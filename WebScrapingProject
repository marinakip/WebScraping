import requests
# import pandas as pd
from bs4 import BeautifulSoup
import time


def scrape_results_page(url):
    webpage_response = requests.get(url)
    webpage = webpage_response.content
    soup = BeautifulSoup(webpage, "html.parser")

    description_list = []  # TODO find empty descriptions
    description = soup.find_all(attrs={"class": "mb-1"})
    for dis in description:
        description_text = dis.get_text().strip()
        description_list.append(description_text)

    results_list = []
    for link in soup.find_all("a", class_="v-align-middle"):
        new_link = "https://github.com" + str(link.get('href'))
        results_list.append(new_link)

    stars_list = []
    stars = soup.find_all(attrs={"class": "muted-link"})
    for star in stars:
        stars_text = star.get_text().split()
        stars_list.append(stars_text)

    info_list = []
    info = soup.find_all("div", class_="mr-3")
    for info in info:
        info_list.append(info.get_text().strip())

    page_links_list = []
    page_links = soup.find_all("div", attrs={"class": "d-flex d-md-inline-block pagination"})
    pages = page_links[0].find_all("a")
    for i in pages:
        url = i.get('href')
        new_url = "https://github.com" + str(url)
        page_links_list.append(new_url)  # TODO remove last link because is page2
    next_page = page_links_list[0]
    last_page = page_links_list[-2]
    return results_list, next_page, last_page


def scrape_repositories(url):
    # TODO for all repositories list
    repository_response = requests.get(url)
    repository = repository_response.content
    soup2 = BeautifulSoup(repository, "html.parser")

    article = soup2.find("p", attrs={"class": "f4 mb-3"})
    article_list = []
    try:
        text = article.get_text()
    except AttributeError:
        text = "NONE"
    article_list.append(text)

    profile = soup2.find("a", attrs={"class": "url fn"})
    url = profile.get('href')
    profile_new = "https://github.com" + str(url)


    stars = soup2.find(attrs={"class": "link-gray no-underline mr-3"})
    try:
        stars_repo = stars.get_text().strip()
    except AttributeError:
        stars_repo = "NONE"

    forks = soup2.find(attrs={"class": "link-gray no-underline"})
    try:
        forks_repo = forks.get_text().strip()
    except AttributeError:
        forks_repo = "NONE"
    return profile_new, stars_repo, forks_repo


def scrape_profile(url):
    # TODO for all profiles list
    profile_response = requests.get(url)
    profile_page = profile_response.content
    soup2 = BeautifulSoup(profile_page, "html.parser")
    try:
        location = (soup2.find(attrs={"class": "p-label"})).text
    except AttributeError:
        location = "NONE"
    try:
        name = (soup2.find(attrs={"class": "p-name vcard-fullname d-block overflow-hidden"})).text
    except AttributeError:
        name = "NONE"
    try:
        nickname = (soup2.find(attrs={"class": "p-nickname vcard-username d-block"})).text
    except AttributeError:
        nickname = "NONE"
    try:
        bio = (soup2.find(attrs={"class": "p-note user-profile-bio mb-3 js-user-profile-bio f4"})).text
    except AttributeError:
        bio = "NONE"

    stats = soup2.find_all(attrs={"class": "link-gray no-underline no-wrap"})
    stats_list = []
    for stat in stats:
        text = stat.get_text().strip().split("\n")
        stats_list.append(text)
    try:
        contributions = ((soup2.find(attrs={"class": "f4 text-normal mb-2"})).text).strip()
    except AttributeError:
        contributions = "NONE"

    profile_info = name + "\n" + nickname + "\n" + bio + "\n" + location + "\n" \
                   + str(stats_list) + "\n" + str(contributions)
    return profile_info, location, stats_list, contributions


def scrape_page(start_url):
    #print("START SCRAPING")
    results_list, next_page, last_page = scrape_results_page(start_url)
    final_results_list = []
    length = len(results_list)
    for i in range(length):
        repository_url = results_list[i]
        profile_url, stars_repo, forks_repo = scrape_repositories(repository_url)
        if profile_url == "NONE":
            dictionary = {
                'location': "NONE",
                'stats_list': "NONE",
                'contributions': "NONE"
            }
            final_results_list.append(dictionary)
        profile_info, location, stats_list, contributions = scrape_profile(profile_url)
        dictionary = {
            'location': location,
            'stats_list': stats_list,
            'contributions': contributions
        }
        final_results_list.append(dictionary)
    #print("END SCRAPING")
    return final_results_list, next_page, last_page


#start_time = time.time()
#------------------- MAIN -------------------------------------------------------------

start_url = "https://github.com/search?q=diabetes"
print("SCRAPING PAGE 1")
final_results_list, next_page, last_page = scrape_page(start_url)
all_final_results_list = [final_results_list]  # results of one page
count = 1
print("OK")
while next_page != last_page:
    count += 1
    if(count % 4 == 0):
        print("SLEEP")
        time.sleep(30)
    print("SCRAPING PAGE " + str(count))
    new_final_results_list, new_next_page, last_page = scrape_page(next_page)
    all_final_results_list.append(new_final_results_list)
    print("OK")
print("SCRAPING LAST PAGE")
last_results_list, next_page, last_page = scrape_page(last_page)
all_final_results_list.append(last_results_list)

print(all_final_results_list)
print("all results length: " + str(len(all_final_results_list)))

#------------------- END MAIN ---------------------------------------------------------


#print("TIME OF EXECUTION --- %s seconds ---" % (time.time() - start_time))

# #--------------------- TEST CODE ------------------------------------------------------
# final_results_list, next_page, last_page = scrape_page("https://github.com/search?p=5&q=diabetes&type=Repositories")
# print(final_results_list)
# print("length: " + str(len(final_results_list)))
# #---------------------- END TEST CODE ---------------------------------------------------
