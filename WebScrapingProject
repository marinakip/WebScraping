import requests
import pandas as pd
from bs4 import BeautifulSoup
import time
import datetime
import random
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry


def scrape_results_page(url):
    session = requests.Session()
    retry = Retry(connect=3, backoff_factor=0.5)
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)

    webpage_response = session.get(url, timeout=2.5)


    # try:
    #     webpage_response = requests.get(url, timeout=2.5)
    #
    #     if webpage_response.status_code == 429:
    #         time.sleep(int(webpage_response.headers["Retry-After"]))
    #
    #     webpage_response.raise_for_status()
    # except (ConnectionError, requests.HTTPError):
    #     print("CONNECTION ERROR, SLEEP 30")
    #     time.sleep(60)


    webpage = webpage_response.content
    soup = BeautifulSoup(webpage, "html.parser")

    description_list = []  # TODO find empty descriptions
    description = soup.find_all(attrs={"class": "mb-1"})
    for dis in description:
        description_text = dis.get_text().strip()
        description_list.append(description_text)

    results_list = []
    for link in soup.find_all("a", class_="v-align-middle"):
        new_link = "https://github.com" + str(link.get('href'))
        results_list.append(new_link)

    stars_list = []
    stars = soup.find_all(attrs={"class": "muted-link"})
    for star in stars:
        stars_text = star.get_text().split()
        stars_list.append(stars_text)

    info_list = []
    info = soup.find_all("div", class_="mr-3")
    for info in info:
        info_list.append(info.get_text().strip())

    page_links_list = []
    page_links = soup.find_all("div", attrs={"class": "d-flex d-md-inline-block pagination"})
    pages = page_links[0].find_all("a")
    for i in pages:
        url = i.get('href')
        new_url = "https://github.com" + str(url)
        page_links_list.append(new_url)  # TODO remove last link because is page2
    next_page = page_links_list[-1]
    last_page = page_links_list[-2]
    #print("NEXT: " +str(next_page))
    #print("LAST: " + str(last_page))
    return results_list, next_page, last_page


def scrape_repositories(url):
    # TODO for all repositories list
    repository_response = requests.get(url)
    repository = repository_response.content
    soup2 = BeautifulSoup(repository, "html.parser")

    article = soup2.find("p", attrs={"class": "f4 mb-3"})
    article_list = []
    try:
        text = article.get_text()
    except AttributeError:
        text = "NONE"
    article_list.append(text)

    profile = soup2.find("a", attrs={"class": "url fn"})
    try:
        url = profile.get('href')
    except AttributeError:
        time.sleep(60)
        scrape_results_page(url)

    profile_new = "https://github.com" + str(url)


    stars = soup2.find(attrs={"class": "link-gray no-underline mr-3"})
    try:
        stars_repo = stars.get_text().strip()
    except AttributeError:
        stars_repo = "NONE"

    forks = soup2.find(attrs={"class": "link-gray no-underline"})
    try:
        forks_repo = forks.get_text().strip()
    except AttributeError:
        forks_repo = "NONE"
    return profile_new, stars_repo, forks_repo


def scrape_profile(url):
    # TODO for all profiles list
    profile_response = requests.get(url)
    profile_page = profile_response.content
    soup2 = BeautifulSoup(profile_page, "html.parser")
    try:
        location = (soup2.find(attrs={"class": "p-label"})).text
    except AttributeError:
        location = "NONE"
    try:
        name = (soup2.find(attrs={"class": "p-name vcard-fullname d-block overflow-hidden"})).text
    except AttributeError:
        name = "NONE"
    try:
        nickname = (soup2.find(attrs={"class": "p-nickname vcard-username d-block"})).text
    except AttributeError:
        nickname = "NONE"
    try:
        bio = (soup2.find(attrs={"class": "p-note user-profile-bio mb-3 js-user-profile-bio f4"})).text
    except AttributeError:
        bio = "NONE"

    stats = soup2.find_all(attrs={"class": "link-gray no-underline no-wrap"})
    stats_list = []
    for stat in stats:
        text = stat.get_text().strip().split("\n")
        stats_list.append(text)
    try:
        contributions = ((soup2.find(attrs={"class": "f4 text-normal mb-2"})).text).strip()
    except AttributeError:
        contributions = "NONE"

    profile_info = name + "\n" + nickname + "\n" + bio + "\n" + location + "\n" \
                   + str(stats_list) + "\n" + str(contributions)
    return profile_info, location, stats_list, contributions


def scrape_page(start_url):
    #print("START SCRAPING")
    results_list, next_page, last_page = scrape_results_page(start_url)
    final_results_list = []
    addresses = []
    length = len(results_list)
    for i in range(length):
        repository_url = results_list[i]
        profile_url, stars_repo, forks_repo = scrape_repositories(repository_url)
        profile_info, location, stats_list, contributions = scrape_profile(profile_url)
        addresses.append(location)
        dictionary = {
            'location': location,
            'stats_list': stats_list,
            'contributions': contributions
        }
        final_results_list.append(dictionary)
    #print("END SCRAPING")
    return final_results_list, next_page, last_page, addresses


start_time = time.time()
#print("Time: " + str(datetime.datetime.now().minute))
#------------------- MAIN -------------------------------------------------------------

#start_url = "https://github.com/search?q=diabetes"
#start_url = "https://github.com/search?p=85&q=diabetes&type=Repositories"
start_url = "https://github.com/search?q=machine+learning"
#start_url = "https://github.com/search?p=89&q=machine+learning&type=Repositories"
#start_url = "https://github.com/search?p=71&q=machine+learning&type=Repositories"
#start_url = "https://github.com/search?utf8=%E2%9C%93&q=diabetes+created%3A2005-01-01..2017-01-01&type=Repositories"
count = 1
#count = 89
#count = 71
print("SCRAPING PAGE " + str(count))
final_results_list, next_page, last_page, addresses = scrape_page(start_url)
all_final_results_list = [final_results_list]  # results of one page
final_addresses = [addresses]

print("OK")
while next_page != last_page:
    count += 1
    duration = time.time() - start_time
    # if datetime.datetime.now().minute % 3 == 0:
    #     print("TIME : " +str(datetime.datetime.now().minute))
    #     secs = random.randint(20, 60)
    #     print("SLEEP " + str(secs))
    #     time.sleep(secs)
    if count % 3 == 0:
        print("SLEEP")
        secs = random.randint(20, 60)
        print("SLEEP " + str(secs))
        time.sleep(secs)
    print("SCRAPING PAGE " + str(count))
    new_final_results_list, new_next_page, last_page, addresses = scrape_page(next_page)
    final_addresses.append(addresses)
    all_final_results_list.append(new_final_results_list)
    next_page = new_next_page
    if(count % 4 == 0):
        dataframe_addresses = pd.DataFrame(final_addresses)
        dataframe_addresses.to_csv('addresses_temp.csv', index=False, header=False)
        print("CSV ADDRESSES TEMP CREATED")

        dataframe = pd.DataFrame(all_final_results_list)
        dataframe.to_csv('scraping_results_temp.csv', index=False, header=False)
        print("CSV TEMP CREATED")
    print("OK")
print("SCRAPING LAST PAGE")
last_results_list, next_page, last_page, addresses = scrape_page(last_page)
final_addresses.append(addresses)
all_final_results_list.append(last_results_list)

#print(all_final_results_list)
print("all results length: " + str(len(all_final_results_list)))

dataframe_addresses = pd.DataFrame(final_addresses)
dataframe_addresses.to_csv('addresses_all.csv', index=False, header=False)

dataframe = pd.DataFrame(all_final_results_list)
dataframe.to_csv('scraping_results.csv', index=False, header=False)
print("CSV FINAL CREATED")

#------------------- END MAIN ---------------------------------------------------------


print("TIME OF EXECUTION --- %s seconds ---" % (time.time() - start_time))


