import requests
# import pandas as pd
from bs4 import BeautifulSoup


def scrape_results_page(url):
    webpage_response = requests.get(url)
    webpage = webpage_response.content
    soup = BeautifulSoup(webpage, "html.parser")
    #print("STARTING RESULTS PAGE SCRAPING")
    #print("=========================")
    #print("DISCRIPTION\n")  # TODO find empty descriptions
    description_list = []
    description = soup.find_all(attrs={"class": "mb-1"})
    for dis in description:
        description_text = dis.get_text().strip()
        description_list.append(description_text)

    #print("=========================")

    #print("LINKS\n")
    results_list = []
    for link in soup.find_all("a", class_="v-align-middle"):
        new_link = "https://github.com" + str(link.get('href'))
        #print(new_link)
        results_list.append(new_link)
    #print("=========================")

    #print("STARS\n")
    stars_list = []
    stars = soup.find_all(attrs={"class": "muted-link"})
    for star in stars:
        stars_text = star.get_text().split()
        #print(stars_text)
        stars_list.append(stars_text)
    #print("=========================")

    #print("INFO\n")
    info_list = []
    info = soup.find_all("div", class_="mr-3")
    for info in info:
        #print(info.get_text().strip())
        info_list.append(info.get_text().strip())
    #print("=========================")

    #print("PAGE LINKS\n")
    page_links_list = []
    page_links = soup.find_all("div", attrs={"class": "d-flex d-md-inline-block pagination"})
    pages = page_links[0].find_all("a")
    for i in pages:
        url = i.get('href')
        new_url = "https://github.com" + str(url)
        #print(new_url)
        page_links_list.append(new_url)  # TODO remove last link because is page2
    #print("=========================")
    #print("END RESULTS PAGE SCRAPING")
    return results_list, page_links_list


def scrape_repositories(url):
    #print("STARTING REPOSITORIES SCRAPING")
    #print("=========================")
    #print("PARSING NEW SOUP FOR REPOS....\n")
    # Scraping a Repository  TODO for all reposirories list
    repository_response = requests.get(url)
    repository = repository_response.content
    soup2 = BeautifulSoup(repository, "html.parser")

    article = soup2.find("p", attrs={"class": "f4 mb-3"})
    article_list = []
    try:
        text = article.get_text()
    except AttributeError:
        text = "NONE"
    ##print(text)
    article_list.append(text)

    #print("=========================")
    profile = soup2.find("a", attrs={"class": "url fn"})
    url = profile.get('href')
    profile_new = "https://github.com" + str(url)
    #print(profile_new)

    #print("=========================")
    stars = soup2.find(attrs={"class": "link-gray no-underline mr-3"})
    stars_repo = stars.get_text().strip()
    #print(stars_repo)

    #print("=========================")
    forks = soup2.find(attrs={"class": "link-gray no-underline"})
    forks_repo = forks.get_text().strip()
    #print(forks_repo)
    #print("=========================")
    #print("END REPOSITORIES SCRAPING")
    return profile_new, stars_repo, forks_repo


def scrape_profile(url):
    #print("STARTING PROFILE SCRAPING")
    #print("=========================")
    #print("PARSING NEW SOUP FOR PROFILE....\n")

    # Scraping a Profile  TODO for all profiles list
    profile_response = requests.get(url)
    profile_page = profile_response.content
    soup2 = BeautifulSoup(profile_page, "html.parser")
    try:
        location = (soup2.find(attrs={"class": "p-label"})).text
    except AttributeError:
        location = "NONE"
    try:
        name = (soup2.find(attrs={"class": "p-name vcard-fullname d-block overflow-hidden"})).text
    except AttributeError:
        name = "NONE"
    try:
        nickname = (soup2.find(attrs={"class": "p-nickname vcard-username d-block"})).text
    except AttributeError:
        nickname = "NONE"
    try:
        bio = (soup2.find(attrs={"class": "p-note user-profile-bio mb-3 js-user-profile-bio f4"})).text
    except AttributeError:
        bio = "NONE"

    stats = soup2.find_all(attrs={"class": "link-gray no-underline no-wrap"})
    stats_list = []
    for stat in stats:
        text = stat.get_text().strip().split("\n")
        stats_list.append(text)
    try:
        contributions = ((soup2.find(attrs={"class": "f4 text-normal mb-2"})).text).strip()
    except AttributeError:
        contributions = "NONE"

    profile_info = name + "\n" + nickname + "\n" + bio + "\n" + location + "\n" \
                   + str(stats_list) + "\n" + str(contributions)
    #print("=========================")
    #print("END PROFILE SCRAPING")
    return profile_info, location, stats_list, contributions


def scrape_page(start_url):
    print("START SCRAPING")
    results_list, page_links_list = scrape_results_page(start_url)
    final_results_list = []
    length = len(results_list)
    for i in range(length):
        repository_url = results_list[i]
        profile_url, stars_repo, forks_repo = scrape_repositories(repository_url)
        profile_info, location, stats_list, contributions = scrape_profile(profile_url)
        dictionary = {
            'location': location,
            'stats_list': stats_list,
            'contributions': contributions
        }
        final_results_list.append(dictionary)
    # print("-------------------------------")
    # print("RESULTS...\n")
    # print(final_results_list)
    # print("LENGTH LIST : " + str(len(final_results_list)))
    print("END SCRAPING")
    return page_links_list, final_results_list


start_url = "https://github.com/search?q=diabetes"
page_links_list, final_results_list = scrape_page(start_url)
length_page_list = len(page_links_list)
all_final_results_list = []
all_final_results_list.append(final_results_list)
new_scraping_url = page_links_list[0]
new_page_links_list, new_final_results_list = scrape_page(new_scraping_url)
all_final_results_list.append(new_final_results_list)
print(all_final_results_list)
print("all results length: " + str(len(all_final_results_list)))
