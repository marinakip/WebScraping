"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""The Applied Artificial Intelligence Workshop\n\n\n\n\nThis is the repository for The Applied Artificial Intelligence Workshop, published by Packt. It contains all the supporting project files necessary to work through the course from start to finish.\nRequirements and Setup\n\nTo get started with the project files, you'll need to:\n\nSetup The Jupyter Notebook\n\nAbout The Applied Artificial Intelligence Workshop\nThe Applied Artificial Intelligence Workshop teaches you the ins and outs of machine learning and neural networks from the ground up, using real-world examples. You'll learn to develop AI and ML models using Python, starting with using the minmax algorithm and alpha-beta pruning to create your first game, and ending with classifying images using neural networks.\nWhat you will learn\n\nCreate your first AI game in Python with the minimax algorithm\nImplement regression techniques to simplify real-world data\nExperiment with classification techniques to label real-world data\nPerform predictive analysis in Python using decision trees and random forests\nUse clustering algorithms to group data without manual support\nUnderstand how to use neural networks to process and classify labeled images\n\nRelated Workshops\nIf you've found this repository useful, you might want to check out some of our other workshop titles:\n\nThe Natural Language Processing Workshop\nThe Computer Vision Workshop\nThe Machine Learning Workshop\n\n""], 'url_profile': 'https://github.com/PacktWorkshops', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Apress Source Code\nThis repository accompanies Agile Artificial Intelligence in Pharo by Alexandre Bergel (Apress, 2020).\n\nThe book provides a complete implementation of a number of complex algorithms. Download the files as a zip using the green button, or clone the repository to your machine using Git.\nHow to load it?\nThe provided code was designed for Pharo and it works Pharo 8 and Pharo 9. Open a playground and execute the instructions:\nMetacello new\n    baseline: 'AgileArtificialIntelligence';\n    repository: 'github://Apress/agile-ai-in-pharo/src';\n    load.\nContent\nThe repository provides the complete implementation of:\n\nNeural network library\nMatrix library\nGenetic algorithm\nZoomorphic creature\nNEAT neuroevolution algorithm\nMario-like game\n\nFurthermore, all the scripts and code snippets are provided in the scripts folder.\nReleases\nRelease v1.0 corresponds to the code in the published book, without corrections or updates.\nContributions\nSee the file Contributing.md for more information on how you can contribute to this repository.\n""], 'url_profile': 'https://github.com/Apress', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': [""Artificial Intelligence for Trading Nanodegree\n\nSource\nWorldQuant, a global quantitative asset management firm, in partnership with global online learning company Udacity announces the launch of a new Artificial Intelligence for Trading Nanodegree program. Students enrolled in the program will analyze real data and build financial models by learning the basics of quantitative trading, as well as how to analyze alternative data and use machine learning to generate trading signals.\nUdacity and WorldQuant have collaborated with top industry professionals with prior experience at leading financial institutions to ensure students are exposed to the latest AI applications in trading and quantitative finance. By learning from industry experts, students will advance their finance knowledge, build a strong portfolio of real-world projects and learn to generate trading signals using natural language processing, recurrent neural networks and random forests. Graduates will gain the quantitative skills currently in demand across multiple functions and roles at hedge funds, investment banks and fintech startups.\nThe program consists of two three-month terms. In the first term, students learn the basics of quantitative analysis, covering data processing, trading signal generation and portfolio management. The second term is focused on AI Algorithms for Trading, where students work with alternative data and use machine learning to generate trading signals and run backtests to evaluate signals\nNanodegree Program Information\nThis nanodegree program is comprised of 8 courses and 8 projects, which are described in detail below.\nBuilding a project is one of the best ways to demonstrate the skills we've learned around the mastery of\nquantitative finance.\n\nProject 1: Trading with Momentum\nProject 2: Breakout Strategy\nProject 3: Smart Beta and Portfolio Optimization\nProject 4: Alpha Research and Factor Modeling\nProject 5: NLP on Financial Statements (generate Alpha Factors from 10-k)\nProject 6: Sentiment Analysis with Neural Networks (LSTM)\nProject 7: Combining Signals for Enhancing Alphas (using Machine Learning)\nProject 8: Backtesting (Barra data)\n\nProject 1: Trading with Momentum\nIn this project, you will learn to implement a trading strategy on your own, and test to see if it has the potential to be profitable. You will be supplied with a universe of stocks and time range. You will also be provided with a textual description of how to generate a trading signal based on a momentum indicator. You will then compute the signal for the time range given and apply it to the dataset to produce projected returns. Finally, you will perform a statistical test on the mean of the returns to conclude if there is alpha in the signal. For the dataset, we'll be using the end of day from Quotemedia.\nProject 2: Breakout Strategy\nIn this project, you will implement the breakout strategy. You'll find and remove any outliers. You'll test to see if it has the potential to be profitable using a Histogram and P-Value. For the dataset, we'll be using the end of day from Quotemedia.\nIn this project, you will code and evaluate a breakout signal. You will run statistical tests to test for normality\nand to find alpha. You will also learn to find outliers and evaluate the effect that filtered outliers could have\non your trading signal. You will run various scenarios of your model with or without the outliers and decide\nif the outliers should be kept or not. You'll test to see if it has the potential to be profitable using a Histogram and P-Value. For the dataset, we'll be using the end of day from Quotemedia.\nProject 3: Smart Beta and Portfolio Optimization\nIn this project, you will build a smart beta portfolio and compare it to a benchmark index. To find out how well the smart beta portfolio did, you’ll calculate the tracking error against the index. You’ll then build a portfolio by using quadratic programming to optimize the weights. Your code will rebalance this portfolio and calculate turn over to evaluate the performance. You’ll use this metric to find the optimal rebalancing Frequency. For the dataset, we'll be using the end of day from Quotemedia.\nProject 4: Multi-Factor Model\nIn this project, you will build a statistical risk model using PCA. You’ll use this model to build a portfolio along with 5 alpha factors. You’ll create these factors, then evaluate them using factor-weighted returns, quantile analysis, sharpe ratio, and turnover analysis. At the end of the project, you’ll optimize the portfolio using the risk model and factors using multiple optimization formulations. For the dataset, we'll be using the end of day from Quotemedia and sector data from Sharadar.\nProject 5: NLP on Financial Statements\nIn this project, you will apply Natural Language Processing (NLP) on corporate filings, such as 10Q and 10K\nstatements, from cleaning data and text processing, to feature extraction and modeling. You will utilize\nbag-of-words and TF-IDF to generate company-specific sentiments. Based on the sentiments, you will decide\nwhich company to invest in, and the optimal time to buy or sell. You will use NLP generate an alpha factor.For the dataset, we'll be using the end of day from Quotemedia and Loughran-McDonald sentiment word lists.\nProject 6: Sentiment Analysis with Neural Networks\nIn this project, you'll build your own deep learning model to classify the sentiment of messages from StockTwits, a social network for investors and traders. Your model will be able to predict if any particular message is positive or negative. From this, you'll be able to generate a signal of the public sentiment for various ticker symbols.\n\nYou will construct and train LSTM networks for sentiment classification. You will run backtests and apply the models to news data for signal generation\n\nProject 7: Combining Signals for Enhanced Alphas\nIn this project, you'll combine signals on a random forest for enhanced alpha. While implementing this, you'll have to solve the problem of overlapping samples. For the dataset, we'll be using the end of day from Quotemedia and sector data from Sharadar.\nProject 8: Backtesting\nIn this project, you will build a fairly realistic backtester that uses the Barra data. The backtester will perform\nportfolio optimization that includes transaction costs, and you'll implement it with computational efficiency\nin mind, to allow for a reasonably fast backtest. You'll also use performance attribution to identify the major\ndrivers of your portfolio's profit-and-loss (PnL). You will have the option to modify and customize the\nbacktest as well.\n""], 'url_profile': 'https://github.com/tatwan', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'NTNU', 'stats_list': [], 'contributions': '211 contributions\n        in the last year', 'description': ['AI-programming\nRepository for the subject IT3105 - Artificial Intelligence Programming, NTNU\n'], 'url_profile': 'https://github.com/bendiknordeng', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/faiqueali017', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['ArtificialIntelligenceExercises\nThis repository holds some implementations of algorithms we learned about in the course aboute Artificial Intelligence.\nThe first package contains different approaches to solve an Eight Puzzle, namely Breadth-First-Search, Depth-First-Search, iterative deepening and A*-Search.\nThe other package uses some test data sets and implements the kNearesNeighbour algorithm with crossvalidation.\n'], 'url_profile': 'https://github.com/MilenaEisemann', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Artificial Intelligence Engines\nComputer code collated from various sources for use with the book:\nArtificial Intelligence Engines: A Tutorial Introduction to the Mathematics of Deep Learning\nby James V Stone\nThis file is: https://github.com/jgvfwstone/ArtificialIntelligenceEngines\nNote that the book is principally about the mathematics of deep learning.\nThis repository is intended to provide \'taster\' code, rather than an exercise in how to program deep learning networks. Because this code has been collated from different sources, the coding style varies between examples.\nEach example has been reproduced with permission from the author.\nDownloading Single Files\nGithub normally insists you download the whole repository.\nHowever, to download a single file\n\ngo to the file so you can see it in the github user interface\nclick on the RAW button in the upper right\nuse the browser ""save as ..."" menu to save the file to your computer.\n\nHow To ...\nThere is a README file within each directory.\nSystem Requirements\nEach example has been tested on a mac (System Version:\tOS X 10.11.3), MacBook Air 1.6GHz.\nPython examples have been tested using the Spyder (3.3.2) python application with Python 3.7.0.\n'], 'url_profile': 'https://github.com/PlanetG3', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'Seattle, Washington', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ryan-moll', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Artificial-Intelligence\n'], 'url_profile': 'https://github.com/peppermintcoding', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Awesome AI Labs \nA curated list of Artificial Intelligence Labs doing cutting edge research.\nContributions welcome.\n\n\nStanford AI Lab\n\n\n\nNeuro AI Lab\n \n\n\nStanford NLP\n \n\n\nStanford AIMI\n\n\n\nHuman Centered AI\n\n\n\nEthics of AI UoT\n\n\n\nPSU Crowd AI Lab PennState\n\n\n\nCenter For Brains Mind and Machines\n \n\n\nCMU AI\n\n\nComputational Cognitive Neuroscience Lab\n\n\n\nComputational Visual Neuroscience Lab\n\n\n\nCenter for Computational Brain Research\n\n\nFlowers Project-Team at INRIA\n \n\n\nArtificial Intelligence Laboratory,  Linz Institute of Technology\n\n\nKording Lab\n \n\n\nWicklow AI in Medicine Research Institute\n \n\n\nVan Djik Lab\n\n\nThe Open Cognition Project\n\n\nThe Alan Turing Institute\n \n\n\nVector Institute for AI\n \n\n\nMila\n \n\n\nQUVA Deep Vision Lab\n \n\n\nNASA Quantum AI Lab\n\n\nAllen NLP\n \n\n\nHugging Face\n \n\n\nIDSIA\n \n\n\nIDIAP\n \n\n\nCriteo AI Lab\n \n\n\nWadhwani AI\n \n\n\nFacebook AI Research\n \n\n\nPeoples AI Research\n\n\n\nGoogle Deepmind\n \n\n\nGoogle Brain\n\n\nOpenAI\n \n\n\nUber AI\n \n\n\nSalesforce Research - Einstein\n \n\n\nLyft Level 5\n \n\n\nMicrosoft Research\n \n\n\nAdobe Research\n \n\n\nMIT-IBM Watson AI LAB\n\n\n\nSiemens AI Lab\n\n\n'], 'url_profile': 'https://github.com/sairampillai', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}"
"{'location': 'Missouri', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['Artificial Intelligence\nCourse: https://github.com/badriadhikari/AI-2020spring\nDescription:\n\nThis course provides an introduction to artificial intelligence (AI). The list of topics may include artificial neural networks, search, planning, knowledge-based reasoning, probabilistic inference, machine learning, natural language processing, and practical applications.\n\nTopics:\n\nThe sequences of topics will be as follows:\n\n\nUse Python, Numpy and Keras to design, train, and evaluate basic feed-forward neural networks\nLearn an overview of artificial intelligence principles and approaches\nLearn a basic understanding of the building blocks of AI as presented in terms of intelligent agents\nSelect and evaluate various searching algorithms\nUnderstand some of the problems and ideas in the field of natural language processing, perception, and robotics\nLearn the philosophical foundations of AI and the future of AI\nImplement various AI algorithms such as DFS, BFS, etc.\n\nAssignments:\n\nData Analysis & Preparation\nModel Selection & Evaluation\nFeature Importance and Reduction\nAddress Peer-Review (Evaluation)\nFinal Assembly\n\n'], 'url_profile': 'https://github.com/zegster', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'Portland, OR', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['artificial_intelligence\nprojects for ai course\n'], 'url_profile': 'https://github.com/charboltron', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'Portland', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Artificial-Intelligence\nThis project is a web application that uses IBM Watson technology for image classification and render plotly js dashboard on the form of gauge chart\n'], 'url_profile': 'https://github.com/moadtahri', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Artificial_Intelligence\ngraphs, heuristics, constraint solvers, game trees, genetic algorithms, learning, natural language processing, agents\n'], 'url_profile': 'https://github.com/kazuyachue', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['Artificial-Intelligence\nBIT Notes for Artificial Intelligence\nBIM Notes for Artificial Intelligence\n'], 'url_profile': 'https://github.com/MohanBhandari', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gGrobinson', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'Orange, CA', 'stats_list': [], 'contributions': '175 contributions\n        in the last year', 'description': ['CPSC_390\nArtificial Intelligence\n'], 'url_profile': 'https://github.com/TobyJChappell', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kronemeyer', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/haroldvelasquez', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['iot_eCare_ai\nPanoramica\nIl progetto consiste in una piattaforma software a supporto della erogazione un servizio innovativo basato su tecnologie biometriche e domotica che supportano la persona (utenti anziani) a prendersi cura di se stessa, attraverso il monitoraggio costante di tutti i parametri vitali, la stimola a seguire uno stile di vita sano e offre un controllo da parte di una centrale operativa 24h per la sicurezza e la gestione delle emergenze. In caso di valori devianti rispetto alla serie storica, la piattaforma di Artificial Intelligence (AI) fa scattare automaticamente delle azioni (da una videochiamata sulla telecamera posizionata in casa dell’utente, al controllo della posizione sull’orologio intelligente che sarà dato in dotazione agli utenti, fino all’intervento di un operatore socio-assistenziale). Si tratta di un progetto di Internet of Things (Internet delle cose – IoT) e Intelligenza Artificiale con tecnologie biometriche applicato al settore dell’assistenza sociale agli anziani. Non esiste un’applicazione simile a livello europeo in corso d’uso che abbracci tutta la catena del valore dalla rilevazione del dato all’intervento sul campo.\nInstallazione\nSeguire le istruzioni di installazione presenti nel file ""Manuale_Installazione_AI.pdf""\nContributi\nSe vuoi contribuire a iot_eCare, leggi il file CONTRIBUTING.md, e restuiscilo firmato\nLicenza\nQuesto progetto è distribuito sotto EUPL License. Vedi il file LICENSE per le informazioni sulla licenza: https://github.com/igcomsrl/iot_eCare_ai/blob/master/LICENSE\n'], 'url_profile': 'https://github.com/igcomsrl', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}"
"{'location': 'Singapore', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['CS3243 Artificial Intelligence\nNational University of Singapore\n'], 'url_profile': 'https://github.com/Olivvvia', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ParthKulkarni', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Broomfield, Colorado', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['CSCI-3202-Artificial-Intelligence\nA repository full of my work from AI (Artifical Intelligence), CSCI 3202 at CU Boulder. Spring 2020.\nDISCLAIMER: Please do not use work in dishonest way! This serves as a place to show the work I have completed in the past.\n'], 'url_profile': 'https://github.com/MuntahaPasha', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': [""CS: AI\nRepository for work done in PSU'S CS Artificial Intelligence Course.\nHomework 1.\nThe goal of Homework 1 was to create an implementation to compare the BFS and\nA* algorithm.\nHomework 2.\nThe goal of Homework 2 is to create a solution to the 8 Queens problem using\na Genetic Algorithm (GA). In this GA, the mutation is to swap two elements at\nrandom until generationally, a solution had been found. More results can be\nfound in the writeup attached.\nFinal\nAs the COVID-19 Pandemic set in, I wanted to focus more on enhancing previous\nstudied material. Thus, I worked on improving the third homework assignment:\nRobby the Robot. Robby is a robot that utilizes a Q-Learning Matrix to identify\nwhat the correct action is to do in a hypothetical n x n map where the actions\nthe robot can perform is to move left, right, up, down, or clear the vacuum.\nThe robot must not crash into a wall, pick up dirt in a square that is clean,\nor not clean in a square that is dirty.\nThe final involved tweaking variables common to the Q-Learning rewards\nalgorithm, and finding what the sweetspot of combinations are and how they\naffect the efficiency of Robby the Robot.\n""], 'url_profile': 'https://github.com/blzzrd', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['CAP6635-Artificial-Intelligence\n2020 Spring\n'], 'url_profile': 'https://github.com/AdamCorbinFAUPhD', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Seoul, Korea', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hyunas1996', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['README\n\u200b\t\t包括课件、实验和作业相应答案。不过我们这一届的考试特别难（比作业难了好多），千万不要以作业为准来复习。\n'], 'url_profile': 'https://github.com/hit-thusz-RookieCJ', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['CarImageArtificialIntelligence\n'], 'url_profile': 'https://github.com/balrajendran', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '1,281 contributions\n        in the last year', 'description': ['Checkers_Game-Artificial-Intelligence\nA console based Checkers Game that I made during the Artificial Intelligence course in my university, played\nbetween the user/player and AI that uses heuristics/Alpha-Beta pruning.\n'], 'url_profile': 'https://github.com/sakiib', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Tijuana, Baja California, Mexico', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Unity Artificial Intelligence\nArtificial Intelligence examples and progress.\n'], 'url_profile': 'https://github.com/JCharlieDev', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}"
"{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '1,281 contributions\n        in the last year', 'description': ['datingAI\nA dating/matching program that I made during the Artificial Intelligence course in my university, which uses a logic base to give the  compatibility score for man/woman looking for woman/man respectively according to their given preference & information.\n'], 'url_profile': 'https://github.com/sakiib', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['CarImageArtificialIntelligence\n'], 'url_profile': 'https://github.com/balrajendran', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '1,281 contributions\n        in the last year', 'description': ['Checkers_Game-Artificial-Intelligence\nA console based Checkers Game that I made during the Artificial Intelligence course in my university, played\nbetween the user/player and AI that uses heuristics/Alpha-Beta pruning.\n'], 'url_profile': 'https://github.com/sakiib', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YulanJS', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,268 contributions\n        in the last year', 'description': ['Notas de Aula / Anotacoes gerais\nDetecção de Anomalia:\nhttps://towardsdatascience.com/best-clustering-algorithms-for-anomaly-detection-d5b7412537c8\nhttps://medium.com/@elutins/dbscan-what-is-it-when-to-use-it-how-to-use-it-8bd506293818\n\nipykernel - Alterar Kernel no Jupyter\nconda install jupyter\nconda install nb_conda\nconda install ipykernel\npython -m ipykernel install --user --name mykernel\njupyter kernelspec remove mykernel\n\nPythonAnyWhere Deploy\nhttps://blog.pythonanywhere.com/169/\nhttps://blog.pythonanywhere.com/121/\n\nSome Data related Posts\nPandas where loc mask good examples:\nhttps://kanoki.org/2019/07/17/pandas-how-to-replace-values-based-on-conditions/\nhttps://amitkushwaha.co.in/data-visualization-part-1.html\nhttps://www.kaggle.com/masumrumi/a-statistical-analysis-ml-workflow-of-titanic\nhttps://blog.minitab.com/blog/understanding-statistics/understanding-qualitative-quantitative-attribute-discrete-and-continuous-data-types\n\nSELENIUM\nAdicionar executavel geckodriever ao path:\nexport PATH=$PATH:/root/pasta.\nhttps://medium.com/@cagriaydogdu2334/3d-visualization-of-k-means-clustering-47d3d3e82117\nhttps://www.bigendiandata.com/2017-04-18-Jupyter_Customer360/\nhttp://scraping.pro/recaptcha-solve-selenium-python/\nhttps://www.howtoforge.com/tutorial/tesseract-ocr-installation-and-usage-on-ubuntu-16-04/\n\n'], 'url_profile': 'https://github.com/havyx', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'Bloomington, Indiana', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['Assignment 1 - Searching\nPart 1 : The Luddy Puzzle\nFormulation:\nThe game board consists of 4 * 4 grid with 15 tiles numbered from 0\nto 15 in a randomized way. In each turn, the player can slide a tile into an adjacent empty space. Our job is to find a set of sliding moves of the blank tile which converts the initial board to the goal state.\nAbstraction\nIntitial State:\nRandom arrangement of tiles from 0-15, 0 numbered tile being a blank tile\nGoal State:\nTiles arranged in an ascending order from 1-15  with 0 at the last position of the grid.\nState Space:\nState of all objects are stored in the fringe. Each state object contains, the heuristic value of that node and an evaluation value i.e g(s) + h(s),  the path to the current state from the initial node.\nSuccessor function\nSuccessor function will give us the next possible states that can be generated by moving the blank tile. In this assignment, we have 3 Successor functions depending on the variant the users passes as an input\n\nOriginal: This Successor function generates successors of its neighboring tiles in all the direction of the current state, Up, down , left, bottom. We use check valid-index method to determine whether node exists specially in the case of edge nodes.\nCircular: In this variant, the successor function is defined same as original variant considering circular moves constraint.\nLuddy: Defined 8 moves with respect to the luddy constraint and successor can be found based on this moves.\n\nHeuristic Function:\nThis heuristic function returns the number of misplaced tiles of current state. That is, it checks every tile of every game board to see if it is in goal-state. We first move the empty tile in all possible direction in the current state and calculate the h-score which is nothing but number of misplaced tiles. Then g-score is evaluated as number of nodes traversed from the start state to the current state. Example\nAdmissibility of the heuristic:\nWe can say that the heuristic function is admissible because we are always underestimating the cost to reach the goal state, i.e. we cannot reach the goal state in fewer number of moves than the number of misplaced tiles in the board. Hence, each tile which is not in place must be moved at least once, this shows the admissibility.\nAlgorithm Used\nThe code was implemented using A* algorithm (Search 3 Algorithm)\nCode Flow\nThe code takes two inputs one is board and other the variant. Then the code takes the initial-board as its first state as a start stage and finds the empty tile that is tile numbered 0. Depending on the variant the start state calculates all of its successor. Once the successors are calculated, it is added it to the closed set. Then for every successor we calculate the heuristic function and adds it to the priority queue. The node having the least heuristic value is returned or popped out of the queue. This process is repeated until we get the final state.\nApproach & Problem Faced\nThe given code snippet was working only for original variant. Our next task was to make it work for two new variants circular and luddy respectively. Original variant was not giving a best solution as it did not take into account the visited state because of this some boards were running into an infinite loop.\nWe first of all implemented the visited state logic to remove the states that were already visited and then thought of implementing heuristic. There was some confusion in choosing heuristic, so this link quite helped me. https://cs.stackexchange.com/questions/37795/why-is-manhattan-distance-a-better-heuristic-for-15-puzzle-than-number-of-til\nPseudo Code for Heuristic: Number of Misplaced Tiles.\nfor i in the state:\nif state[i] is not equal to i+1\n#this will ensure all the nodes are in ascending order\nincrement count\nreturn count\n#count will only increment if the two adjacent tiles are not in the ascending order\nThe next step was to implement priority queue to get the least value of the heuristic. Some state space was quite huge because of which our A* algorithm was not fast enough and consumed lots of time. To handle this case, I first of all thought may be using different heuristic could solve my problem at ease or perhaps it is due to heuristic I am getting large number of state space and traversing in the same loop. I tried using manhattan distance heuristic but was in the same loop hole of infinite loop and timeout error. Also read about IDA * search algorithm as it is the fastest solution to get the output. I tried implementing IDA * search but because the implementation was too complicated was not able to do so.\nPart 2 - Road trip!\nI) a description of how you formulated the search problem, including precisely defning the state space, the successor function, the edge weights, the goal state, and (if applicable) the heuristic function(s) you designed, including an argument for why they are admissible;\nFormulation: The user will give a to and from city for which we have to find a route which will be optimal according to the constraint given by the user.\n1.Initial State: All the segment containing start city\n2.Goal State: The Current hop made has the End city (Example: if my end city is B and my current hop is B to C then its my                    goal state)\n3.Successor Function and edge weight and heuristic: According to the constraint the cost of every succesor is returned\n-distance - haversine distance for next city in the hop and destination i.e the end_city is calculated and added to the         existing cost\n-segments - for every hop segment++\n-time - distance given in the data divided by speed limit given in the data i.e the road-segments.txt and added to the          existing cost\n-mpg - calculated using the formula in the Instructions for ""V"" in the formula used the speed limit for that hop and added      to the existing cost\n4.Admissibality: As we are haversine distance it will always give optimal solution\nII) a brief description of how your search algorithm works\nList of Functions and Code Flow\n\nMain function\n-we take command line arguments in variables\n-we check if the given constraint is correct and the given cities name are in the data set\n-call parse_document function\n-call A star function\nparse_document()\n-parse road-segments.txt in gv_rd_seg_lst list structure\n-parse city-gps.txt in gv_cty_gps_lst list structure\nUsing haversine function of Python directly\n-to calculate the GPS distance between two location which will be used in\nA* search heuristic for distance constraint\ndist_calc()\n-it passes values to haversine_distance()\n-the values are extracted from gv_cty_gps_lst - latitude and longitude of current and destination city\ncost_calc()\n-for every next hop possible we need to calculate cost of that hop which is done in this function\na_star_srch()\n-we chack if initial state is same as goal state or not\n-if not we initialize a priority queue\nwe use priority queue and the priority queue uses the cost to pop the next segment with lowest cost calculated in cost_clac\n-Queue is initialized with the initial segments based on start city\n-the while loop runs till queue is empty in this case printing ""INF"" or till we reach goal state\n-we pop the value with the lowest cost an duse it as the current state\n-successors for this current state are calculated if the current state is not equal to goal state\n-for every successor we calculate the next cost, distance, time, total gas gallon, segments\n-we append the route as well over here for every successor\n-when the goal state is found we print the segment, distance travelled, time required, total gas gallons, and path\n\nIII)discussion of any problems you faced, any assumptions,simplications, and/or design decisions.\n1.initially both files were parsed in list structure for time optimization it was changed to dictionary after comparison of      run time list structure took less time\n2.bidirectional case handling : we switched the city names  where needed in the successor states\nCities which didn\'t have GPS details case handling : in dist_calc() if we do not find details in the given data we return 0.\nif the returned value from dist_calc is 0 we consider the distance                                        given for that segment in the dataset\n3.the haversine module is not available on SICE server - added a function for it haversine_distance()\n4.For mpg ""V"" in the formula given assumed the speed limit for that hop\n5.total gas gallons = (hop distance/mpg for the hop)\nPart 3 - Choosing a team\nProble Statement\nChoose a team of robots that has the greatest possible skill\nGiven Code :\nThe sceleton code used a greedy approach to choose the team of robots. Firstly the list of robots is picked in decreasing order of skill per unit cost. But the solution was not giving the team of robots in whole number.\nSo to give the solution in more optimized way we reversed the robots in the sorted list and removed the logic for fraction solution in the Else part of If Statement. This gave me the answer for the team of robots in whole number(1.0000).\ndef approx_solve(people, budget): \n\nThis function solves the code using Greedy Search\nWhich may or may not be optimal for every scenario\n\nProblem Faced :\nWe tried using multiple datasets for the above approch but the code didn\'t gave the optimal solution for all the case. This was because the code considers the perfect solution is the one that it finds at that moment.\nTo solve this we thought of using the Knapsack algorithm which is similar used to solve similar problems as the problem statement.\nThen we started implementing the 0/1 Knapsack problem with the Dynamic programming approach. Though if using the matrix technique approach for calculating the max cost and perfect team of robots. But as the values for cost and skills were in the floating point we used naive way of taking the round of that number and solving it. But this didn\'t gave the optimal solution for the robots who were having cost/skill relatively similar values.\nFinal Approach :\nKnapsack Branch and Bound Function \n\ndef approx_solve_branchandbound(people, budget):\n\n\n\nAlgorith Used : 0/1 Knapsack using Branch and Bound\nAs the problem statement is based on solving combinatory optimization 0/1 knapsack algorithm provides optimized solution for choosing robots based on the budget given.\nThis algorithm solved both the problems faced in 2 earlier approaches and gave the optimum solution for every single dataset and case. The backtracking solution helps in ignoring the infeasible solutions.\n\n\nFlow :\nCreated 2 classes for storing the Robots details and Robots State in the node of tree\nCode starts with reading the file and storing the data in the dictionary.\nSorted all the robot items in the descending order of ratio of cost per skills\nInitialize the max_skill =0 and also Added a dummy node of Decision tree and added it to Dequeue.\nAfter that untill the deque is not empty we deque one by one element and find profit of next node. If the profit is more then we update the max_skill. Then calculate Upper Bound and if bound > max_skill then add node to the dequeue.\nConsider the case when next node is not considered but we have to add the next level to the dequeue without updating the max_skill.\n\n\nClass to calculate the upper bound for element\n\ndef calculate_bound(u, capacity, item_count, items):\n\n'], 'url_profile': 'https://github.com/Kaustubh-DB', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'Pabna, Bangladesh', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Artificial-Intelligence-and-Robotics-Lab\n'], 'url_profile': 'https://github.com/6shihab', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['IF.03.01-11 Basic Web Techniques - Dynamic Css 3 Features\nThis coding assignment shall make you practice the dynamic css3 features. Make sure that you read the section Required Tasks  in CodingAssignment.md carefully and to complete all the tasks listed there.\n'], 'url_profile': 'https://github.com/zzArchive-if-03-01-C-fall-2019', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'Orlando, Florida', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': [""CAP4630-Artificial-Intelligence-UCF\nAbdool Shakur's Repo for the UCF course CAP4630 aka Artificial Intelligence and Machine Learning.\n""], 'url_profile': 'https://github.com/salamshakur', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'Türkiye', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mustafakoca99', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['yemolysis\nARTIFICIAL INTELLIGENCE (AI)\n'], 'url_profile': 'https://github.com/Yemolysis', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2,734 contributions\n        in the last year', 'description': ['AI Challenges\nResearching narrow artificial intelligence.\nContents\n\nImage Classification\nImage Synthesis\nLanguage Model\nSpeech Recognition\nSpeech Synthesis\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Weak_AI\n\nLicense\nMIT licensed\n'], 'url_profile': 'https://github.com/wurde', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'Seoul, Korea', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/guiyomj', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/venkatesh405', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Arbert407', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'Fairfax, VA ', 'stats_list': [], 'contributions': '442 contributions\n        in the last year', 'description': ['Slides of presentations I have done at various meetups\n'], 'url_profile': 'https://github.com/vineetk1', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Welcome to GitHub Pages\nYou can use the editor on GitHub to maintain and preview the content for your website in Markdown files.\nWhenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.\nMarkdown\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\nFor more details see GitHub Flavored Markdown.\nJekyll Themes\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings. The name of this theme is saved in the Jekyll _config.yml configuration file.\nSupport or Contact\nHaving trouble with Pages? Check out our documentation or contact support and we’ll help you sort it out.\n'], 'url_profile': 'https://github.com/adityajadhav3', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Adsanvar', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '602 contributions\n        in the last year', 'description': ['intro-artif-intel\nOld Artificial Intelligence coursework using search algorithms on search spaces.\nReferences\nSonya Allin / Bahar Aameri, University of Toronto Winter 2020 \n'], 'url_profile': 'https://github.com/22victoryy', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['aicodes\nArtificial intelligence works\n'], 'url_profile': 'https://github.com/kenbravo2017', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}"
"{'location': 'Bloomington, Indiana', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['a0\nPart 1: Finding Your Way\nPython program for Finding Your Way from a given location to the final location.\nThis program demonstrates search algorithm named Breadth First Search. It basically finds for best possible shortest path for a given set of input map. The set of components for the program are: 1] The map given in a ‘.Txt’ File. 2] Certain blocked positions are marked by ‘&’. 3] The starting position marked by #. 4] A end position or the target position marked by @.\nSolution of Finding Your Way problem we check the possible valid paths that is, all the (‘.‘) positions, in the same row and in the same column, starting with the ‘#’, and moving onto the next valid adjacent nodes, considering & as a block-way and thereby restricting its path.\nState of states will be all the possible moves from # point to @. Initial state will be the starting location of traversal(#). Successor function Succ:S will be all the valid positions(“.”) in the same row and column for that particular state.\nObservations and Experiment,\nAt the first point, Initial node is inserted into the fringe, and then popped out(Queue approach) and its valid successor is being appended to the fringe. And this task is being carried out until we reach our goal state. At Initial stage, main difficulty that I found going through this code was it was going in infinite loop as it was searching for all possible moves, and also considered the recurring moves. Hence, to reduce the number of states I used visited_node list, thereby adding visited states into fringe. Also to get all the possible directions(N,S,W,E), I passed *move(present) and *curr_move(successor) to see the comparison between row or column values, for instance if curr_move’s row valueis 1 less than move’s value then it is travelling North so used literal ‘N’ and thereby appended it in the fringe along with the cost of moving to its successor node that is 1. This loop was carried until we reached our goal state thereby increasing its distance by one while moving to its successor and adding path of its traversal. Also if solution is not found, and is going into infinite loop then it will return Inf.\nPart 2: Hide and Seek\nPython program for arranging K friends, such that no two friends can see one another.\nThis program demonstrates search algorithm named Depth First Search. The set of components for the program are: 1] The map given in a ‘.Txt’ File. 2] Certain blocked positions that are marked by ‘&’.\nSolution of arranging is that we iterate from left to right and top to bottom over the map to get the first empty position. So now is the time where we can place the 1st Friend at that point. The Initial State will be defined by placing the first friend and moving this state to the fringe. Successor function Succ:S will be all the valid position that the second friend could be placed with a condition that it can’t be placed in the same row, column respective to the friend. New Friend can only be placed after the block between the same row and column. Check if new friend is visible on the east, west, south and north from this side walk, if the friend is not visible in the same path, then place that friend on the board and push it on the fringe. Check this loop till all the friends are placed and no more fringe items are left. That will be our goal state.\nObservation and Experiment,\nAt the initial stage, I was stuck with a problem of exploring all the possible states including the one that is visited recursively for the same row and column, I thereby reduced the number of states to explore by adding traversed states list in the fringe. I made a try to use recursive method in a way to avoid using fringe but was not able to do so. Hence maximizing the time complexity of the code. The code can be made more optimizable by breaking the loop for traversal if we are not able to find the friend’s placement in the same row or column after the first iteration and was not able to achieve that goal. I will in fact try to learn more about recursive functionality and avoiding unnecessary loop traversals in the coming weeks and try to implement it in the next assignment making the code optimum as possible.\n'], 'url_profile': 'https://github.com/Kaustubh-DB', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'Taiwan', 'stats_list': [], 'contributions': '4,192 contributions\n        in the last year', 'description': ['Data Science and Artificial Intelligence Note\nPython\n\nBasic\nData Wrangling\n\nMachine Learning\n\nBasic\nVectors, Matrices, And Arrays\nPreprocessing Structured Data\nPreprocessing Images\nPreprocessing Text\n\nDeep Learning\nReference\nhttps://chrisalbon.com/?fbclid=IwAR0DqM9JoqeDV2JVeLX7Jr3k5SsCax3J2BgDSjDpddI9UnmR8LH1WRs27hU\n'], 'url_profile': 'https://github.com/Offliners', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'Torino', 'stats_list': [], 'contributions': '129 contributions\n        in the last year', 'description': ['Landmark Recognition\nLandmark Recognition is a large-scale classification task\nwhich is raising the interest of machine learners from all\nover the world for its peculiar challenging aspects. A huge\namount of landmarks have been captured and gathered in\nthe Google Landmark Dataset. This extreme classification\nscenario is combined with the necessity to confirm the correctness\nof the obtained prediction, since the test set contains\n’tricky’ samples.\nTo do so, a retrieval mechanism\nbased on Deep Local Features is implemented. This solution\nprovides a full pipeline composed by three main stages:\n\nA pre-processing and filtering step to\ndeal with constraints imposed by both time and computational\nresources available;\nThe classification phase has been performed with a ResNet50 model exploiting transfer\nlearning from ImageNet dataset;\nDELF module has been adapted to our specific application with a thresholdbased\ndecision system for an efficient verification of the predictions.\n\nIn the following paper the results will be analyzed.\n'], 'url_profile': 'https://github.com/iliodipietro', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'Montreal', 'stats_list': [], 'contributions': '2,614 contributions\n        in the last year', 'description': ['maiart\n\nMaster in Art - Artificial Intelligence\n\nX\nx 200117 1528 - Receive variations\n--@STCGoal TouchDesigner and Arduino Talks to each other\n\nx/x__arduino_td__200117/README.md\n\n09\n\nIntegrating another channel to control rotation of the noize : FAILED - Can not connect param, sucks I know !!\n\n10\n\nINtegrating RENDER\n\n'], 'url_profile': 'https://github.com/jgwill', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'Vienna, Austria', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['ARTICLE\nARTificial IntelligenCe powered Literature sEarch\nhttps://github.com/tyiannak/pyScholar/blob/master/pyScholar.py\n'], 'url_profile': 'https://github.com/TheCabbageBaggage', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'Zurich, Switzerland', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tucaman', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""AIProject_FraudDetection\nProject on Artificial Intelligence Course. A model tailored to the exact business that detect fraud users.\nRead more in the report\nGetting Started\nYou don't need to clone this project to run it! However, we are not able to provide you the full data due to the NDA.\nThe part of the code that you can run is marked with --REPRODUCIBLE PART--. Thus you can check how the saved models perform on the test data.\nPrerequisites\nRead more in requirements.txt\nRunning the code\nFor accessing the saved models and test data, click here. You need to save it in your Drive in order that Colab could access the files.\nAfter that you should mount your drive in Colab(cell in our notebook with code as below) and run imports(next cell).\nYou can scroll through the non-reproducible part and still see the code results.\nfrom google.colab import drive\ndrive.mount('/content/drive/')\n\nBuilt With\n\nGoogle Colab - Jupyter notebook environment that runs in the cloud\n\nAuthors\n\nSofiya Hevorhyan - SofiyaHevorhyan\nOlya Zubyk - olyazub\n\nAcknowledgments\n\nBase of this project was developed during the course of internship at Corevalue Inc.\nSpecial thanks to our mentor, Olena Domanska\n\n""], 'url_profile': 'https://github.com/SofiyaHevorhyan', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'Royaume uni des pays bastille', 'stats_list': [], 'contributions': '19,375 contributions\n        in the last year', 'description': [""What is QuanticoDB ?\nQuanticoDB is a fast, simple ( 4 command ), Fully Encrypted NoSQL DB + A.I. with auto training, can talk and learn from: Google Prediction - Microsoft Cognitive - IBM Watson with one simple language.\nrequire 'Quantico.php';\n\nQuantico\\DB::in();  // --- insert data\nQuantico\\DB::ver(); // --- verify data\nQuantico\\DB::del(); // --- delete data\nQuantico\\DB::out(); // --- extract data\n\n// *******************\n// ** Best Practice **\n// *******************\n\nuse Quantico as Q;\n\nQ\\DB::in();  // ---------- insert data\nQ\\DB::ver(); // ---------- verify data\nQ\\DB::del(); // ---------- delete data\nQ\\DB::out(); // ---------- extract data\nRequirements\nQuanticoDB requires Linux + PHP 5.6 or greater.\n\n\nLicense\nQuanticoDB is released under the MIT license.\n\n\nContributing\nArtificial Intelligence & language translations are welcome.\n\n\nDocumentation\nI'm working to Wiki ... keep watching QuanticoDB.\n\n""], 'url_profile': 'https://github.com/nondejus', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['AI/ML Learning Path\nA list of (mostly free) ressources to learn Artificial Intelligence and Machine Learning.\nUniversity Courses\nThe following courses have their slide, notes, homework, and/or videos available online:\n\nCS188: Intro to AI, based on Artificial Intelligence: A Modern Approach\nCS229: Machine Learning\nCS230: Deep Learning\nCS231n: Convolutional Neural Networks for Visual Recognition\nCS224n: Natural Language Processing with Deep Learning\nCS285: Deep Reinforcement Learning\nCS246: Mining of Massive Datasets\nStat212b: Topics Course on Deep Learning\n\nOnline Courses\n\nFast.ai\nOpenAI Spinning Up\n\nBooks\n\n🎁 Free The Hundred-Page Machine Learning Book\nHands-On Machine Learning  with Scikit-Learn & TensorFlow, the associated Github repository has tutorials as Jupyter notebooks.\nDeep Learning with Python, by the creator of Keras\n\nTextbooks\nMathematical background for machine learning (but you probably want to look elsewhere if you are not used to maths):\n\n🎁 Free Mathematics for Machine Learning\n\nTo have a general tour of techniques in artificial intelligence read AIMA:\n\nArtificial Intelligence: A Modern Approach\n\nUse these textbooks to get a grasp of machine and statistical learning:\n\n🎁 Free Foundations of Data Science\n🎁 Free A Course in Machine Learning\n🎁 Free Elements of Statistical Learning\n🎁 Free Pattern Recognition and Machine Learning\nMachine Learning: A Probabilistic Perspective\nLearning from Data, starts to be old and is not enough by itself but can help understand the very basics of the VC\n\nLearn deep learning with these:\n\n🎁 Free Neural Networks and Deep Learning\n🎁 Free Deep Learning\n\nIf you want to learn reinforcement learning, this one is the reference:\n\n🎁 Free Reinforcement Learning, 2nd Edition\n\nFor data mining specific methods:\n\n🎁 Free Mining of Massive Datasets\n\nPapers\nTo get into Deep Reinforcement Learning, this review is nice:\n\nAn Introduction to Deep Reinforcement Learning\n\nUse the Arxiv Sanity Preserver to find new papers to read.\nInteresting Blog Posts\n\nDeep Learning: Our Miraculous Year 1990-1991\nBetter Language Models\nand Their Implications\nEmergent Tool Use from\nMulti-Agent Interaction\n\nFrameworks/Libraries\nnumpy and pandas are essential, numpy is basically everything math-related whereas pandas is all about manipulating data.\n\nnumpy\npandas\n\nscikit-learn is part of the bigger scipy framework but works as a standalone. It contains a lot of non-deep learning machine learning algorithm and many preprocessing methods. Perfect for learning linear/logistic regression, SVMs, or tree-based methods.\n\nscikit-learn\n\nPyTorch and TensorFlow are pretty much competitors, chose one and implement everything with it. Keep a distant eye on the other.\n\nPyTorch\nTensorFlow\n\nKeras is a layer of abstraction on top of TensorFlow, CNTK or Theano (which it uses as a backend).\n\nKeras\n\n'], 'url_profile': 'https://github.com/Qu3tzal', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '267 contributions\n        in the last year', 'description': ['AI For Connect 4\n\nA Connect 4 AI which uses the minimax algorithm and alpha beta pruning to search for the possible best move. A transposition table is incorporated storing previous calculations and iterative deepening is used to make the AI search progressively deeper as the transposition table becomes larger. The pygame module is used for the GUI in which the player can compete with the AI on a 6 x 7 board. The difficulty of the AI can be adjusted by changing the time available to the AI.\n\nUsage\nTo run the script vs AI:\n$ python Connect4_AI.py\nSee the section on adjustable parameters to change behaviour of the AI\nThe transposition and zobrists tables are already present with game data.\nTo reset the tables:\n$ python Cache_Init.py\nor, delete the cachetable.pickle and zobtable.pickle files.\nRunning the AI script will automatically check for these files and run the Cache_Init.py script if the files are not found.\nTo run the 2 player script:\n$ python Connect4_Basic.py\nDependencies\nThe following external modules are used\n\npygame\nnumpy\n\nCommands for installation of pygame 2.0 and numpy\n$ pip install pygame\n$ pip install numpy\nSearching algorithms\nMinimax\n\nThis AI uses the minimax algorithm to search for all possible board outcomes and returns the best move which will result in the best scoring board position if the AI always plays the move maximising its chance of winning and the opponent player always plays the move minimising the AI’s chance of winning. The minimax will go down each possible board position and recursively score that board position using the minimax algorithm at a lower depth, until all board positions and their scores up to a given depth are calculated. The AI will then return the best board position if it is the AI’s move, and return the worst board position if it is the player’s move. The algorithm will therefore finally return the best move to play at the current moment to maximise the score of the eventual board position if the player plays the best moves as expected.\n\nAlpha Beta Pruning\n\nThe minimax algorithm, when used naively, is slow and inefficient as it explores all possible outcomes up to certain depth, meaning with each depth the search time increases 7x. Alpha beta pruning significantly reduces the search time by eliminating branches of the search that will never be explored as it is undesirable for either the AI, or the player, and so the minimax will never have returned a final value from that branch. The nature of the algorithm means if the best move is found early in the search, more branches can be disregarded. Therefore the searching algorithm is altered to search moves form the centre column first, then spread the search outwards to neighbouring columns, as the best move is more likely to be nearer to the centre.\n\nScoring Function\nBasic Scoring System\n\nThe AI uses a scoring function to assess board positions in order to implement the minimax algorithm. After testing different values, the following were found to work the best:\n\n3 AI and 1 EMPTY  ------>  +5\n2 AI and 2 EMPTY  ------>  +2\n\n3 PLAYER and 1 EMPTY  -->  -5\n2 PLAYER and 2 EMPTY  -->  -2\n\n\nAlongside this basic scoring system, the odd-even strategy was implemented to more accurately calculate the value of a given position so the AI can improve its decision making.\n\nOdd-Even Strategy\n\nThe odd-even strategy dictates that is is favourable for the odd player to have a potential win with an empty position which completes the win on an odd row, with the opposite true for the even player. The odd player is the player that starts first and vice versa for the even player. This is because when the board becomes full the other player will be forced to play their coin in the column that will give the opponent the win. Therefore the lower down the empty space is, the better. The power the strategy means it must be scored highly:\n\nOdd-Even for AI  ------->  Distance from top * 100\nOdd-Even for PLAYER  --->  Distance from top * -100\n\nWinning Move\n\nThe winning move must be given ultimate value as it is the aim of the game.\n\nWinning Move  ---------->  +1000000\nLoosing Move  ---------->  -1000000\n\n\nThe AI can be optimised by anticipating a win when the board is in a position in which any one player can win the game in the next move. This effectively means that a depth 4 AI can peak into what will happen at depth 5, and can prevent a loss or push to a win.\n\nAnticiapted Win  ------->  +100000\nAnticiapted Loss  ------>  -100000\n\n\nThis value is 10x less than the value give to a win, because is must be much greater than all other board positions, however a win/loss is still prioritised over an imminent win/loss as the AI must not opt for a potential win and loose the game as a result.\n\nFurther Optimisations\nTransposition table\n\nThe transposition table uses zobrist hashing to store hash values of board positions calculated during the game to save the AI from starting from scratch every time it is called to calculate the best move. The transposition table stores the hash of the board, the score of the board, and the calculation depth that has resulted in the score. The transposition table is referred to every time the minimax algorithm is assessing a board, and if the board hash is in the table and the calculation depth is greater than or equal to the depth of calculation the minimax algorithm must perform, the score stored in the table is used and the calculation doesn’t need to be done. If the table does not contain a hash, or the table contains a hash with a lower corresponding depth than the one calculated, the entry in the table is (over)written as the new calculation. The calculations made in previous games are saved in the transposition table so the minimax can build upon the previous calculations instead of repeating them in future games.\n\nIterative Deepening\n\nUsing the introduction of the transposition table, iterative deepening can be implemented. Instead of limiting the depth of the search, the time available to the AI can be the limiting factor. The AI will iterative increase the depth of the search until the time available is up, at which point the AI is terminated and the move returned by the highest depth that could be calculated in the given time is used as the final move.\n\nAdjustable Parameters\nThe time available to the AI in seconds can be adjusted to any integer value. A higher time value means the AI will achieve a higher depth of calculation.\n# maximum seconds AI can take\nAI_TIME = 6\nThis can be switched to true to train the AI by making it playing itself. This will improve the AI by increasing the number or boards stored in the transposition table.\nAI_VS_AI = False\nThe play order can be adjusted to give the first move to the PLAYER or to the AI. The order of play is randomised by default.\nPLAY_ORDER = [PLAYER, AI]\n\n# PLAY_ORDER.reverse()\nrandom.shuffle(PLAY_ORDER)\n'], 'url_profile': 'https://github.com/dhruvnps', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}"
"{'location': 'Cleveland, OH', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/annasedla', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/simalcubuk', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Minimax_Checkers\n\n\nMinimax based artificial intelligence that plays checkers programmed in C++ and C\n\n\nVariable depth of future moves the minimax algorithm will analyze, anything higher than 8 runs slowly\n\n\nAlpha beta pruning is used to optimize runtime when searching ahead a large number of moves\n\n\nGame is displayed on a 32x32 LED matrix controlled by an Arduino Mega\n\n\nAlso contains Player vs. Player gamemode\n\n\nFull project report for Software Engineering 101 class included in repository\n\n\nCreated alongside Connor Byers, Nick Makharinets, and Andrew Wang\n\n\n'], 'url_profile': 'https://github.com/n-faria', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/simalcubuk', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'Poznań', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aleksykrolczyk', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/federica-collab', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'Jakarta, Indonesia', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Bang Bang\nBang Bang adalah sebuah game FPS yang dibuat menggunakan Unity. Game ini memiliki genre Survival, dimana Player harus bertahan hidup dengan cara membunuh musuh-musuh yang akan respawn terus-menerus. Bang Bang juga menggunakan sistem skor, dimana setiap saat Player membunuh Enemy, maka skor Player akan bertambah sesuai dengan jenis musuh yang dibunuh.\nOur Team\n\n\n2101628783 -\nKristi Handayani -\nhttps://www.linkedin.com/in/kristi-handayani-9a11a5178\n\n\n2101651786 -\nRio Rafelino -\nhttps://www.linkedin.com/in/rio-rafelino-91a30a18b\n\n\n2101665450 -\nHans Michael -\nhttps://www.linkedin.com/in/hansmichaels/\n\n\n2101659025 -\nAnton -\nhttps://www.linkedin.com/in/anton-anton-5951b01a0\n\n\n2101627465 -\nIvan Andi -\nhttps://www.linkedin.com/in/ivan-andi-10a940183/\n\n\n'], 'url_profile': 'https://github.com/hansmichaels', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Matt Tetreau\nTim Van Dyke\nChristian Lundy\nJiri Hoffmann\n'], 'url_profile': 'https://github.com/tetreaum', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['CSCI-561\nAssignments for Artificial Intelligence course- Fall 2019\n'], 'url_profile': 'https://github.com/nidhi-86', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'Orange County, California', 'stats_list': [], 'contributions': '129 contributions\n        in the last year', 'description': ['Sudoku-Game-Solver\nProject for CS271P Intro to Artificial Intelligence\n'], 'url_profile': 'https://github.com/yulinzhang0822', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '195 contributions\n        in the last year', 'description': ['ai-projects-uni\nArtificial Intelligence - projects for University Course\nThis project contains my projects, which were creted for certain University courses.\nIntro to AI\nThis course was as an intoductory subject for AI and ML. We have been learning there about Machine Learning algoroithms and implementing some of them in Jupyter Notebook]\nArtificial Life with Cognitive Science\nThis course teached us about basics of simulation and evolutionary algorithms\n'], 'url_profile': 'https://github.com/trebacz626', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'Bryan, Texas', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/achaar', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/simalcubuk', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '615 contributions\n        in the last year', 'description': ['AI-Accessed-Excercise\nAssessed Exercise for Artificial Intelligence (H)\n'], 'url_profile': 'https://github.com/iShauny', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Music-AI-LSTM\nArtificial Intelligence for Music Generation, Columbia University\n\n\n\n\n\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/skandupmanyu', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gurmeetsdigitalworld', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'Torino', 'stats_list': [], 'contributions': '129 contributions\n        in the last year', 'description': ['K-Nearest Neighbors (KNN) and Support Vector Machines (SVMs)\nK-Nearest Neighbors (KNN) and Support Vector Machines (SVMs) are a set of learning methods used for\nregression and classification (also density estimation in the case of KNN). The aim of this experience is to apply\nboth KNN and SVM to the same dataset, analysing a set of parameters (k in the case of KNN and C, Gamma for\nSVM) in order to achieve the best accuracy on the validation set and then trying to predict correctly the labels\non the test set. At the end, K-fold Cross-Validation will be applied to improve the quality of the classification.\n'], 'url_profile': 'https://github.com/iliodipietro', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['CSCI 446 Artificial intelligence assignement 1\nAssignement goal\nThe goal of this assignment was to generate a map randomly: a set of points on a plane connected to each other. Then, to color each point so the connected points do not have the same color. The four-color theorem states that for a planar a graph, such a coloring is alway possible with at most 4 colors.\nAbstract\nIn order to solve the planar graph coloring problem, a CSP, we implemented five different algorithms on a randomly generated graph. The first 3 algorithms build off of each other. All three of the backtracking algorithms reliably give a correct graph coloring. But, the difference comes in the resources required for each. Simple backtracking is horribly expensive because it has no checks and balances. This is where forward checking improves the algorithm, but in this application, it does very little good after about 50 regions. Implementing arc consistency provides a huge leap in performance and resources needed. Performing much better than initially anticipated. Even up to 100 regions, the algorithm swiftly delivers the correct coloring. Genetic is a very interesting approach but seems unadapted for this problem. The algorithm will not provide consistent assignment of colors for a graph with size 30, this is due to the unguided nature of the search, which is driven by fitness, but occurs with random mutations. Simulated annealing performs much better as the mutations are not random but based on a min conflict heuristic. The space is also explored more thoroughly because of the possibility of accepting a worse state. This minimizes the local optima problem.\nScreenshots\n\n\n'], 'url_profile': 'https://github.com/olimar718', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/AIDA-UIUC', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['AITND-P1-TRADING-WITH-MOMENTUM\nThis is the first project in the nanodegree, artificial intelligence for trading\nThe aim is to implement a trading strategy and test to see if has the potential to be profitable. Supplied are a universe of stocks and time range.\nThe project requires to implement some functions:\nResample Adjusted Prices\nThe trading signal for this project isn\'t based on daily prices, and therefore a resample of the daily adjusted closing prices into monthly buckets is performed\nCompute Log Returns\nCompute the log returns (Rt) from prices (Pt) for each ticker and date\nRt = log e(Pt) - log e(Pt-1)\nShift Returns\nImplement a shift_return function to shift the log returns to the previous or future returns in the time series. The parameter shift_n is 2 and returns is the following and shifts the stocks in the future, if it\'s a negative number, the stock are shift in the past\nGenerate Trading signal\nA trading signal is a sequence of trading actions, or results that can be used to take trading actions. A common form is to produce a ""long"" and ""short"" portfolio of stocks on each date (e.g. end of each month, or whatever frequency you desire to trade at). This signal can be interpreted as rebalancing your portfolio on each of those dates, entering long (""buy"") and short (""sell"") positions as indicated.\nWe tried to implement the following:\nFor each month-end observation period, rank the stocks by previous returns, from the highest to the lowest. Select the top performing stocks for the long portfolio, and the bottom performing stocks for the short portfolio.\nImplementing the get_top_n function is to get the top performing stock for each month.\nProjected Returns\nHere we check the trading signal if it has the potential to become profitable.\nTherefore we compute the net returns that this portfolio would return. For simplicity, we\'ll assume every stock gets an equal dollar amount of investment. This makes it easier to compute a portfolio\'s returns as the simple arithmetic average of the individual stock returns.\nImplement the portfolio_returns function to compute the expected portfolio returns. Using df_long to indicate which stocks to long and df_short to indicate which stocks to short, calculate the returns using lookahead_returns.\nT-TEST\nThe null hypothesis ( 𝐻0 ) that the actual mean can return from the signal is zero. We  performed a one-sample, one-sided t-test on the observed mean return, to see if it can reject 𝐻0.\nFirst compute the t-statistic and then find it\'s corresponding p-value. The p-value will indicate the profitability of observing a t-statistic equally or more extreme than the one that is observed in the null hypothesis were true.\nA small p-value means that the chance of observing the t-statistic we observed under the null hypothesis is small, and thus casts doubt on the null hypothesis. It\'s good practice to set a desired level of significance or alpha ( 𝛼 ) before computing the p-value, and then reject the null hypothesis if  𝑝 < 𝛼.\nFor this project, we\'ll use  𝛼=0.05 , since it\'s a common value to use\nRESULT\nIn our test the alpha was set as 0.05, the result above show a value (0.073359) > 0.05, therefore we cannot reject the null hypothesis and moreover we can conclude that the strategy doesn\'t contains an alpha.\n'], 'url_profile': 'https://github.com/CDA70', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RUGMJ7443', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Berkeley, CA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ericjortiz', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Bangkok, Thailand', 'stats_list': [], 'contributions': '135 contributions\n        in the last year', 'description': ['Udacity Build a Sudoku Solver\nFirst Project from the Udacity Artificial Intelligence Nanodegree\nSynopsis\nIn this project, students will extend the Sudoku-solving agent developed in the classroom lectures to solve diagonal Sudoku puzzles. A diagonal Sudoku puzzle is identical to traditional Sudoku puzzles with the added constraint that the boxes on the two main diagonals of the board must also contain the digits 1-9 in each cell (just like the rows, columns, and 3x3 blocks).\nInstructions\nFollow the instructions in the classroom lesson to install and configure the AIND Anaconda environment. That environment includes several important packages that are used for the project.\n'], 'url_profile': 'https://github.com/AekachaiTang', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Kansas City, MO', 'stats_list': [], 'contributions': '2,340 contributions\n        in the last year', 'description': ['Paradigms of Artificial Intelligence Programming\nby Peter Norvig\n\nJust some notes I took while reading the book for tinkering along with his examples.\nThis will not be useful to anyone; just something I want to keep in case I revisit the chapters in the future.\n'], 'url_profile': 'https://github.com/Fedreg', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Berkeley, CA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ericjortiz', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Reutlingen, Germany', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['brain\nB.R.A.I.N (Best Reutlingen Artificial Intelligence Network)\n'], 'url_profile': 'https://github.com/rtlion', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['ECSE526\nAI for the course ECSE-526 Artificial Intelligence\n'], 'url_profile': 'https://github.com/acoulombe', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Beijing, China', 'stats_list': [], 'contributions': '218 contributions\n        in the last year', 'description': ['Fundamentals_of_AI\nCourse projects and reports for Fundamentals_of_AI by Prof. @ZhangChangshui and Prof. @JiangRui from Department of Automation in Tsinghua University. Project details in respective folders.\n'], 'url_profile': 'https://github.com/Iceblaze9527', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '171 contributions\n        in the last year', 'description': ['AI4Games\nImplementations of various Artificial Intelligence Behaviours for videogames in C++\n'], 'url_profile': 'https://github.com/MarcosJLR', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Montreal, Quebec', 'stats_list': [], 'contributions': '283 contributions\n        in the last year', 'description': ['SentimentClassifier\nCOMP 472 Project 2\n'], 'url_profile': 'https://github.com/SashSubba', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}"
"{'location': 'Portugal', 'stats_list': [], 'contributions': '1,398 contributions\n        in the last year', 'description': ['IART\n2019/2020 - 3rd Year, 2nd Semester\nCourse: Inteligência Artificial | Artificial Intelligence\nProjects developed by:\n\nMartim Silva\nLuís Ramos\nFrancisco Gonçalves \n\n\n\n\nProject Number\nProject Name\nDescription\n\n\n\n\n1\nSelf Driving Rides\nPython implementation of optimization algorithms (solution to google hashcode 2018)\n\n\n2\nFootball Predictions\nMachine Learning and Python to predict the outcome of football matches\n\n\n\nDisclaimer - This repository was created for educational purposes and we do not take any responsibility for anything related to its content. You are free to use any code or algorithm you find, but do so at your own risk.\n'], 'url_profile': 'https://github.com/motapinto', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Waterloo, Ontario', 'stats_list': [], 'contributions': '328 contributions\n        in the last year', 'description': [""Synviz\n\nDevpost submission\nSynviz is an IoT device that uses state of the art artificial intelligence to decode text from the movement of a speaker's mouth.\nInspiration\nThere were two primary sources of inspiration. The first one was a paper published by University of Oxford researchers, who proposed a state of the art deep learning pipeline to extract spoken language from video. The paper can be found here. The repo for the model used as a base template can be found here.\nThe second source of inspiration is an existing product on the market, Focals by North. Focals are smart glasses that aim to put the important parts of your life right in front of you through a projected heads up display. We thought it would be a great idea to build onto a platform like this through adding a camera and using artificial intelligence to gain valuable insights about what you see, which in our case, is deciphering speech from visual input.\nPipeline Overview\n\n\nThe user presses the button on the glasses to start a recording\nThe user clicks the button again to stop recording\nThe data is passed to a Google Cloud Platform bucket as an mp4 file\nSimultaneously, the glasses ping the Flask backend server to let it know there's something to be processed\nThe backend downloads the video file\nThe backend runs the video through a Haar Cascade classifier to detect a face\nThe video is cropped so that it tracks the mouth of the speaker\nThe cropped video is fed through a transformer network to get a transcript\nThe backend passes the transcript and file URL to the frontend through a socket\nThe frontend displays the transcript it got from the backend, and also allows playback of the mp4 file found on Google Cloud Platform\n\n\nUse Cases\n\nFor individuals who are hard-of hearing or deaf\nNoisy environments where automatic speech recognition is difficult\nCombined with speech recognition for ultra-accurate, real-time transcripts\nLanguage learners who want a transcript or translation\n\nSocial Impact\nThis hack can help in situations where communication is difficult. One of the most promising use cases is when\nthis technology is combined with automatic speech recognition. All-in-one solutions for real-time transcription and translation\nare becoming more viable as our technology progresses.\nThis proof-of-concept is another key piece that would\nimprove human computer interaction.\nNext Steps\nWith stronger on-board battery, 5G network connection, and a computationally stronger compute server, we believe it will be possible to achieve near real-time transcription from a video feed that can be implemented on an existing platform like North's Focals to deliver a promising business appeal.\nThe Team\n\n\nWaleed Ahmed - PM, Backend (Flask) & Cloud (GCP)\nSinclair Hudson - Hardware (Raspberry Pi) & Computer Vision (OpenCV)\nMartin Ethier - Deep Learning (TensorFlow)\nWilliam Lu - Frontend (React)\n\n""], 'url_profile': 'https://github.com/w29ahmed', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Guntur', 'stats_list': [], 'contributions': '150 contributions\n        in the last year', 'description': ['Introduction-to-TensorFlow-for-Artificial-Intelligence-Machine-Learning-and-Deep-Learning-\nLink to course: https://www.coursera.org/learn/introduction-tensorflow/home/welcome\nThis is a 4 week course where you will be introduced to Keras and use it to implement different models and learn about different callbacks.\nLearnt the following concepts:\nWeek1: Working with Google Colab,Introduction to Keras that uses Tensorflow as backend\nWeek2: Working with MNIST Dataset using ANNs,using callbacks to control training of neural network\nWeek3: Working with MNIST Dataset using CNNs\nWeek4: Working with complex Datasets\n'], 'url_profile': 'https://github.com/sonusajid004', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/u8karshgupta', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Aveiro, Portugal', 'stats_list': [], 'contributions': '213 contributions\n        in the last year', 'description': [""iia-ia-bomberman\nBomberman clone for AI teaching\n\nHow to install\nMake sure you are running Python 3.5.\n$ pip install -r requirements.txt\nTip: you might want to create a virtualenv first\nHow to play\nopen 3 terminals:\n$ python3 server.py\n$ python3 viewer.py\n$ python3 client.py\nto play using the sample client make sure the client pygame hidden window has focus\nKeys\nDirections: arrows\nA: 'a' - detonates (only after picking up the detonator powerup)\nB: 'b' - drops bomb\nDebug Installation\nMake sure pygame is properly installed:\npython -m pygame.examples.aliens\nTested on:\n\nUbuntu 18.04\nOSX 10.14.6\nWindows 10.0.18362\n\nDevelopers:\n\nRenato Valente\nJacinto Luf\n\n""], 'url_profile': 'https://github.com/renatovalente5', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Tangerang, Banten, INDONESIA', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NicholasDominic', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Bucharest', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Madaist', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['intro-to-ai\n[2019 Fall] course materials for FDU DATA130008: Introduction to Artificial Intelligence 人工智能\nUnfortunately no course website is available for this semester. There is an old one for 2019 Spring, where you can find lecture slides, videos, codes, etc.\nTextbook: Stuart J. Russell, Peter Norvig (2009) Artificial Intelligence A Modern Approach, 3rd Edition.2009, Prentice Hall\n'], 'url_profile': 'https://github.com/sgallon-rin', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '219 contributions\n        in the last year', 'description': ['UnityZombies\nProject for passing the subject ""Fundamentals of Artificial Intelligence"" at the Jagiellonian University\n'], 'url_profile': 'https://github.com/Deusald', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Varanasi, India', 'stats_list': [], 'contributions': '828 contributions\n        in the last year', 'description': ['CSE-241N-AI-Lab-Codes\nThis repository contains the Lab codes of the course Artificial Intelligence (CSE-241) at IIT BHU Varanasi - Even Semester 2019-20.\n\nGuided By: Dr. Anil Kumar Singh, Associate Professor, CSE, IIT (BHU) Varanasi.\n\nContents:\n\nAssignment 0: Tools Installation.\nAssignment 1: Sudoku Solver.\nAssignment 2: Linear Regression.\nAssignment 3: Logistic Regression.\nAssignment 4: K Means Clustering.\nAssignment 5: HMM Viterbi.\n\n'], 'url_profile': 'https://github.com/krashish8', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JeongbokSon', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MEHEDl', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,079 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aridavis', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': [""ai-uoi-coursework\nExercise for artificial intelligence course at cse-uoi, spring of 2019.\nWhole project is written in Java.\n\nCollaborators\n\n@DimitrisSintos\n@Billy54\n\n\nContents\n\nAI_part1 : a project that simulates a robot movements inside a labyrinth.\nThe highlighted algorithms are Uniform Cost Search and A-Star.\neuretic.pdf explains the choice behind the heuristic function on the A-Star Algorithm.\nAI_part2 : a two player game, where the opponent is the computer.\nAlgorithm MiniMax is used to calculate the computer's actions.\n\nComments\n\nyou can find the requirements at exercise-instructions(greek).pdf\n\nTO-DO LIST_\n\n translate requirements and other pdf files to english\n clean code\n\n""], 'url_profile': 'https://github.com/ThThoma', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['sudoku\nsolve sudoku game using artificial intelligence (Backtracking algorithm , Forward checking , Simulated Annealing)\n\n'], 'url_profile': 'https://github.com/an9080', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'Tangerang, Banten, INDONESIA', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NicholasDominic', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'Bologna, Italy', 'stats_list': [], 'contributions': '552 contributions\n        in the last year', 'description': ['uni-notes\nA collection of notes in markdown from my Master in Artificial Intelligence:\nhttps://www.unibo.it/it\n'], 'url_profile': 'https://github.com/nihil21', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'Gujarat', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prachi-ag', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'Bangalore, Karnataka', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Data Science Projects\nData Science Projects on Artificial Intelligence, Python, R , SAS, Adv. EXCEL, IBM SPSS, IBM Modeller etc.,\nRapid miner Projects will be updated soon....\nAdvanced Excel\n'], 'url_profile': 'https://github.com/sanjaytallolli', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'Tangerang, Banten, INDONESIA', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NicholasDominic', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '277 contributions\n        in the last year', 'description': ['02445_Statistical_evaluation_of_AI\nScripts used in the course 02445 Statistical evaluation of Artificial Intelligence\n'], 'url_profile': 'https://github.com/s183920', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': [""\nLive Subtitles\n\nDisplay what people are saying as they are talking in real time!\nA quick app built by David Shen, Roger Wang, Jerry Han, and Alex Sun.\nCreated using React, Tensorflow.js, and Chrome's speech-to-text API.\n\n \n\nTry It Yourself\n\n\n\n\n\n\nSee Full Video Demonstration!\n\n\n\n\n\n\n\n\nGetting Started\n\nAll the code required to get started\nImages of what it should look like\n\nClone\n\nClone this repo to your local machine\ngit clone https://github.com/rogerwangcs/ar-dialogue-subtitles.git\n\nSetup\nInstall dependencies\nyarn install or npm install\nInstall dependencies\nyarn install or npm install\nRun\nyarn start or npm npm start\nEnjoy!\n\nContributors\n\nRoger Wang\nDavid Shen\nJerry Han\nAlexander Sun\n\n\nLicense\n\nMIT license\n\n""], 'url_profile': 'https://github.com/heladegachi', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': [""fake-news-detection\nIntro\nThe goal of this repository is to build a machine learning model which is able to detect fake news by estimating the relative perspective (stance) of two pieces of text relative to a topic or claim.\nInput:\nA headline and a body text - either from the same news article or from two different articles.\nOutput:\nClassify the stance of the body text relative to the claim made in the headline into one of four categories:\n\nAgrees: The body text agrees with the headline.\nDisagrees: The body text disagrees with the headline.\nDiscusses: The body text discuss the same topic as the headline, but does not take a position.\nUnrelated: The body text discusses a different topic than the headline.\n\nEvaluation\nPerformance is measured based on a weighted, two-level scoring system:\n\nLevel 1: Classify headline and body text as related or unrelated: 25% score weighting.\nLevel 2: Classify related pairs as agrees, disagrees, or discusses: 75% score weighting.\n\nRationale: The related/unrelated classification task is expected to be much easier and is less relevant for detecting fake news, so it is given less weight in the evaluation metric. The Stance Detection task (classify as agrees, disagrees or discuss) is both more difficult and more relevant to fake news detection, so is to be given much more weight in the evaluation metric.\nConcretely, if a [HEADLINE, BODY TEXT] pair in the test set has the target label unrelated, evaluation score will be incremented by 0.25 if it labels the pair as unrelated.\nIf the [HEADLINE, BODY TEXT] test pair is related, the evaluation score will be incremented by 0.25 if it labels the pair as any of the three classes: agrees, disagrees, or discusses.\nThe evaluation score will so be incremented by an additional 0.75 for each related pair if gets the relationship right by labeling the pair with the single correct class: agrees, disagrees, or discusses.\nData\nTraining sets: Pairs of headline and body text with the appropriate class label for each.\n\ntrain_bodies.csv - shape (1683, 2): contains the article body column ('Body ID') with corresponding body text of article ('article Body') column.\ntrain_stances.csv - shape (49952, 3): contains article headline ('Headline') for pairs of article body ('Body ID'), and labeled stance ('Stance') columns.\n\nTest sets: Pairs of headline and body text without class labels used to evaluate systems.\n\ntest_bodies.csv - shape (905, 2): contains the article body column ('Body ID') with corresponding body text of article ('article Body') column.\ntest_stances_unlabeled.csv - shape (25414, 3): contains article headline ('Headline') for pairs of article body ('Body ID') (no Stance column).\n\nData source: The data is derived from the Emergent Dataset created by Craig Silverman. For more information, visit Fake news challenge.\nModels\nI implemented 3 methods:\n1. Perceptron\n(44 features)\n\nScore on the development set (Hold-out split): 3307.25 out of 4448.5 (74.3%).\nScore on the test set: 8461.75 out of 11651.25 (72.6%).\n\n2. Softmax linear model with additional tf-idf features\n(44 features + tf-idf features)\n\nScore on the development set (Hold-out split): 3857.75 out of 4448.5 (86.7%).\nScore on the test set: 8963.25 out of 11651.25 (76.9%)\n\n3. Feedforward neural network with additional tf-idf features\n(44 features + tf-idf features)\n\nScore on the development set (Hold-out split): 3907.25 out of 4448.5 (87.8%).\nScores on the test set: 8984.25 out of 11651.25\t(77.1%).\n\nUsage\npython main.py -method [method's name]\n\nmethod's name: choose one method for training: perceptron, softmax_linear_model (softmax linear model with additional tf-idf features), or feedforward_NN (feedforward neural network with additional tf-idf features).\nReference\nData source and evaluation: Fake news challenge\nDate uploaded: Jan 11, 2020\nDate updated: Dec 22, 2020\n""], 'url_profile': 'https://github.com/huyenkn', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['This file is used for the machine learning and artificial intelligence. In recent decades, the artificial intelligence has been widely used in many fields.\nHowever, the advantages of the technology have not been fully understood.\n'], 'url_profile': 'https://github.com/Datalearn168', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'Alexandria, Egypt', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['Deft-Eval\nThis is an implementation of definition evaluation project as a class project within the Artificial Intelligence class.\nMotivation\nThis project aimed to learn more about the classification algorithms such as naive byes, word2vec tool in classification and familiarity with machine learning in general.\n'], 'url_profile': 'https://github.com/ahmedfawzy98', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'Montreal, QC, Canada', 'stats_list': [], 'contributions': '580 contributions\n        in the last year', 'description': ['https://github.com/razine-bensari/IndonesianDotPuzzle\nIndonesian Dot Puzzle\nlink of repo above\nTo run the dfs, you need to have python 3.7.x\nsimple go to run the main.py file\npython3 main.py (in the terminal)\n'], 'url_profile': 'https://github.com/razine-bensari', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bentrey', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'Pemba', 'stats_list': [], 'contributions': '564 contributions\n        in the last year', 'description': ['FEVER NOTIFICATION SYSTEM USING EMAIL AND SMS\nThe purpose and aim of this application is to actualise the ability of a system to connect a fever status detection AI system to a computer/mobile client application and use clients SMS/notification systems. Below are some of the components in its implementation.\nDevelopmental enviromental installation\nDo not use ordinary python. Rather use condo enviroment for ANN and ML\n--------------------------------\nconda activate atslearning\n\npython serverMaker1.py\npython serverMaker2.py\npython serverMaker3.py\n\n---------------------------\npip install twilio\npip install flask\n\npython server.py\npython request.py\n----------------------------------\n\nDevices and Components\n\nSensors and PC server\nServer and Website-UI\nServer, Tablet-UI and Phone-UI\nSMS Python/NodeJS\nEmail Python/Nodejs\n\nUser Interface UX\nHere are screens shots from the application accuracy plots and User experiece interfaces\n\n\n\n\n\n'], 'url_profile': 'https://github.com/LINOSNCHENA', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['sick-tac-toe\n2 player iOS game with Artificial Intelligence: Player vs AI, Swift 3\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/Lizz1102', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'Porto, Portugal', 'stats_list': [], 'contributions': '1,250 contributions\n        in the last year', 'description': ['FEUP-AIAD\nProjects developed for the AIAD (Distributed Artificial Intelligence Agents) course unit.\nContext\n\nDate: 4th Year, 1st Semester, 2019/2020\nTopic:  Exercises\nCourse: Agentes de Inteligência Artificial Distribuidos (AIAD) | Distributed Artificial Intelligence Agents\nCourse Link: https://sigarra.up.pt/feup/pt/UCURR_GERAL.FICHA_UC_VIEW?pv_ocorrencia_id=436453\n\nDisclaimer\nThis repository, and every other FEUP-COURSE* repos on GitHub correspond to school projects from the respective COURSE. The code on this repo is intended for educational purposes. I do not take any responsibility, liability or whateverity over any code faults, inconsistency or anything else. If you intend on copying most or parts of the code for your school projects, keep in mind that this repo is public, and that your professor might search the web for similar project solutions or whatnot and choose to fail you for copying.\n(Credit to miguelpduarte for the original README layout, adapted by me)\n'], 'url_profile': 'https://github.com/xRuiAlves', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}"
"{'location': 'Hoboken, NJ', 'stats_list': [], 'contributions': '572 contributions\n        in the last year', 'description': ['Tic-tac-toe with AI - CLI\nAbout\nA CLI version of Tic-tac-toe with 0-, 1-, and 2-player modes:\n\nA 0-player game has two computer players playing against each other with no interaction from the user.\nA 1-player game has a human playing against a computer.\nA 2-player game has two human players.\n\nInitial Setup\nThis project is supported by Bundler and includes a Gemfile.\nRun bundle install before running the app.\nPlaying The Game\nStart the game by entering bin/tictactoe into the command line, then follow the instructions in the console\n'], 'url_profile': 'https://github.com/grangerl330', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'Porto, Portugal', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': [""aima-cpp\nC++ implementation of algorithms from Russell And Norvig's Artificial Intelligence - A Modern Approach 3rd Edition. You can use this in conjunction with a course on AI, or for study on your own.\n""], 'url_profile': 'https://github.com/ei06125', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'Sri Lanka', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['\nBL Studio\nBL Studio is an open-source collection of tools for generating music using Artificial Intelligence. Learn More...\n'], 'url_profile': 'https://github.com/Buddhilive', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'Porto, Portugal', 'stats_list': [], 'contributions': '1,250 contributions\n        in the last year', 'description': ['FEUP-AIAD\nProjects developed for the AIAD (Distributed Artificial Intelligence Agents) course unit.\nContext\n\nDate: 4th Year, 1st Semester, 2019/2020\nTopic:  Exercises\nCourse: Agentes de Inteligência Artificial Distribuidos (AIAD) | Distributed Artificial Intelligence Agents\nCourse Link: https://sigarra.up.pt/feup/pt/UCURR_GERAL.FICHA_UC_VIEW?pv_ocorrencia_id=436453\n\nDisclaimer\nThis repository, and every other FEUP-COURSE* repos on GitHub correspond to school projects from the respective COURSE. The code on this repo is intended for educational purposes. I do not take any responsibility, liability or whateverity over any code faults, inconsistency or anything else. If you intend on copying most or parts of the code for your school projects, keep in mind that this repo is public, and that your professor might search the web for similar project solutions or whatnot and choose to fail you for copying.\n(Credit to miguelpduarte for the original README layout, adapted by me)\n'], 'url_profile': 'https://github.com/xRuiAlves', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'Poland', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/karol57', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': '24 Rue Pasteur, 94270 Le Kremlin-Bicêtre', 'stats_list': [], 'contributions': '259 contributions\n        in the last year', 'description': ['AIA_gomoku_2019\n3rd year artificial intelligence project in Python in which the goal is to implement a Gomoku Narabe game bot/ai.\nIt is a 2-player game that is played on a 19x19 game board by default (20x20 on ""Piskvork""). Each player plays a stone at his/her turn, and the game ends as soon as one has a 5 stones in a row (vertically, horizontally or diagonaly) and thus wins.\nThe bot is compliant with the communication protocol and can be upload on ""Piskvork"" (windows Gomoku software plateform).\nThe program is built using the Min-max method and can be played against a human and ai player. Other than on Piskvork it can also be manually tested on Linux/Macos with commands sequences detailed below.\nUSAGE :\n./pbrain-minMax.py\n\nCOMMANDS :\nSTART [size >= 20] - Select board sizes.\n\nBEGIN - To start the game.\n\nTURN [X],[Y] - The parameters are coordinate of the opponent\'s move. All coordinates are numbered from zero.\n\nBOARD - This command imposes entirely new playing field. It is suitable for continuation of an opened match or for undo/redo user commands.\nAfter this command the data forming the playing field are send. Every line is in the form: [X],[Y],[field]\nwhere [X] and [Y] are coordinates and [field] is either number 1 (own stone) or number 2 (opponent\'s stone) or number 3      (only if continuous game is enabled, stone is part of winning line or is forbidden according to renju rules).\nThen Data are ended by DONE command.\n\nINFO [key] [value] - Informations about the current game (time remaining in the game, time remaining for each moves...) :\n\nThe key can be:\ntimeout_turn  - time limit for each move (milliseconds, 0=play as fast as possible)\ntimeout_match - time limit of a whole match (milliseconds, 0=no limit)\nmax_memory    - memory limit (bytes, 0=no limit)\ntime_left     - remaining time limit of a whole match (milliseconds)\ngame_type     - 0=opponent is human, 1=opponent is brain, 2=tournament, 3=network tournament\nrule          - bitmask or sum of 1=exactly five in a row win, 2=continuous game, 4=renju\nevaluate      - coordinates X,Y representing current position of the mouse cursor\nfolder        - folder for persistent files\n\nEND - To end the current game.\n\nABOUT - Informations about the current player such as author, country, www, email etc...\n\nMore detailed commands can be found here : https://svn.code.sf.net/p/piskvork/code/trunk/source/doc/protocl2en.htm\n\n\n'], 'url_profile': 'https://github.com/WoshiWoshu', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tonyliu6ix', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['8puzzle_AI_Assignment\nProgramming Assignment for module CE213 Artificial Intelligence. The task is to build a program to solve the 8-puzzle\n'], 'url_profile': 'https://github.com/MarcosLaydner', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': [""image_classification-project\nThis repository contains the files for my final project for Udacity's Nanogree program- Artificial Intelligence with Python.\n""], 'url_profile': 'https://github.com/Phylliac', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Lexden12', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}"
"{'location': 'PH', 'stats_list': [], 'contributions': '2,275 contributions\n        in the last year', 'description': ['Artificial Intelligence: N-Queens Puzzle\nA java program that solves the n-queens puzzle using Hill Climbing and Random Restart algorithm in Artificial Intelligence.\nProblem\nN-Queens is a famous computer science problem. The goal is to place “N” Number of queens on an “N x N” sized chess board such that no queen is under attack by another queen.\nBelow, you can see one possible solution to the N-queens problem for N = 4.\n\nNo two queens are on the same row, column, or diagonal.\nSolution\nResources\n'], 'url_profile': 'https://github.com/shaniadicen', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['hello-datascience\nArtificial Intelligence,Machine Learning,Deep Learning,Data Analysis,Data Mining,Big Data,R Programming,Python For Data Science.\nI am a Freelance Programmer now Learning Data Science.\n'], 'url_profile': 'https://github.com/seeniappan', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Magdeburg, Germany', 'stats_list': [], 'contributions': '251 contributions\n        in the last year', 'description': ['Shortest-Path-Finding-Algorithm\nThis is an implementation of A-Star Algorithm to find the shortest path of two provided nodes. This is widely used algorithm for Artificial Intelligence in Games and in general.\n'], 'url_profile': 'https://github.com/JalajVora', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Berkeley, California', 'stats_list': [], 'contributions': '645 contributions\n        in the last year', 'description': ['Tic-Tac-Toe\nA Python application that allows users to play games of Tic-Tac-Toe against a minimax-based artificial intelligence.\n# How to use:\n- In terminal, navigate to the project folder and run the application using the command line command:\n\tpython __init__.py \n- Then, indicate which player starts first 0 = user, 1 = AI\n\nInformation\nThis application is a work-in-progress and will be constantly updated at varying intervals of time. This project was created as a means for me to learn more about the minimax descision rule and its use cases for creating artificial intelligences that will always tie or win solved games.\nRoom for improvement\n\nAdd to the user interface, allowing users to click buttons to indicate which player starts first (player or ai).\nImplement a ""restart"" feature that allows users to play multiple games without having to restart the program.\n\nScreenshots\n\nNotes\nWhen running the game using a system running macOS, the final moves that cause an end state (win/lose/tie) will not be drawn. This is not the case with systems running Windows 10, and I am currently unsure of the issue.\n'], 'url_profile': 'https://github.com/mattau13', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '619 contributions\n        in the last year', 'description': ['\nteX-annotator for F-RCNN\nA PDF documents annotator, based on lateX files downloaded from arXiv.org, which outputs annotated documents\' pages used for a Faster RCNN trainining.\n1. Repo contents\nIn this repo I present a PDF annotator, used to obtains data input for F-RCNN training tasks.\nThe PDF annotator has the aim to detect:\n\nTitles\nFigures\nLists\nTables\n\nThe teX-annotator uses two type of files: a PDF file and its related lateX source code. It works thanks to the\ninformations retrived from two parsing tasks: the first is done with tex_parser.py, a python script here\npresented, that analyzes all the .tex files linked with its PDF file; then, the second parsing task is done\nusing PDFMIner, a very useful tool which collects information from each PDF\nline. Once the parsing is finished, the knowledge obtained from those two steps is merged in order to match lateX and PDF\nPDF ""objects"" (titles, figures, tables and lists) and save their xy coordinates from bbox PDFMiners elements\'\nproperty. So, each object of each page of each PDF file saved in the PDF_files directory is identified and located\nthanks to its coordinates: a list is created with all these obejct. A detected object is uniquely represented with\nthese values memorized into a list:\ndetected_object = [page, x_min, x_max, y_min, y_max, object_category]\n\npage: The object page\nx_min, y_max: the bottom left bounding box point\nx_max, y_max: the top right bounding box point\nobject_category: the object category\n\nThese are used for annotations: a summary images_annotations.csv file, from which is obtained a .txt file\nused to indicate to the Faster RCNN the training images. The test images are produced during the main program runs.\nPDF files and their source files (lateX) have been downloaded from arXiv.org; there were not\ndocument layout distinctions in downloading files.\n2. Project structure\nThe project is organized as follows:\nmain.py\n\nIt\'s the main, and does the following operations:\n\nIt generates as many .png files as many pdf pages for each downloaded paper.\nIt does parsing tasks and retrievs objects coordinates.\nIt divides train and test images, listed inside PNG_files dir, parsing TEX and PDF files, which are stored in\nPDF_files and TEX_files directories respectively. The 90% of pdf files will generates train images,\nthe rest 10% the test images ones.\nIt generates annotations_images.csv and annotated_train_images.txt files; this last one will be given in input\nto the frcnn.\n\nThe main.py can be launched from shell. There are two optioned commands:\npython3 main.py --help\nusage: main.py [-h] [--csv_file_path CSV_FILE_PATH]\n               [--annotations ANNOTATIONS]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --csv_file_path CSV_FILE_PATH\n                        Type the .csv file name to convert. By default is:\n                        images_annotations. The converter then will generate\n                        test_annotations_images.txt and\n                        train_annotations_images.txt .\n  --annotations ANNOTATIONS\n                        Choose if generate annotated images where: red=\n                        titles; green= figures; blu= lists; aqua green=\n                        tables; yellow= text; typing yes or no.\n\nThe most of the work is done by PDF_parser.py, which calls tex_parser.py and optionally the\nimages_annotator.py. images_annotator.py highlights different objects categories whit different colors:\n\nRED -------------> titles\nGREEN ---------> images\nBLUE ------------>  lists\nTURQUOISE ---> tables\nYELLOW --------> text NOT USED IN FRCNN.\n\nWhen the main finishes, all is set-up for start with the frcnn training.\nNB: the graphic pages annotations (specifying --annotations=yes) could be slow since also text is annotated.\n3. Download files from arXiv.org\nThe PDF and teX files are downloaded thanks to the arXiv_download_script.py. This script has to be launched before\nthe main one because it creates PDF_files and TEX_files populating them (unless you don\'t have >10K pdf and related\nteX).\narXiv_download_script.py can be launched from bash; there are some optional commands:\npython3 arXiv_download_script.py --h\nusage: arXiv_download_script.py [-h] [--year YEAR] [--month MONTH]\n                                [--counter COUNTER] [--max_items MAX_ITEMS]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --year YEAR           Choose the year from which you want to start\n                        downloading papers from arXIv. Default 20.\n  --month MONTH         Choose the month, once you have choseh the year, from\n                        which you want to start downloading papers from arXIv.\n                        Default 1(janaury)\n  --counter COUNTER     Choose the starting file counter. With 0 you will\n                        download all files from the year and the month\n                        specified.Default 0.\n  --max_items MAX_ITEMS\n                        Chose how many files you will download\n\n\nThese commands take into accounts how papers are saved on arXiv.org.\n4. F-RCNN\nThe annotated images serve as F-RCNN input data; the goal of this project is to test how much correctly the net can\ndetects titles, figures, lists and tables. For this task I\'ve chosen the F-RCNN net; further informations about it\nare available on R-CNN, Fast R-CNN, Faster R-CNN, YOLO — Object Detection Algorithms.\nI\'ve cloned the repo from this git repo keras-frcnn and the I\'ve followed\nthis guide for implementation.\nStart with training is very easy: open the shell, go to /DDM_Project/venv/frcnn/ and start!\ncd frcnn\npython3 train_frcnn.py -o simple -p annotated_train_images.txt\n\nThis command will start training which will generate an h5 model, that is the input for test:\npython3 test_frcnn.py -p ../png_files/test_images/\n\nThe test procedure will outputs the annotated test images basing on the training task results.\n5. Test evaluation\nTest evaluation is available. After test task, a .txt file will be generated with the result images: predicted_test_images.txt.\nThis file contains all the details about all the instances detected during the test (obviously concerning the test set).\nFurthermore, it\'s important to generate a similar file which constitutes the Ground Truth. Such a file has to be created\nafter the FRCNN test phase, running the test_images_annotator.py script.\npython3 test_images_annotator.py\n\nThis script will generate the annotated_test_images.txt and the parse_error_test_files.txt, which will contains the paper name\nthat have been badly elaborated (parsing errors); this file is very important because, before the test evaluation, it is crucial\nto remove all the lines which refers to the erroneous papers in the annotated_test_images.txt: such an operation insures\nan unique correspondence between the GT and the predictions (annotated_test_images.txt and predicted_test_images.txt).\nOnce you have done this operation, you can evaluate your FRCNN predictions simply running the evaluate.py script:\nNB: It is very important that you sort the predicted_test_images.txt and the annotated_test_images.txt\nrunning this command:\nsort -u -o <file_to_sort> <file_where_output_sort_operation>\n\nThen, run:\npython3 evaluate.py\n\nThis script will generate:\n\na log with all the GT papers and another with all the PREDICTION papers; these logs file\ncould be useful if correspondence problems occur.\na test_results.txt file which contains all the information about the test evaluation of all the single papers.\nThis file is so constructed:\n\nPRED and GT different pages lists\nPrecision\nRecall\nTrue Positives, False Positives, False Negatives\nF1 score\n\n\n\nAll these statistics are evaluated varying a threshold value used in the Intersection Over Union\n(IoU) algorithm used for predicted papers instances classification. The used thresholds are:\n[0.1, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80].\nAn F1-PRECISION-RECALL plot it is shown after the computation, giving an idea of the test accuracy.\n6. Requirements\nThe dipendencies required for this project are listed inside requirements.txt; you should simply open a shell and\nuse pip3 running:\npip3 install -r requirements.txt\n\nFor this project I used Python 3.7; I recommend to use PyCharm.\n7: Numbers, examples, results\nI would like to share with you some numbers:\n\n10.200 pdf files processed (papers and various scientific articles) downloaded from arXIv.\n\n9 180 for training set\n1 020 for test set\n\n\n204 658 relative papers source files downloaded from arXiv\n187 485 png files generated, one image for one paper page\n466452 instances (titles, images, lists and tables) found and analyzed with frcnn.\n\n8. References\nThis project has been inspired by PubLayNet, a project where PDF taken from the PubMed Central dataset\n(over 360 thousands of articles!) are annotated with theirs relative XML files. PubLayNet paper is\navailable here.\nHere an example of the PubLayNet paper annotated using tex-annotator!\n\n'], 'url_profile': 'https://github.com/pisalore', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Gurgaon, Haryana, India', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Welcome to GitHub Pages\nYou can use the editor on GitHub to maintain and preview the content for your website in Markdown files.\nWhenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.\nMarkdown\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\nFor more details see GitHub Flavored Markdown.\nJekyll Themes\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings. The name of this theme is saved in the Jekyll _config.yml configuration file.\nSupport or Contact\nHaving trouble with Pages? Check out our documentation or contact support and we’ll help you sort it out.\n'], 'url_profile': 'https://github.com/thumbarnirmal', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Artificial Intelligence Website\nThis is the website I created for the Independent Study assignment. This website was made with a parralax effect. Keep scrolling and enjoy! Download instructions are below.\nDownloading the Website\'s Contents\nThe first step is to download the website. Click the green ""Clone or download"" button, and download the ZIP.\nExtracting the Contents and Wiewing the Site\nThe next step is to extract the contents of the folder. After this is done, navigate to the folder with the uncompressed files (the non-ZIP folder) and open it up. Open the ""AI_and_Robotics_main.htm file. Enjoy!\n'], 'url_profile': 'https://github.com/sanjram', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Dhanbad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rackson3861', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Las Palmas de Gran Canaria', 'stats_list': [], 'contributions': '1,485 contributions\n        in the last year', 'description': ['Análisis de Sentimientos mediante Redes neuronales\n\nUniversidad Politécnica\nMáster Universitario en Inteligencia Artificial\nWeb Science\nEnero 2020\n\nAutores\n\nAntonio Sejas Mustafá\nElena Saa Noblejas\n\nINTRODUCCIÓN\nEl objetivo de esta práctica es realizar una aplicación implementando alguno de los métodos vistos en clase. Por tanto tuvimos que elegir entre Sistema de recomendación, Clasificación de documentos siguiendo Topic Models, Reconocimiento de Entidades o Análisis de Sentimiento.\nNosotros decidimos desarrollar este último proyecto. De este modo, nuestro objetivo es realizar una aplicación que dado un texto sea capaz de identificar si caracter positivo o negativo.\nMás concretamente hemos decidido trabajar sobre un dataset ya conocido. El dataset de reviews de películas de IMDB. En el siguiente apartado ampliamos la información sobre el dataset y comentamos dónde está disponible para su descarga.\nNuestros textos, como ya hemos comentado son reviews de películas, y el sentimiento será si una review le ha gustado a un usuario o no.\n¿Pero cómo vamos a evaluar nuestra aplicación?\nUn texto puede estar lleno de ambiguedad, incluso una misma review puede tener comentarios pariales de caracter positivo y otras críticas negativas. Nosotros vamos a ignorar estas situaciones y seguiremos el gold standard marcado por el artículo descrito en Maas, A. L. et. al. 2011 [1]\nEn este artículo se describe que las reviews positivas son aquellas que tengan una nota de 7 estrellas o más. Y de forma equivalente las negativas son las que tengan asociada una valoración de 0 a 4 estrellas. De esta forma no se tienen en cuenta los textos más ambiguos o indecisos, las reseñas Neutrales.\nEl Dataset de IMDB\nEste dataset ha sido realizado por los investigadores de Stanford autores del artículo original [1] .\nEl dataset original está disponible en: http://ai.stanford.edu/~amaas/data/sentiment/\nEl dataset cuenta con 50.000 reviews, que ellos utilizaron 25.000 para entrenamiento y 25.000 para testing. Nosotros para reducir complejidad solo usaremos una mitad, que hemos tratado previamente para eliminar caracteres raros y poner los textos en minúsculas.\nAdemás este dataset cuenta con un porcentaje equilibrado de reseñas. Siendo la mitad positivas y la otra mitad negativas.\nCada review tiene una etiqueta que lo categoriza de positiva o negativamente.\nTecnologías utilizadas\nA continuación describimos la metodología que hemos seguido para el analizador de sentimientos. Cada uno de estos puntos corresopnde con una sección del código.\nLimpieza del dataset\nEl primer paso es analizar y hacer un tratamiento de los textos del dataset. En el área de procesamiento del lenguaje natural hay una gran cantidad de alternativas y posibilidades. Es posible realizar distintas representación de los textos del corpus, y una gran variedad de extracción de características.\nEl planteamiento del analizador de sentimientos puede verse de alguna forma con un clasificador de textos, en el que se intenta clasificar un text (review) como positivo o negativo.\nPor este motivo todos los métodos de representación utiizados en PLN son válidos. Algunos de estos modelos son: bag of words, vector space model, tf-idf, topic model.\nNosotros hemos decidido obtener una bolsa de palabras (bag of words), teniendo en cuenta la frecuencia relativa con respecto a la otra clase. La idea es similar a un TF-IDF pero a nivel de clase. Esta representación nos permitirá darle más peso a las palabras más polarizadas.\nPor mantener determinar un límite en esta práctica no aplicaremos ningún procedimiento lematización ni stemming. Tampoco utilizaremos n-gramas. Únicamente eliminaremos las palabras vacías, stopwords, para reducir el ruido de los datos. La tokenización utilizada consiste en convertir las palabas en índices de un array. Todo este tipo de técnicas las hemos visto en clase y también se describen en más detalle en el libro""Natural Language Processing in Action"" [3]\nEntrenamiento\nDe forma similar a la limipieza del dataset y la extracción de variables predictoras, en el entrenamiento podemos utilizar prácticamente cualquier algoritmo de clasificación. Desde un Naive Bayes, Support Vector Machine, árboles de decisión o redes neuronales son algunas de las opciones más utilizadas.\nNosotros al no haber cursado ninguna asignatura de redes neuronales, hemos decidido utilizar una red neuronal, en concreto un Perceptrón multicapa.\nLa primera capa, capa de entrada, tendrá tantos nodos como palabras haya en nuestro vocabulario. La segunda capa tendrá 30 nodos, de forma experimental hemos observado un buen comportamiento con 10 a 30 nodos.\nPor último la capa de salida tendrá un solo nodo que dará un valor comprendido entre 0 y 1. Cuanto más cerca del 1 , más positiva se considerará la reseña. Un valor cercano al 0.5 se considerará la reseña ""neutral"".\nValidación\nPor último, nosotros hemos preferido evaluar la precisión de nuestro algoritmo utilizando un dropout 70/30 por sencillez de implementación. Una solución más profesional requeriría utilizar métodos de validación más sofistiados como un k-fold.\nAdemás hemos observado que incluso usando la mitad del dataset de entrenamiento, obtenemos valores muy cercanos al SVM descrito en el artículo [1]. En el artículo se alcanzan precisiones de entorno al 0.88, mientras que como veremos nuestra red se queda en 0.86 debido a falta de reducción de ruido comentada anteriormente.\nExtra\nDe forma adicional, hemos creado una celda con una caja de texto para que se pueda comprobar el fucionamiento con textos fuera del dataset. Esta caja de texto está identificada bajo el título ""Inserta un texto para probar el analizador de sentimientos"". Hay que escribir un texto y ejecutar esa celda y la siguiente para ver los resultados.\n\nEl código está autocontenido en este Jupyter Notebook. El cual está disponible online: https://colab.research.google.com/drive/11ZvUGrctfSbuTqa_tk3J-SNkEolqmZbK\nAdemás el código fuente y el dataset están disponibles en Github: https://github.com/sejas/muia-imdb-sentiment-analysis\n\nReferencias\n\n\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011, June). Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1 (pp. 142-150). Association for Computational Linguistics. Descargar Artículo\n\n\nHochreiter, Sepp & Schmidhuber, Jürgen. (1997). Long Short-term Memory. Neural computation. 9. 1735-80. 10.1162/neco.1997.9.8.1735.\n\n\nLane, H., Howard, C., & Hapke, H. M. (2019). Natural Language Processing in Action: Understanding, Analyzing, and Generating Text with Python. Manning Publications Company.\n\n\nCARGA DEL DATASET\n# Importar librerías\nimport sys\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n# Carga de los datos en un dataframe\ndf = pd.read_csv(\'imdb.csv\', index_col=0)\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nlabels\nreviews\n\n\n\n\n0\nPOSITIVE\nbromwell high is a cartoon comedy . it ran at ...\n\n\n1\nNEGATIVE\nstory of a man who has unnatural feelings for ...\n\n\n2\nPOSITIVE\nhomelessness  or houselessness as george carli...\n\n\n3\nNEGATIVE\nairport    starts as a brand new luxury    pla...\n\n\n4\nPOSITIVE\nbrilliant over  acting by lesley ann warren . ...\n\n\n\n\nlen(df)\n25000\n\nLos datos de imdb.csv han sido preprocesados y el contenido está preparado para contener solo caracteres en minúsculas. Esto es para simplificar el la identificación de las palabras, independientemente de cómo hayan sido escritas.\nANÁLISIS Y TRATAMIENTO PREVIO DEL DATASET\nUtilizando tres objetos Counter podemos calcular la frequencia absoluta para cada tipod e clase, positiva y negativa y un tercer counter para la contabilizar la frecuencia total de cada palabra en el corpus.\npositive_freq = Counter()\nnegative_freq = Counter()\ntotal_freq = Counter()\nAdemás de contabilizar la frecuencia de cada palabra en cada clase y en total, aprovechamos para eliminar las palabras vacías previamente conocidas y facilitadas por sklearn.\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\ncustom_stopwords = stop_words.ENGLISH_STOP_WORDS.union([\'br\', \'.\'])\ndef remove_stopwords(text):\n  return [word for word in text.split(\' \') if word not in custom_stopwords]\nfor _, (label, review) in df[df[\'labels\']==\'POSITIVE\'].iterrows():\n  positive_freq += Counter(remove_stopwords(review))\nfor _, (label, review) in df[df[\'labels\']==\'NEGATIVE\'].iterrows():\n  negative_freq += Counter(remove_stopwords(review))\n\ntotal_freq = positive_freq + negative_freq\n\nExtraemos las palabras de las reseñas positivas y negativas ordenándolas de más a menos comunes.\npositive_freq.most_common(20)\n[(\'\', 550468),\n (\'s\', 33815),\n (\'film\', 20937),\n (\'movie\', 19074),\n (\'t\', 13720),\n (\'like\', 9038),\n (\'good\', 7720),\n (\'just\', 7152),\n (\'story\', 6780),\n (\'time\', 6515),\n (\'great\', 6419),\n (\'really\', 5476),\n (\'people\', 4479),\n (\'best\', 4319),\n (\'love\', 4301),\n (\'life\', 4199),\n (\'way\', 4036),\n (\'films\', 3813),\n (\'think\', 3655),\n (\'movies\', 3586)]\n\nnegative_freq.most_common(20)\n[(\'\', 561462),\n (\'s\', 31546),\n (\'movie\', 24965),\n (\'t\', 20361),\n (\'film\', 19218),\n (\'like\', 11238),\n (\'just\', 10619),\n (\'good\', 7423),\n (\'bad\', 7401),\n (\'really\', 6262),\n (\'time\', 6209),\n (\'don\', 5336),\n (\'story\', 5208),\n (\'people\', 4806),\n (\'make\', 4722),\n (\'plot\', 4154),\n (\'movies\', 4080),\n (\'acting\', 4056),\n (\'way\', 3989),\n (\'think\', 3643)]\n\nAunque hayamos quitado las palabras vacías de un diccionario, hay un gran número de palabras vacías intrínsecas a nuestro dominio. En nuestro caso estas palabras que no aportan valor a la hora de distinguir entre una polarización positivia o negativa deberían ser consideradas como palabras vacías. Un ejemplo de estas palabras son muchas de las que aparecen en las listas de arriba. film, movie, acting y muchos nombres de actores y películas.\nA continuación calculamos el ratio de las palabras positivas entre las negativas, esto nos indicará si una palabra es muy positiva, neutra o nada positiva.\nLa forma de calcular este ratio de frecuencia es:\nnúmero de usos positivos / (número de usos negativos+1)\nSe le añade +1 al denominador para no dividir entre 0.\nMIN_FREQ = 200\npositive_negative_prop = Counter()\n\nfor word,freq in list(total_freq.most_common()):\n    if(freq > MIN_FREQ):\n        proportion = positive_freq[word] / float(negative_freq[word]+1)\n        positive_negative_prop[word] = proportion\nExaminamos el ratio de algunas palabras:\ndef check_words(words_list):\n  for word_to_check in words_list:\n    print(""Word \'%s\' = %s""%(word_to_check, positive_negative_prop[word_to_check]))\ncheck_words([\'film\', \'fantastic\', \'bad\'])\nWord \'film\' = 1.089390707112753\nWord \'fantastic\' = 4.503448275862069\nWord \'bad\' = 0.2576330721426641\n\nComo podemos ver, las palabras positivas tendrán valores muy altos. (>1)\nLas palabras neutrales que aparecen en reviews positivas o negativas, tendrán valores muy cercanos a 1. (Equilibradas)\nY las palabras negativas estarán muy próximas a 0.\nfor word,ratio in positive_negative_prop.most_common():\n    positive_negative_prop[word] = np.log(ratio)\nUna forma sencilla de normalizar estos valores y conseguir que las palabras neutrales estén en torno al 0 en vez de entorno al 1, es usando la función logaritmo.\nA continuación comprobamos las mismas palabras anterioremente comprobadas y observamos los nuevos valores normalizados.\ncheck_words([\'film\', \'fantastic\', \'bad\'])\nWord \'film\' = 0.08561855565085673\nWord \'fantastic\' = 1.5048433868558566\nWord \'bad\' = -1.3562189073456823\n\nArriba vemos que film, apenas aporta un valor discriminatorio.\nA continuación vemos la lista de palabras más polarizadas y sus nuevos valores.\npositive_negative_prop.most_common(20)\n[(\'victoria\', 2.681021528714291),\n (\'captures\', 2.038619547159581),\n (\'wonderfully\', 2.0218960560332353),\n (\'powell\', 1.978345424808467),\n (\'refreshing\', 1.8551812956655511),\n (\'delightful\', 1.8002701588959635),\n (\'beautifully\', 1.7626953362841438),\n (\'underrated\', 1.7197859696029656),\n (\'superb\', 1.7091514458966952),\n (\'welles\', 1.667706820558076),\n (\'sinatra\', 1.6389967146756448),\n (\'touching\', 1.637217476541176),\n (\'stewart\', 1.611998733295774),\n (\'brilliantly\', 1.5950491749820008),\n (\'friendship\', 1.5677652160335325),\n (\'wonderful\', 1.5645425925262093),\n (\'magnificent\', 1.54663701119507),\n (\'finest\', 1.546259010812569),\n (\'jackie\', 1.5439233053234738),\n (\'freedom\', 1.5091151908062312)]\n\nlist(reversed(positive_negative_prop.most_common()))[0:20]\n[(\'unfunny\', -2.6922395950755678),\n (\'waste\', -2.6193845640165536),\n (\'pointless\', -2.4553061800117097),\n (\'redeeming\', -2.3682390632154826),\n (\'lousy\', -2.307572634505085),\n (\'worst\', -2.286987896180378),\n (\'laughable\', -2.264363880173848),\n (\'awful\', -2.227194247027435),\n (\'poorly\', -2.2207550747464135),\n (\'sucks\', -1.987068221548821),\n (\'lame\', -1.981767458946166),\n (\'insult\', -1.978345424808467),\n (\'horrible\', -1.9102590939512902),\n (\'amateurish\', -1.9095425048844386),\n (\'pathetic\', -1.9003933102308506),\n (\'wasted\', -1.8382794848629478),\n (\'crap\', -1.8281271133989299),\n (\'tedious\', -1.802454758344803),\n (\'dreadful\', -1.7725281073001673),\n (\'badly\', -1.753626599532611)]\n\nLa aparición de ""Victoria"" parece indicar que sus películas tienen asociadas muy buenas críticas. Pero sabemos que en inglés hace referencia a un nombre propio, por lo que  para mejorar nuestra predicción habría considerar los nombres propios como stopwords.\npd.DataFrame(positive_negative_prop.most_common()).plot.hist(bins=50)\n<matplotlib.axes._subplots.AxesSubplot at 0x7ffb79d86fd0>\n\n\nEste histograma nos enseña la polaridad de las palabras en todo el corpus. Podemos observar que sigue una distribución normal con media en torno al 0. Es decir, la mayoría de las palabras están categorizadas como neutrales. Esto es ruido en nuestro clasificador. Esto se podría solucionar teniendo en cuenta aquellas palabas que aporten un valor discriminatorio mayor de |0.5|\nGENERANDO NUESTRO VOCABULARIO\nA continuación para ""tokenizar"" nuestros textos y convertirlos en vector de palabras, vamos a crear un vocabulario que será la entrada de nuestra red neuronal.\nvocab = set(total_freq.keys())\nvocab_size = len(vocab)\nprint(vocab_size)\n73759\n\nword2index = {}\nfor i,word in enumerate(vocab):  \n    word2index[word] = i\nword2index\n{\'\': 0,\n \'pork\': 1,\n \'cancer\': 2,\n \'hypermacho\': 3,\n \'beam\': 4,\n \'didja\': 5,\n \'sires\': 6,\n \'colonised\': 7,\n \'jest\': 8,\n \'fem\': 9,\n \'mitochondrial\': 10,\n \'azuma\': 11,\n \'stunk\': 12,\n \'attracting\': 13,\n \'cathernine\': 14,\n \'ventricle\': 15,\n \'ding\': 16,\n \'religous\': 17,\n \'training\': 18,\n \'cranks\': 19,\n \'hobbs\': 20,\n \'novac\': 21,\n \'millennia\': 22,\n \'zinn\': 23,\n \'sacrilage\': 24,\n \'mistry\': 25,\n \'sensualists\': 26,\n \'giff\': 27,\n \'bungling\': 28,\n \'raechel\': 29,\n \'swedes\': 30,\n \'miffed\': 31,\n \'ultimate\': 32,\n \'dought\': 33,\n \'plagiaristic\': 34,\n \'limned\': 35,\n \'jee\': 36,\n \'aracnophobia\': 37,\n \'centerpiece\': 38,\n \'unfaithal\': 39,\n \'knievel\': 40,\n \'ecstacy\': 41,\n \'trudged\': 42,\n \'alun\': 43,\n \'habituation\': 44,\n \'cannibalism\': 45,\n \'alarmist\': 46,\n \'looney\': 47,\n \'sudser\': 48,\n \'min\': 49,\n \'michelle\': 50,\n \'winninger\': 51,\n \'deployment\': 52,\n \'menzel\': 53,\n \'demonstrative\': 54,\n \'overpowered\': 55,\n \'seema\': 56,\n \'psychotics\': 57,\n \'coughthe\': 58,\n \'rollin\': 59,\n \'interferring\': 60,\n \'shimbei\': 61,\n \'orientated\': 62,\n \'traumatized\': 63,\n \'meriwether\': 64,\n \'kind\': 65,\n \'gruff\': 66,\n \'palsey\': 67,\n \'substories\': 68,\n \'acquittal\': 69,\n \'movecheck\': 70,\n \'compromised\': 71,\n \'zarustica\': 72,\n \'maadri\': 73,\n \'kaiser\': 74,\n \'budgetary\': 75,\n \'mt\': 76,\n \'factors\': 77,\n \'goulding\': 78,\n \'transposing\': 79,\n \'chineese\': 80,\n \'herbal\': 81,\n \'orkly\': 82,\n \'murderer\': 83,\n \'stephan\': 84,\n \'tage\': 85,\n \'forefathers\': 86,\n \'plays\': 87,\n \'dysfunction\': 88,\n \'gramophone\': 89,\n \'pendleton\': 90,\n \'juxtapositions\': 91,\n \'upto\': 92,\n \'excitement\': 93,\n \'ruphert\': 94,\n \'ultimo\': 95,\n \'mallorquins\': 96,\n \'lunacy\': 97,\n \'pratfalls\': 98,\n \'skyraiders\': 99,\n \'varela\': 100,\n \'rexes\': 101,\n \'mattresses\': 102,\n \'shvollenpecker\': 103,\n \'oversexed\': 104,\n \'taiwanese\': 105,\n \'toyota\': 106,\n \'neds\': 107,\n \'sugarman\': 108,\n \'facebuster\': 109,\n \'doel\': 110,\n \'veal\': 111,\n \'druidic\': 112,\n \'wary\': 113,\n \'extravaganzas\': 114,\n \'spiteful\': 115,\n \'sublime\': 116,\n \'nyfd\': 117,\n \'enthuses\': 118,\n \'wheaton\': 119,\n \'pharmaceutical\': 120,\n \'fulfill\': 121,\n \'innocence\': 122,\n \'undertake\': 123,\n \'infantile\': 124,\n \'crapfest\': 125,\n \'nec\': 126,\n \'shroyer\': 127,\n \'flour\': 128,\n \'valseuses\': 129,\n \'text\': 130,\n \'breasted\': 131,\n \'tachigui\': 132,\n \'additives\': 133,\n \'vanlint\': 134,\n \'mcphillip\': 135,\n \'impersonated\': 136,\n \'fictionalization\': 137,\n \'hitler\': 138,\n \'burry\': 139,\n \'curses\': 140,\n \'worn\': 141,\n \'thirbly\': 142,\n \'spitted\': 143,\n \'calhoun\': 144,\n \'hoyden\': 145,\n \'peculiarities\': 146,\n \'crops\': 147,\n \'blinding\': 148,\n \'gossemar\': 149,\n \'genghis\': 150,\n \'dusting\': 151,\n \'mausoleum\': 152,\n \'braincell\': 153,\n \'carrer\': 154,\n \'thumper\': 155,\n \'wale\': 156,\n \'beresford\': 157,\n \'coleman\': 158,\n \'deix\': 159,\n \'porkys\': 160,\n \'weasel\': 161,\n \'norton\': 162,\n \'garmes\': 163,\n \'croquet\': 164,\n \'aristocats\': 165,\n \'cigliutti\': 166,\n \'amore\': 167,\n \'casket\': 168,\n \'pending\': 169,\n \'mutated\': 170,\n \'probate\': 171,\n \'favourable\': 172,\n \'grandeurs\': 173,\n \'cavelleri\': 174,\n \'exasperated\': 175,\n \'kak\': 176,\n \'conflictive\': 177,\n \'paradoxically\': 178,\n \'aamir\': 179,\n \'aauugghh\': 180,\n \'onhand\': 181,\n \'deshimaru\': 182,\n \'strolls\': 183,\n \'grete\': 184,\n \'sickroom\': 185,\n \'clouded\': 186,\n \'baguettes\': 187,\n \'unabsorbing\': 188,\n \'sarajevo\': 189,\n \'sulk\': 190,\n \'chart\': 191,\n \'explore\': 192,\n \'permitted\': 193,\n \'malkovichian\': 194,\n \'whys\': 195,\n \'schlitz\': 196,\n \'disingenuous\': 197,\n \'hustle\': 198,\n \'immortel\': 199,\n \'insightfully\': 200,\n \'workforces\': 201,\n \'lyndon\': 202,\n \'aden\': 203,\n \'dunham\': 204,\n \'disbelieving\': 205,\n \'dunbar\': 206,\n \'segal\': 207,\n \'laroche\': 208,\n \'shakespearian\': 209,\n \'peasant\': 210,\n \'retention\': 211,\n \'concerted\': 212,\n \'serve\': 213,\n \'getz\': 214,\n \'discos\': 215,\n \'fused\': 216,\n \'looong\': 217,\n \'deceiving\': 218,\n \'ancients\': 219,\n \'brigadier\': 220,\n \'sistahs\': 221,\n \'violin\': 222,\n \'unengineered\': 223,\n \'deranged\': 224,\n \'lachlin\': 225,\n \'veoh\': 226,\n \'clung\': 227,\n \'ran\': 228,\n \'swabby\': 229,\n \'rataud\': 230,\n \'endearment\': 231,\n \'comity\': 232,\n \'bookend\': 233,\n \'waaaaaayyyy\': 234,\n \'siren\': 235,\n \'misleads\': 236,\n \'alrite\': 237,\n \'examination\': 238,\n \'panned\': 239,\n \'themsleves\': 240,\n \'wandered\': 241,\n \'simper\': 242,\n \'pliers\': 243,\n \'rump\': 244,\n \'cripplingly\': 245,\n \'scrawl\': 246,\n \'lewinski\': 247,\n \'gearheads\': 248,\n \'ktla\': 249,\n \'ambience\': 250,\n \'dozens\': 251,\n \'presumes\': 252,\n \'awards\': 253,\n \'surpressors\': 254,\n \'edits\': 255,\n \'difficulties\': 256,\n \'remar\': 257,\n \'wheelchairs\': 258,\n \'fiascos\': 259,\n \'claimed\': 260,\n \'waldeman\': 261,\n \'dangles\': 262,\n \'aloud\': 263,\n \'luncheon\': 264,\n \'cliffhangers\': 265,\n \'reminding\': 266,\n \'protected\': 267,\n \'serafinowicz\': 268,\n \'sorrell\': 269,\n \'bused\': 270,\n \'vulnerability\': 271,\n \'kaleidoscope\': 272,\n \'lizard\': 273,\n \'plateful\': 274,\n \'subbed\': 275,\n \'mpkdh\': 276,\n \'majkowski\': 277,\n \'eroticism\': 278,\n \'latecomers\': 279,\n \'outreach\': 280,\n \'visualizes\': 281,\n \'ramotswe\': 282,\n \'scientific\': 283,\n \'mcgaw\': 284,\n \'zb\': 285,\n \'mole\': 286,\n \'macho\': 287,\n \'uninstructive\': 288,\n \'resourceful\': 289,\n \'pumba\': 290,\n \'soleil\': 291,\n \'whopper\': 292,\n \'adhering\': 293,\n \'slobber\': 294,\n \'ai\': 295,\n \'lifelike\': 296,\n \'finisher\': 297,\n \'eponymous\': 298,\n \'shoudln\': 299,\n \'oyl\': 300,\n \'carrefour\': 301,\n \'argonne\': 302,\n \'golovanov\': 303,\n \'gunmen\': 304,\n \'palestinians\': 305,\n \'precocious\': 306,\n \'teapot\': 307,\n \'somtimes\': 308,\n \'aiden\': 309,\n \'curmudgeon\': 310,\n \'opting\': 311,\n \'imagery\': 312,\n \'stitches\': 313,\n \'irresistibly\': 314,\n \'ezra\': 315,\n \'hypesters\': 316,\n \'spritely\': 317,\n \'honeymooners\': 318,\n \'mined\': 319,\n \'muggings\': 320,\n \'fallow\': 321,\n \'grimm\': 322,\n \'fiddler\': 323,\n \'daneille\': 324,\n \'carelessness\': 325,\n \'braveheart\': 326,\n \'cahoots\': 327,\n \'reflexivity\': 328,\n \'agekudos\': 329,\n \'abdu\': 330,\n \'tick\': 331,\n \'kindling\': 332,\n \'flowed\': 333,\n \'terrifically\': 334,\n \'montegna\': 335,\n \'rest\': 336,\n \'unperceptive\': 337,\n \'fannin\': 338,\n \'hindersome\': 339,\n \'monique\': 340,\n \'einstein\': 341,\n \'lea\': 342,\n \'portrayed\': 343,\n \'garrett\': 344,\n \'arcaica\': 345,\n \'parlor\': 346,\n \'blight\': 347,\n \'abusing\': 348,\n \'gainful\': 349,\n \'infects\': 350,\n \'twiggy\': 351,\n \'storszek\': 352,\n \'tediousness\': 353,\n \'tigerland\': 354,\n \'spirited\': 355,\n \'skipping\': 356,\n \'gills\': 357,\n \'barrels\': 358,\n \'soni\': 359,\n \'guanajuato\': 360,\n \'burkhalter\': 361,\n \'ingela\': 362,\n \'emulations\': 363,\n \'estefan\': 364,\n \'adlai\': 365,\n \'trainor\': 366,\n \'attraction\': 367,\n \'adma\': 368,\n \'flippantly\': 369,\n \'irritated\': 370,\n \'pendant\': 371,\n \'annoyed\': 372,\n \'storaro\': 373,\n \'az\': 374,\n \'punters\': 375,\n \'radical\': 376,\n \'unresponsive\': 377,\n \'printer\': 378,\n \'hmmmmmmmm\': 379,\n \'portrayer\': 380,\n \'gained\': 381,\n \'lars\': 382,\n \'willed\': 383,\n \'appreciation\': 384,\n \'herilhy\': 385,\n \'campy\': 386,\n \'fahrenheit\': 387,\n \'rodrix\': 388,\n \'nordham\': 389,\n \'underfoot\': 390,\n \'woolgathering\': 391,\n \'bs\': 392,\n \'aldonova\': 393,\n \'elequence\': 394,\n \'suspending\': 395,\n \'incubates\': 396,\n \'sans\': 397,\n \'misfire\': 398,\n \'reassuring\': 399,\n \'jerri\': 400,\n \'rework\': 401,\n \'utilities\': 402,\n \'handlers\': 403,\n \'margineanus\': 404,\n \'cos\': 405,\n \'masters\': 406,\n \'widened\': 407,\n \'excuse\': 408,\n \'pinkish\': 409,\n \'split\': 410,\n \'kewl\': 411,\n \'attract\': 412,\n \'wavy\': 413,\n \'alda\': 414,\n \'recognizable\': 415,\n \'whip\': 416,\n \'securing\': 417,\n \'insular\': 418,\n \'idiosyncratic\': 419,\n \'hayseed\': 420,\n \'tukur\': 421,\n \'advisedly\': 422,\n \'proposal\': 423,\n \'espeically\': 424,\n \'astrotech\': 425,\n \'shoufukutei\': 426,\n \'muncie\': 427,\n \'notoriety\': 428,\n \'escapism\': 429,\n \'outburst\': 430,\n \'hipper\': 431,\n \'condon\': 432,\n \'prix\': 433,\n \'glop\': 434,\n \'lespart\': 435,\n \'occupational\': 436,\n \'slacken\': 437,\n \'kerkhof\': 438,\n \'gymnasts\': 439,\n \'rigorous\': 440,\n \'jame\': 441,\n \'definetly\': 442,\n \'someway\': 443,\n \'caresses\': 444,\n \'deepak\': 445,\n \'sutdying\': 446,\n \'da\': 447,\n \'groundwork\': 448,\n \'ford\': 449,\n \'pentimento\': 450,\n \'hanns\': 451,\n \'drab\': 452,\n \'der\': 453,\n \'underwear\': 454,\n \'casper\': 455,\n \'puppetry\': 456,\n \'pakis\': 457,\n \'pearlman\': 458,\n \'bets\': 459,\n \'deservingly\': 460,\n \'hesitates\': 461,\n \'liberty\': 462,\n \'inconvenience\': 463,\n \'grosbard\': 464,\n \'steam\': 465,\n \'mounts\': 466,\n \'warnercolor\': 467,\n \'matt\': 468,\n \'beatific\': 469,\n \'colwell\': 470,\n \'slumping\': 471,\n \'doings\': 472,\n \'miswrote\': 473,\n \'jodoworsky\': 474,\n \'floods\': 475,\n \'enticement\': 476,\n \'rigueur\': 477,\n \'starsky\': 478,\n \'nick\': 479,\n \'monumentous\': 480,\n \'naffness\': 481,\n \'scratched\': 482,\n \'mays\': 483,\n \'starblazers\': 484,\n \'doves\': 485,\n \'wellpaced\': 486,\n \'growls\': 487,\n \'mist\': 488,\n \'ropes\': 489,\n \'baltimoreans\': 490,\n \'touch\': 491,\n \'aja\': 492,\n \'valga\': 493,\n \'recur\': 494,\n \'contreras\': 495,\n \'unbearded\': 496,\n \'cassetti\': 497,\n \'cascading\': 498,\n \'megapack\': 499,\n \'bandido\': 500,\n \'sprays\': 501,\n \'smuttiness\': 502,\n \'ladder\': 503,\n \'dosage\': 504,\n \'milwall\': 505,\n \'competent\': 506,\n \'hilltop\': 507,\n \'discomfort\': 508,\n \'stutter\': 509,\n \'draughtswoman\': 510,\n \'stockpile\': 511,\n \'littlekuriboh\': 512,\n \'bootie\': 513,\n \'disappoints\': 514,\n \'koz\': 515,\n \'proceeded\': 516,\n \'solimeno\': 517,\n \'avian\': 518,\n \'wicked\': 519,\n \'scales\': 520,\n \'howls\': 521,\n \'pleasaunces\': 522,\n \'shead\': 523,\n \'wickerman\': 524,\n \'xylophonist\': 525,\n \'companys\': 526,\n \'lorado\': 527,\n \'undertook\': 528,\n \'utopia\': 529,\n \'chihiro\': 530,\n \'courtesan\': 531,\n \'democratically\': 532,\n \'broad\': 533,\n \'conniving\': 534,\n \'photographic\': 535,\n \'davidbathsheba\': 536,\n \'glum\': 537,\n \'militaries\': 538,\n \'unfairly\': 539,\n \'ohio\': 540,\n \'talosian\': 541,\n \'grafted\': 542,\n \'cof\': 543,\n \'evers\': 544,\n \'bogglingly\': 545,\n \'overheating\': 546,\n \'mammothly\': 547,\n \'unfurnished\': 548,\n \'loves\': 549,\n \'battle\': 550,\n \'qi\': 551,\n \'tragedy\': 552,\n \'blonde\': 553,\n \'dystopic\': 554,\n \'cineasts\': 555,\n \'antonius\': 556,\n \'tarka\': 557,\n \'bloodthirst\': 558,\n \'milieu\': 559,\n \'vivant\': 560,\n \'censured\': 561,\n \'stinkpile\': 562,\n \'differential\': 563,\n \'affirmation\': 564,\n \'lydia\': 565,\n \'superlivemation\': 566,\n \'financially\': 567,\n \'pac\': 568,\n \'funiest\': 569,\n \'revolving\': 570,\n \'applauds\': 571,\n \'sperr\': 572,\n \'sybil\': 573,\n \'pedestrians\': 574,\n \'promise\': 575,\n \'elam\': 576,\n \'gazongas\': 577,\n \'categorised\': 578,\n \'tura\': 579,\n \'jeb\': 580,\n \'opportune\': 581,\n \'furgusson\': 582,\n \'irl\': 583,\n \'refuge\': 584,\n \'enacting\': 585,\n \'disenchantment\': 586,\n \'tis\': 587,\n \'breads\': 588,\n \'transposed\': 589,\n \'sivan\': 590,\n \'johan\': 591,\n \'siu\': 592,\n \'beswick\': 593,\n \'vlkava\': 594,\n \'auburn\': 595,\n \'gurl\': 596,\n \'figuring\': 597,\n \'numbingly\': 598,\n \'soft\': 599,\n \'centred\': 600,\n \'harrowed\': 601,\n \'hearkens\': 602,\n \'joeseph\': 603,\n \'moovies\': 604,\n \'witchie\': 605,\n \'cigs\': 606,\n \'stage\': 607,\n \'mitevska\': 608,\n \'roulette\': 609,\n \'rolly\': 610,\n \'ramchand\': 611,\n \'mulit\': 612,\n \'ameteurish\': 613,\n \'supplicant\': 614,\n \'compositor\': 615,\n \'pointer\': 616,\n \'dooooosie\': 617,\n \'rembrandt\': 618,\n \'skolimowski\': 619,\n \'vangelis\': 620,\n \'dzundza\': 621,\n \'cherri\': 622,\n \'harvested\': 623,\n \'filmmakers\': 624,\n \'essendon\': 625,\n \'nicolie\': 626,\n \'reassigned\': 627,\n \'calvins\': 628,\n \'refinery\': 629,\n \'amrish\': 630,\n \'lesson\': 631,\n \'nris\': 632,\n \'clerical\': 633,\n \'oooo\': 634,\n \'medication\': 635,\n \'phenomenons\': 636,\n \'santoni\': 637,\n \'moronfest\': 638,\n \'soviet\': 639,\n \'harden\': 640,\n \'relationsip\': 641,\n \'roofer\': 642,\n \'afar\': 643,\n \'neptune\': 644,\n \'unforgetable\': 645,\n \'sorcha\': 646,\n \'ditz\': 647,\n \'mehemet\': 648,\n \'advice\': 649,\n \'romantisised\': 650,\n \'ulcerating\': 651,\n \'millimeter\': 652,\n \'snorer\': 653,\n \'glady\': 654,\n \'daylights\': 655,\n \'anorexia\': 656,\n \'gettysburg\': 657,\n \'foe\': 658,\n \'suck\': 659,\n \'ising\': 660,\n \'johar\': 661,\n \'cradled\': 662,\n \'womennone\': 663,\n \'clampets\': 664,\n \'ishwar\': 665,\n \'dandies\': 666,\n \'jughead\': 667,\n \'themself\': 668,\n \'chundering\': 669,\n \'shipment\': 670,\n \'owed\': 671,\n \'wrestlemanias\': 672,\n \'commercisliation\': 673,\n \'vooren\': 674,\n \'shipped\': 675,\n \'brogues\': 676,\n \'nectar\': 677,\n \'kitties\': 678,\n \'buyer\': 679,\n \'tapers\': 680,\n \'leidner\': 681,\n \'perverted\': 682,\n \'vaticani\': 683,\n \'insouciance\': 684,\n \'iannaccone\': 685,\n \'succulently\': 686,\n \'apprehending\': 687,\n \'mitchel\': 688,\n \'workday\': 689,\n \'titty\': 690,\n \'oppenheimer\': 691,\n \'eser\': 692,\n \'tassel\': 693,\n \'sumptuousness\': 694,\n \'intonations\': 695,\n \'cherubic\': 696,\n \'franklin\': 697,\n \'propane\': 698,\n \'senegalese\': 699,\n \'compiled\': 700,\n \'arret\': 701,\n \'intrusively\': 702,\n \'wrinkle\': 703,\n \'urmila\': 704,\n \'buds\': 705,\n \'librarians\': 706,\n \'cubbyholes\': 707,\n \'portends\': 708,\n \'interconnecting\': 709,\n \'posterity\': 710,\n \'norseman\': 711,\n \'episodic\': 712,\n \'bleating\': 713,\n \'frumpy\': 714,\n \'ofcourse\': 715,\n \'rouged\': 716,\n \'voerhoven\': 717,\n \'stun\': 718,\n \'beret\': 719,\n \'scrutinized\': 720,\n \'sequenes\': 721,\n \'inhumanity\': 722,\n \'merkle\': 723,\n \'vomitum\': 724,\n \'gobbler\': 725,\n \'plastique\': 726,\n \'frownbuster\': 727,\n \'turaqui\': 728,\n \'sanju\': 729,\n \'x\': 730,\n \'chakraborty\': 731,\n \'curator\': 732,\n \'strategies\': 733,\n \'orientals\': 734,\n \'poorly\': 735,\n \'glass\': 736,\n \'fellowship\': 737,\n \'spaz\': 738,\n \'decomp\': 739,\n \'warbler\': 740,\n \'aonghas\': 741,\n \'withouts\': 742,\n \'bergqvist\': 743,\n \'dutt\': 744,\n \'maclaine\': 745,\n \'prowls\': 746,\n \'millie\': 747,\n \'turbulent\': 748,\n \'clunks\': 749,\n \'shards\': 750,\n \'conaughey\': 751,\n \'pounced\': 752,\n \'lineal\': 753,\n \'justicia\': 754,\n \'ksm\': 755,\n \'parnell\': 756,\n \'alcoholic\': 757,\n \'seafood\': 758,\n \'marienbad\': 759,\n \'mander\': 760,\n \'rowdy\': 761,\n \'designates\': 762,\n \'cheerless\': 763,\n \'hallgren\': 764,\n \'bastidge\': 765,\n \'aubrey\': 766,\n \'panoramas\': 767,\n \'ke\': 768,\n \'blige\': 769,\n \'nicks\': 770,\n \'taunts\': 771,\n \'thingie\': 772,\n \'zerifferelli\': 773,\n \'fisticuff\': 774,\n \'dakota\': 775,\n \'stettner\': 776,\n \'relaxers\': 777,\n \'cared\': 778,\n \'entrenchments\': 779,\n \'jaipur\': 780,\n \'rosco\': 781,\n \'murkily\': 782,\n \'karogi\': 783,\n \'sharpe\': 784,\n \'msb\': 785,\n \'kelemen\': 786,\n \'anal\': 787,\n \'entities\': 788,\n \'hagerthy\': 789,\n \'hyderabadi\': 790,\n \'indulgent\': 791,\n \'chicatillo\': 792,\n \'capabilities\': 793,\n \'leguizamo\': 794,\n \'couleur\': 795,\n \'apostrophe\': 796,\n \'uncynical\': 797,\n \'sadomasochism\': 798,\n \'retreated\': 799,\n \'kimmell\': 800,\n \'artistry\': 801,\n \'helen\': 802,\n \'stagnation\': 803,\n \'globalizing\': 804,\n \'puh\': 805,\n \'prosaically\': 806,\n \'redhead\': 807,\n \'footsteps\': 808,\n \'longtime\': 809,\n \'axiomatic\': 810,\n \'fans\': 811,\n \'xtianity\': 812,\n \'alucard\': 813,\n \'predominant\': 814,\n \'lynchings\': 815,\n \'fielding\': 816,\n \'contessa\': 817,\n \'fried\': 818,\n \'abortive\': 819,\n \'underscored\': 820,\n \'adroitly\': 821,\n \'awkwardness\': 822,\n \'sinese\': 823,\n \'travelcard\': 824,\n \'maryam\': 825,\n \'intact\': 826,\n \'ads\': 827,\n \'northam\': 828,\n \'nafta\': 829,\n \'matlock\': 830,\n \'madchen\': 831,\n \'swung\': 832,\n \'numero\': 833,\n \'genetics\': 834,\n \'ashley\': 835,\n \'scot\': 836,\n \'zeffirelli\': 837,\n \'slayers\': 838,\n \'duquenne\': 839,\n \'quibble\': 840,\n \'fumes\': 841,\n \'zues\': 842,\n \'pap\': 843,\n \'lasciviousness\': 844,\n \'cukor\': 845,\n \'lemuria\': 846,\n \'pejorative\': 847,\n \'toto\': 848,\n \'midway\': 849,\n \'vadis\': 850,\n \'sliced\': 851,\n \'businesspeople\': 852,\n \'homey\': 853,\n \'artisticly\': 854,\n \'refracted\': 855,\n \'dysfunctions\': 856,\n \'atlanteans\': 857,\n \'baby\': 858,\n \'bassis\': 859,\n \'reconstructions\': 860,\n \'johannesburg\': 861,\n \'jaret\': 862,\n \'hungarian\': 863,\n \'useless\': 864,\n \'indestructible\': 865,\n \'jacuzzi\': 866,\n \'kayyyy\': 867,\n \'mi\': 868,\n \'milinkovic\': 869,\n \'dioz\': 870,\n \'discombobulation\': 871,\n \'abm\': 872,\n \'vise\': 873,\n \'lovesick\': 874,\n \'faraway\': 875,\n \'unheated\': 876,\n \'bogie\': 877,\n \'messmer\': 878,\n \'foreseeing\': 879,\n \'labouf\': 880,\n \'phoenicia\': 881,\n \'overhears\': 882,\n \'remunda\': 883,\n \'unraveling\': 884,\n \'daisies\': 885,\n \'aristide\': 886,\n \'bedingfield\': 887,\n \'cheesecake\': 888,\n \'terrace\': 889,\n \'flagship\': 890,\n \'pickup\': 891,\n \'escalating\': 892,\n \'uttara\': 893,\n \'gunshots\': 894,\n \'meres\': 895,\n \'savales\': 896,\n \'horrorfilm\': 897,\n \'sheeple\': 898,\n \'twine\': 899,\n \'aboriginies\': 900,\n \'hoydenish\': 901,\n \'reshipping\': 902,\n \'wouln\': 903,\n \'speckle\': 904,\n \'befuddled\': 905,\n \'liebe\': 906,\n \'mopey\': 907,\n \'steffen\': 908,\n \'noises\': 909,\n \'install\': 910,\n \'barren\': 911,\n \'workaholics\': 912,\n \'alcoholism\': 913,\n \'bashing\': 914,\n \'ilu\': 915,\n \'disrupts\': 916,\n \'republics\': 917,\n \'briliant\': 918,\n \'rheubottom\': 919,\n \'eludes\': 920,\n \'endearing\': 921,\n \'alexanderplatz\': 922,\n \'judgment\': 923,\n \'colorfully\': 924,\n \'darlene\': 925,\n \'buckaroo\': 926,\n \'machettes\': 927,\n \'moncia\': 928,\n \'gaita\': 929,\n \'doctresses\': 930,\n \'thunderbolt\': 931,\n \'flak\': 932,\n \'vanquishes\': 933,\n \'supermen\': 934,\n \'shuddup\': 935,\n \'pinkie\': 936,\n \'sensations\': 937,\n \'elvira\': 938,\n \'guessed\': 939,\n \'consults\': 940,\n \'amatuerish\': 941,\n \'corsair\': 942,\n \'munitions\': 943,\n \'git\': 944,\n \'hectic\': 945,\n \'septic\': 946,\n \'flapper\': 947,\n \'atherton\': 948,\n \'anons\': 949,\n \'motos\': 950,\n \'bambou\': 951,\n \'childish\': 952,\n \'reviczky\': 953,\n \'graystone\': 954,\n \'mandate\': 955,\n \'heightens\': 956,\n \'petron\': 957,\n \'rods\': 958,\n \'excell\': 959,\n \'collection\': 960,\n \'macrae\': 961,\n \'wiping\': 962,\n \'delli\': 963,\n \'poltergeist\': 964,\n \'minutia\': 965,\n \'malikka\': 966,\n \'upbringings\': 967,\n \'noisier\': 968,\n \'ifyou\': 969,\n \'manchu\': 970,\n \'prophets\': 971,\n \'mice\': 972,\n \'leit\': 973,\n \'morrocco\': 974,\n \'korman\': 975,\n \'reviving\': 976,\n \'slowed\': 977,\n \'epstein\': 978,\n \'testified\': 979,\n \'bassett\': 980,\n \'bendan\': 981,\n \'punkris\': 982,\n \'scolded\': 983,\n \'okinawan\': 984,\n \'poisoning\': 985,\n \'blueprints\': 986,\n \'impalement\': 987,\n \'bethune\': 988,\n \'ted\': 989,\n \'dissertations\': 990,\n \'oops\': 991,\n \'deesh\': 992,\n \'chaya\': 993,\n \'gunnerside\': 994,\n \'mano\': 995,\n \'advision\': 996,\n \'negro\': 997,\n \'postino\': 998,\n \'dumbed\': 999,\n ...}\n\nCLASIFICADOR MEDIANTE RED NEURONAL\nYa nos acercamos al final de nuestro analizador de sentimientos. Hemos decidido utilizar una red neuronal clásica, que como ya hemos comentado en la introducción posee 3 capas. La primera que es de entradas y tiene la longitud de nuestro vocabulario. Cada review se codificará como un vector en el que cada cada elemento representa la frequencia de apariciones de esa palabra en el texto. La capa intermedia tiene 30 nodos, y finalmente la capa final tiene un solo nodo de salida.\nNuestro clasificador puede en modo entrenamiento y test devuelve 2 etiquetas Positivo o Negativo. Y en modo interactivo (logger), además devuelve el nivel de confianza y una tercera etiqueta Neutral.\nclass SentimentReviewClassifier:\n    def __init__(self, learning_rate = 0.01):\n        np.random.seed(7)\n        self.input_nodes = len(vocab)\n        self.middle_nodes = 30\n        self.final_nodes = 1\n        self.learning_rate = learning_rate\n\n        # Initialize net\n        self.hidden_0_1 = np.zeros((self.input_nodes,self.middle_nodes))\n        self.hidden_1_2 = np.random.normal(0.0, self.middle_nodes**-0.5, \n                                                (self.middle_nodes, self.final_nodes))\n        self.first_layer = np.zeros((1,self.middle_nodes))\n    \n    translate_label = {\n        \'POSITIVE\': 1,\n        \'NEGATIVE\': 0,\n    }\n        \n    def sigmoid(self,x):\n        return 1 / (1 + np.exp(-x))\n    \n    def sigmoid_output_2_derivative(self,output):\n        return output * (1 - output)\n    def show_progress(self, i, total, correct):\n        progress =  str(100 * i/float(total))[:4]\n        accuracy =  str(correct * 100 / float(i+1))[:4]\n        sys.stdout.write(""\\r - Progress:%s %%| Correct:%s | Accuracy:%s%%""%(progress, correct, accuracy))\n        \n    def train(self, reviews_corpus, labels_t):\n        """""" Update weights from corpus""""""\n        reviews = list()\n        for review in reviews_corpus:\n            indices = set()\n            for word in remove_stopwords(review):\n                if(word in word2index.keys()):\n                    indices.add(word2index[word])\n            reviews.append(list(indices))\n\n        correct = 0\n        for i in range(len(reviews)):\n            review = reviews[i]\n            label = labels_t[i]\n            # Training\n            self.first_layer *= 0\n            for index in review:\n                self.first_layer += self.hidden_0_1[index]\n            second_layer = self.sigmoid(self.first_layer.dot(self.hidden_1_2))            \n\n            # Output error\n            second_layer_error = second_layer - self.translate_label[label]\n            second_layer_delta = second_layer_error * self.sigmoid_output_2_derivative(second_layer)\n\n            # Backpropagated error\n            first_layer_error = second_layer_delta.dot(self.hidden_1_2.T)\n            first_layer_delta = first_layer_error\n            self.hidden_1_2 -= self.first_layer.T.dot(second_layer_delta) * self.learning_rate\n\n            for index in review:\n                self.hidden_0_1[index] -= first_layer_delta[0] * self.learning_rate\n\n            if(second_layer >= 0.5 and label == \'POSITIVE\'):\n                correct += 1\n            elif(second_layer < 0.5 and label == \'NEGATIVE\'):\n                correct += 1\n            self.show_progress(i, len(reviews), correct)\n            if(i % 2500 == 0):\n                print("""")\n    \n    def test(self, reviews, testing_labels):\n        """""" Test and don\'t update the weights """"""\n        correct = 0\n        for i in range(len(reviews)):\n            pred = self.run(reviews[i])\n            if(pred == testing_labels[i]):\n                correct += 1\n            self.show_progress(i, len(reviews), correct)\n    \n    def run(self, review, logger = False):\n        """""" Evaluate a single review""""""\n        self.first_layer *= 0\n        unique_indices = set()\n        for word in remove_stopwords(review):\n            if word in word2index.keys():\n                unique_indices.add(word2index[word])\n        for index in unique_indices:\n            self.first_layer += self.hidden_0_1[index]\n        second_layer = self.sigmoid(self.first_layer.dot(self.hidden_1_2))\n        out = second_layer[0]\n        threshold = 0\n        if logger:\n          print(out)\n          threshold = 0.05\n        if out >= 0.5 + threshold:\n            return ""POSITIVE""\n        elif out < 0.5 - threshold:\n            return ""NEGATIVE""\n        else:\n            return ""NEUTRAL""\n# Dividimos el dataset en 70% Training y 30% Testing\nDROPOUT_PARTITION = 0.7\nSPLIT_PART = int(len(df)*DROPOUT_PARTITION)\ndf_train = df.iloc[:SPLIT_PART]\ndf_test = df.iloc[SPLIT_PART:]\nprint(""DROPOUT SPLIT Train: %d, Test: %d, TOTAL: %d""%(len(df_train), len(df_test), len(df_train)+len(df_test)))\ndf_test.head()\nDROPOUT SPLIT Train: 17500, Test: 7500, TOTAL: 25000\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nlabels\nreviews\n\n\n\n\n17500\nPOSITIVE\none reason pixar has endured so well  and been...\n\n\n17501\nNEGATIVE\ni saw the film and i got screwed  because the ...\n\n\n17502\nPOSITIVE\na scanner darkly  minority report  blade runne...\n\n\n17503\nNEGATIVE\nwhat  s happening to rgv  he seems to repeat h...\n\n\n17504\nPOSITIVE\ni  ve seen this film in avant  premiere at ima...\n\n\n\n\nnet = SentimentReviewClassifier(learning_rate=0.02)\nnet.train(df_train[\'reviews\'],df_train[\'labels\'])\n - Progress:0.0 %| Correct:1 | Accuracy:100.%\n - Progress:14.2 %| Correct:2028 | Accuracy:81.0%\n - Progress:28.5 %| Correct:4129 | Accuracy:82.5%\n - Progress:42.8 %| Correct:6277 | Accuracy:83.6%\n - Progress:57.1 %| Correct:8469 | Accuracy:84.6%\n - Progress:71.4 %| Correct:10629 | Accuracy:85.0%\n - Progress:85.7 %| Correct:12785 | Accuracy:85.2%\n - Progress:99.9 %| Correct:14934 | Accuracy:85.3%\n\n# Cambiamos la forma de indexar por problemas en algún dato en el dataframe.\nnet.test(df_test.iloc[:,1].values, df_test.iloc[:,0].values)\n - Progress:99.9 %| Correct:6453 | Accuracy:86.0%\n\n# Example of a single review\nnet.run(\'This a great film fantastic actors\', logger=True)\n[0.69505086]\n\n\n\n\n\n\'POSITIVE\'\n\n#@title ### Inserta un texto para probar el analizador de sentimientos\nreview = ""This movie is the best in the world"" #@param {type:""string""}\nprint(\'Review a analizar: ""%s""\'%review)\nprint(\'La Review es: %s\'% net.run(review, logger=True))\nReview a analizar: ""This movie is the best in the world""\n[0.6628306]\nLa Review es: POSITIVE\n\nCONCLUSIONES\nComo hemos podido observar, hemos obtenido resultados muy semejantes a los propuestos en el paper [1].\nExisten multitud de aproximaciones a un mismo problema.\nDeterminar la polaridad positiva o negativa de una reseña se puede conseguir con modelos relativamente sencillos.\nObtener datos más precisos, como qué tipo de sentimiento expresa, enfado, ira, amor, felicidad son un reto todavía en investigación.\nTécnicas muy similares propuestas en esta práctica se pueden utilizar para detectar reseñas fraudulentas, la dificultad está en conseguir un dataset etiquetado.\nUn analizador de sentimiento se puede simplificar a un clasificador de textos, en el que cada tópico es el sentimiento que queremos clasificar.\nPosibles mejoras, un mayor tratamiento en la reducción ruido, mediante la eliminación de palabras vacías aumentaría la precisión de nuestra red.\nExisten algoritmos más avanzados que posilemente den mejores resultados. Modelizar el corpus como word embeddings es una alternativa. Otra opción sería utilizar LSTM (Long short-term memory) [2]. Ambos sistemas tienen en cuenta las palabras que están cercanas y tenemos seguridad de que producirían mejores resultados.\nAunque nosotros hemos tomado una vía muy rudimentaria para ir comentando y describiendo la metodología paso a paso, existen varias librerías que pueden simplificar nuestro código. Nosotros aconsejamos la utilización de estas librerías en sistemas reales. Algunas de estas librerías son: sklearn, pytorch, tensorflow, keras, nltk, scipy entre otras.\nPor último quremos destacar que este analizador de sentimientos funcionará bien con el dominio de películas en el idioma inglés, pero no sería el más adecuado para corpus de otros dominios, y por supuesto el resultado no sería fiable en el caso de clasificar documentos que no tengan ninguna palabra de nuestro vocabulario.\nDe hecho al realizar esta prueba, se puede observar que el clasificador tiene un bias positio. Si no introduces ninguna palabra clasifica el texto como positivo con una confianza de 0.55.\n'], 'url_profile': 'https://github.com/sejas', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Ottawa, Ontario, Canada', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['CanDev\n'], 'url_profile': 'https://github.com/sourabhagarwal07', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}"
"{'location': 'Greece', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['8puzzle\nSolving the 8puzzle game using BFS and A* search algorithm and compare their effectivenes. Group project for Artificial Intelligence, UOWM Semester 2019-2020\n'], 'url_profile': 'https://github.com/AndrewManitsas', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gabbirenee', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'NIT Durgapur', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Flutter App :\nADKit: Smartphone Based Artificial Intelligence Enabled Portable Low-cost Anemia Detection Kit based on Observation of Nail and Palm Pallor\nUnder MEITY\nStatus : Ongoing\nChange Log\n\n\nEnabled feature to upload video from gallery or record a new one\n\n\nAdded Different files for Home Page, Login Page, and Auth File.\n\n\nIntegrated Firebase to Primary Email\n\n\nAdded Firebase Authentication with Custom Email of Gmail\n\n\nEnabled Firebase Storage\n\n\nMinor UI Changes\n\n\nApp now Uploads videos to Firebase Storage\n\n\n'], 'url_profile': 'https://github.com/kulkarni-rajas', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'Southampton, UK', 'stats_list': [], 'contributions': '273 contributions\n        in the last year', 'description': ['Artificial Intelligence for Web Accessibility\nThis is the GitHub repository for my Masters dissertation titled: Artificial Intelligence for Web Accessibility which I completed as a part of my MSc in\nData Science course in the University of Southampton, UK under the supervision of Prof. Mike Wald\nThis project provided me an opportunity to apply my knowledge in machine Learning and Deep Learning to a problem that impacts people\'s lives.\nThe project mainly focuses on applying AI technologies to make the web more accessible to people who are differently abled. We take the knowledge and information\navailable on the internet for granted; but not everyone is so fortunate. This project is my modest attempt to work on this problem. There is a lot of scope to extend\nthis work and please feel free to contact me if you have any ideas/queries/suggestions!\nLinkedIn: Shaunak Sen\nEmail: shaunak1105@gmailcom\nThe project mainly focuses on two parts:\n\nAutomatic Image Captioning System\nContextual Hyperlink Detection\n\nThis document only provides an overview. For details please refer to the full report here\nAutomatic Image Captioning System\nThe Problem\nThe World Wide Web Consortium (W3C) is an organization responsible for developing and maintaining web standards such as HTML, CSS, etc. (57). The Web Content Accessibility Guidelines (WCAG) is developed through the W3C process and it aims to create and maintain a single set of guidelines and recommendations for individuals, organizations, and governments internationally to follow to make web content more accessible and inclusive, especially for people with disabilities (58; 64).\nGuideline H37 (56) of the WCAG focuses on the proper use of alternative (alt) texts for images to help visually impaired people understand the message the image is trying to convey. Often developers fail to provide the above-mentioned alt texts and even if they do, the text does not really convey the message of the image.\nAutomatic Image captioning is a challenging task because it combines the workings of both CNNs and RNNs together. The CNN must understand the high-level features of the image and the RNN must translate these features into relevant captions\nThe Dataset\nThere are several options for a dataset of images accompanied by their corresponding captions. Some of these are Flickr8k (17), Microsoft COCO: Common objects in context (MSCOCO) (27), Conceptual captions dataset by Google (44). I have used the Flickr8k dataset for this task\nData Cleaning and pre-processing\nIn this task, we are dealing with both image data as well as textual data, which has been crowdsourced (17). Data cleaning and preprocessing is very important for the performance of the deep learning model.\nThe pre-processing steps vary for images and text. The pre-processing for the images involves:\n\nResize the images to dimensions: 224x224x3 - 224 is the image height and width (in pixels). 3 denotes the number of color channels (RGB)\nNormalize the images by mean centering them. The mean RGB value was subtracted from each pixel value of the image\n\nFor natural language processing based tasks it is a good practice to clean text data and create a text cleaning pipeline using tools like python nltk (2; 6). The steps in the text cleaning pipeline suitable for our task include:\n\nTokenize the captions into separate words\nCase Normalization - Convert all words to lowercase\nRemove punctuations from the words\nRemove non-alphanumeric characters from the words\n\nFor more traditional machine learning tasks additional steps like stemming and lemmatization need to be carried out, but because our model is going to have an embedding layer, it does not make sense to perform additional preprocessing (6).\nModel for Image Classiﬁcation\nWe use CNN generally for image classification.In this task,  we strip away the last layer of the CNN model.  This is because we areonly interested in the high-level features that the CNN learns from the image, and noton the final classification.  These features can be fed into the RNN along with part ofthe corresponding text of the caption.  The features and the text together are used topredict the next text in the caption\n\nFor  the  purpose  of  this  task,  I  initially  tried  training  my  own  CNN  models,  but  theresults were not good.  Then,  I used Transfer learning,  where we re-use a model thathas already been developed for a certain task for a related but unidentical task (5).  Wecan use a pre-trained network for recognizing and classifying images and use it to getthe high-level features of the images.  Transfer learning helps reduce running time as themodel does not have to be trained from scratch.\nVGG 16 was selected as the final model and it was used it to extract the photofeatures.  Each feature has a dimensionality of 4096.\nOptimization of the VGG-16 model\nRunning each image through the entire VGG network takes a long time and had scopefor optimization.  The process was:\nAssign  a unique  id  to  each  image  in  the  dataset\n\nCreate  a  dictionary  of  form{i d    :  [ . . . ,  features ,]}\n\nFor  each  image\n    Run the  image  through  the VGG network  except  for  the  last  layer\n    Extract  the  features  for  that  image\n    Store  the  image  id  as  the  key and  the  list  of  features  as  the  value  in the  dictionary\n    Return  the  formed  dictionary\n\nOnce the dictionary is formed, we can easily look up the corresponding features for animage using the id of the image\nAt the end of this step,we have computed the high-level features of an image.Animportant point to note here is that the choice of stripping away exactly one layer fromthe model is experimental.  As discussed in the prev section 2.1, as we go deeper into theCNN, it learns more complicated features.  One can argue that it might be a good ideato explore the results after stripping away the last two layers so that slightly less specificfeatures are taken into account.  I tried this process, however, the performance of themodel by stripping away 2 layers reduced dramatically and the network became underfitted. That means by stripping away 2 layers the network could not understandthe complexities of the image well enough to associate the features with the captions.\n\nCreating the training set\nThe next step is to create the dataset that will combine the image features and the cap-tions together which we can then feed into our RNN model.  While generating captionswe have to set a limit for the model to stop predicting the next word in the caption.We do this by appending two special tokensstartseqandendseqto the beginning andend of each caption respectively.  These tokens tell the system when to start and whento stop predicting the sequence of words.\n\nX1, X2 and y are now our training lists.  The model should receive the pair[X1, X2]and predicty. [X1, X2] are like the photo features and the corresponding tokens fromthe caption combined andyis the next token that the model should learn to predict.Thesame algorithm is used to create the corresponding test sequences.\nThe figure represents X2 and y as  words  for  readability  and  understanding  purpose.However  neural  networks  cannot  understand  text  features.   We  have  to  encode  thesenumbers in some form.  The paper (31) discusses the benefits of using word embeddings.So we convert the input text into a one-hot vector and then feed them into an Embeddinglayer, which is built into Keras (8).  This layer basically converts these sparse one-hotvectors into a dense vector representation by embedding them into a lower dimension.\nThe  choice  of  how  many  dimensions  to  use  for  embedding  is  arbitrary,  and  throughexperimentation it was found that300dimensions gave the best results.\nModel for Image Captioning\nAs mentioned in (65), the task of automatically generating captions from an image is very similar to the task of machine translation.  Encoder-decoder based architectures have achieved  excellent  results  in  machine  translation  (7).   Encoder-decoder  based  modelsgenerally consist of an encoder RNN, which maps the input sequence to a vector of fixedlength, and the decoder maps this representation to a target sequence. This architecture,when incorporated with attention-based networks like LSTM (16) achieve state-of-the-art  results  in  machine  translation  tasks.   Also,  the  models  are  very  interpretable  andquite simple compared to other complex models for similar tasks. The final model has been generated absed on a number of experiments\n\n\nGenerating the captions\nNow,  we  have  our  final  model  which  has  been  trained  on  6000  images  and  their  cor-responding captions.  We can generate the captions on the test set (1000 images) andevaluate the results.  To generate the captions we use the following algorithm:\n\nAt this stage, we have the captions for all the 1000 images in the test set.\nEvaluating the model\n(50)  mentions  a  variety  of  metrics  to  evaluate  the  quality  of  the  generated  captions. Initially, the metric Bilingual Evaluation Understudy Score (BLEU) (36) was used forevaluating  the  generated  captions  against  the  real  captions  in  the  test  dataset.   As discussed in (4), BLEU offers some advantages that apply to this project like:\n\nSimple to understand\nEasy to implement - nltk (14) in python has an implementation of BLEU\nIt can be used to compare the performance of our model against the model de-scribed in (65) and (50)\nIt correlates highly with human evaluation\nThe score is calculated irrespective of the order of the words\nA cumulative BLEU score can be calculated based on N-gram (62) matches\n\nThe metric ranges from 0 to 1; 1 being a perfect match.  We can calculate cumulative BLEU score for N-grams.  For example, while considering the BLEU-2 score we see thepercentage of matching 2-grams in the real and generated caption.  Using this metric, our final model Figure 3.5 has the following scores:\n\nBLEU-1:  0.535031\nBLEU-2:  0.282928\nBLEU-3:  0.196293\nBLEU-4:  0.091624\n\nSample results\nSome examples of the captions generated by our model are shown in Figure 3.6.  It canbe seen that there are cases when the model gets the caption correct (green), partiallycorrect (yellow) and completely incorrect (red)\n\n\nBLEU metrics - The problem\nEven though the BLEU metrics for our model look promising a closer inspection revealeda problem.The  BLEU  scores  between  the  real  and  generated  captions  as  shown  in  table  3.3  arevery low although the captions are quite close to the real ones.  The reason why BLEUscore fails to capture this is because it tries to match every word exactly consideringvarious N-grams.  It fails to understand the words in their context and synonyms.\nA Proposed Solution\nIt is clear from the above discussion that we would require a more representative methodof evaluating the generated captions.  One possible solution is to use a word embeddingtechnique like word2vec (31).  The motivation behind using word embeddings for thisproject is:\n\n\nWhen words are trained by deep learning algorithms, they should be representedin a manner which can capture the context in some manner\n\n\nDetecting phrases which have close context is essential not only for evaluating the current model but also for developing the second part of this project.\n\n\nBy embedding the words in vector space we can apply metrics like Cosine similarity between  them  which  can  give  us  a  better  idea  about  the  similarity  than  BLEUs cores or Euclidean similarity.  Cosine similarity is insensitive to the relative sizesof the documents (28)\n\n\nWorking of a word embedding model - word2vec\nFor this project, we use the famous word embedding technique - word2vec (31). Word2vecis a shallow neural network that learns the association between the words (52).  FromFigure 3.7, we can see that the model takes as input the one-hot encoded form of thewords.  The size of this input vector is the same as our vocabulary size.  Then there is ahidden layer, the size of which determines the vector size in which all the words will beembedded.  The final output layer again is the same size as the vocabulary, but usuallyhas a softmax (63) activation function, which outputs a probability distribution of all possible words in the vocabulary.  By training this network on a large corpus of words,it gradually learns to maximize the probability of words which are in close proximity toit.  An important assumption that this model makes is that words in close proximity areoften similar or contextual.  Finally, after the network has learned the associations, wecan extract the hidden (embedding) dimension, and words which have similar contextwill represent similar vector space embeddings.\n\nWord Movers Distance\nNow  that  we  have  a  brief  idea  about  how  word2vec  works,  we  can  go  back  to  ourproblem of understanding if the generated captions are similar to the original captionsor not.  Basically, we have to compare two sentences, and in the field of natural languageprocessing, sentences are often referred to as documents.  So we need a metric that cangive us a similarity score based on the distances between the documents, incorporatingthe features of word2vec.\nWord Movers Distance (WMD) (23) is such a metric.  WMD utilized word2vec embed-dings.  The words and hence, the documents can be represented in vector space.  WMDcomputes the cumulative distance between two documents in vector space as the mini-mum distance the cloud of points for one document will need to travel to merge with thecloud of points for the above document.  Because it uses word2vec embeddings, similarwords will be close together in vector space, and, as a result, similar documents will haveless WMD between them.\nThe motivation for using WMD for our use case of identifyingthe similarity between captions are:\n\nCan leverage pre-trained embeddings of the word2vec model\nCan  capture  the  semantic  similarity  between  documents  that  other  metrics  likeTF-IDF (23; 37) fail to do\nThe algorithm is hyperparameter free - so a lot of time and memory for hyperpa-rameter optimization can be saved.  As discussed in section 1.3, RAM managementis a crucial factor\nThe algorithm can be easily implemented using the gensim package in python (39)\n\nImplementing the proposed solution\nTo implement WMD, we use pre-trained word2vec embeddings from the Google Newsdataset (12).  These are a set of pre-trained vectors, each representing a unique word,which have been extracted from Google news data.  There are about 100 billion uniquewords, and the vector (embedding) size is 300.  It is a safe assumption to consider that all of the words in our dataset is a part of this massive 100 billion words dataset, so wedo not have to train our own word2vec model for this.  Another important catch whilecomputing WMD is it considers Euclidean distance (60), and not Cosine Similarity (28).So if two documents have different lengths the Euclidean distance will be large, so wenormalize the embedded vectors so that they have the same lengths.\n\nThe implementation of the WMD distance on our test dataset is as follows:\n\nThe average WMD Distance score is 1.17, which suggests that the generated captionsare quite close to the original ones.  Through experimentation,  we can see that if thecaptions are not similar the WMD score is generally over 1.25, and often over 1.5\nResults\n\nIn  table  3.3,  we  had  observed  few  samples  of  real  and  generated  captions  for  whichthe N-gram BLEU scores were very less, even though the captions were quite similar.We compute the WMD Distance metric between these captions using pre-trained GoogleNews word embeddings (12), and the results are summarized in table 3.4.  It is clear fromthe above results that applying WMD metric to the captions generate better results.\n\nSome optimizations for deploying\nWe  optimized  the  CNN  part  of  the  model  by  pre-computing  and  storing  the  image  features.   When  we  deploy  the  model  to  the  web,we  need  to  consider  the  running  time  of  the  model.   For  that,  we  should  store  all the variables, data structures and weights of the models which have been trained and optimized on disk so that while predicting, the application can just read from these file sand  run  the  data  through  the  model  to  get  the  predictions.   We  should  not  have  to re-create the datasets or re-train the model every time.  Most importantly, the creation of the training dataset takes a long time (30 mins on 25GBRAM) and should be stored on disk.\n\nOptimizing word embeddings\nOur model has an embedding layer of fixed dimensionality which learns dense vector space embeddings of words. Once trained on the whole dataset,8763 words are learned by the model.  This is a significant number and it does not make sense to precompute the embeddings every time.  So, the embedding matrix once learned can be stored as a numpy array of dimensionality(vocabularysize, embeddingsize)and then be stored on disk as a pickle, which the model can refer to while training.  This significantly reduced the training time of the model (by almost 10 minutes on a batchof 1000 new images for 20 epochs).  Also, to make this approach work, we keep a list of words in our vocabulary.  If newer data comes in and the percentage of words that are out-of-vocabulary is beyond a particular threshold, we re-train the embedding.  This process is summarized in Figure 3.8.\nUsing model checkpoints\nAlso, because the model will be deployed on the cloud, we may need to re-train the model as we receive new data. So we should always ensure that we are using the best model for the predictions. An easy way to do this is to monitor the loss via callbacks. Callbacks allow us to monitor important statistics of a model like loss and accuracy while training [29]. Using callbacks we can create checkpoints of the model  The way we do this is:\nSet the format of the model file as: model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\nCreate a new ModelCheckpoint instance\nSet the appropriate parameters:\n    monitor = ‘val_loss’\n    save_best_only = True: Only save the model with the lowest validation score\n    mode = ‘min’: overwrite the current file if the model has the minimum validation loss\nSet the checkpoint as a callback while training the model\n\n\nThus, only the model with the lowest validation loss will be stored on disk. If the current model has a lower validation loss, the model on disk will be replaced by the current one.\n\nFile management\nTable 3.5 shows a possible configuration of the files that have to be maintained on disk, their types and refresh rates. Generally files of sizes less than 1GB can be maintained as pickle files. Files larger than that have to be stored in HDF (HD5) format.\n\nNow that we have developed a system for automatically captioning images and we have also applied metrics to test the quality of the generated captions keeping in mind the complexities of natural language. Additionally, we explore some additional extended features that we can provide users to improve their experiences on the web.\n\nObject Detection\nImage captions provide a visually impaired user with an overview of what the image is trying to convey. However, what we noticed was that the captions generated were often vague, like “children playing in the park” instead of “three children playing with football in the park”. One way to do this would be to incorporate an attention mechanism (65) in our model. However, this would not solve the problem perfectly as the results in (65) and (50) are quite comparable.\nA solution is to use object detection. As mentioned, image captioning provides a general overview of what the image is trying to convey. The motivation behind this is that to get a detailed understanding of what is going on in the image, users can toggle through the objects in the image and the system will read out what that object is.\nThere are many object detection libraries and APIs available, but we use Microsoft Azure Vision API (30 ) for this purpose, due to the following reasons:\n\nThe API returns the objects detected as well as the coordinates of the bounding boxes\nThe API returns a confidence level of the detection\nThe API returns parent objects for any detected object. For e.g bicycle helmet - helmet - headwear. bicycle helmet is the detected object here.\nThe API returns the data in JSON (JavaScript Object Notation) format which is easy to interpret using JavaScript and Python\n\nA sample response from the API, when the model has detected the object \\textbf{dog} is shown below:\n{\n    ""url"": url for the image,\n    ""response"": {\n        ""objects"": [{\n          ""rectangle"": {\n            ""x"": 154,\n            ""y"": 23,\n            ""w"": 102,\n            ""h"": 122\n          },\n          ""object"": ""dog"",\n          ""confidence"": 0.845,\n          ""parent"": {\n            ""object"": ""mammal"",\n            ""confidence"": 0.849,\n            ""parent"": {\n              ""object"": ""animal"",\n              ""confidence"": 0.906\n            }\n          }\n        }],\n        ""requestId"": unique request id,\n        ""metadata"": {\n          ""width"": 356,\n          ""height"": 277,\n          ""format"": ""Png""\n        }\n      }\n}\nSome of the features that this application should have are the following:\n\nThe image should have a generic caption which will be the output of the image captioning model\nIf the user wants, they can explore more. On the click of a button, the object detection API should be executed\nThe bounding boxes for each object in the image should be drawn\nUser can hover over these objects and the information should be provided via text and speech\nUser can also toggle through the objects by pressing a specific key (completely blind users will not know where to hover on the image)\n\nKeeping these features in mind, a demonstration application was built. The link to this application is:\nhttps://codepen.io/shaunak1105/full/dybZEXa\n\nThe above figure shows how users can interact with the app. The information about the objects detected is provided both by text and speech. The bounding boxes are also overlaid on the image. Users can choose to hover over the objects or toggle through them by pressing the space key.\nRelevance of image on a web page\nOften, we come across web sites which are cluttered with images. These images may be present for the purpose of advertisements and they do not convey any real meaning to the topic being discussed. These images are thus confusing and distracting and for someone who is using screen readers, the experience will be worse.\nThe motivation behind this extension is to detect these irrelevant images from the text surrounding it and automatically flag these images so that they can be ignored by the screen readers.\nWe have already built an image captioning system and evaluated metrics for testing caption-to-caption similarity using WMD. To extract the text surrounding the image, we can consider a window of w words around the image tag. So we have the generated caption, the words surrounding the image and we want to apply WMD to detect of the text and image caption are in context to each other or not.\nWe initially pre-process both the caption text and the surrounding text . Then we computed the WMD score. however, we were not getting proper results (the scores were always higher than 1.25).\nThe caption text does not have a fixed length of words. So we cannot directly consider it as a document and compute WMD between the texts. Additionally, only a part of the surrounding text might be discussing the caption. So if we simply compute the WMD between the caption and surrounding text, it will return a high value, but that does not mean that they are not similar.\nHowever,at least one part of the surrounding text must be discussing the caption. We can detect this by splitting both the surrounding text and the caption into N-grams (62). For example, if the caption has 5 words, we can consider 2-gram sequences, 3-gram sequences, 4-gram sequences, and 5-gram sequences between the caption and the surrounding text and then compute WMD similarity. The intuition is that at least one of these N-grams between the caption and the surrounding text should have a close match i.e have less WMD score.\nThe algorithm to do this is described below:\nPreprocess both the caption text and surrounding text\nCompute length of caption\nFor i in range of 2 to length of caption:\n    Create i-grams of caption text\n    Create i-grams of surrounding text\n    For each such i-gram pair\n        Compute WMD between caption i-gram and surrounding i-gram\n            If WMD < 1.15\n                The caption is similar\n                    Return true\n            Else\n                continue\nReturn false - the caption is not similar\n\n\nThis process is visualized in teh figure below. It is clear for this scenario, simply computing the WMD score between the caption and surrounding text resulted in a high score of 1.34. However using the algorithm discussed, we get a much lower score and we can also visually see which parts of the text received a close match (shown in green in the figure). By removal of stopwords, we have ensured that common words are not taken into the formation of N-grams.\n\nIn progress; to be continued\n\n'], 'url_profile': 'https://github.com/ShaunakSen', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jhuebotter', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'Nagercoil', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['RPA WISHING ROBOT\nThis project was developed to help to send a mail Invitation or wishes like new year, Christmas, Pongal.etc. The project is automated in UI Path using Artificial Intelligence.\n'], 'url_profile': 'https://github.com/sherbin29', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'Louisville, KY', 'stats_list': [], 'contributions': '369 contributions\n        in the last year', 'description': ['UofL-Final-Undergrad-Semester\nIn prior semesters I have made a repository for each class.  This semester I will be keeping all classes in a single repository because none of these individual classes should be as large as previous ones such as Artificial Intelligence.\nClass Syllabi\n\nSemiconductor Development Fundamentals (ECE 542) - Prof. Shamus McNamara\nControl Systems (ECE 560/561) - Prof. Tamir Inanc\nIntroduction to Databases (CECS 535) - Prof. Antonio Badia\nEngineering Methods, Tools, and Practices (ENGR 110) - Prof. Campbell Bego\nDeep Learning (CECS 590) - Prof. Daniel Sierrasosa\n\n'], 'url_profile': 'https://github.com/jtcass01', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'Sydney', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Corvus AI\nCorvus est une Intelligence Artificielle capable de prédire les résultats de football en Ligue 1. Algorithme basé sur les 3 dernières saisons\n'], 'url_profile': 'https://github.com/nicolasguerin8', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'Hangzhou', 'stats_list': [], 'contributions': '153 contributions\n        in the last year', 'description': ['sokoban-game-AI-solver\nBreif Introduction of the project\nAn individual project from the course ""Introduction to Artificial Intelligence"", developped a “Sokoban Game” AI solver with multiple searching algorithms (Depth First Search, Breadth First Search, Uniform Cost Search and Heuristic A* Search)\nIn this project the game rule is a little bit different from the classic Sokoban Game, there can be multible ""workers"" in the game.\nThe original game state can be input as a SokobanState object in sokoban.py, the following example is corresponding to the case shown in picture below:\nSokobanState(""START"", 0, None, 6, 6, # dimensions\n                 ((0, 0), (0, 2), (0, 4), (5, 5)), #robots\n                 frozenset(((1, 0), (4, 1), (1, 2), (4, 3), (1, 4), (4, 5))), #boxes\n                 frozenset(((5, 0), (0, 1), (5, 2), (0, 3), (5, 4), (0, 5))), #storage\n                 frozenset() #obstacles\n                 )\n\nProgram solves for the solution and search for the final state\n\nFor a detailed explanation please check ""project_description.pdf""\n'], 'url_profile': 'https://github.com/YixiaoHong', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/globalcoach11', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Global Coach IT Academy\nGlobal Coach IT Academy Hyderabad is a global leading  company that professionalizes SAP Training ,Digital Marketing Training and other Software Courses like Python , Big Data , Hadoop, Machine learning and Artificial Intelligence etc. Operating  since 2011.We have trained more than 5000+professionals across the globe  we provide both online and off line training in hyderabad.\n'], 'url_profile': 'https://github.com/globalcoach11', 'info_list': ['Updated Jan 17, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': [""XAIIC-classify-the-characteristics-of-Xi-an\n\nXi'an was called Chang'an in the ancient times, and it is one of the birthplaces of the Chinese nation. The them of this competition is Xi'an tourism, using artificial intelligence technology to classify Xi'an popular attractions, food, special products, folk customs, crafts and other pictures.\n\n\n\n\ntraining\n\nRunning environment ubuntu 16.04 with pytorch 1.3.1 and torchvision 0.4.2.\n\n\nTo start the training procedure, just run python go.py. Then you could see something like this\n\n\n\n\nAfter finishing training procedure, there will be some .pth files:\n\n\nmention\n\nThis is only the preliminary version.\n\n\nWe download some pictures from internet using script to enlarge the train&val set, but there are still some problems(you do not need to take much care about this):\n\n\n\nAnother thing you should pay attention to is the plateform provided by HUAWEI which is called ModelArts, this version of our code can not be directly used on this plateform. We made some modificaions with the baseline using our developing environment and got a great improvement(but not the best, we do not pay much more time on finetuning). Meanwhile, we found that without our modification(just using more pictures), we can also get an excellent result.\n\n""], 'url_profile': 'https://github.com/WWWangHan', 'info_list': ['Updated Jan 17, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': [""Feature Engineering And Statistical Analysis\nThis project was from my CSC3060 module (Artificial Intelligence and Data Analytics). It is the first assignment in a 2 part project.\nThere were 3 parts to this project:\n(1) Create a dataset of handwritten symbols.\n(2) Calculate features from the handwritten symbols which may be useful for distinguishing between the different symbols automatically.\n(3) Perform statistical analysis of the datasets, using methods of statistical inference.\nSection 1\nWe created the handwritten symbols by using a tool called GIMP, where we used the touchscreen software of the PCs in the computer science lab to draw our own handwritten symbols. Each separate symbol had 8 different attempts so as to have variance within each symbol group. We then had to export these symbols to a pgm file which were binary where 1 is black and 0 is whitespace. These pgm files were converted to csv files via the notebook in the directory section1_code.\nSection 2\nAfter this, our handwritten symbols were now more easily processable. We used this to our advantage by calculating a range of features for each handwritten symbol. This is done in the notebook in section2_code. In this notebook we calculate a range of features, such as how many neighbours are around each pixel or if the symbol has an 'eye' (for example letters like a and e have eyes but c and f do not and we assign calculated features to each training sample which gives each sample a feature vector.\nSection 3\nThese features were then statistically analysed and visualised in section 3 using R code. Comparison between the symbol groups was visualised using boxplots and also calculated using the ANOVA statistical method as well as t-tests for when the comparison was between two groups rather than multiple. The Tukey HSD method was used especially often as a statistical method to compare across multiple features for different groups of symbols.\n""], 'url_profile': 'https://github.com/fionnmcconville', 'info_list': ['Updated Jan 17, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Teach-a-Neural-Network-to-Read-Handwriting\nNeural networks and deep learning are two success stories in modern artificial intelligence. They’ve led to major advances in image recognition, automatic text generation, and even in self-driving cars. To get involved with this exciting field, you should start with a manageable dataset. The MNIST Handwritten Digit Classification Challenge is the classic entry point. Image data is generally harder to work with than “flat” relational data. The MNIST data is beginner-friendly and is small enough to fit on one computer. Handwriting recognition will challenge you, but it doesn’t need high computational power. Build a neural network from scratch that solves the MNIST challenge with high accuracy.\n'], 'url_profile': 'https://github.com/SRIKARREDDY-dotorg', 'info_list': ['Updated Jan 17, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Feb 3, 2020']}","{'location': 'INDIA', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': [""Machine-Learning\nThis repository contains sample codes that are coded and test by me using Python 3 and python 4. This repository hold foundational data explaining about the things and about's it's existence and why it really matter in machine learning before actually moving into Artificial intelligence or Machine Learning field.  Understanding Why, How, Who, When, Where, What about things is necessary before getting to start working right away in any field.  Here mostly i teach myself by relating math related stuff to real life. Math is main ground for ML.\nAditya Gurav\nTogether we learn and grow as time passes by...\n""], 'url_profile': 'https://github.com/CodeThinkingMachine', 'info_list': ['Updated Jan 17, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Feb 3, 2020']}",,,,,
