"{'location': 'London', 'stats_list': [], 'contributions': '856 contributions\n        in the last year', 'description': ['AI for Trading\nUdacity nano-degree to learn practical AI application in trading algo.\nDesigned by WorldQuant.\nSyllabus:\n\nBasic Quantitative Trading - Trading with Momentum\nAdvanced Quantitative Trading - Breakout Strategy\nStocks, Indices, and ETFs - Smart Beta and Portfolio Optimization\nFactor Investing and Alpha Research - Alpha Research and Factor Modeling\nSentiment Analysis with Natural Language Processing\nAdvanced Natural Language Processing with Deep Leaning\nCombining Multiple Signals for Enhanced Alpha\nSimulating Trades with Historical Data - Backtesting\n\nMain Libraries:\n\nNumpy, Pandas, Matplotlib\nScikit-learn\nPytorch\nQuantopian/zipline\nQuantmedia\n\nMy Course:\n\nStarted: September 2019\nTarget End: February 2020\nActual End: January 2020\n\nProject Details:\n1. Basic Quantitative Trading - Trading with Momentum\n\nimport pandas, numpy, helper\nLoad Quatemedia EOD Price Data\nResample to Month-end close_price.resample(\'M\').last()\nCompute Log Return\nShift Returns returns.shift(n)\nGenerate Trading Signal\n\nStrategy tried:\n\nFor each month-end observation period, rank the stocks by previous returns, from the highest to the lowest. Select the top performing stocks for the long portfolio, and the bottom performing stocks for the short portfolio.\n\n\n\n for i, row in prev_price:\n   top_stock.loc[i] = row.nlargest(top_n)\n\n\n\n\nProjected Return portfolio_returns = (lookahead_returns * (df_long - df_short))/n_stocks\nStatistical Test\n\nAnnualized Rate of Return (np.exp(portfolio_returns.T.sum().dropna().mean()*12) - 1) * 100\nT-Test\n\nNull hypothesis (H0): Actual mean return from the signal is zero.\nWhen p value < 0.05, the null hypothesis is rejected\nOne-sample, one-sided t-test (t_value, p_value) = scipy.stats.ttest_1samp(portfolio_return, hypothesis)\n\n\n\n\n\n2. Advanced Quantitative Trading - Breakout Strategy\n\nimport pandas, numpy, helper\nLoad Quatemedia EOD Price Data\nThe Alpha Research Process\n\nWhat feature of markets or investor behaviour would lead to a persistent anomaly that my signal will try to use?\nExample Hypothesis:\n\nStocks oscillate in a range without news or significant interest\nTraders seek to sell at the top of the range and buy at the bottom\nWhen stocks break out of the range,\n\nthe liquidity traders seek to cover the losses, which magnify the move out of the range\nthe move out of the range attract other investor interst due to herd behaviour which favor continuation of the trend\n\n\n\n\nProcess:\n\nObserv & Research\nForm Hypothesis\nValidate Hypothesis, back to #1\nCode Expression\nEvaluate in-sample\nEvaluate out-of-sample\n\n\n\n\nCompute Highs and Lows in a Window\n\ne.g., Rolling max/min for the past 50 days\n\n\nCompute Long and Short Signals\n\nlong = close > high, short = close < low, position = long - short\n\n\nFilter Signals (5, 10, 20 day signal window)\n\nCheck if there was a signal in the past window_size of days\nhas_past_signal = bool(sum(clean_signals[signal_i:signal_i+window_size]))\nUse the current signal if there\'s no past signal, else 0/False\nclean_signal.append(not has_past_signal and current_signal)\nApply the above to short (signal[signal == -1].fillna(0.astype(int))) and long, add them up\n\n\nLookahead Close Price\n\nHow many days to short or long close_price.shift(lookahead_days*-1)\n\n\nLookahead Price Return\n\nLog return between lookahead_price and close_price\n\n\nCompute the Signal Return\n\nsignal * lookahead_returns\n\n\nTest for Significance\n\nPlot a histogram of the signal returns\n\n\nCheck Outliers in the histogram\nKolmogorov-Smirnov Test (KS-Test)\n\nCheck which stock is causing the outlying returns\nRun KS-Test on a normal distribution against each stock\'s signal returns\nks_value, p_value = scipy.stats.kstest(rvs=group[\'signal_return\'].values, cdf=\'norm\', args=(mean_all, std_all))\n\n\nFind outliers\n\nSymbols that pass the null hypothesis with a p-value less than 0.05\nSymbols that with a KS value above ks_threshod(0.8)\nRemove them by good_tickers = list(set(close.column) - outlier_tickers)\n\n\n\n3. Stocks, Indices, and ETFs - Smart Beta and Portfolio Optimization\n\nLoad large dollar volume stocks from quotemedia\n\nSmart Beta by alternative weighting - dividend yield to choose the portfolio weight\n\nCalculate Index Weights (dollar volume weights)\nCalculate Portfolio Weights based on Dividend\nCalculate Returns, Weighted Returns, Cumulative Returns\nTracking Error  np.sqrt(252) * np.std(benchmark_returns_daily - etf_returns_daily, ddof=1)\n\nPortfolio Optimization - minimize the portfolio variance and closely track the index\n$Minimize \\left [ \\sigma^2_p + \\lambda \\sqrt{\\sum_{1}^{m}(weight_i - indexWeight_i)^2} \\right  ]$ where $m$ is the number of stocks in the portfolio, and $\\lambda$ is a scaling factor that you can choose.\n6. Calculate the covariance of the returns np.cov(returns.fillna(0).values, rowvar=False)\n7. Calculate optimal weights\n\nPortfolio Variance: $\\sigma^2_p = \\mathbf{x^T} \\mathbf{P} \\mathbf{x}$\n\ncov_quad = cvx.quad_form(x, P)\n\n\nDistance from index weights: $\\left | \\mathbf{x} - \\mathbf{index} \\right |2$ = $\\sqrt{\\sum{1}^{n}(weight_i - indexWeight_i)^2}$\n\nindex_diff = cvx.norm(x, p=2, axix=None)\n\n\nObjective function = $\\mathbf{x^T} \\mathbf{P} \\mathbf{x} + \\lambda \\left | \\mathbf{x} - \\mathbf{index} \\right |_2$\n\ncvx.Minimize(cov_quad + scale * index_diff)\n\n\nConstraints\n\n\nx = cvx.Variable()\nconstraints = [x >= 0, sum(x) == 1]\n\n\n\n\nOptimization\n\n\nproblem = cvx.Problem(objective, constraints\nproblem.solve()\n\n\n\n\n\n\nRebalance Portfolio over time\nPortfolio Turnover\n\n$ AnnualizedTurnover =\\frac{SumTotalTurnover}{NumberOfRebalanceEvents} * NumberofRebalanceEventsPerYear $\n$ SumTotalTurnover =\\sum_{t,n}{\\left | x_{t,n} - x_{t+1,n} \\right |} $ Where $ x_{t,n} $ are the weights at time $ t $ for equity $ n $.\n$ SumTotalTurnover $ is just a different way of writing $ \\sum \\left | x_{t_1,n} - x_{t_2,n} \\right | $\nMinimum volatility ETF\n\n\n\n4. Factor Investing and Alpha Research - Alpha Research and Factor Modeling\n\nimport cvxpy, numpy, pandas, time, matplotlib.pyplot\nLoad equitiies EOD price (zipline.data.bundles)\n\nbundles.register(bundle_name, ingest_func)\nbundles.load(bundle_name)\n\n\nBuild Pipeline Engine\n\nuniverse = AverageDollarVolume(window_length=120).top(500) <- 490 Tickers\nengine = SimplePipelineEngine(get_loader, calendar, asset_finder)\n\n\nGet Returns\n\ndata_portal = DataPotal()\nget_pricing = data_portal.get_history_window()\nreturns = get_pricing().pct_change()[1:].fillna(0) <- e.g. 5 year: 1256x490\n\n\n\nStatistical Risk Model\n\nFit PCA\n\npca = sklearn.decomposition.PCA(n_components, svd_solver=\'full\')\npca.fit()\npca.components_ <- 20x490\n\n\nFactor Betas\n\npd.DataFrame(pca.components_.T, index=returns.columns.values, columns=np.arange(20)) <- 20x490\n\n\nFactor Returns\n\npd.DataFrame(pca.transform(returns), index=returns.index , columns=np.arange(20)) <- 490x20\n\n\nFactor Coveriance Matrix\n\nnp.diag(np.var(factor_returns, axix=0, ddof=1)*252) <- 20x20\n\n\nIdiosyncratic Variance Matrix\n\n\n_common_returns = pd.DataFrame(np.dot(factor_returns, factor_betas.T), returns.index, returns.columns)\n_residuals = (returns - _common_returns)\npd.DataFrame(np.diag(np.var(_residuals)*252), returns.columns, returns.columns) <- 490x490\n\n\n\n\nIdiosyncratic Variance Vector\n\n# np.dot(idiosyncratic_variance_matrix, np.ones(len(idiosyncratic_variance_matrix)))\npd.DaraFrame(np.diag(idiosyncratic_variance_matrix), returns.columns)\n\n\nPredict Portfolio Risk using the Risk Model\n\n$ \\sqrt{X^{T}(BFB^{T} + S)X} $ where:\n\n$ X $ is the portfolio weights\n$ B $ is the factor betas\n$ F $ is the factor covariance matrix\n$ S $ is the idiosyncratic variance matrix\n\n\nnp.sqrt(weight_df.T.dot(factor_betas.dot(factor_cov_matrix).dot(factor_betas.T) + idiosyncratic_var_matrix).dot(weight_df))\n\n\n\nCreate Alpha Factors\n\nMomentum 1 Year Factor\nMean Reversion 5 Day Sector Neutral Factor\nMean Reversion 5 Day Sector Neutral Smoothed Factor\nOvernight Sentiment Factor\nOvernight Sentiment Smoothed Factor\nCombine the Factors to a single Pipeline\n\n\npipeline = Pipeline(screen=universe)\npipeline.add(momentum_1yr(252, universe, sector), \'Momentum_1YR\')\n:\nall_factors = engine.run_pipeline(pipeline, start, end)\n\n\n\n\n\nEvaluate Alpha Factors\n\nGet Pricing Data\n\nassets = all_factors.index.level[1].values.tolist()\n\n\nFormat Alpha Factors and Pricing for Alphalens\n\nclean_factor_data = {factor: alphalens.get_clean_factor_and_forward_returns(factor, prices, period=[1])}\nunixt_factor_data = {factor: factor_data.set_index(pd.MultiIndex.from_tuples([(x.timestamp(), y) for x, y in factor_data.index.values], names=[\'date\', \'asset\']))}\n\n\nQuantile Analysis\n\nFactor Returns:\n\nalphalens.performance.factor_returns(factor_data).iloc[:, 0].cumprod().plot()\nThis should be generally move up and to the right\n\n\nBasis Points Per Day per Quantile\n\nalphalens.performance.mean_return_by_quantile(factor_data)[0].iloc[:, 0].plot.bar()\nShould be monotonic, not too much on short that is not practical to implement\nReturn spread (Q1 minus Q5)*252, considering transaction cost to cut this half, should be clear that these alphas can only survive in an institutional setting and that leverage will likely need to be applied\n\n\n\n\nTurnover Analysis\n\nLight test before full backtest to see the stability of the alphas over time\nFactor Rank Autocorrelation (FRA) should be close to 1\nalphalens.performance.factor_rank_autocorrelation(factor_data).plot()\n\n\nSharpe Ratio of the Alphas\n\npd.Series(data=252*factor_returns.mean()/factor_returns.std())\n\n\nThe Combined Alpha Vector\n\nML like Random Forest to get a single score per stock\nSimpler approach is to jsut average\n\n\n\nOptimal Portfolio Constrained by Risk Model\n\nObjective and Constraints\n\nObjective Function:\n\nCVXPY objective function that maximizes $ \\alpha^T * x \\ $, where $ x $ is the portfolio weights and $ \\alpha $ is the alpha vector.\ncvx.Minimize(-alpha_vector.values.flatten()*weights)\n\n\nConstraints\n\n$ r \\leq risk_{\\text{cap}}^2 \\ $ risk <= self.risk_cap **2\n$ B^T * x \\preceq factor_{\\text{max}} \\ $ factor_betas.T*weights <= self.factor_max\n$ B^T * x \\succeq factor_{\\text{min}} \\ $ factor_betas.T*weight >= self.factor_min\n$ x^T\\mathbb{1} = 0 \\ $ sum(weights) == 0.0\n$ |x|_1 \\leq 1 \\ $ sum(cvs.abs(weights)) <= 1.0\n$ x \\succeq weights_{\\text{min}} \\ $ weights >= self.weights_min\n$ x \\preceq weights_{\\text{max}} $ weights <= self.weights_max\nWhere $ x $ is the portfolio weights, $ B $ is the factor betas, and $ r $ is the portfolio risk\n\n\nOptimalHoldings(ABC).find()\n\n\nweights = cvx.Variable(len(alpha_vector))\nrisk = cvx.quad_form(f, X) + cvx.quad_form(weights, S)\nprob = cvx.Problem(obj, constraints)\nprob.solve(max_iter=500)\noptimal_weights = np.asarray(weights.value).flatten()\nreturns pd.DataFrame(data=optimal_weights, index=alpha_vector.index)\n\n\n\n\n\n\nOptimize with a Regularization Parameter\n\nTo enforce diversification, change Objective Function\nCVXPY objective function that maximize $ \\alpha^T * x + \\lambda|x|_2\\ $, where $ x $ is the portfolio weights, $ \\alpha $ is the alpha vector, and $ \\lambda $ is the regularization parameter.\nobjective = cvx.Minimize(-alpha_vector.values.flatten()*weights + self.lambda_reg*cvx.norm(weights, 2))\n\n\nOptimize with a Strict Factor Constrains and Target Weighting\n\nAnother common constraints is to take a predefined target weighting, $x^*$ (e.g., a quantile portfolio), and solve to get as close to that portfolio while respecting portfolio-level constraints.\nMinimize on on $ |x - x^|_2 $, where $ x $ is the portfolio weights  $ x^ $ is the target weighting\nobjective = cvs.Minimize(cvx.norm(alpha_vector.values.flatten()-weights), 2)\n\n\n\n5. Sentiment Analysis with NLP - NLP on Financial Statement\n\nimport nltk, numpy, pandas, pickle, pprint, tqdm.tqdm, bs4.BeautifulSoup, re\n\nnltk.download(\'stopwords\'), nltk.download(\'wordnet\')\n\n\nGet 10-k documents\n\nLimit number of request per second by @limits\nfeed = BeautifulSoup(request.get.text).feed\nentries = [entry.content.find(\'filing-href\').getText(), ... for entry in feed.find_all(\'entry\')]\nDownload 10-k documents\nExtract Documents\n\ndoc_start_pattern = re.compile(r\'<DOCUMENT>\')\ndoc_start_position_list = [x.end() for x in doc_start_pattern.finditer(text)]\n\n\nGet Document Types\n\ndoc_type_pattern = re.compile(r\'<TYPE>[^\\n]+\')\ndoc_type = doc_type_pattern.findall(doc)[0][len(""<TYPE>""):].lower()\n\n\n\n\nProcess the Data\n\nClean up\n\ntext.lower()\nBeautifulSoup(text, \'html.parser\').get_text()\n\n\nLemmatize\n\nnltk.stem.WordNetLemmatizer, nltk.corpus.wordnet\n\n\nRemove Stopwords\n\nnltk.corpus.stopwords\n\n\n\n\nAnalysis on 10ks\n\nLoughran and McDonald sentiment word list\n\nNegative, Positive, Uncertainty, Litigious, Constraining, Superfluous, Modal\n\n\nSentiment Bag of Words (Count for each ticker, sentiment)\n\n\nsklearn.feature_extraction.text.CountVectorizer(analyzer=\'word\', vocabulary=sentiment)\nX = vectorizer.fit_transform(docs)\nfeatures = vectorizer.get_feature_names()\n\n\n\n\nJaccard Similarity\n\nsklearn.metrics.jaccard_similarity_score(u, v)\nGet the similarity between neighboring bag of words\n\n\nTF-IDF\n\nsklearn.feature_extraction.text.TfidfVectorizer(analyzer=\'word\', vocabulary=sentiments)\n\n\nCosine Similarity\n\nsklearn.metrics.pairwise.cosine_similarity(u, v)\nGet the similarity between neighboring IFIDF vectors\n\n\n\n\nEvaluate Alpha Factors\n\nUse yearly pricing to match with 10K frequency of annual production\nTurn the sentiment dictionary into a dataframe so that alphalens can read\nAlphalens Format\n\ndata = alphalens.utils.get_clean_factor_and_forward_return(df.stack(), pricing, quantiles=5, bins=None, period=[1])\n\n\nAlphalens Format with Unix Timestamp\n\n{factor: data.set_index(pd.MultiIndex.from_tuples([(x.timestamp(), y) for x, y in data.index.values], names=[\'date\', \'asset\'])) for factor, data in factor_data.items()}\n\n\nFactor Returns\n\nalphalens.performance.factor_returns(data)\nShould move up and to the right\n\n\nBasis Points Per Day per Quantile\n\nalphalens.performance.mean_return_by_quantile(data)\nShould be monotonic in quantiles\n\n\nTurnover Analysis\n\nFactor Rank Autocorrelation (FRA) to measure the stability without full backtest\nalphalens.factor_rank_autocorrelation(data)\n\n\nSharpe Ratio of the Alphas\n\nShould be 1 or higher\nnp.sqrt(252)*factor_returns.mean() / factor_returns.std()\n\n\n\n\n\n6. Advanced NLP with Deep Leaning - Analizing Stock Sentiment from Twits (requiring GPU)\n\nimport json, nltk, os, random, re, torch, torch.nn, torch.optim, torch.nn.functional, numpy\nImport Twits\n\njson.load()\n\n\nPreprocessing the Data\n\nPre-Processing\n\n\nnltk.download(\'wordnet\')\nnltk.download(\'stopwords\')\ntext = message.lower()\ntext = re.sub(\'https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*\', \' \', text)\ntext = re.sub(\'\\$[a-zA-Z0-9]*\', \' \', text)\ntext = re.sub(\'\\@[a-zA-Z0-9]*\', \' \', text)\ntext = re.sub(\'[^a-zA-Z]\', \' \', text)\ntokens = text.split()\nwnl = nltk.stem.WordNetLemmatizer()\ntokens = [wnl.lemmatize(wnl.lemmatize(word, \'n\'), \'v\') for word in tokens]\n\n\n\n\nBag of Words\n\nbow = sorted(Counter(all_words), key=counts.get, reverse=True)\n\n\nRemove most common words such as \'the, \'and\' by high_cutoff=20, rare words by low_cutoff=1e-6\nCreate Dictionaries\n\n\nvocab = {word: ii for ii, word in enumarate(filtered_words, 1)}\nid2vodab = {v: k for k, v in vocab.items()}\nfiltered = [[word for word in message if word in vocab] for message in tokenized]\n\n\n\n\nBalancing the classes\n\n50% is neutral --> make it 20% by dropping some neutral twits\nRemove messages with zero length\n\n\n\n\nNeural Network\n\nEmbed -> RNN -> Dense -> Softmax\nText Classifier\n\n\nclass TextClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.lstm_size = lstm_size\n        self.output_size = output_size\n        self.lstm_layers = lstm_layers\n        self.dropout = dropout\n\n        self.embedding = nn.Embedding(vodab_size, embed_size)\n        self.lsfm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout, batch_first=False)\n        self.dropout = nn.Dropout(-0.2)\n        self.fc = nn.Linear(lstm_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n                  weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n        return hidden\n    def forward(self, nn_input, hidden_state)\n        batch_size = nn_input.size(0)\n        nn_input = nn_input.long()\n        embeds = self.embedding(nn_input)\n        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n        lstm_out = lstm_out[-1,:,:] # Stack up LSMT Outputs\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        logps = self.softmax(out)\n        return logps, hidden_state\n\n\n\n\n\n\nTraining\n\nDataLoaders and Batching\n\nInput Tensor shape should be (sequence_length, batch_size)\nLeft pad with zeros if a message has less tokens than sequence_length.\nIf a message has more token than sequence_length, keep the first sequence_length tokens\nBuild a DataLoader as a generator\ndef dataloader(): \n    yield batch, label_tensor # both variables are torch.tensor()\n\n\n\n\nTraining and Validation\n\nSplit data to training set and validation set, then check the model\ntext_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\nmodel = TextClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\nhidden = model.init_hidden(64)\nlogps, hidden = model.forward(text_batch, hidden)\nprint(logps)\n\n\nModel\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\nmodel = TextClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\nmodel.embedding.weight.data.uniform_(-1,1)\nmodel.to(device)\n\n\nTrain!\nepochs = 3\nbatch_size = 1024\nlearning_rate = 0.001\nclip = 5\nprint_every = 100\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nmodel.train()\nfor epoch in range(epochs):\n    print (\'Starting epoch {}\'.format(epoch + 1))\n    hidden = model.init_hidden(batch_size)\n    steps = 0\n    for text_batch, labels in dataloader(train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n        steps += 1\n        if text_batch.size(1) != batch_size:\n            break\n        hidden = tuple([each.data for each in hidden])\n        text_batch, labels = text_batch.to(device), labels.to(device)\n        for each in hidden:\n            each.to(device)\n        model.zero_grad()\n        output, hidden = model(text_batch, hidden)\n        loss = criterion(output, labels)\n        loss.backwards()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step() # Optimize\n        if steps % print_every == 0:\n            model.eval()\n            valid_losses = []\n            accuracy = []\n            valid_hidden = model.init_hidden(batch_size)\n            for text_batch, labels in dataloader(valid_features, valid_labels, batch_size=batch_size, sequence_length=20, shuffle=False):\n                if text_batch.size(1) != batch_size:\n                    break\n                valid_hidden = tuple([each.data for each in valid_hidden])\n                text_batch, lables = text_batch.to(device), labels.to(device)\n                for each in valid_hidden:\n                    each.to(device)\n                valid_output, valid_hidden = model(text_batch, valid_hidden)\n                valid_loss = criterion(valid_output.squeeze(), labels)\n                valid_losses.append(valid_loss.item())\n                ps = torch.exp(valid_output)\n                top_p, top_class = ps.topk(1, dim=1)\n                equals = top_class == labels.view(*top_class.shape)\n                accuracy.append(torch.mean(equals.type(torch.FloatTensor)).item())\n            model.train()\n            print(""Epoch: {}/{}..."".format(epoch+1, epochs),\n                  ""Step: {}..."".format(steps),\n                  ""Loss: {:.6f}..."".format(loss.item()),\n                  ""Val Loss: {:.6f}"".format(np.mean(valid_losses)),\n                  ""Accuracy: {:.6f}"".format(np.mean(accuracy)))\n\n\n\n\n\n\nMaking Predictions\n\npreprocess, filter non-vocab words, convert words to ids, add a batch dimention (torch.tensor(tokens).view(-1,1))\nhidden = model.init_hidden(1)\nlogps, _ = model.forward(text_input, hidden)\npred = torch.exp(logps)\n\n\n\n\nTesting\n\n7. Combining Multiple Signals for Enhanced Alpha\n\nimport numpy, pandas, tqdm, matplotlib.pyplot\nData Pipeline\n\nzipline.data.bundles - register, load\nzipline.pipeline.Pipeline\nuniverse = zipline.pipeline.AverageDollarVolume\nzipline.utils.calendar.get_calendar(\'NYSE\')\nzipline.pipeline.loaders.USEquityPricingLoader\nengine = zipline.pipeline.engine.SimplePipelineEngine\nzipline.data.data_portal.DataPortal\n\n\nAlpha Factors\n\nMomentum 1 Year Factor\n\nzipline.pipeline.factors.Returns().demean(groupby=Sector).rank().zscore()\n\n\nMean Reversion 5 Day Sector Neutral Smoothed Factor\n\nunsmoothed = -Returns().demean(groupby=Sector).rank().zscore()\nsmoothed = zipline.pipeline.factors.SimpleMovingAverage(unsmoothed).rank().zscore()\n\n\nOvernight Sentiment Smoothed Factor\n\nCTO(Returns), TrainingOvernightReturns(Returns)\n\n\nCombine the three factors by pipeline.add()\n\n\nFeatures and Labels\n\nUniversal Quant Features\n\nStock Volatility 20d, 120d: pipeline.add(zipline.pipeline.factors.AnnualizedVolatility)\nStock Dollar Volume 20d, 120d: pipeline.add(zipline.pipeline.factors.AverageDollarVolume)\nSector\n\n\nRegime Features\n\nHigh and low volatility 20d, 120d: MarketVolatility(CustomFactor)\nHigh and low dispersion 20d, 120d: SimpleMovingAverage(MarketDispersion(CustomFactor))\n\n\nTarget\n\n1 Week Return, Quantized: pipeline.add(Returns().quantiles(2)), pipeline.add(Returns().quantiles(25))\n\n\nengine.run_pipeline()\nDate Feature\n\nJanuary, December, Weekday, Quarter, Qtr-Year, Month End, Month Start, Qtr Start, Qtr End\n\n\nOne-hot encode Sector\nShift Target\nIID Check (Independent and Identically Distributed)\n\nCheck rolling autocorelation between 1d to 5d shifted target using scipy.stats.speamanr\n\n\nTrain/Validation/Test Splits\n\n\nRandom Forests\n\nVisualize a Simple Tree\n\nclf = sklearn.tree.DecisionTreeClassifier()\nGraph: IPython.display.display\nRank features by importance clf.feature_importances_\n\n\nRandom Forest\n\nclf = sklearn.ensemble.RandomForestClassifier()\nScores: clf.score(), clf.oob_score_, clf.feature_importances_\n\n\nModel Results\n\nSharpe Ratios sqrt(252)*factor_returns.mean()/factor_returns.std()\nFactor Returns alphalens.performance.factor_returns()\nFactor Rank Autocorelation alphalens.performance.factor_rank_autocorrelation()\nScores: clf.predict_proba()\n\n\nCheck the above for Training Data and Validation Data\n\n\nOverlapping Samples\n\nOption 1) Drop Overlapping Samples\nOption 2) Use sklearn.ensemble.BaggingClassifier\'s max_samples with base_clf = DecisionTreeClassifier()\nOption 3) Build an ensemble of non-overlapping trees\n\n\nsklearn.ensemble.VotingClassifier\nsklearn.base.clone\nsklearn.preprocessing.LavelEncoder\nsklearn.utils.Bunch\n\n\n\n\n\n\nFinal Model\n\nRe-Training Model using Training Set + Validation Set\n\n\n\n8. Simulating Trades with Historical Data - Backtesting\n\n\n\nLoad Price, Covariance and Factor Exposure from Barra - data.update(pickle.load())\n\n\nShift daily returns by 2 days\n\n\nWinsorize\n\nnp.where(x <= a,a, np.where(x >= b, b, x)) and Density plot\n\n\n\nFactor Exposures and Factor Returns\n\nmodel = ols (Ordinary Least Squares)\nuniverse = Market Cap > 1e9, Winsorize\nvariable: dependent = Daily Return, independent = Factor Exposures\nestimation: Factor Returns\n\n\n\nChoose 4 Alpha Factors\n\n1 Day Reversal, Earnings Yield, Value, Sentiment\n\n\n\nMerge Previous Portfolio Holdings and Add h.opt.previous with 0\n\n\nConvert all NaN to 0, and median for 0 Specific Risk\n\n\nBuild Universe - (df[\'IssuerMarketCap\'] >= 1e9) | (abs(df[\'h.opt.previous\']) > 0.0)\n\n\nSet Risk Factors (B)\n\nAll Factors - Alpha Factors\npatsy.dmatrices to one-hot encode categories\n\n\n\nCalculate Specific Variance\n\n(Specific Risk * 0.01)**2\n\n\n\nBuild Factor Covariance Matrix\n\nTake off diagonal\n\n\n\nEstimate Transaction Cost\n\nLambda\n\n\n\nCombine the four Alpha Factors\n\nsum(B_Alpha(Design Matrix)) * 1e-4\n\n\n\nDefine Objective Function\n\n$$ f(\\mathbf{h}) = \\frac{1}{2}\\kappa \\mathbf{h}_t^T\\mathbf{Q}^T\\mathbf{Q}\\mathbf{h}t + \\frac{1}{2} \\kappa \\mathbf{h}t^T \\mathbf{S} \\mathbf{h}t - \\mathbf{\\alpha}^T \\mathbf{h}t + (\\mathbf{h}{t} - \\mathbf{h}{t-1})^T \\mathbf{\\Lambda} (\\mathbf{h}{t} - \\mathbf{h}{t-1}) $$\n\n\n\nDefine Gradient of Objective Function\n\n$$ f\'(\\mathbf{h}) = \\frac{1}{2}\\kappa (2\\mathbf{Q}^T\\mathbf{Qh}) + \\frac{1}{2}\\kappa (2\\mathbf{Sh}) - \\mathbf{\\alpha} + 2(\\mathbf{h}{t} - \\mathbf{h}{t-1}) \\mathbf{\\Lambda} $$\n\n\n\nOptimize Portfolio\n\nh = scipy.optimize.fmin_l_bfgs_b(func, initial_guess, func_gradient)\n\n\n\nCalculate Risk Exposure\n\nB.T * h\n\n\n\nCalculate Alpha Exposure\n\nB_Alpha.T * h\n\n\n\nCalculate Transaction Cost\n\n$$ tcost = \\sum_i^{N} \\lambda_{i} (h_{i,t} - h_{i,t-1})^2 $$\n\n\n\nBuild Tradelist\n\nh - h_previous\n\n\n\nSave optimal holdings as previous optimal holdings\n\nh_previous = h\n\n\n\nRun the Backtest\n\nLoop #6 to #21 for all the dates\n\n\n\nPnL Attrribution\n\n$$ {PnL}{alpha}= f \\times b{alpha} $$\n$$ {PnL}{risk} = f \\times b{risk} $$\n\n\n\nBuild Portfolio Characteristics\n\ncalculate the sum of long positions, short positions, net positions, gross market value, and amount of dollars traded.\n\n\n\n'], 'url_profile': 'https://github.com/yuki678', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'Daejeon', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['CH485---Artificial-Intelligence-and-Chemistry\nLecturer: Prof. Woo Youn Kim,  TA: Jaechang Lim\n\nAI has become a big social issue as it spreads rapidly to science, industry, and even daily life. Deep\nlearning techniques has attracted great attention as a new powerful tool for chemical research. In this course, we will discuss the role of artificial intelligence in modern chemistry and look at the latest trends in this\nfield. It aims to learn practical knowledge that can be used in actual research field through theory and\npractice focused on deep learning.\nThis is the repository for materials of KAIST 2019 fall Artificial Intelligence and Chemistry.\nWe use Google colab for all practices.\nWebiste: https://aceteamkaist.wixsite.com/home\n\nPractice\nPractice 02-06: Predicting molecular property, LogP in this practice, using various architectures of neural networks\nPractice 07: Generating new SMILES string using variational autoencoder\n\nPractice 02: Linear regression\nPractice 03: Multilayer perceptron with non-linear activation\nPractice 04: Convolutional neural network\nPractice 05: Recurrent neural network\nPractice 06: Graph convolutional neural network\nPractice 07: Variational autoencoder\n\n'], 'url_profile': 'https://github.com/jaechanglim', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Künstliche Intelligenz\nAufgaben.\n\n Open Tech School AI Anmeldung (zum 06.01.2020).\n Open Tech School AI Anmeldung (zum 09.01.2020).\n Open Tech School besuchen.\n Kennenlernen der Pythonbibliotheken:\n\n matplot\n numpy\n[-] keras\n[-] TensorFlow etc.\n\n\n Xor gate K.I. Programm implementieren.\n pytest zum laufen kriegen.\n Text ""Noahs Lernmodell"" fertigstellen\n\n[-] Formeln besser leserlich machen\n\n\n Erklärung zu ""Neuronale Netzwerke"" outline\n Codereview von Tic Tac Toe mit Mat\n logfile fertigstellen\n Erklärung zu ""Neuronale Netzwerke"" fertigstellen\n\nLang-Zeit-Aufgaben.\n\n TicTacToe K.I. Programmieren.\n\nLOG.\n06.01.2020:\nBetriebspraktikumsbeginn.\nUeber die Bibliothek “Numpy” recherchiert.\nMit dem gesammelten Wissen hab ich anschließend einen Mathematischen Graphenzeichner programmiert.\nIn den Zwischenzeiten hab ich mich über neuronale Netzwerke informiert.\n07.01.2020:\nAm zweiten Tag des Praktikums hab ich mich weiter über neuronale Netzwerke informiert.\n08.01.2020:\nAm Mittwoch hab ich mich weiter informiert und mich für das Thema meines ersten neuronalen Netzwerks in Python entschieden.\n09.01.2020:\nDas “Xor Gate Neuronales Netzwerk” fertig programmiert, aber es funktioniert noch nicht.\nAbends bei dem “Open Tech School AI Meetup” habe ich einige neue “Numpy” Funktionen kennengelernt.\n10.01.2020:\nIch habe über neuronale Netzwerke gelernt und “Bugfixes” an dem “Xor Gate” Programm durchgeführt aber es funktioniert immer noch nicht. Ich bin zur Erkenntnis gelangt das die Fehler meines Programmes schwer zu finden sein werden da ich “unordentlich” programmiert habe.\n11.01.2020:\nPause.\n12.01.2020:\nPause.\n13.01.2020:\nAm Montag habe ich über ein Konzept mit dem Namen “Test Oriented Programming” gelernt, und die nötigen “Python libraries” installiert und ausprobiert.\n14.01.2020:\nAm Dienstag der zweiten Woche habe ich ein “Lernprozess für Neuronale Netzwerke” entwickelt und begonnen zu dokumentieren.\n15.01.2020:\nIch habe am Mittwoch meine Dokumentation zu meinem “Lernprozess für Neuronale Netzwerke” abgeschlossen, Pytest zum laufen gebracht und mich mit “Clean Coding” auseinandergesetzt.\nSpäter besuchte ich wieder die Open Tech Schule.\n16.01.2020 - 17.01.2020:\nAls ansatz um mit meiner Tic Tac Toe K.I. zu beginnen habe ich ein gewöhnliches 2 Spieler Tic Tac Toe Spiel programmiert.\n18.01.2020 - 22.01.2020:\nIn dieser Zeitspanne begann und beendete ich ein mein Tic Tac Toe neuronales Netzwerk. Es funktioniert auf Basis meines Lernprozesses. Nach 3.5 Stunden übung, was in diesem Kontext nicht viel ist, hat das Netzwerk schon zum Teil Gewinnorientiert gespielt.\n23.01.2020 - 24.01.2020:\nIn den letzten 2 Tagen machte ich ""Bugfixes"" an meinem neuralen Netzwerk. Mit hilfe einer der Mitarbeiter wurde mein neuronales Netzwerk dann anschließend zum Tic Tac Toe üben, auf einem Server deponiert, wo es ungestört eine Woche lang trainieren wird.\nEinige meiner recherchierten Links.\nhttps://www.youtube.com/watch?v=GB9ByFAIAH4 : Ein Numpy Lernvideo.\nhttps://www.youtube.com/watch?v=CliW7kSxxWU : Was sind Tensoren.\nhttps://www.youtube.com/watch?v=f5liqUk0ZTw&t=7s : Erlärt was Tensoren sind.\nhttps://en.wikipedia.org/wiki/Tensor : Tensoren.\nhttps://www.youtube.com/watch?v=oJNHXPs0XDk : neuronale Netzwerke: Topologische Architekturen.\nhttps://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2&t=0s : Neural networks: Deep learning, chapter 1 (3Blue1Brown).\nhttps://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3&t=0s : Neural networks: Deep learning, chapter 2 (3Blue1Brown).\nhttps://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4&t=1s : Neural networks: Deep learning, chapter 3 (3Blue1Brown).\nhttps://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5&t=0s : Neural networks: Deep learning, chapter 4 (3Blue1Brown).\nhttps://www.youtube.com/watch?v=K-wIZuAA3EY : Eine K.I. erlernt das Gehen.\nhttps://www.youtube.com/watch?v=kvo1B7r7Xtk : Induktions Urteil.\nhttps://www.youtube.com/watch?v=ZDa-Z5JzLYM&t=62s : Python OOP Tutorial 1: Classes and Instances.\nhttps://en.wikipedia.org/wiki/Types_of_artificial_neural_networks : Unterschiedliche Sorten des neuronalen Netzwerks.\nhttps://en.wikipedia.org/wiki/Rectifier_(neural_networks) : Rectifier (neuronale Netzworke).\nhttps://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464 : Erklärte Topologien für Neuronale Netzwerke.\nhttps://www.youtube.com/watch?v=QVdf0LgmICw : Python: ""variable scopes"".\n'], 'url_profile': 'https://github.com/Orangetree5', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fsjal', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'Malvern, PA', 'stats_list': [], 'contributions': '187 contributions\n        in the last year', 'description': ['Artificial-Intelligence\nWhat is Artificial-Intelligence and Machine Learning?\n\n\nArtifical Intelligence is used go perform tasks that requires human intelligence, such as speech recognition and decision making.\n\n\nMachine learning is part of AI which provides the system for the ability to learn from experiences in order for these\ntechnologies to become better.\n\n\nArtificial-Intelligence in Software Testing\nArtificial-Intelligence Under Software Development\n\n\nAlthough AI has been small as comapred to technolgoies, such as smart speakers or self driving cars, it is still driving foward\n\n\nTools to test software are being made using AI are being used to make software development life cycle easier.\n\n\nIt can be used to reduce the amount of tasks in development and testing.\n\n\nAI enabled bots can identify recent code changes and check current state of tests.\n\n\nAI Techniques\nNeural Machine Translation\n\nIt predicts the sequence of words.\nIt uses neural machine translation.\nIt learns from experience.\n\nHow much memory do they need?\nThey only need a fraction a memory unlike statistical machine translation.\nHow much time do they need?\nThey do not need as much time as statistical machine translation.\nHow does neural machine translation work?\n\nThe neural network takes an input and convert it to vectors in matrices for the computer to understand.\nThe neural network converts the number back.\n\nExamples of nerual machine translation\n\nGoogle translate\nFacial recognition\nSnapchat\n\nHow does neural machine translation work?\n\nDeeping learning is worked through neural networks that handles the info. After that, machine learning does its job where it takes its data to do what it is supposed to do.\nHow does a artificial neural network apply to nerual machine translation ?\n\n\nModel of a Artificial Neural Network\nReal World Example: Google Translate\n\nA word is taken and the neural network converts it to vertices\nThe vertices convert it back to the input in a different language\n\n\nIn this case music can be passed to a neural network in order to make better music.\nResources\n\n\nhttps://blog.parasoft.com/what-is-artificial-intelligence-in-software-testing\n\n\nhttps://youtu.be/bfmFfD2RIcg\n\n\n'], 'url_profile': 'https://github.com/fayedraza', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NikolaiDimitrov', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Artificial Intelligence projects\n'], 'url_profile': 'https://github.com/shafaaf', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/touzanimo', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'Bhopal', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SRsk786', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'Bozeman, MT', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Artificial-Intelligence\nMontana State University, CSCI 446 projects\nProjects Overview\nProject 1: Search Algorithms\nProject 1 uses breadth/depth first searches, greedy algorithm, and A-star to ultimately solve puzzles, in this case, randomly generated mazes of different sizes.\nThis involves structuring the maze such that it can be represented as a graph and or tree.\nProject 2: Constraint Satisfaction Problems\nProject 2 focuses on CSP, a manner in which the agent must satisfy constaints using heuristics. In this case, a common phone game is implemented but rather a user solving the maze, the agent solves it. The constraints are used to tether the agent in which it will actually solve the maze. This is implemented as a decision tree such that the contraints eliminate possible options.\nProject 3: First-Order Logic\nProject 3 uses reinforcement learning with the application of first-order logic. The program is known as ""Wumpus World"" where an agent is to perform a task while avoiding obstacles using the first order logic. The overall idea is that the agent makes a decision with respect to its current state in training. Once the training is complete, a pruning process is used to eliminate decisions in which the agent would not receive a reward.\n'], 'url_profile': 'https://github.com/Jrkeeling23', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Artificial-Intelligence\n'], 'url_profile': 'https://github.com/jawahar17', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'Lahore, Pakistan', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Hammad-Ikhlaq', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'Raleigh, North Carolina', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Artificial-Intelligence\nThis repository contains problems of artificial intelligences and their solutions in python3\nAll code files are written in python3\nSuitable sample data is also given to run the code\n'], 'url_profile': 'https://github.com/shaival2905', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bilaleluneis', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['AI\nArtificial Intelligence\n'], 'url_profile': 'https://github.com/pradeeprt10', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'Dublin', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sajevk', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gokul-code', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'Chicago, Illinois', 'stats_list': [], 'contributions': '171 contributions\n        in the last year', 'description': ['UnityArtificialIntelligencePlayground\n\n\nA playground for my personal learning of AI using Unity\n\nFor a technical write up about the making of this game, please check out my portfolio entry.\n'], 'url_profile': 'https://github.com/hodge47', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Artificial-Intelligence-Research\n'], 'url_profile': 'https://github.com/ackais', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/renanaya48', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}"
"{'location': 'Evanston', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Intro-to-Artificial-Intelligence\nadversarial games, classification, and regression\nWPI CS:4341 (Introduction to Artificial Intelligence)\nGroup 28 (Nikolas Gamarra, Adam Moran, Ying Zhang)\nTerm Project: Bomberman AI\nProject Strategy\nOur overall strategy for this project was to begin with implementing A* to ensure\nwe had a simple basic algorithm that could tackle most of the challenges. Once A* was\nimplemented we set about improving our A* heuristic until we hit a limit of what we\nthought was achievable with A*. In order to try and solve some of the shortcomings of\nA*, we attempted to implement more advanced algorithms like expectimax and policy\niteration.\nA* Search Algorithm\nIn order to make our path we did an implementation of A*. There were no major\ndifferences between our implementation and traditional A* except that our priority\nqueue was structured as a tuple of a location tuple and a priority. We also created a\ncouple of simple helper functions to find the distance between two tuples, and check if\na monster is within a certain radius of a cell in order to inform our heuristic. We also\nneeded to implement a simple priority queue class. For our heuristic, normal cells were\ngiven a value of 1, walls were given a value proportional to the fuse time +20, cost of 60\nwas given to cells within a radius of 5 of a monster and a cost of 80 was given to cells\nwithin a radius of 2 of a monster. The A* algorithm returned a dictionary of the came\nfrom path. This dictionary was iterated backward into a list and then the next move was\nextracted. In the event that A* could not find a path (which happened when we were\nstuck in a corner between an explosion) a try-catch statement would prevent the\nprogram from dying and move the character to a random safe cell. Because walls had a\nvery high cost, our agent generally avoids them but will path to them if there is no other\noption or if the other option gets too close to monsters. In the general motion of the\ncharacter, a bomb is placed whenever a move is attempted into a wall.\nIn order to avoid pathing into cells that would be exploding in the next turn,\nseveral helper functions were added to keep track of where a bomb had been placed\nand how long till it exploded. If the character ever detects that it is attempting to move\ninto a cell that will explode it will instead make a random safe choice.\nA* 7 was made in an attempt to solve the issue of pathing into corners with\navoid_monster3. Unfortunately, this implementation did not produce any better results\nfor any variant than the other implementations we had.\nUtility Functions\nIn order to tackle the problem, we created several helper functions that are\nuniversally helpful.\nPlace smart bombs and several other helper functions keep a record of which\ncells will be exploding and when. Using this A* can have the ability to peer into the\nfuture without the help of looking into future states of the map or using expectimax.\nUsing this we could simply and effectively create an A* solution that would never path\ninto a cell that is about to explode.\nAnother important helper function we created for our A* was avoid_monster. This\nfunction was improved over three versions. In the event that a monster is found within a\ncertain radius of the character A* stops and instead the character tries to run away from\nthe monster. If the monster is in line with a potential bomb placement or within a certain\nradius a bomb will be placed. The avoid_monster2 behavior has a three part heuristic.\nFirst, the valid neighbors are considered. Of those, cells closer to the exit, further away\nfrom the monster, and bordering fewer walls score better. Avoid walls was added to\nattempt to avoid getting cornered. Ultimately We found that this was not enough to\nalways avoid getting trapped in corners.\nTo solve getting cornered we created avoid_monster3. This function does a\nsimple look into the future with sensed worlds and checks if it will survive running in for\na certain depth in each direction if the monster is aggressive and chases it. The set of\nresults where it survived is examined and the one furthest from the closest monster is\nchosen as the next move.\nExpectimax\nIn expectimax search, we estimated the probabilistic model of how the monster\nwill move in any state from the sensed world, we obtained utilities from the outcomes of\nthe current states of the world, and emerge the action. The implement of the\nprobability-based algorithm and reward system allows our agent to evaluate the risks\nand rewards of each move, which largely increased the pass rate.\nThe pseudocode of Expectimax we used is as follow:\ndef value(s):\nif s is a max node\nreturn maxValue(s)\nif s is an exp node\nreturn expValue(s)\nif s is a terminal node\nreturn evaluation(s)\ndef maxValue(s):\nvalues = [value(s’) for s’ in successors(s)]\nreturn max(values)\ndef expValue(s):\nvalues = [value(s’) for s’ in successors(s)]\nweights = [probability(s, s’) for s’ in successors(s)]\nreturn expectation(values, weights)\nFor future improvement, we would spend more time on developing the reward\nsystem. The current one is working but sometimes making unnecessary actions which\nmade the agent vulnerable from the placed bomb and monsters. More importantly, we\nwould increase the search level to go deeper for more accurate results.\nPolicy Iteration\nIn policy iteration, we basically wanted the agent to find the most beneficial\nactions in any given state. The idea was that we used a “SensedWorld” to modify the\nreturned world by performing actions or change the state of the environment without\naffecting other existing world instances. With the exception of the agent occupying the\nsame coordinates as the exit cell, all rewards would be negative to incentivize moving\ntowards the exit. It used the same code used to avoid corners, calculate proximity to a\nmonster, detect explosions before they happen, and calculate distances between two\ntuples to produce rewards. Unfortunately, how we would end up finding states would\nnot include all of the possible states (especially since we would have to consider each\ncombination of agent position, monster position, bomb position, explosion position, and\nwall position to have all the states and therefore the best action to do for each state)\nand would therefore create an incomplete policy, which defeats the purpose of doing\npolicy iteration.\nSolutions\nIn order to evaluate which test character we would use for each variant, we created a\nspreadsheet to track the performance of each one across multiple unseeded random trials. This\nway we could ensure we had the best performance possible for each variant and scenario.\nBelow this table is a simple explanation of why we chose that algorithm and why it performed\nwell. For an in-depth explanation of how the algorithm works look into the above sections on\neach algorithm.\nA4(retreat) A5(normal) A6(cheese) Expectimax 3 Expectimax 4 Policy Iteration A7(avoid3) BEST\nTotal: 72.36% 74.27% 33.00% 64.00% 77.73% 0.00% 67.00% 81.36%\nS1 ADV 66.0% 72.5% 66.0% 70.0% 76.0% 0.0% 74.0% 80.0%\nS1 V1 100.0% 100.0% 100.0% 100.0% 100.0% 0.0% 100.0% 100.0%\nS1 V2 90.0% 100.0% 100.0% 90.0% 90.0% 0.0% 100.0% 100.0%\nS1 V3 50.0% 72.7% 80.0% 50.0% 90.0% 0.0% 70.0% 90.0%\nS1 V4 40.0% 50.0% 50.0% 60.0% 60.0% 0.0% 50.0% 60.0%\nS1 V5 50.0% 40.0% 0.0% 50.0% 40.0% 0.0% 50.0% 50.0%\nS2 ADV 78.7% 76.0% 0.0% 58.0% 79.5% 0.0% 60.0% 82.7%\nS2 V1 100.0% 100.0% 0.0% 100.0% 100.0% 0.0% 100.0% 100.0%\nS2 V2 100.0% 100.0% 0.0% 90.0% 90.0% 0.0% 100.0% 100.0%\nS2 V3 75.0% 66.7% 0.0% 40.0% 73.3% 0.0% 70.0% 75.0%\nS2 V4 68.6% 66.7% 0.0% 50.0% 64.0% 0.0% 20.0% 68.6%\nS2 V5 50.0% 46.7% 0.0% 10.0% 70.0% 0.0% 10.0% 70.0%\nScenario 1:\nVariant 1:\nWe simply used our simplest version of A* (#5) for maps without monsters as it was reliable and\neffective. We chose to use the simplest versions for these as it would be less likely to run into\nerrors.\nVariant 2:\nFor scenario 1 we used our normal A* (#5) as it scored the best in our trials\nVariant 3:\nFor this variant we used our implementation of Expectimax (#4) as is scored best in our trials.\nVariant 4:\nFor this variant we used our implementation of Expectimax (#4) as is scored best in our trials.\nVariant 5:\nFor scenario 1 we used our retreating A* (#4) as it scored the best in our trials\nScenario 2:\nVariant 1:\nWe simply used our simplest version of A* (#5) for maps without monsters as it was reliable and\neffective. We chose to use the simplest versions for these as it would be less likely to run into\nerrors.\nVariant 2:\nWe again used A* (#5) for this variant as it consistently scored 100%\nVariant 3:\nFor scenario 2 we used a special version of A* (#4) that would retreat to the y location of the\nstart position. This behavior was implemented because we realized it was dangerous to enter\nthe tight spaces while a monster was present.\nVariant 4:\nFor scenario 2 we used a special version of A* (#4) that would retreat to the y location of the\nstart position. This behavior was implemented because we realized it was dangerous to enter\nthe tight spaces while a monster was present.\nVariant 5:\nFor this variant we used our implementation of Expectimax (#4) as is scored best in our trials.\nThis was because the different heuristic that was used for it performed better in variants with a\nlot of monsters as it moved more cautiously and placed more bombs.\nConclusion\nIn conclusion we were able to successfully exit the map over 80% of the time in our own testing\nwhen using the best algorithm for each scenario and variant. While we would have liked to\nimplement some more advanced AIs given more time, we successfully met our target of being\nable to get a good grade based on the rubric in our own tests.\n'], 'url_profile': 'https://github.com/PRINTF-yzhang', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/faheelsattar', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '928 contributions\n        in the last year', 'description': ['MNIST-Artificial-Intelligence\nArtificial intelligence projects using the MNIST dataset and Python, Scikit-learn, and TensorFlow.\n\nPerceptron, Logistic Regression, SVM 분류기를 이용하여 최상의 MNIST학습 결과 도출하기\nSVM Classifier + Stochastic Gradient Descen 으로 MNIST의 Multi-Label Classification Problem 해결하기\nStandard Scaling, PCA, Convolution, Polynomial 전처리작업 및 작업물을 그래프로 도식화시키기\n\n\nPBL01\nThe given PBL Case is making a system to read the serial numbers of the car parts produced in their factory.\nNamely, It is creating an accurate digit classification system.\nAnd then, our object is finding the best model out of the given classifiers and hyperparameters.\nThe classifiers are Logistic Regression and Support Vector Machine, and a data is MNIST as we saw in class, and a criterion is F1-score, which is the harmonic mean of precision and recall.\nThe problem of our project is to find out which Machine Learning model would be best to classify this MNIST.\n\nPBL02\nUsing an SVM classifier, implement a maximized margin aolong with the data can be classified. \nStochastic Gradient Descent would then be used along with Mini-Batches to update the weights in order to create a solution for the multi-label classification prolbem at hand.\nTo conclude,\nData scaling proved incredibly important, having increased the accuracy in every case where we implemented it.\nPCA also allowed our algorithms to run in a much shorter time however retained accuracy all the while.\nHowever, ran on its own PCA without scaling is not recommendable since normalization is needed prior to PCA.\nGridSearch although time consuming proved incredibly important in the grand scheme of hyperparameter tuning.\n\nPBL03\nThe training data consisted of a total of 80,000 data points including the D1, D2 and the new1k data set. Of the 80,000 data, 30% of it was separated and used as the test set.\nWe set our hyperparameters; C, learning rate and batch size to the following values.\nConvolution, PCA and Polynomial were used to preprocess the data.\n\n'], 'url_profile': 'https://github.com/ChaeLinYeo', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'Stony Brook', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Artificial-Intelligence-Projects\nProjects of AI Course\n'], 'url_profile': 'https://github.com/DeepthiCherukuri', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'Dhaka', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/showkoth', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'Luxembourg', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': [""Artificial Intelligence cheatsheets for Stanford's CS 221\nAvailable in English - Français - Türkçe\nGoal\nThis repository aims at summing up in the same place all the important notions that are covered in Stanford's CS 221 Artificial Intelligence course, and include:\n\nCheatsheets for each artificial intelligence field\nAll elements of the above combined in an ultimate compilation of concepts, to have with you at all times!\n\nContent\nVIP Cheatsheets\n\n\n\n\n\n\n\n\n\n\n\nReflex-based models\nStates-based models\nVariables-based models\nLogic-based models\n\n\n\nSuper VIP Cheatsheet\n\n\n\n\n\n\n\n\nAll the above gathered in one place\n\n\n\nWebsite\nThis material is also available on a dedicated website, so that you can enjoy reading it from any device.\nAuthors\nAfshine Amidi (Ecole Centrale Paris, MIT) and Shervine Amidi (Ecole Centrale Paris, Stanford University)\n""], 'url_profile': 'https://github.com/Moado', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['This assignment is from Free University of Tbilisi\'s AI course, which is based on University of California, Berkeley\'s ""CS 188 | Introduction to Artificial Intelligence"" course.\n'], 'url_profile': 'https://github.com/lkito', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['This assignment is from Free University of Tbilisi\'s AI course, which is based on University of California, Berkeley\'s ""CS 188 | Introduction to Artificial Intelligence"" course.\n'], 'url_profile': 'https://github.com/lkito', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['AI-ACW\nArtificial Intelligence ACW project\n'], 'url_profile': 'https://github.com/dcontini5', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'Visakhapatnam', 'stats_list': [], 'contributions': '364 contributions\n        in the last year', 'description': ['Article-on-AI\nAn article on Artificial Intelligence.\n'], 'url_profile': 'https://github.com/SudeepaNoble', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': [""Nestor\nAn artificial intelligence based chatbot powered by Google's Dialogflow that possesses human capabilities.\nCurrently putting a currency conversion and language-deciphering components.\n""], 'url_profile': 'https://github.com/ykabusalah', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['intellect-supreme\nArtificial Intelligence research\n'], 'url_profile': 'https://github.com/josephaan', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'Montreal, Quebec', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Aritifical Intelligence - Winter 2020\nProject 1 - State Space Search\nProject 2 - Naive Bayes Classifier\n'], 'url_profile': 'https://github.com/matteo-esposito', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Pet-Dog-Recommendation-Expert-System\nProject of Artificial Intelligence\n'], 'url_profile': 'https://github.com/qjwmelody', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mertunal0', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'Iran', 'stats_list': [], 'contributions': '205 contributions\n        in the last year', 'description': ['AI951\nArtificial Intelligence - Fall 2016\n'], 'url_profile': 'https://github.com/SadraSamadi', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['This assignment is from Free University of Tbilisi\'s AI course, which is based on University of California, Berkeley\'s ""CS 188 | Introduction to Artificial Intelligence"" course.\n'], 'url_profile': 'https://github.com/lkito', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['This assignment is from Free University of Tbilisi\'s AI course, which is based on University of California, Berkeley\'s ""CS 188 | Introduction to Artificial Intelligence"" course.\n'], 'url_profile': 'https://github.com/lkito', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/plamenpasliev', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'Colombo, Sri Lanka', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sanuja91', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}"
"{'location': 'Paraguay', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['PrivAI\n\nPrivacy Preserving Artificial Intelligence Algorithms\n\nIntroduccion\nImplementacion de algoritmos para entrenar modelos de machine learning preservando la privacidad de los usuarios\nRequirements\n\npysyft[udacity]\n\n'], 'url_profile': 'https://github.com/sgaseretto', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Artificial Intelligence\nThis is the pallanghuzhi game code. Pallanguzhi is a two player game. It is a sister game to mancala which is popular in AI since a lot of agents were created to play mancala. Pallenguzhi differs slightly in its rules when compared to mancala, regardless, it’s still a counting game.\nAI Agent: This projects work with different agents playing the game “Pallanguzhi”. The agents employed the algorithms: Greedy, Min-Max, Min-Max with alpha-beta pruning, Genetic Algorithm and A3C. Each agent played against another agent to test how well the different heuristics (strategies) helped the agent win and the observations were recorded.\nThere are two python files game.py and board.py.\nboard.py file contains game board, no of pits and pieces in each pit along with some of the methods.\ngame.py file contains all agent algorithms that is naive algorithm, greedy algorithm, min max algorithm, min max with alpha beta prunning algorithm.\nTo run the program.\nYou can use python version 2.7 or 3.0 or greater.\nOpen Console\nGo to the directory of the program file and then execute below line in the console\npython game.py\n\nYou can see an output in the console.\nDEMO\nNaïve vs Naïve\n\n\n\n\nNaïve vs Greedy\n\n\n\n\nNaïve vs Min-Max with alpha-beta pruning\n\n\n\n\nMin-Max vs Greedy\n\n\n\n\nCheckout project section for more details: https://kepy.online/\nCheckout Demo of this project here: https://youtu.be/TBmMEyluORQ\n'], 'url_profile': 'https://github.com/kepy97', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['CS181-BBTan-Project\nWe are 4 sophomores from ShanghaiTech University. This is our final project for course CS181 which aims to play BBTan using basic Q-learning and advanced DQN.\nOur work contains 2 parts.\n1. Using basic Q-learning with human-manufactured features.\n2. Combining convolution neutral network to Q-learning based on visual input.\n'], 'url_profile': 'https://github.com/sheyining', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Fairfax, VA', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['CS221: Artificial Intelligence - Autumn 2019 - 2020\nFor overview, look at cheatsheet by Shervine Amidi\n'], 'url_profile': 'https://github.com/domhuh', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/LubnaFirdouse', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Matrix\nMy first project on artificial intelligence\n'], 'url_profile': 'https://github.com/Uganda-Knuckles', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A5\nVision recognition technology based on Artificial Intelligence\n'], 'url_profile': 'https://github.com/songshiweilai', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['A5\nVision recognition technology based on Artificial Intelligence\n'], 'url_profile': 'https://github.com/fallinlovewitheattingshit', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '329 contributions\n        in the last year', 'description': ['2048 - Solving using Artificial Intelligence\nObjective:\nIn this project we aim to create an artificial intelligence to solve the 2048 game consistently i.e. trying to reach the score of 2048. We will be using the minimax algorithm for the main game play with alpha-beta pruning to reduce the redundant moves and use the combination of several heuristics to get the ideal heuristic to compare the validity of the different generated states in helping us reach the 2048 tile most consistently and efficiently.\n'], 'url_profile': 'https://github.com/tskk97', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Bucharest', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Madaist', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': [""rl-hypothesis-1\nThis is an attempt at artificial general intelligence (AGI), defining AGI to be capable of arbitrary problem solving even if not particularly intelligent. To date, the closest solution is reinforcement learning (RL). RL is great at solving simulatable games, so is now used to solve video games. However, a true automaton should not be constrained to video games. To an RL agent, the only difference between reality and video games is the amount of data--there is far less data in the real world. To work with this constraint, I fit a variational autoencoder (VAE) to the data distribution and can thus pad the dataset with simulants. So, the fundamental question is: can VAEs pad AGI agents' data sets sufficiently to achieve real-world RL? This repo test this question.\nOne major caveat is that VAEs aren't powerful enough to simulate totally-accurate game transitions (RL data). To get around this, I'm applying transfer learning. The transfer learning transform is summarized in this notebook. The majority of my q-Net (RL deep net) is trained on a very large RL dataset. Only the upper-most dense net will be fit on VAE-simulated data. Simulation quality is analyzed in this notebook. This enables the VAE to simulate an embedding instead of exact game states. This should also abstract-away visual processing information, leaving distilled strategic information. Transfer learning will gives VAEs a chance to succeed. I feel this concession is acceptable because a similar design choice has effectively been applied in biological evolution--even humans' preceptive mechanisms are largely based in our animal origins.\nThis experiment's engineering has been fascinating, involving non-trivial scaling challenges. PySpark manages work communication and execution, but only executes workers' Python as child processes. This enables workers' Python to be arbitrarily complex, not constrained to PySpark's definitions nor the JVM, and is necessary for RL and VAE software. PySpark is executed in containers orchestrated by Kubernetes. Containers' builds abstract-away RL and VAE complexity, exposing a simple Python interface to PySpark.\nThe hypothesis is tested by executing an RL experiment with variably padded data, playing Breakout. If VAE-padding can increase the average score, we are one important step closer to releasing AGI into the real world. Latest experimental results are summarized here.\nResources\nFast and easy experimentation is prioritized. Content will be copied from public repos to avoid manual programming when possible.\n\nbase q-net software here\nexecution environment here\n\nNVidia TOS denies distribution of cudnn lib, so environment is CPU-only. This isn't a big setback since game simulation eats most of the time and is largely CPU-driven.\n\n\ncVAE here\n\nI used a conditional generative method because I would otherwise doubt its capability to simulate non-continuous values.\nDiscrete values are simulated from their empirical distributions.\nOriginally, I used GANs but prefer to work with the VAE's clearly defined loss function.\n\n\n\nI've tested the q-net and VAE software. It's good stuff. The q-net really needs some heavy parallelization, ideally a parameter server. However, I don't have the dev time and will eat the cycles instead. Fortunately, it seems to be using available CPU cores (up to 14) and does seem to need a lot of RAM--so, I'll use a beefy node.\nBuild\n\nBuild environment: GCP console.\nConfigure with config.sh\n\nService account required.\n\n\nExecute build with bash build.sh\n\nExecution\n\nFit initial model\n\nRun bash 1-initial-fit.sh\n\n\nRun transfer learning transform\n\nRun bash 2-transform.sh\n\n\nFit VAE\n\nRun bash 3-fit-vae.sh\n\n\nSimple evaluation\n\nRun bash 4-scaled-simple-eval.sh\nLaunches Spark on Kubernetes cluster\nGenerates samples from VAE\nFits a transfer-learned q-net\nReturns game play performance statistics\n\n\n\nHow it works\n\nEngineering details\n\nCompute environment is a single, multi-purpose container.\nVMs and Clusters will auto-terminate after executing work.\nSpark worker nodes are assigned to preemptible machines, reducing compute costs by about 5 times.\n\nResults\nData suggests a weak negative result. VAE-padding isn't providing statistically significant lift. Fortunately, the hypothesis isn't necessarily voided. Sample sizes have been low so far, so increasing sample sizes could illustrate small, yet-positive effect sizes. Also, not enough time has been put into designing a high-quality generative process. There's still plenty of room for a positive result here.\n""], 'url_profile': 'https://github.com/wdurno', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '157 contributions\n        in the last year', 'description': ['SYNBIO_AI\nArtificial Intelligence tools for synthetic biology\n'], 'url_profile': 'https://github.com/Gonza10V', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'Bhubaneswar', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amjadbhai', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['This assignment is from Free University of Tbilisi\'s AI course, which is based on University of California, Berkeley\'s ""CS 188 | Introduction to Artificial Intelligence"" course.\n'], 'url_profile': 'https://github.com/lkito', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'Kerala, India', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rans0000', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['HexAI\nHex game with artificial intelligence using Monte Carlo simulation.\nAuthor:\nRomain Garnier <rom1{dot}garnier{at}yahoo{dot}fr>.\nLicensing provisions:\nApache 2.0 license.\nCompile with\n$ g++ -Wall -Wextra -Wpedantic -Wconversion HexAI.cpp -o HexAI\nExecute as\n$ ./HexAI dimension HumanVsHuman\nHuman can play against human if second argument > 0.\nMachine chooses positions in the hex table and computes best move from\na chosen number of Monte Carlo simulations (minimum 100, default 1000).\nHex table is showed on terminal with played positions.\nPositions are numbered as a grid.\nFor Hex board description see https://en.wikipedia.org/wiki/Hex_(board_game)\nX should take left<->right path to win.\nO should take up<->down path to win.\nThe human might want to play in first or take machine position.\nMachine might want to take human position if the latest plays first.\nPlayer should hit (row number enter button, then column enter).\nThe algorithm finds shortest paths from src to all other vertices\nwith union-find data structure\n(see https://en.wikipedia.org/wiki/Disjoint-set_data_structure)\n'], 'url_profile': 'https://github.com/4Rom1', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'Hangzhou, China', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['AIUE\n💫 Artificial Intelligence UI Exploration for Web/Desktop/APP testing\n'], 'url_profile': 'https://github.com/slxiao', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['atificial_intellegence_neu\n'], 'url_profile': 'https://github.com/wepstein712', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['JavascriptUAI\nUser interface Artificial Intelligence\n'], 'url_profile': 'https://github.com/JavaScriptUAI', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'Hanover, NH', 'stats_list': [], 'contributions': '363 contributions\n        in the last year', 'description': ['AI for basic game logic (Tic-Tac-Toe).\nThis algorithm utilizes a minimax decision tree in order to determine the optimal move when playing tic-tac-toe.\nBuilt using React and JS, this application runs in the browser and can be run here: http://tictactoe-ai314.herokuapp.com/\n\nAvailable Scripts\nIn the project directory, you can run:\nyarn start\nRuns the app in the development mode.\nOpen http://localhost:3000 to view it in the browser.\nThe page will reload if you make edits.\nYou will also see any lint errors in the console.\nyarn test\nLaunches the test runner in the interactive watch mode.\nSee the section about running tests for more information.\nyarn build\nBuilds the app for production to the build folder.\nIt correctly bundles React in production mode and optimizes the build for the best performance.\nThe build is minified and the filenames include the hashes.\nYour app is ready to be deployed!\nSee the section about deployment for more information.\nyarn eject\nNote: this is a one-way operation. Once you eject, you can’t go back!\nIf you aren’t satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project.\nInstead, it will copy all the configuration files and the transitive dependencies (Webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own.\nYou don’t have to ever use eject. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.\nLearn More\nYou can learn more in the Create React App documentation.\nTo learn React, check out the React documentation.\nCode Splitting\nThis section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting\nAnalyzing the Bundle Size\nThis section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size\nMaking a Progressive Web App\nThis section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app\nAdvanced Configuration\nThis section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration\nDeployment\nThis section has moved here: https://facebook.github.io/create-react-app/docs/deployment\nyarn build fails to minify\nThis section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify\n'], 'url_profile': 'https://github.com/johnmccambridge7', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}"
"{'location': 'Florianopolis', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['\nProjeto-X\n""Project-X"" is a program that aims to create an artificial intelligence system that can recognize people and communicate with them.\nThe system is based on Python and its own functions to create chatbot artificial intelligence.\n\n\nThe purpose of this project is to create an artificial intelligence that most closely resembles the AI \u200b""Jarvis used in the Iron Man movie.\nHow artificial intelligence works\nThe communication system searches your database for the closest phrase to what you said. If she finds nothing and training mode is enabled, she will ask if she wants to add her phrase to the database and a possible answer.\nIf training mode is disabled and the AI \u200b\u200bconfidence level is low, it will be ignored and say ""I don\'t understand"".\nInstallation\nFor project startup the repository must be cloned:\ngit clone https://github.com/bobyzoo/ProjetoX.git\n\nBasic Usage\nfrom classBot import *\n\nBot = classBot(\'Assist.db\', mode_train=False)\n\nwhile True:\n    Quest = input(\'You: \')\n    answer = Bot.procuraResposta(Quest)\n    print(f\'Bot: {answer}\')\n\n\nDevelopment pattern for contributors\n\nCreate a fork of\nthe main ProjetoX repository on GitHub.\nMake your changes in a branch named something different from master, e.g. create\na new branch my-pull-request.\nCreate a pull request.\n\n'], 'url_profile': 'https://github.com/bobyzoo', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Face-recognition-implementation-using-state-of-the-art-modern-artificial-intelligence-technique\n'], 'url_profile': 'https://github.com/eobi', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'Cyprus, Nicosia', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Perceptron-Algorithms\nIntroduction to artificial intelligence.The perceptron algorithm, which is one of the introduction topics of artificial intelligence, was developed in Python Language.\n'], 'url_profile': 'https://github.com/RamazanBakir', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['inmersivetech\nRepository of Inmersive tech as Virtual, augmented reality and artificial intelligence\n'], 'url_profile': 'https://github.com/camilomoal', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': [""PACMAN\nAStar Search, Alpha-Beta Pruning, Minimax Algorithms, Depth-first Search, Breadth-first Search etc.\nThese AI algorithms' implementations on the Pacman game\n""], 'url_profile': 'https://github.com/senihcerit', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['SetupIAEnv\nInstall all packages required to perform machine learning and artificial intelligence. Works on Windows and Linux.\n'], 'url_profile': 'https://github.com/alexandreauda', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['imperial_courseworks\n'], 'url_profile': 'https://github.com/arnoldcheung', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Intro-to-AI-projects\nProjects done in CS188 at UC Berkeley(Intro to Artificial Intelligence)\n\nSearch\nGames\nReinforcement Learning\nGhostbusters(HMMs and BNs)\nMachinelearning\n\nSearch: In this project, your Pacman agent will find paths through his maze world, both to reach a particular location and to collect food efficiently. You will build general search algorithms and apply them to Pacman scenarios.\nGames: In this project, you will design agents for the classic version of Pacman, including ghosts. Along the way, you will implement both minimax and expectimax search and try your hand at evaluation function design.\nReinforcement Learning: In this project, you will implement value iteration and Q-learning. You will test your agents first on Gridworld (from class), then apply them to a simulated robot controller (Crawler) and Pacman.\nGhostbusters(HMMs and BNs): In this project, you will design Pacman agents that use sensors to locate and eat invisible ghosts. You’ll advance from locating single, stationary ghosts to hunting packs of multiple moving ghosts with ruthless efficiency.\nMachine Learning: This project will be an introduction to machine learning. In this project you will build a neural network to classify digits, and more!\n'], 'url_profile': 'https://github.com/ejee2020', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['fiat-lux\nThis is my first GitHub repository. I am interested in artificial intelligence in content industry.\nWhat will happen if a story can be predicted?\n'], 'url_profile': 'https://github.com/huneyk', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JeeSa', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['MIT-6.034-AI-problem-sets\nMy solutions for MIT OCW 6.034 Artificial Intelligence assignments.\nLink to the online course:\nhttps://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/assignments/\nMost of the code was provided in the course resources and only parts were supposed to be written by me. The majority of the solutions to these problem sets are in the files lab0.py, lab1.py etc.\n'], 'url_profile': 'https://github.com/oscar0325', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '253 contributions\n        in the last year', 'description': ['PIAIC\nLearning AI Course from PIAIC\n'], 'url_profile': 'https://github.com/waleedbutt98', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['momofboy\nI have studied artificial intelligence and advanced intelligence in my university for two semesters. And this very useful so I can pick up easily.\n'], 'url_profile': 'https://github.com/Deepi4179', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'Los Angeles, USA', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['CSCI-561-AI\nContains my work for CSCI-561 (Artificial Intelligence) course assignments during fall 2019 at University of Southern California\n'], 'url_profile': 'https://github.com/RushabhK02', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'Yogyakarta', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['blencong.io\nBlencongIO is an artificial intelligence-based image recognition application for the introduction of the atmosphere at the WAYANG KULIT show\n'], 'url_profile': 'https://github.com/helloaltop', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['DS442\nCourse Website for DS/CMPSC 442 Artificial Intelligence course offered at Penn State University in Spring 2020\n'], 'url_profile': 'https://github.com/AmulyaYadav', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'Moscow, Russia', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Paper: https://ieeexplore.ieee.org/document/8911054\n'], 'url_profile': 'https://github.com/SebyakinAndrei', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'Via Cossila 13, 10153 Turin, Italy', 'stats_list': [], 'contributions': '306 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nopesir', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'Via Cossila 13, 10153 Turin, Italy', 'stats_list': [], 'contributions': '306 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nopesir', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['AIoT-LPWAN\nArtificial Intelligence of Things project focused on Low Power Wide Area Networks to achieve more efficient and scalable subGHz 802.15.4g mesh networks.\n'], 'url_profile': 'https://github.com/SmartCityEdge', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}"
"{'location': 'Via Cossila 13, 10153 Turin, Italy', 'stats_list': [], 'contributions': '306 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nopesir', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 1, 2020', 'Updated Jan 3, 2020', '2', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': [""deeplant.ai\nwe make innovation in the world of agriculture using artificial intelligence. We help increase agricultural production and find out various information about the state of plants in real time\n\nInspiration\nThe inspiration of deeplant.ai is to combine agriculture and technology using artificial intelligence. Where artificial intelligence in the form of machine learning is made with the help of python and tensorflow.\nWhat it does\ndeeplant.ai serves to provide information on plants or plantations in real time, and is able to detect pests or diseases in plants making it easier to overcome. This will help increase agricultural production in the future.\nHow I built it\nwe started by making a model using python and backend tensorflow with google colabs to training the data. Our data is taken secondary from several open sources, especially Kaggle. Then we save the model obtained in the form of .tflite file and then it is implemented in the Android application to detect diseases and do other things about agriculture\nChallenges I ran into\nthe challenges that we have encountered, especially on computers that lack support to develop applications and impact on slowing our performance and we found some error with code or application. then the data is a little difficult to find. Knowledge in several programming languages \u200b\u200bat once is still lacking. Changing from .flite to tensorflow.js is a bit complicated and also the application of tensorflow with the Internet of Things (IoT) has not been implemented due to the limitations of the above.\nAccomplishments that I'm proud of\nUntil now, we have successfully created a model and saved it as a .tflite file and implemented it in the application and have been able to apply it to tensorflow.js. besides that we try to make a smooth framework for deeplant.ai applications\nWhat I learned\nIn its application we learned a lot that tensorflow 2.0 is easier to use than version 1, and produces better output even though we have to adjust the changes and learn more, but we are happy with that. and we think tensorflow will be widely used to help many people in the future, especially the application of AI\nWhat's next for deeplant.ai\nThe next step, we will apply to tensorflow.js so that it is more flexible than the .tflite file, then we also intend to develop this application for the IoT. In addition to getting data directly and funding to overcome problems, we want to try to work with various parties including researchers, the government and its expectations with Google and Tensorflow\nBuilt With\nandroid-studio\ncss3\nhtml5\njava\njavascript\nnumpy\npandas\npython\n""], 'url_profile': 'https://github.com/linggaajiandika', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 1, 2020', 'Updated Jan 3, 2020', '2', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zeeshanhussainbhatti', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 1, 2020', 'Updated Jan 3, 2020', '2', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['MobileChatBotFramework\nThe mobile chatbot  framework helps you find out all possible answers that your Bot can give for various formats of questions asked\nby your users. Aritificial Intelligence or training your Bot to understand different intents requires you to first understand how\nyour Bot currently behaves for various questions.\nThis framework can read the questions from a spreadsheet and run on a mobile emulator to test the Bot and records the answers given in an accurate manner.\n'], 'url_profile': 'https://github.com/seemacalibrecode', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 1, 2020', 'Updated Jan 3, 2020', '2', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Supermarket-sales-Prediction\nA data analysis work with prediction on the dataset of a supermarket sales carried out by my fellow team members and I during the Nigerian-German Center(Implemented by GIZ) sponsored boot-camp on Data science and Artificial Intelligence training. The Dataset was gotten from (kaggle.com). The problem statement was pointed out to be  constant expiration of a supermarket's product caused by overstocking which frequently led to much losses. Hence, this led to the aim of this project which is to predict the quantity of goods to be purchased in each day of the first quarter of subsequent years. This is achieved by using the model that best yields approximately the same Quantities as in the Sales record/Dataset for each transaction in the first quarter of 2019.  The growth of supermarkets in most populated cities are increasing and market competitions are getting high hence the need for precision on the quantity of the individual product line to be stocked in First quarter of any year to avoid overstocking, expiration of goods and boost profit.\nFrom our analysis, Random Forest Regressor Model gave the most accurate result of all the other Models.\n""], 'url_profile': 'https://github.com/Christabel-Ajaero', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 1, 2020', 'Updated Jan 3, 2020', '2', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['A-Simple-Chatbot-\nA chatbot (also known as a talkbot, chatterbot, Bot, IM bot, interactive agent, or Artificial Conversational Entity)The classic historic early chatbots are ELIZA (1966) and PARRY (1972).More recent notable programs include A.L.I.C.E., Jabberwacky and D.U.D.E (Agence Nationale de la Recherche and CNRS 2006). While ELIZA and PARRY were used exclusively to simulate typed conversation, many chatbots now include functional features such as games and web searching abilities. In 1984, a book called The Policeman\'s Beard is Half Constructed was published, allegedly written by the chatbot Racter (though the program as released would not have been capable of doing so).  One pertinent field of AI research is natural language processing. Usually, weak AI fields employ specialized software or programming languages created specifically for the narrow function required. For example, A.L.I.C.E. uses a markup language called AIML, which is specific to its function as a conversational agent, and has since been adopted by various other developers of, so called, Alicebots. Nevertheless, A.L.I.C.E. is still purely based on pattern matching techniques without any reasoning capabilities, the same technique ELIZA was using back in 1966. This is not strong AI, which would require sapience and logical reasoning abilities.  Jabberwacky learns new responses and context based on real-time user interactions, rather than being driven from a static database. Some more recent chatbots also combine real-time learning with evolutionary algorithms that optimise their ability to communicate based on each conversation held. Still, there is currently no general purpose conversational artificial intelligence, and some software developers focus on the practical aspect, information retrieval.  Chatbot competitions focus on the Turing test or more specific goals. Two such annual contests are the Loebner Prize and The Chatterbox Challenge (offline since 2015, materials can still be found from web archives).  According to Forrester (2015), AI will replace 16 percent of American jobs by the end of the decade.Chatbots have been used in applications such as customer service, sales and product education. However, a study conducted by Narrative Science in 2015 found that 80 percent of their respondents believe AI improves worker performance and creates jobs.[citation needed] is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods. Such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the Turing test. Chatbots are typically used in dialog systems for various practical purposes including customer service or information acquisition. Some chatterbots use sophisticated natural language processing systems, but many simpler systems scan for keywords within the input, then pull a reply with the most matching keywords, or the most similar wording pattern, from a database.  The term ""ChatterBot"" was originally coined by Michael Mauldin (creator of the first Verbot, Julia) in 1994 to describe these conversational programs.Today, most chatbots are either accessed via virtual assistants such as Google Assistant and Amazon Alexa, via messaging apps such as Facebook Messenger or WeChat, or via individual organizations\' apps and websites. Chatbots can be classified into usage categories such as conversational commerce (e-commerce via chat), analytics, communication, customer support, design, developer tools, education, entertainment, finance, food, games, health, HR, marketing, news, personal, productivity, shopping, social, sports, travel and utilities. Background\n'], 'url_profile': 'https://github.com/Yogapriya2512', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 1, 2020', 'Updated Jan 3, 2020', '2', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020']}",,,,
