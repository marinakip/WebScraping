"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Flask-based Model API\nEXPLORE Data Science Academy Regression Predict\nTable of Contents\n\nFlask-based Model API\n- EXPLORE Data Science Academy Regression Predict\n\n1) Overview\n\n1.1) Wait, what is an API again?\n1.2) How our API will work\n\nDescription of files\n\n\n\n\n2) Usage Instructions\n\n2.1) Creating a copy of this repo\n2.2) Running the API on your local machine\n2.3) Updating the API to use your own model\n\nPrerequisites\nMaking the changes\n\n\n2.4) Running the API on a remote AWS EC2 instance\n\n\n3) FAQ\n\n\n\n1) Overview\nThis repository forms the basis of Task 2 for the Regression Predict within EDSA\'s Data Science course. It hosts template code which will enable students to deploy their own developed models through a web server-based API.\n1.1) Wait, what is an API again?\n\nAn API - or Application Programming Interface - refers to a set of procedures and protocols that allows us to send and request information between ourselves and remote applications. You can think of this as a channel of communication to a remote server using specific commands that allow you to use their applications without needing to host that functionality yourself. Many types of API\'s exist, but for this predict task we are interested specifically in Web API\'s. These allow us to send and receive information using web development languages, such as HTML and JSON. The video above provides a simple and intuitive explanation of how API\'s operate.\n1.2) How our API will work\n\nDescription of files\nSeveral files within this repository enable the correct functioning of our API. We provide a high-level description of these salient files within the table below:\n\n\n\nFile Name\nDescription\n\n\n\n\napi.py\nFlask web server application definition and instantiation.\n\n\nmodel.py\nContains helper functions to separate model specific code from our API definition.\n\n\nutils/request.py\nSimple script to simulate a POST request sent to our API.\n\n\nutils/train_model.py\nCode used to train the simple model used for demonstration of the API\'s functioning.\n\n\n\n2) Usage Instructions\n2.1) Creating a copy of this repo\n\n\n\nâš¡ WARNING âš¡\n\n\n\n\nDo NOT clone this repository. Instead follow the instructions in this section to fork the repo.\n\n\n\nAs described within the Predict instructions for the Regression Sprint, this code represents a template from which you can base your own model\'s API. As such, in order to modify the template to serve your own model (and the associated code changes which are required for this), you will need to fork this repository. Failing to do this will lead to complications when trying to work on the API remotely.\n\nTo fork the repo, simply ensure that you are logged into your GitHub account, and then click on the \'fork\' button at the top of this page as indicated within the figure above.\n2.2) Running the API on your local machine\nAs a first step to becoming familiar with our API\'s functioning, we recommend setting up a running instance on your own local machine.\nTo do this, follow the steps below by running the given commands within a Git bash (Windows), or terminal (Mac/Linux):\n\nEnsure that you have the prerequisite Python libraries installed on your local machine:\n\npip install -U flask numpy pandas scikit-learn\n\nClone the forked repo to your local machine.\n\ngit clone https://github.com/{your-account-name}/regression-predict-api-template.git\n\nNavigate to the base of the cloned repo, and run the API web-server initialisation script.\n\ncd regression-predict-api-template/\npython api.py\nIf the web server was able to initialise successfully, the following message should be displayed within your bash/terminal session:\n----------------------------------------\nModel succesfully loaded\n----------------------------------------\n * Serving Flask app ""api"" (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: off\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n\n\nLeave the web server script running within the current bash/terminal session. Open a new session, and navigate to the utils subfolder of the cloned repo.\n\ncd {your/custom/path}/regression-predict-api-template/utils/\n\n\nRun the request.py script located within the utils subfolder to simulate a POST request for our running API.\n\npython request.py\n\nIf you receive an error at this point, please ensure that the web server is still running in your original bash/terminal session. If the script ran successfully, you should receive similar output to the message shown below:\nSending POST request to web server API at: http://127.0.0.1:5000/api_v0.1\n\nQuerying API with the following data:\n [\'Order_No_21660\', \'User_Id_1329\', \'Bike\', 3, \'Business\', 31, 5, \'12:16:49 PM\', 31, 5, \'12:22:48 PM\', 31, 5, \'12:23:47 PM\', 31, 5, \'12:38:24 PM\', 4, 21.8, nan, -1.2795183, 36.8238089, -1.273056, 36.811298, \'Rider_Id_812\', 4402, 1090, 14.3, 1301]\n\nReceived POST response:\n**************************************************\nAPI prediction result: 1547.3014476106036\nThe response took: 0.004323 seconds\n**************************************************\n\nCongratulations! You\'ve now officially deployed your first web server API, and have successfully received a response from it.\nWith these steps completed, we\'re now ready to both modify the template code to place our own model within the API, and to host this API within an AWS EC2 instance. These processes are outlined within the sections below.\n2.3) Updating the API to use your own model\n\n\n\nâ„¹ï¸ NOTE â„¹ï¸\n\n\n\n\nWe strongly encourage you to be familiar with running the API as described in Section 2.2 before attempting to use your own model.\n\n\n\nPrerequisites\nBefore you can update the API code-base to use your own custom model, you will need to have the following:\n\n\nYour own sklearn model, trained and saved as a .pkl file.\nFor a simple example of how to pickle your model, review the script found in utils/train_model.py. For further instructions, consult the \'Saving and Restoring Models in Python\' train in Athena.\n(Note: You are not limited to use only a single model within the API. Furthermore, other sklearn structures which have saved parameters may be required for your model to function as well. Obviously, you are expected to handle the loading of such structures in a similar way as described within this section.)\n\n\nCode for the data preprocessing pipeline used to train your model.\nThis code should cover aspects such as data cleaning, feature engineering, feature selection, and feature transformations.\nThe requirement of this code is vital as your API is built to provide a standard interface for POST requests. I.e. someone asking your API to make a prediction shouldn\'t have to worry about what specific features your model uses internally. Instead, anyone who sends a request with the standard features within the public dataset, should expect to receive a prediction result. This design principle makes it far easier to swap out an old model for a newer one, even if ends up using radically different features.\n\n\nMaking the changes\nOnce you\'ve gathered the prerequisites from the above section, making the changes to API is relatively straight forward. It involves three steps:\n\n\nPlace your .pkl file within the assets/trained-models/ directory of the repo.\n\n\nModify the api.py file by changing the path_to_model variable to reflect the new model .pkl file.\n\n\nModify the model.py file by adding your data preprocessing code to the _preprocess_data() helper function.\n\n\nIf the following steps were carried out successfully, running the API should now produce a new prediction result.\n2.4) Running the API on a remote AWS EC2 instance\n\n\n\nâ„¹ï¸ NOTE â„¹ï¸\n\n\n\n\nYou will only be able to work on this section of the API setup once you\'ve completed the \'Introduction to Amazon AWS - Part I\' train on Athena.\n\n\n\nThe following steps will enable you to run your web server API on a remote EC2 instance, allowing it to the accessed by any device/application which has internet access.\nWithin these setup steps, we will be using a remote EC2 instance, which we will refer to as the Host, in addition to our local machine, which we will call the Client. We use these designations for convenience, and to align our terminology with that of common web server practices. In cases where commands are provided, use Git bash (Windows) or Terminal (Mac/Linux) to enter these.\n\n\nEnsure that you have access to a running AWS EC2 instance with an assigned public IP address. Instructions for this process are found within the \'Introduction to Amazon AWS - Part I\' train on Athena.\n\n\nInstall the prerequisite python libraries on both the Host (EC2 instance), and Client (local machine):\n\n\npip install -U flask numpy pandas scikit-learn\n\nClone your copy of the API repo onto both the Host and Client machines, then navigate to the base of the cloned repo:\n\ngit clone https://github.com/{your-account-name}/regression-predict-api-template.git\ncd regression-predict-api-template/\n[On the Host]:\n\nRun the API web-server initialisation script.\n\npython api.py\nIf this command ran successfully, the following output should be observed on the Host:\n----------------------------------------\nModel succesfully loaded\n----------------------------------------\n * Serving Flask app ""api"" (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: off\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n\n[On the Client]:\n\nNavigate to the utils subdirectory within the repo.\n\ncd utils/\n\n\nOpen the request.py file using your favourite text editor.\nChange the value of the url variable to reflect the public IP address of the Host. (Instructions for getting the public IP address are provided within the â€˜Introduction to Amazon AWS - Part Iâ€™ train on Athena.)\n\n\nurl = \'http://{public-ip-address-of-remote-machine}:5000/api_v0.1\'\n\nOnce the editing is completed, close the file and run it:\n\npython request.py\nIf the command ran successfully, you should see output similar to the following:\nSending POST request to web server API at: http://54.229.152.221:5000/api_v0.1\n\nQuerying API with the following data:\n [\'Order_No_21660\', \'User_Id_1329\', \'Bike\', 3, \'Business\', 31, 5, \'12:16:49 PM\', 31, 5, \'12:22:48 PM\', 31, 5, \'12:23:47 PM\', 31, 5, \'12:38:24 PM\', 4, 21.8, nan, -1.2795183, 36.8238089, -1.273056, 36.811298, \'Rider_Id_812\', 4402, 1090, 14.3, 1301]\n\nReceived POST response:\n**************************************************\nAPI prediction result: 1547.3014476106036\nThe response took: 0.406473 seconds\n**************************************************\n\nIf you have completed the steps in 2.3), then the prediction result should differ from the one given above.\n[On the Host]\nYou should also see an update to the web server output, indicating that it was contacted by the Client (the values of this string will differ for your output):\n102.165.194.240 - - [08/May/2020 07:31:31] ""POST /api_v0.1 HTTP/1.1"" 200 -\n\nIf you are able to see these messages on both the Host and Client, then your API has succesfully been deployed to the Web. Snap âš¡!\n3) FAQ\nThis section of the repo will be periodically updated to represent common questions which may arise around its use. If you detect any problems/bugs, please create an issue and we will do our best to resolve it as quickly as possible.\nWe wish you all the best in your learning experience ðŸš€\n\n'], 'url_profile': 'https://github.com/Explore-AI', 'info_list': ['2', 'Python', 'Updated May 27, 2020', '100', 'Jupyter Notebook', 'Updated May 29, 2020', '84', 'Python', 'Updated Jan 31, 2021', '14', 'TypeScript', 'GPL-3.0 license', 'Updated Feb 3, 2021', '17', 'Julia', 'Updated Jan 7, 2021', '17', 'Jupyter Notebook', 'Updated Jan 7, 2021', '8', 'R', 'Updated Mar 24, 2020', '3', 'Updated Aug 16, 2020', '127', 'Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Shell', 'Updated Jun 2, 2020']}","{'location': 'Hangzhou, China', 'stats_list': [], 'contributions': '295 contributions\n        in the last year', 'description': ['\nML assignments, about Regression, Classification, CNN, RNN, Explainable AI, Adversarial Attack, Network Compression, Seq2Seq, GAN, Transfer Learning, Meta Learning, Life-long Learning, Reforcement Learning. It will be challenge but cheerful work!\n\nLearning Map\n\nintroduction\nå­¦ä¹ ç¬”è®°ï¼šhttps://github.com/Sakura-gh/ML-notes\n\n\nAssignments\n\n\n1_Regression\n\n\n2_Classification\n\n\n3_CNN\n\n\n4_RNN\n\n\n5_Explainable AI\n\n\n6_Adversarial Attack\n\n\n7_Network Compression\n\n\n8_Seq2Seq\n\n\n9_Unsupervised Learning\n\n\n10_Anomaly Detection\n\n\n11_GAN\n\n\n12_Transfer Learning\n\n\n13_Meta Learning\n\n\n14_Life Long Learning\n\n\n15_Reinforcement Learning\n\n\n'], 'url_profile': 'https://github.com/Sakura-gh', 'info_list': ['2', 'Python', 'Updated May 27, 2020', '100', 'Jupyter Notebook', 'Updated May 29, 2020', '84', 'Python', 'Updated Jan 31, 2021', '14', 'TypeScript', 'GPL-3.0 license', 'Updated Feb 3, 2021', '17', 'Julia', 'Updated Jan 7, 2021', '17', 'Jupyter Notebook', 'Updated Jan 7, 2021', '8', 'R', 'Updated Mar 24, 2020', '3', 'Updated Aug 16, 2020', '127', 'Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Shell', 'Updated Jun 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': [""DecoMR\nCode repository for the paper:\n3D Human Mesh Regression with Dense Correspondence\n[Wang Zeng, Wanli Ouyang, Ping Luo, Wentao Liu, Xiaogang Wang]\nCVPR 2020\n[paper]\n.\nInstallation instructions\nThis project is tested on ubuntu 16.04 with python 3.6, PyTorch 1.1. We recomend using Anaconda to create a new enviroment:\nconda create -n DecoMR python=3.6\nconda activate DecoMR\n\nInstall dependecies: This project utilizes OpenDR to render 3D mesh.\nIn order to install OpenDR on Python 3.6, libosmesa6-dev needs to be installed firstï¼š\nsudo apt-get install libglu1-mesa-dev freeglut3-dev mesa-common-dev\nsudo apt-get install libosmesa6-dev\nsudo apt-get install gfortran\npip install --force-reinstall pip==19\npip install -r requirements.txt\n\nAfter finishing with the installation,\nyou can continue with running the demo/evaluation/training code.\nIf you want to do the evaluation on Human3.6M,\nyou also need to manually install the\npycdf package of the spacepy library\nto process some of the original files.\nIf you face difficulties with the installation,\nyou can find more elaborate instructions\nhere.\nFetch data\nIn order to run our code, you need to download some additional\nfile here.\nThis package contains neccessary files for UV mapping function and a model trained on Human3.6M + UP-3D.\nAfter unzip the files, you need to put the folder under this project.\nIn addition, you need to fetch the neutral SMPL model from the\nUnite the People repository:\nwget https://github.com/classner/up/raw/master/models/3D/basicModel_neutral_lbs_10_207_0_v1.0.0.pkl --directory-prefix=data\n\nThe model falls under the SMPLify license.\nIf you find this model useful for your research, please follow the\ncorresponding citing insturctions.\nIn order to perform the evaluation on 3DPW or SURREAL dataset, you also need to get the\nmale and female models.\nPlease go to the websites for the corresponding projects and register\nto get access to the downloads section.\nRun demo code\nTo run our method, you need a bounding box around the person.\nThe person needs to be centered inside the bounding box and\nthe bounding box should be relatively tight.\nYou can either supply the bounding box directly or provide an\nOpenPose detection file.\nIn the latter case we infer the bounding box from the detections.\nIn summary, we provide 3 different ways to use our demo code and models:\n\nProvide only an input image (using --img), in which case it is assumed that\nit is already cropped with the person centered in the image.\nProvide an input image as before,\ntogether with the OpenPose detection .json (using --openpose).\nOur code will use the detections to compute the bounding box and crop the image.\nProvide an image and a bounding box (using --bbox).\nThe expected format for the json file can be seen in examples/im1010_bbox.json.\n\nExample with OpenPose detection .json\npython demo.py --checkpoint=data/model/h36m_up3d/checkpoints/h36m_up3d.pt --config=data/model/h36m_up3d/config.json --img=examples/im1010.jpg --openpose=examples/im1010_openpose.json\n\nExample with predefined Bounding Box\npython demo.py --checkpoint=data/model/h36m_up3d/checkpoints/h36m_up3d.pt --config=data/model/h36m_up3d/config.json --img=examples/im1010.jpg --bbox=examples/im1010_bbox.json\n\nExample with cropped and centered image\npython demo.py --checkpoint=data/model/h36m_up3d/checkpoints/h36m_up3d.pt --config=data/model/h36m_up3d/config.json --img=examples/im1010.jpg\n\nRunning the previous command will save the results in examples/im1010_{render, render_side}.png.\nThe file im1010_render.png shows the overlayed reconstructions of the non-parametric shapes.\nThe file im1010_render_side.png shows the mesh in the side view.\nRun evaluation code\nWe provide code to evaluate our models on the datasets we employ for our empirical evaluation.\nBefore continuing, please make sure that you follow the\ndetails for data preprocessing.\nExample usage:\npython eval.py --checkpoint=data/model/h36m_up3d/checkpoints/h36m_up3d.pt  --dataset=h36m-p1 --log_freq=20\n\nRunning the above command will compute the MPJPE and MPJPE-PA on the Human3.6M dataset (Protocol I). The --dataset option can take different values based on the type of evaluation you want to perform:\n\nHuman3.6M Protocol 1 --dataset=h36m-p1\nHuman3.6M Protocol 2 --dataset=h36m-p2\nUP-3D --dataset=up-3d\nLSP --dataset=lsp\n3DPW --dataset=3dpw\nMPI-INF-3DHP --dataset=mpi-inf-3dhp\nSURREAL --dataset=surreal\n\nRun training code\nDue to license limitiations, we cannot provide the SMPL parameters for Human3.6M\n(recovered using MoSh).\nSo you may need to get the training data of Human3.6M by yourself.\nThe training process is two-stage. In the first stage, the correspondence net is trained.\nExample usage:\npython train.py --name=sample_dp --stage=dp --num_epoch=5\n\nRunning the above command will start the training process of the Correspondence Net.\nIt will also create the folders logs/sample_dp that is used to save model checkpoints\nand Tensorboard logs.\nIf you start a Tensborboard instance pointing at the directory logs/sample_dp/tensorboard,\nyou should be able to look at the logs stored during training.\nIn the second stage, the pretrained Correspondence Net is loaded\nand trained with the Location Net end-to-end. Example usage:\npython train.py --name=sample_end --stage=end --pretrained_checkpoint=logs/sample_dp/checkpoints/final.pt\n\nRunning the above command will start the training process of the full model.\nIt will create the folders logs/sample_end to save model checkpoints and Tensorboard logs.\nThe default training pararmeters use 8 gpus and the default batch size is 128.\nYou can change the setting by resetting --ngpu and --batch_size.\nYou can view the full list of command line options by running python train.py --help.\nThe default values are the ones used to train the models in the paper.\nLatest Update\n2020.09.01\nWe recently added the training and evaluation code on several datasets:\nSURREAL,\n3DPW and\nMPI-INF-3DHP.\nYou can get the preprocess details from here.\nWe add the training code to use the\nSPIN fits\nas supervision and provide the\npretrained models.\n2020.09.24\nWe fixed the bug of the gender label in\nSURREAL dataset,\nand retrained the model on SURREAL dataset.\nIf you are using the codes updated on 2020.09.01,\nyou need to delete the annotation files of SURREAL dataset\n(surreal_train.npz and surreal_val.npz) and then\nrun preprocess_extra_datasets.py again to get the right gender labels.\nFortunately, you do NOT need to remove the rendered IUV images of SURREAL dataset,\nbecause the rendered IUV images are with right gender labels.\nSo the preprocess won't take too much time.\nCiting\nIf you find this code useful for your research, please consider citing the following paper:\n@Inproceedings{zeng20203d,\n  Title={3D Human Mesh Regression with Dense Correspondence},\n  Author={Zeng, Wang and Ouyang, Wanli and Luo, Ping and Liu, Wentao and Wang, Xiaogang},\n  Booktitle={CVPR},\n  Year={2020}\n}\n\nAcknowledgements\nPart of the code and data are adapted from\n(Graph-CMR,\nDenseBody,\nSPIN) .\nWe gratefully appreciate the impact they had on our work.\nWe also appreciate wangzheallen for\nmaking the installation easier.\n""], 'url_profile': 'https://github.com/zengwang430521', 'info_list': ['2', 'Python', 'Updated May 27, 2020', '100', 'Jupyter Notebook', 'Updated May 29, 2020', '84', 'Python', 'Updated Jan 31, 2021', '14', 'TypeScript', 'GPL-3.0 license', 'Updated Feb 3, 2021', '17', 'Julia', 'Updated Jan 7, 2021', '17', 'Jupyter Notebook', 'Updated Jan 7, 2021', '8', 'R', 'Updated Mar 24, 2020', '3', 'Updated Aug 16, 2020', '127', 'Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Shell', 'Updated Jun 2, 2020']}","{'location': 'Melbourne Australia', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Time Series Extrinsic Regression\nThis repository contains the source code for Time Series Extrinsic Regression.\nWe aim to learn the relationship between a time series and a scalar value.\nWe note the similarity with a parallel field in the Statistics community known as\nScalar-on-Function-Regression (SoFR) and is working on implementing those methods.\nData\nThe archive containing 19 time series regression datasets can be found at Monash UEA UCR Time Series Extrinsic Regression Archive.\nWe recommend you to read the paper for a detailed discussion of the datasets and their sources.\nFor demo, please use the data in the data folder provided in this repository\nModels\nThe following models are implemented in this repository:\nClassical ML models\n\nSupport Vector Regression (SVR) - A wrapper function for sklearn SVR\nRandom Forest Regressor (RF) - A wrapper function for sklearn RandomForestRegressor\nXGBoost (XGB) - A wrapper function for XGBoost package\nLinear Regression (LR) - A wrapper function for sklearn LinearRegression\nRidge Regression (Ridge) - A wrapper function for sklearn RidgeCV\n\nDeep Learning for TSC\n\nFully Convolutional Network (FCN)\nResidual Network (ResNet)\nInception Time (InceptionTime)\n\nTSC models\n\nRandom Convolutional Kernels Transform (Rocket)\n\nFeatures Transform\nSome simple feature transformation have been implemented.\n\nPrincipal Component Analysis (PCA)\nFunctional Principal Component Analysis (FPCA)\nFPCA with BSpline Smoothing (FPCA-BSpline)\n\nCode\nThe code is mainly divided as follows:\n\nThe demo.py file contains demo code for a single experiment run.\n\nArguments:\n-d --data_path      : path to dataset\n-p --problem        : dataset name\n-r --regressor      : name of the model\n-t --transformer    : name of the transformer\n-i --iteration      : iteration number\n-n --normalisation  : normalisation (none, standard, minmax)\n\n\nThe run_experiments file contains code to run a set of experiments.\nThe models folder contains the models used for regression.\nThe utils folder contains helper functions for the program.\nAfter each run, the results will be saved to the output folder.\n\nDependencies\nAll python packages needed are listed in requirements.txt file\nand can be installed simply using the pip command.\nSome of the main packages are:\n\nkeras\nmatplotlib\nnumba\nnumpy\npandas\nscikit-fda\nsklearn\nscipy\ntqdm\nxgboost\n\nResults\nThese are the results on the 19 Time series regression datasets from Monash UEA UCR Time Series Regression Archive.\nThe initial benchmark results in the paper showed that a simple linear model such as Rocket\nperforms best for the time series regression task.\nThe full results can be obtained here.\n\nReference\nIf you use any part of this work, please cite:\n@article{Tan2020TSER,\n  title={Time Series Extrinsic Regression},\n  author={Tan, Chang Wei and Bergmeir, Christoph and Petitjean, Francois and Webb, Geoffrey I},\n  journal={arXiv preprint arXiv:2006.12672},\n  year={2020}\n}\n\nAcknowledgement\nWe appreciate the data donation from all the donors.\nWe would also like to thank Hassan Fawaz for providing the base code for Deep Learning models and\nAngus Dempster for providing the code for Rocket.\n'], 'url_profile': 'https://github.com/ChangWeiTan', 'info_list': ['2', 'Python', 'Updated May 27, 2020', '100', 'Jupyter Notebook', 'Updated May 29, 2020', '84', 'Python', 'Updated Jan 31, 2021', '14', 'TypeScript', 'GPL-3.0 license', 'Updated Feb 3, 2021', '17', 'Julia', 'Updated Jan 7, 2021', '17', 'Jupyter Notebook', 'Updated Jan 7, 2021', '8', 'R', 'Updated Mar 24, 2020', '3', 'Updated Aug 16, 2020', '127', 'Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Shell', 'Updated Jun 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""StructuralCausalModels.jl\n\n\n\n\n\n\nIntroduction\nStructuralCausalModels.jl is part of the StatisticalRethinkingJulia eco system and contains functionality to analyse directed acyclic graph (DAG) based causal models as described in StatisticalRethinking, Causal Inference in Statistics and Cause and Correlation in Biology.\nMy initial goal for this package is to have a way to apply SCM ideas to the examples in StatisticalRethinking.jl, i.e. a working version of basis_set(), d_separation() m_separations() and adjustment_sets().\nFrom the point of view of above functionality, I believe the package is close to R's ggm (including most of Sadeghi's additions). I'm hoping version 1.0.0 has a similar API but many more test cases, including more comparisons with R's dagitty.\nStructuralCausalModels.jl will be registered and, once registered, can be installed using\n] add StructuralCausalModels.\nVersions\n0.1.0\n\nInitial commit to Julia's registry.\n\nAcknowledgements\nImportant links are:\n\nDagitty\nR dagitty package\nR ggm package\nSadeghi, K. (2011). Stable classes of graphs containing directed acyclic\ngraphs, implementation as included in ggm.\n\nThe latter two have been used for the Julia implementations of most fuctions\nin this package, e.g. basis_set(), d_separation(), m_separation,\nshipley_test(), pcor_test() and ancestral_graph.\nReferences\n\nStatisticalRethinking\nCausal Inference in Statistics - a primer\nCause and Correlation in Biology\nSadeghi, K. (2011). Stable classes of graphs containing directed acyclic\ngraphs.\nRichardson, T.S. and Spirtes, P. (2002).  Ancestral graph Markov\nmodels {Annals of Statistics}, 30(4), 962-1030.\nSeparators and Adjustment Sets in Causal Graphs: Complete Criteria and an Algorithmic Framework\n\n""], 'url_profile': 'https://github.com/StatisticalRethinkingJulia', 'info_list': ['2', 'Python', 'Updated May 27, 2020', '100', 'Jupyter Notebook', 'Updated May 29, 2020', '84', 'Python', 'Updated Jan 31, 2021', '14', 'TypeScript', 'GPL-3.0 license', 'Updated Feb 3, 2021', '17', 'Julia', 'Updated Jan 7, 2021', '17', 'Jupyter Notebook', 'Updated Jan 7, 2021', '8', 'R', 'Updated Mar 24, 2020', '3', 'Updated Aug 16, 2020', '127', 'Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Shell', 'Updated Jun 2, 2020']}","{'location': 'Hong Kong', 'stats_list': [], 'contributions': '202 contributions\n        in the last year', 'description': ['Hierarchical Regression Network for Spectral Reconstruction from RGB Images\nThe README file for NTIRE 2020 Spectral Reconstruction Challenge of Team OrangeCat: Hierarchical Regression Network for Spectral Reconstruction from RGB Images. Our method achieves the 1st place in track 2: real-world images.\nPaper can be downloaded: https://openaccess.thecvf.com/content_CVPRW_2020/html/w31/Zhao_Hierarchical_Regression_Network_for_Spectral_Reconstruction_From_RGB_Images_CVPRW_2020_paper.html\nHRNet architecture\n\nThe main network (different layers are connected by PixelShuffle and PixelUnShuffle):\n\n\n\nThe proposed ResDB and ResGB used in main network:\n\n\nFile structure\nNTIRE 2020 Spectral Reconstruction Challenge\nâ”‚   README.md\nâ”‚   validation*.py\nâ”‚   test*.py\nâ”‚   ensemble*.py\nâ”‚\nâ””â”€â”€â”€track1 (saving the trained models of track1)\nâ”‚   â”‚   code1_G_epoch9000_bs8.pth\nâ”‚   â”‚   code1_second_G_epoch8000_bs8.pth\nâ”‚   â”‚   ...\nâ”‚\nâ””â”€â”€â”€track2 (saving the trained models of track2)\nâ”‚   â”‚   code1_bs2_G_epoch6000_bs2.pth\nâ”‚   â”‚   code2_G_epoch6000_bs8.pth\nâ”‚   â”‚   ...\n|\nâ””â”€â”€â”€NTIRE2020_Test_Clean\nâ”‚    â”‚   ARAD_HS_0468_clean.mat\nâ”‚    â”‚   ARAD_HS_0508_clean.mat\nâ”‚    â”‚   ...\nâ”‚\nâ””â”€â”€â”€NTIRE2020_Test_RealWorld\nâ”‚    â”‚   ARAD_HS_0477_RealWorld.mat\nâ”‚    â”‚   ARAD_HS_0502_RealWorld.mat\nâ”‚    â”‚   ...\nâ”‚\nâ””â”€â”€â”€test (will generate by test1.py or test2.py)\nâ”‚   â””â”€â”€â”€track1\nâ”‚       â”‚   ARAD_HS_0468_clean.mat\nâ”‚       â”‚   ARAD_HS_0508_clean.mat\nâ”‚       â”‚   ...\nâ”‚   â””â”€â”€â”€track2\nâ”‚       â”‚   ARAD_HS_0477_RealWorld.mat\nâ”‚       â”‚   ARAD_HS_0502_RealWorld.mat\nâ”‚       â”‚   ...\nâ”‚\nâ””â”€â”€â”€ensemble (will generate by ensemble1.py or ensemble2.py)\nâ”‚   â””â”€â”€â”€track1\nâ”‚       â”‚   ARAD_HS_0468_clean.mat\nâ”‚       â”‚   ARAD_HS_0508_clean.mat\nâ”‚       â”‚   ...\nâ”‚   â””â”€â”€â”€track2\nâ”‚       â”‚   ARAD_HS_0477_RealWorld.mat\nâ”‚       â”‚   ARAD_HS_0502_RealWorld.mat\nâ”‚       â”‚   ...\nâ”‚   \n\nRequirements\n\nPython 3.6\nPytorch 1.0.0\nCuda 8.0\n\nTrain\n\nRun train.py.\nChange baseroot that contains training data.\nChange save_path corresponding to track 1 or track 2.\nChange other parameters.\n\nTest\nNote that the data should be first generated from different models (please run test*.py). Then compute the average of all results by running ensemble_track*_8methods.py. Finally, all the results for both tracks are saved in ./ensemble/track1 and ./ensemble/track2.\ntrack 1 generation\n\nRun test1.py.\nIt will output 8 results of 8 networks.\n\ntrack 1 ensemble\n\nRun ensemble_track1_8methods.py.\nIt will output 1 ensemble result of 8 generated data.\n\ntrack 2 generation\n\nRun test2.py.\nIt will output 8 results of 8 networks.\n\ntrack 2 ensemble\n\nRun ensemble_track2_8methods.py.\nIt will output 1 ensemble result of 8 generated data.\n\nFor each track, we use the ""best"" epoch for ensemble:\n\nVisualize\n\nRun train_visualize.py or validation_visualize.py or test_visualize.py.\n\nGenerated spectral images\n\nTrack 1 comparison with other methods:\n\n\n\n\n\n* Track 2 comparison with other methods:\n\n\n\n\n* Track 1 all 31 bands (400nm - 700nm) of one image:\n\n* Track 2 all 31 bands (400nm - 700nm) of one image:\n\nGenerated infrared images\n\nLink to pre-trained models and testing results\n\nPre-trained models: OneDrive link. After downloading it, please put them to right folders.\nTesting results: OneDrive link\n\nReference\nIf you have any question, please do not hesitate to contact yzzhao2-c@my.cityu.edu.hk\nIf you find this code useful to your research, please consider citing:\n@inproceedings{zhao2020hierarchical,\n  title={Hierarchical Regression Network for Spectral Reconstruction from RGB Images},\n  author={Zhao, Yuzhi and Po, Lai-Man and Yan, Qiong and Liu, Wei and Lin, Tingyu},\n  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},\n  year={2020},\n}\n\nCVPRW poster\n\n'], 'url_profile': 'https://github.com/zhaoyuzhi', 'info_list': ['2', 'Python', 'Updated May 27, 2020', '100', 'Jupyter Notebook', 'Updated May 29, 2020', '84', 'Python', 'Updated Jan 31, 2021', '14', 'TypeScript', 'GPL-3.0 license', 'Updated Feb 3, 2021', '17', 'Julia', 'Updated Jan 7, 2021', '17', 'Jupyter Notebook', 'Updated Jan 7, 2021', '8', 'R', 'Updated Mar 24, 2020', '3', 'Updated Aug 16, 2020', '127', 'Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Shell', 'Updated Jun 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Projecting the transmission dynamics of SARS-CoV-2 through the post-pandemic period\nThis R code is the basis of the regression analysis presented in this manuscript, now available on medRxiv here.\nThe following datasets are included in this repo:\n\nCorona4PP_Nat.csv: Weekly percent testing positive for each coronavirus strain based on reports to the National Respiratory and Enteric Virus Surveillance System (NREVSS). This data (from Mar 2018 through Feb 2020) is publicly available on the CDC website here. Full data used in paper is available through a data use agreement with the CDC.\nILINet.csv: Weekly data from the U.S. Outpatient Influenza-like Illness Surveillance Network (ILINet), including the weekly number and percent of patients presenting with ILI. This data is available through the FluView Interactive dashboard here.\n\nThe code is organized in the following chunks:\n\nPrep: Download required packages and load in data.\nCleaning: Clean datasets and calculate weekly incidence proxy over the time period (percent of clinic visits for ILI multiplied by percent positive for each CoV strain).\nCalculate R: Use the Wallinga-Teunis method to estimate effective reproduction numbers (3-week moving geometric mean).\nRegression: Calculate depletion of susceptibles for each strain and perform regression.\n\nAcknowledgements: Many thanks to te Beest, et al. The code to calculate the effective reproduction numbers was modified from the code for their paper ""Driving factors of influenza transmission in the Netherlands"" (AJE 2013).\n'], 'url_profile': 'https://github.com/c2-d2', 'info_list': ['2', 'Python', 'Updated May 27, 2020', '100', 'Jupyter Notebook', 'Updated May 29, 2020', '84', 'Python', 'Updated Jan 31, 2021', '14', 'TypeScript', 'GPL-3.0 license', 'Updated Feb 3, 2021', '17', 'Julia', 'Updated Jan 7, 2021', '17', 'Jupyter Notebook', 'Updated Jan 7, 2021', '8', 'R', 'Updated Mar 24, 2020', '3', 'Updated Aug 16, 2020', '127', 'Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Shell', 'Updated Jun 2, 2020']}","{'location': 'Sydney', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tuanvnguyen', 'info_list': ['2', 'Python', 'Updated May 27, 2020', '100', 'Jupyter Notebook', 'Updated May 29, 2020', '84', 'Python', 'Updated Jan 31, 2021', '14', 'TypeScript', 'GPL-3.0 license', 'Updated Feb 3, 2021', '17', 'Julia', 'Updated Jan 7, 2021', '17', 'Jupyter Notebook', 'Updated Jan 7, 2021', '8', 'R', 'Updated Mar 24, 2020', '3', 'Updated Aug 16, 2020', '127', 'Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Shell', 'Updated Jun 2, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '375 contributions\n        in the last year', 'description': [""Machine Learning Projects\n  \n\nWhy this repository?\nâ€¢ The main purpose of making this repository is to keep all my Machine Learning projects at one place, hence keeping my GitHub clean!\nâ€¢ It looks good, isn't it?\nOverview\nâ€¢ This repository consists of all my Machine Learning projects.\nâ€¢ Datasets are provided in each of the folders above, and the solution to the problem statements as well.\nAlgorithms used\nRegression:\nâ€¢ Linear Regression\nâ€¢ Multiple-Linear Regression\nâ€¢ Logistic Regression\nâ€¢ Polynomial Regression\nâ€¢ Lasso and Ridge Regression (L1 & L2 Regularization)\nâ€¢ Elastic-Net Regression\nClassification:\nâ€¢ K-Nearest Neighbours\nâ€¢ Support Vector Machine\nâ€¢ Naive Bayes\nâ€¢ Decision Tree\nClustering:\nâ€¢ K-Means\nEnsemble:\nâ€¢ Random Forest\nâ€¢ Adaptive Boosting (AdaBoost)\nâ€¢ Extreme Gradient Boosting (XGBoost)\nâ€¢ Voting (Hard/Soft)\nDo â­ the repository, if it helped you in anyway.\n""], 'url_profile': 'https://github.com/anujvyas', 'info_list': ['2', 'Python', 'Updated May 27, 2020', '100', 'Jupyter Notebook', 'Updated May 29, 2020', '84', 'Python', 'Updated Jan 31, 2021', '14', 'TypeScript', 'GPL-3.0 license', 'Updated Feb 3, 2021', '17', 'Julia', 'Updated Jan 7, 2021', '17', 'Jupyter Notebook', 'Updated Jan 7, 2021', '8', 'R', 'Updated Mar 24, 2020', '3', 'Updated Aug 16, 2020', '127', 'Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Shell', 'Updated Jun 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['kabanero-regression-testing\nWork items and scripts for builds and test framework\nPrerequisuites\n\nIf running any yaml test scripts, install ansible\n\nHow to Run Tests\n\nClone this repository\n\ngit clone https://github.com/kabanero-io/kabanero-regression-testing.git\ncd kabanero-regression-testing\n\n\nLog in to OCP\n\noc login -u kubeadmin -p somepassword\nor\noc login --token=tometoken --server=https://api.mauler.os.fyre.ibm.com:6443\n\n\nRun maintest.sh\n\n./maintest.sh\n\nHow to add a test to this repository\n\nAdd a directory to icpa-build-and-test/tests\n\nmkdir kabanero-regression-testing/tests/mynewtest\n\n\n\nIn that directory create your new XXXX-mynewtest.sh and / or XXXX-mynewtest.yaml\n\nwhere XXXX is some numeric which orders the test sequence based on alphanumeric sort\n\n\n\nAny other required / support files should be placed in that directory, do not prefix with XXXX\n\n\nmaintest.sh will iterate through all the test directories and run each test\n\n\nTest Framework Project Structure\nâ”œâ”€â”€ tests                                     # Top level directory\nâ”‚   â””â”€â”€ testsuite-example                     # Testsuite directory\nâ”‚       â”œâ”€â”€ 00-example.sh                     # Testcase file .sh/yaml (Numbered in execution order)\nâ”‚       â”œâ”€â”€ ...                               # Testcase file .sh/yaml\nâ”‚       â””â”€â”€ 99-example.sh                     # Testcase file .sh/yaml\nâ””â”€â”€ build                                     # Top level output directory (git ignored)\n    â””â”€â”€ testsuite-example                     # Testsuite directory\n        â””â”€â”€ XX-example                        # Testcase directory (Same name as testcase, no file extension)\n            â”œâ”€â”€ PASSED.TXT/FAILED.TXT         # File name result\n            â”œâ”€â”€ output                        # OCP/Kabanero/misc testcase log directory\n            â””â”€â”€ results                       # Testcase output directory\n                â”œâ”€â”€ junit.html                # Junit results\n                â”œâ”€â”€ XX-example.stderr.txt     # Testcase standard error log\n                â””â”€â”€ XX-example.stdeout.txt    # Testcase standard out log\n\n'], 'url_profile': 'https://github.com/kabanero-io', 'info_list': ['2', 'Python', 'Updated May 27, 2020', '100', 'Jupyter Notebook', 'Updated May 29, 2020', '84', 'Python', 'Updated Jan 31, 2021', '14', 'TypeScript', 'GPL-3.0 license', 'Updated Feb 3, 2021', '17', 'Julia', 'Updated Jan 7, 2021', '17', 'Jupyter Notebook', 'Updated Jan 7, 2021', '8', 'R', 'Updated Mar 24, 2020', '3', 'Updated Aug 16, 2020', '127', 'Jupyter Notebook', 'Updated Sep 27, 2020', '1', 'Shell', 'Updated Jun 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\npoissonreg\n\n\n\n\n\npoissonreg enables the parsnip package to fit various types of Poisson\nregression models including ordinary generalized linear models, simple\nBayesian models (via rstanarm), and two zero-inflated Poisson models\n(via pscl).\nInstallation\nYou can install the released version of poissonreg from\nCRAN with:\ninstall.packages(""poissonreg"")\nInstall the development version from GitHub with:\nrequire(""devtools"")\ninstall_github(""tidymodels/poissonreg"")\nExample\nA log-linear model for categorical data analysis:\nlibrary(poissonreg)\n#> Loading required package: parsnip\n\n# 3D contingency table from Agresti (2007): \npoisson_reg() %>% \n  set_engine(""glm"") %>% \n  fit(count ~ (.)^2, data = seniors)\n#> parsnip model object\n#> \n#> Fit time:  6ms \n#> \n#> Call:  stats::glm(formula = count ~ (.)^2, family = stats::poisson, \n#>     data = data)\n#> \n#> Coefficients:\n#>               (Intercept)               marijuanayes  \n#>                    5.6334                    -5.3090  \n#>              cigaretteyes                 alcoholyes  \n#>                   -1.8867                     0.4877  \n#> marijuanayes:cigaretteyes    marijuanayes:alcoholyes  \n#>                    2.8479                     2.9860  \n#>   cigaretteyes:alcoholyes  \n#>                    2.0545  \n#> \n#> Degrees of Freedom: 7 Total (i.e. Null);  1 Residual\n#> Null Deviance:       2851 \n#> Residual Deviance: 0.374     AIC: 63.42\nContributing\nThis project is released with a Contributor Code of\nConduct.\nBy contributing to this project, you agree to abide by its terms.\n\n\nFor questions and discussions about tidymodels packages, modeling,\nand machine learning, please post on RStudio\nCommunity.\n\n\nIf you think you have encountered a bug, please submit an\nissue.\n\n\nEither way, learn how to create and share a\nreprex (a minimal, reproducible example),\nto clearly communicate about your code.\n\n\nCheck out further details on contributing guidelines for tidymodels\npackages and how to get\nhelp.\n\n\n'], 'url_profile': 'https://github.com/tidymodels', 'info_list': ['8', 'R', 'Updated Oct 28, 2020', '9', 'Python', 'GPL-3.0 license', 'Updated Dec 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '2', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '5', 'R', 'Updated Feb 26, 2021', '2', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '2', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '122 contributions\n        in the last year', 'description': ['Integrating Spatial Configuration into Heatmap Regression Based CNNs for Landmark Localization\nUsage\nThis example implements the networks of the papers Regressing Heatmaps for Multiple Landmark Localization Using CNNs and Integrating Spatial Configuration into Heatmap Regression Based CNNs for Landmark Localization.\nLook into the subfolders of this repository for individual examples and more details.\nYou need to have the MedicalDataAugmentationTool framework downloaded and in you PYTHONPATH for the scripts to work.\nIf you have problems/questions/suggestions about the code, write me a mail!\nTrain models\nRun the main.py files inside the example folders to train the network.\nTrain and test other datasets\nIn order to train and test on other datasets, modify the dataset.py files. See the example files and documentation for the specific file formats. Set the parameter save_debug_images = True in order to see, if the network input images are reasonable.\nCitation\nIf you use this code for your research, please cite our MIA paper or MICCAI paper:\n@article{Payer2019a,\n  title   = {Integrating Spatial Configuration into Heatmap Regression Based {CNNs} for Landmark Localization},\n  author  = {Payer, Christian and {\\v{S}}tern, Darko and Bischof, Horst and Urschler, Martin},\n  journal = {Medical Image Analysis},\n  volume  = {54},\n  year    = {2019},\n  month   = {may},\n  pages   = {207--219},\n  doi     = {10.1016/j.media.2019.03.007},\n}\n\n@inproceedings{Payer2016,\n  title     = {Regressing Heatmaps for Multiple Landmark Localization Using {CNNs}},\n  author    = {Payer, Christian and {\\v{S}}tern, Darko and Bischof, Horst and Urschler, Martin},\n  booktitle = {Medical Image Computing and Computer-Assisted Intervention - {MICCAI} 2016},\n  doi       = {10.1007/978-3-319-46723-8_27},\n  pages     = {230--238},\n  year      = {2016},\n}\n\n'], 'url_profile': 'https://github.com/christianpayer', 'info_list': ['8', 'R', 'Updated Oct 28, 2020', '9', 'Python', 'GPL-3.0 license', 'Updated Dec 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '2', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '5', 'R', 'Updated Feb 26, 2021', '2', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '2', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Practice\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['8', 'R', 'Updated Oct 28, 2020', '9', 'Python', 'GPL-3.0 license', 'Updated Dec 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '2', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '5', 'R', 'Updated Feb 26, 2021', '2', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '2', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KongRay', 'info_list': ['8', 'R', 'Updated Oct 28, 2020', '9', 'Python', 'GPL-3.0 license', 'Updated Dec 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '2', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '5', 'R', 'Updated Feb 26, 2021', '2', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '2', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Yogyakarta, Indonesia', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Covid 19 case prediction using regession\nThis is group assigment for Google Bangkit Program\nBy Hendra Darwintha, Reizkian Yesaya, Virgiawan Huda\nProblem\nSince December 2019 patient that got covid-19 keep increasing.\nWe want predict case number in the future based on previous data\nData Source\nhttps://data.world/markmarkoh/coronavirus-data\nhttps://www.kaggle.com/atilamadai/covid19/data - NOT USED\nPrerequisite\n\nPython 3.x\nNvidia CUDA Runtime 10.1, Download Here\nPip\n\nSetup\n    pip install -r requirements.txt\n    python main.py\n'], 'url_profile': 'https://github.com/hendradarwin', 'info_list': ['8', 'R', 'Updated Oct 28, 2020', '9', 'Python', 'GPL-3.0 license', 'Updated Dec 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '2', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '5', 'R', 'Updated Feb 26, 2021', '2', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '2', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['8', 'R', 'Updated Oct 28, 2020', '9', 'Python', 'GPL-3.0 license', 'Updated Dec 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '2', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '5', 'R', 'Updated Feb 26, 2021', '2', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '2', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '408 contributions\n        in the last year', 'description': ['\nfwildclusterboot\n\n\n\n\nThe fwildclusterboot package is an R port of STATAâ€™s\nboottest package.\nIt implements the fast wild cluster bootstrap algorithm developed in\nRoodman et al\n(2019) for\nregression objects in R. It currently works for regression objects of\ntype lm, felm and fixest from base R and the lfe and fixest\npackages.\nThe packageâ€™s central function is boottest(). It allows the user to\ntest two-sided, univariate hypotheses using a wild cluster bootstrap.\nImportantly, it uses the â€œfastâ€ algorithm developed in Roodman et al,\nwhich makes it feasible to calculate test statistics based on a large\nnumber of bootstrap draws even for large samples â€“ as long as the number\nof bootstrapping clusters is not too large.\nThe fwildclusterboot package currently supports multi-dimensional\nclustering and one-dimensional, two-sided hypotheses. It supports\nregression weights, multiple distributions of bootstrap weights, fixed\neffects, restricted (WCR) and unrestricted (WCU) bootstrap inference and\nsubcluster bootstrapping for few treated clusters (MacKinnon & Webb,\n(2018)).\nFor a quick introduction to the packageâ€™s key function, boottest(),\nplease follow this\nlink.\nThe boottest() function\nlibrary(fixest)\nlibrary(fwildclusterboot)\n\ndata(voters)\n\n# fit the model via fixest::feols(), lfe::felm() or stats::lm()\nfeols_fit <- feols(proposition_vote ~ treatment  + log_income | Q1_immigration + Q2_defense, data = voters)\n\n# bootstrap inference via boottest()\nfeols_boot <- boottest(feols_fit, clustid = c(""group_id1""), B = 9999, param = ""treatment"")\n\nsummary(feols_boot)\n#> boottest.fixest(object = feols_fit, clustid = c(""group_id1""), \n#>     param = ""treatment"", B = 9999)\n#>  \n#>  Observations: 300\n#>   Bootstr. Type: rademacher\n#>  Clustering: 1-way\n#>  Confidence Sets: 95%\n#>  Number of Clusters: 40\n#> \n#>        term estimate statistic p.value conf.low conf.high\n#> 1 treatment    0.079     4.123       0    0.039     0.118\nBenchmarks\nResults of timing benchmarks of boottest(), with a sample of\n(N = 50000), (k = 27) and one fixed effect of 25 groups (10\niterations each).\n\nBenchmark with one cluster of dimension (N_G = 40)\nBenchmark two clusters of dimensions (N_{G1}= 40),\n(N_{G2} = 20), (N_{G12} = 800)\n\n\nInstallation\nYou can install fwildclusterboot from CRAN or the development version\nfrom github by following the steps below:\n# from CRAN \ninstall.packages(""fwildclusterboot"")\n\n# dev version from github\n# note: installation requires Rtools\nlibrary(devtools)\ninstall_github(""s3alfisc/fwildclusterboot"")\n'], 'url_profile': 'https://github.com/s3alfisc', 'info_list': ['8', 'R', 'Updated Oct 28, 2020', '9', 'Python', 'GPL-3.0 license', 'Updated Dec 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '2', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '5', 'R', 'Updated Feb 26, 2021', '2', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '2', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'karachi, Pakistan', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['MACHINE LEARNING\n'], 'url_profile': 'https://github.com/Javeria-Arif', 'info_list': ['8', 'R', 'Updated Oct 28, 2020', '9', 'Python', 'GPL-3.0 license', 'Updated Dec 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '2', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '5', 'R', 'Updated Feb 26, 2021', '2', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '2', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['8', 'R', 'Updated Oct 28, 2020', '9', 'Python', 'GPL-3.0 license', 'Updated Dec 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '2', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '5', 'R', 'Updated Feb 26, 2021', '2', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '2', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Turin, Italy', 'stats_list': [], 'contributions': '144 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rinaldoclemente', 'info_list': ['8', 'R', 'Updated Oct 28, 2020', '9', 'Python', 'GPL-3.0 license', 'Updated Dec 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '2', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jun 13, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '5', 'R', 'Updated Feb 26, 2021', '2', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '2', 'Jupyter Notebook', 'Updated Mar 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'MATLAB', 'Updated Mar 26, 2020', '4', 'Jupyter Notebook', 'Updated Apr 16, 2020', '11', 'MATLAB', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['GTR\nGeneralized Tensor Regression for Hyperspectral Image Classification, TGRS, 2020\n'], 'url_profile': 'https://github.com/liuofficial', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'MATLAB', 'Updated Mar 26, 2020', '4', 'Jupyter Notebook', 'Updated Apr 16, 2020', '11', 'MATLAB', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Neural Regression Discontinuity (NeRD)\nNeural Regression Discontinuity (NeRD) is a framework for performing Regression Discontinuity estimation, a quasi-experimental method for causal inference in time-series, using Convolutional Long Short-Term Memory Networks.\nThis work builds off ""From Econometrics, With Love: Causes and Counterfactuals for Machine Learning"", presented at the SAP Conference for Machine Learning (CML) 2020, St. Leon Rot.\nReferences\nPapers\n[1] Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use\nInterpretable Models Instead (Rudin 2019): https://arxiv.org/pdf/1811.10154.pdf \n[2] Machine Learning & Econometrics: https://web.stanford.edu/class/ee380/Abstracts/140129-slides-Machine-Learning-and-Econometrics.pdf \n[3] Regression Discontinuity: https://www.princeton.edu/~davidlee/wp/RDDEconomics.pdf \n[4] Huber Loss: https://arxiv.org/abs/1906.03751\nData\n[3] Predict Future Sales: https://www.kaggle.com/c/competitive-data-science-predict-future-sales/ \nPackages and Tutorials\n[4] Google CausalImpact: https://github.com/tcassou/causal_impact \n[5] Causal Inference with Python: http://www.degeneratestate.org/posts/2018/Mar/24/causal-inference-with-python-part-1-potential-outcomes/ \n[6] Facebook Prophet: https://facebook.github.io/prophet/docs/quick_start.html\n'], 'url_profile': 'https://github.com/roccojhu', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'MATLAB', 'Updated Mar 26, 2020', '4', 'Jupyter Notebook', 'Updated Apr 16, 2020', '11', 'MATLAB', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['STK: a Small (Matlab/Octave) Toolbox for Kriging\nThis README file is part of\nSTK: a Small (Matlab/Octave) Toolbox for Kriging\nhttp://sourceforge.net/projects/kriging\nSTK is free software: you can redistribute it and/or modify it under\nthe terms of the GNU General Public License as published by the Free\nSoftware Foundation,  either version 3  of the License, or  (at your\noption) any later version.\nSTK is distributed  in the hope that it will  be useful, but WITHOUT\nANY WARRANTY;  without even the implied  warranty of MERCHANTABILITY\nor FITNESS  FOR A  PARTICULAR PURPOSE.  See  the GNU  General Public\nLicense for more details.\nYou should  have received a copy  of the GNU  General Public License\nalong with STK.  If not, see http://www.gnu.org/licenses/.\nGeneral information\nVersion:      See stk_version.m\nAuthors:      See AUTHORS.md file\nMaintainers:  Julien Bect julien.bect@centralesupelec.fr\nand Emmanuel Vazquez emmanuel.vazquez@centralesupelec.fr\nDescription:  The STK is a (not so) Small Toolbox for Kriging. Its\nprimary focus is on the interpolation/regression\ntechnique known as kriging, which is very closely related\nto Splines and Radial Basis Functions, and can be\ninterpreted as a non-parametric Bayesian method using a\nGaussian Process (GP) prior. The STK also provides tools\nfor the sequential and non-sequential design of\nexperiments. Even though it is, currently, mostly geared\ntowards the Design and Analysis of Computer Experiments\n(DACE), the STK can be useful for other applications\nareas (such as Geostatistics, Machine Learning,\nNon-parametric Regression, etc.).\nCopyright:    Large portions are Copyright (C) 2011-2014 SUPELEC\nand Copyright (C) 2015-2019 CentraleSupelec.\nSee individual copyright notices for more details.\nLicense:      GNU General Public License, version 3 (GPLv3).\nSee COPYING for the full license.\nURL:          http://sourceforge.net/projects/kriging\nOne toolbox, two flavours\nThe STK toolbox comes in two flavours:\n\nan ""all purpose"" release, which is suitable for use both with\nGNU Octave\nand with Matlab.\nan Octave package, for people who want to install and use STK as a\nregular Octave package.\n\nHint: if you\'re not sure about the version that you have...\n\nthe ""all purpose"" release has this file (README.md) and the stk_init\nfunction (stk_init.m) in the top-level directory,\nthe Octave package has a DESCRIPTION file in the top-level directory\nand this file in the doc/ subdirectory.\n\nQuick Start\nQuick start with the ""all purpose"" release (Matlab/Octave)\nDownload and unpack an archive of the ""all purpose"" release from the\nSTK project\nfile release system\non SourceForge.\nRun stk_init.m in either Octave or Matlab.\nAfter that, you should be able to run the examples located in the examples\ndirectory.  All of them are scripts, the file name of which starts with\nthe stk_example_ prefix.\nFor instance, type stk_example_kb03 to run the third example in the ""Kriging\nbasics"" series.\nQuick start with the Octave package release (Octave only)\nAssuming that you have a working Internet connection, typing pkg install -forge stk\n(from within Octave) will automatically download the latest STK package tarball from the\nOctave Forge\nfile release system\non SourceForge and install it for you.\nAlternatively, if you want to install an older (or beta) release, you can download\nthe tarball from either the STK project FRS or the Octave Forge FRS, and install it\nwith pkg install FILENAME.tar.gz.\nAfter that, you can load STK using pkg load stk.\nTo check that STK is properly loaded, try for instance stk_example_kb03 to run\nthe third example in the ""Kriging basics"" series.\nRequirements and recommendations\nCommon requirement\nYour installation must be able to compile C mex files.\nRequirements and recommendations for use with GNU Octave\nThe STK is tested to work with GNU Octave 3.8.2 or newer, but\nshould probably also work with Octave 3.8.0 and 3.8.1.\nOlder versions of Octave (<= 3.6.2) are no longer supported, and\nare known to contain bugs that prevent some STK functions from\nworking properly.\nRequirements and recommendations for use with Matlab\nThe STK works with Matlab R2007a or newer.\nThe Optimization Toolbox is recommended.\nThe Parallel Computing Toolbox is optional.\nContent\nBy publishing this toolbox, the  idea is to provide a convenient and\nflexible research tool for  working with kriging-based methods.  The\ncode of the  toolbox is meant to be  easily understandable, modular,\nand reusable.  By  way of illustration, it is very  easy to use this\ntoolbox for implementing the EGO algorithm [1].\nBesides, this toolbox  can serve  as a basis for  the implementation\nof  advanced algorithms such as Stepwise Uncertainty Reduction (SUR)\nalgorithms [2].\nThe toolbox consists of three parts:\n\n\nThe  first part is the  implementation of a  number of covariance\nfunctions, and tools to  compute covariance vectors and matrices.\nThe structure  of the STK  makes it possible  to use any  kind of\ncovariances:  stationary  or  non-stationary covariances,  aniso-\ntropic covariances, generalized  covariances, etc.\n\n\nThe  second part  is the implementation  of a REMAP  procedure to\nestimate the parameters of the covariance. This makes it possible\nto  deal with generalized  covariances and  to take  into account\nprior knowledge about the parameters of the covariance.\n\n\nThe third part consists of prediction procedures.  In its current\nform,  the STK has been optimized  to deal with  moderately large\ndata sets.\n\n\nReferences\n[1] D. R. Jones, M. Schonlau, and William J. Welch. Efficient global\noptimization of expensive black-box functions.  Journal of Global\nOptimization, 13(4):455-492, 1998.\n[2] J. Bect, D. Ginsbourger, L. Li, V. Picheny, and E. Vazquez.\nSequential design of computer experiments for the estimation of a\nprobability of failure.  Statistics and Computing, pages 1-21, 2011.\nDOI: 10.1007/s11222-011-9241-4.\nWays to get help, report bugs, ask for new features...\nUse the ""help"" mailing-list:\nkriging-help@lists.sourceforge.net\nhttps://sourceforge.net/p/kriging/mailman\nto ask for help on STK, and the ticket manager:\nhttps://github.com/stk-kriging/stk/issues\nto report bugs or ask for new features (do not hesitate to do so!).\n'], 'url_profile': 'https://github.com/stk-kriging', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'MATLAB', 'Updated Mar 26, 2020', '4', 'Jupyter Notebook', 'Updated Apr 16, 2020', '11', 'MATLAB', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Key Concepts of Supervised Learning\n\n\n\n\n'], 'url_profile': 'https://github.com/Develop-Packt', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'MATLAB', 'Updated Mar 26, 2020', '4', 'Jupyter Notebook', 'Updated Apr 16, 2020', '11', 'MATLAB', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Coimabtore', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/guruaathavanalu', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'MATLAB', 'Updated Mar 26, 2020', '4', 'Jupyter Notebook', 'Updated Apr 16, 2020', '11', 'MATLAB', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Bengalore', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AkshayAnvekar0707', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'MATLAB', 'Updated Mar 26, 2020', '4', 'Jupyter Notebook', 'Updated Apr 16, 2020', '11', 'MATLAB', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Regression\nRegression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables).\nSimple Linear Regression\nSimple Linear regression is the approximation of a linear model used to describe the relationship between two or more variables.\nDataset used is FuelConsumption of customer\nMulti Linear Regression\nMultiple linear regression (MLR) is to model the linear relationship between the independent variables and dependent variable.\nDataset used is FuelConsumption of customer\nNon Linear Regression\nNonlinear regression is a form of regression analysis in which observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables.\nDataset used is Telephone calls of customer\nPolynomial Regression\nPolynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial.\nDataset used is FuelConsumption of customer\n'], 'url_profile': 'https://github.com/HarishGuragol', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'MATLAB', 'Updated Mar 26, 2020', '4', 'Jupyter Notebook', 'Updated Apr 16, 2020', '11', 'MATLAB', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nikitavig', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'MATLAB', 'Updated Mar 26, 2020', '4', 'Jupyter Notebook', 'Updated Apr 16, 2020', '11', 'MATLAB', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Russia, Saint-Petersburg', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alexeyk500', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'MATLAB', 'Updated Mar 26, 2020', '4', 'Jupyter Notebook', 'Updated Apr 16, 2020', '11', 'MATLAB', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Regression\nReferences:\nhttps://www.hackerearth.com/de/practice/machine-learning/machine-learning-projects/python-project/tutorial/\nhttps://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n'], 'url_profile': 'https://github.com/chamarthisireesha', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Apache-2.0 license', 'Updated Mar 26, 2020', 'C#', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Updated Mar 24, 2020', '9', 'Java', 'Apache-2.0 license', 'Updated Feb 5, 2021', 'Updated Mar 28, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['regression\n'], 'url_profile': 'https://github.com/benardgodwin', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Apache-2.0 license', 'Updated Mar 26, 2020', 'C#', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Updated Mar 24, 2020', '9', 'Java', 'Apache-2.0 license', 'Updated Feb 5, 2021', 'Updated Mar 28, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SabrinM', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Apache-2.0 license', 'Updated Mar 26, 2020', 'C#', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Updated Mar 24, 2020', '9', 'Java', 'Apache-2.0 license', 'Updated Feb 5, 2021', 'Updated Mar 28, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bluck90', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Apache-2.0 license', 'Updated Mar 26, 2020', 'C#', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Updated Mar 24, 2020', '9', 'Java', 'Apache-2.0 license', 'Updated Feb 5, 2021', 'Updated Mar 28, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SumruNayir', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Apache-2.0 license', 'Updated Mar 26, 2020', 'C#', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Updated Mar 24, 2020', '9', 'Java', 'Apache-2.0 license', 'Updated Feb 5, 2021', 'Updated Mar 28, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'Columbus, OH', 'stats_list': [], 'contributions': '428 contributions\n        in the last year', 'description': [""\n\n\n\n\nPredicting House prices in King County, WA\nOverview\nThe King County housing challenges data scientists to create a model for predicting home prices. We used the available features in addition to some engineered features in this pursuit, ultimately identifying location as the primary feature determing the price of a home.\nRepository Navigation\nCode               : Modeling Notebook \nPresentation       : Slide Deck\n\nReadME Navigation\nData -\nModel -\nResults -\nRecommendations -\nFuture -\nProject Info -\nData\nThe dataset is obtained from Kaggle Housing Prediction. It includes just over 21,000 observations, each representing a home sold in King County, Washington, between May 2014 and 2015. The median price of the homes are $450,000, and 99% of the homes sold for less than $2 million.\n\nModeling\nFeatures\nContinuous\n\nPrice (target)\nsquare feet of living space\nsquare feet of lot\nfloors\neffective build: the number of years since the house was last renovated or built. Also, see heat map of house ages across King County; it is interesting to see the newer renovations/constructions closer to city center correlating with higher prices; gentrification?\n\n\nCategorical\n\nZipcode (one hot encoded; total of 70)\nCondition: 1 to 5 rating; an objective assessment of the cphysical condition of the home\nView: 0 to 4 rating; a subjective assessment of the view from the property\nWaterfront: boolean\n\n\n\nhas basement: boolean\n\nThese features were fitted using statmodels OLS.\nResults\nExplained variance: 80%\nWe recognized early on that location has a prominent role in the selling price of a home. Our model clarified this hypothesis through zipcode feature weights:\n\n\n\nZipcode\nMedian Price\nPremium\n\n\n\n\nBase\n$193,094.98\n1.00x\n\n\n98039\n$668,108.63\n3.46x\n\n\n98004\n$587,005.76\n3.04x\n\n\n98040\n$463,427.95\n2.40x\n\n\n98033\n$409,361.36\n2.12x\n\n\n\nThis visual shows the price (size of point) by zipcode (color) for the dataset. One can visually confirm that the selling price of many houses is higher in certain zipcodes\n\nA major feature of location is the view from the property. We discovered homes with a higher view rating are typically located on a waterfront or proximal to Seattle's central business district; these houses may provide visibility to downtown Seattle or another scene with high appeal. Additionally, the homes sell for more.\n\nThis scatter/heatmap displays the view-price relationship. The relative geography of King County can be inferred; large blue regions are bodies of water (Pugot sound), larger points are higher prices. Notice the high view ratings and prices along water and near city center.\n\nFuture\nTo derive even more accurate results, we'd like to expand the project with additional data, specifically housing prices over time, quality of schools, crime metrics. Additionally, we know the kitchen is often the selling point in a house, so kitchen features would provide even further insight on a home's sellability.\nProject Info\nContributors : Alphonso Woodbury\n               Joseph McHugh\n\nLanguages    : Python\nTools/IDE    : Anaconda, Colab\nLibraries    : pandas, matplotlib, statsmodels, sklearn\n\nDuration     : March 2020\nLast Update  : 06.08.2020\n\n""], 'url_profile': 'https://github.com/a-woodbury', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Apache-2.0 license', 'Updated Mar 26, 2020', 'C#', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Updated Mar 24, 2020', '9', 'Java', 'Apache-2.0 license', 'Updated Feb 5, 2021', 'Updated Mar 28, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['Logistic regression and Poisson regression\nYY<-c(0 ,0,1,1,1,2,0,0,1,2,\n      0 ,1,1,0,2,0,0,3,0,2,\n      1 ,0,0,0,0,0,4,0,0,1,\n      12,1,0,2,2,4,0,1,0,1)\n\nXX<-cbind(1,rep(c(0,0.1,0.2,0.3),each=10))\n\nnn<-c(15, 3, 9,12,13,13,16,11,11, 8,\n      6,14,12,10,14,12,14,14,10,12,\n      12,12,11,13,12,14,15,14,12, 6,\n      12,12,13, 8,12,13,13,13,12, 9)\n\nbeta<-rep(0,2)\nfor(iter in 1:100){\n  mu<-exp(XX%*%beta+log(nn))\n  vv<-mu\n  Delta<-1/mu\n  WW<-1/vv/(Delta)^2\n  UU<-t(XX)%*%diag(c(WW*Delta))%*%(YY-mu)\n  II<-t(XX)%*%diag(c(WW))%*%XX\n  beta<-beta+solve(II,UU)\n}\nbeta\n\nbeta<-rep(0,2)\n\nfor(iter in 1:100){\n  mu<-exp(XX%*%beta+log(nn))\n  vv<-mu\n  Delta<-1/mu\n  WW<-1/vv/(Delta)^2\n  zz<-XX%*%beta+c(Delta)*c(YY-mu)\n  beta<-solve(t(XX)%*%diag(c(WW))%*%XX,t(XX)%*%diag(c(WW))%*%zz)\n}\nbeta\n\nglm(YY~-1+XX+offset(log(nn)),family=poisson())\n\n##########\n\npp<-YY/nn\ntemp<-1/nn\n\nglm(cbind(YY,nn-YY)~-1+XX,family=binomial())\nglm(pp~-1+XX,weights=nn,family=binomial())\n\nbeta<-rep(0,2)\nfor(iter in 1:100){\n  mu<-exp(XX%*%beta)/(1+exp(XX%*%beta))\n  vv<-temp*mu*(1-mu)\n  Delta<-1/(mu*(1-mu))\n  WW<-1/vv/(Delta)^2\n  UU<-t(XX)%*%diag(c(WW*Delta))%*%(pp-mu)\n  II<-t(XX)%*%diag(c(WW))%*%XX\n  beta<-beta+solve(II,UU)\n}\nbeta\n\nbeta<-rep(0,2)\nfor(iter in 1:100){\n  mu<-exp(XX%*%beta)/(1+exp(XX%*%beta))\n  vv<-temp*mu*(1-mu)\n  Delta<-1/(mu*(1-mu))\n  WW<-1/vv/(Delta)^2\n  zz<-XX%*%beta+c(Delta)*c(pp-mu)\n  beta<-solve(t(XX)%*%diag(c(WW))%*%XX,t(XX)%*%diag(c(WW))%*%zz)\n}\nbeta\n\n'], 'url_profile': 'https://github.com/youyugithub', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Apache-2.0 license', 'Updated Mar 26, 2020', 'C#', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Updated Mar 24, 2020', '9', 'Java', 'Apache-2.0 license', 'Updated Feb 5, 2021', 'Updated Mar 28, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Gojira\n\n\n\nGojira is a record and replay based regression testing tool.\nFeatures\n\nRecord and Replay framework for single request-response scope executions, that goes beyond just recording http request and response data, by additionally enabling recording of any call, external to the jvm, and storing them against a single test-id.\nStart your JVM in 5 modes: PROFILE(for recording), TEST(when replaying), NONE and SERIALIZE(test de-serialization of recorded data), DYNAMIC(for request level mode setting).\njavax.servlet based Filter for capturing HTTP request(uri, headers, method, body, queryparams) and response(status code, headers, body) data.\nRequest sampling capabilities based on URI signature and time-based sampling.\nAnnotation based method interception support with Guice to capture method data - arguments before and after method execution, and return or exception data.\nCustom serialization handlers, compare handlers and hash handlers per method argument and return or exception data.\nIntermediate storage during recording in a BigQueue before flushing to data-store.\nInterfaces to plug-in data-store for storing recorded data.\nTest executors for running tests in replay mode.\nVery low overhead during NONE and PROFILE mode. TODO: Add metrics.\nAdding new mode called TRANSFORM which will provide hooks to client to modify existing profiled data to adhere to contract changes\n\nChangelog\nChangelog\nGetting Started\nSample Application\nUsers\nFlipkart\nContribution, Bugs and Feedback\nFor bugs, questions and discussions, please use Github Issues.\nFor contributions, please check Contributions\nLicense\nCopyright 2020 Flipkart Internet, pvt ltd.\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n'], 'url_profile': 'https://github.com/flipkart-incubator', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Apache-2.0 license', 'Updated Mar 26, 2020', 'C#', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Updated Mar 24, 2020', '9', 'Java', 'Apache-2.0 license', 'Updated Feb 5, 2021', 'Updated Mar 28, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SumruNayir', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Apache-2.0 license', 'Updated Mar 26, 2020', 'C#', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Updated Mar 24, 2020', '9', 'Java', 'Apache-2.0 license', 'Updated Feb 5, 2021', 'Updated Mar 28, 2020', 'HTML', 'Updated Apr 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sunilnaidu27', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Apache-2.0 license', 'Updated Mar 26, 2020', 'C#', 'Updated Mar 30, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Aug 12, 2020', 'Updated Mar 24, 2020', '9', 'Java', 'Apache-2.0 license', 'Updated Feb 5, 2021', 'Updated Mar 28, 2020', 'HTML', 'Updated Apr 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': [""alps-mef.edu.tr\nDecisionTree Regression\nÄ°n this repository I include my codes written on Python to seperate dataset according to its optimal X and Y values. Decision tree uses entropy values for each point in data and choose most pure coordinates for X and Y. Thus the most optimum coordinates found and data seperated as successful as possible.\nThis is also my 4.th grade engineering lesson's (Machine Learning) assignment. Thus repo includes assignment description and it's report for detailed information in it.\n""], 'url_profile': 'https://github.com/sefalp', 'info_list': ['Python', 'Updated Mar 28, 2020', 'Python', 'Updated Sep 7, 2020', 'Python', 'Updated Mar 23, 2020', 'Scala', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'Updated May 2, 2020', 'Updated Apr 6, 2020']}","{'location': 'Tel Aviv', 'stats_list': [], 'contributions': '971 contributions\n        in the last year', 'description': ['Linear_Regression\nA class designed to read datasets and perform linear regression\n'], 'url_profile': 'https://github.com/adanikel', 'info_list': ['Python', 'Updated Mar 28, 2020', 'Python', 'Updated Sep 7, 2020', 'Python', 'Updated Mar 23, 2020', 'Scala', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'Updated May 2, 2020', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Boston-Dataset\nLinear Regression\nIntroduction\n          Linear Regression is a statistical approach for modelling relationship between a dependent variable with a given set of independent variable\n\nBoston Dataset\n            I take the Housing dataset which contains information about different houses in Boston . There are 506 samples and 13 features in this dataset\n\nChallenge\n           To predict the Value of prices of the house using the given features.\n\nDependencies\n                Numpy\n                Pandas\n                matplotlib\n                seaborn\n                GridSearchCv\n                Linear_model\n                statsmodel\n                mean_squared_error\n\nGoal\n          Data Handling\n               Importing Data with Pandas\n               Cleaning Data\n               Exploring Data through Visualizations with Matplotlib\n          Data Analysis\n               Supervised Machine learning Techniques: LinearRegression + Predicting Results + Ploting the results + R value + mean_squared_error\n\n'], 'url_profile': 'https://github.com/saravanan8015', 'info_list': ['Python', 'Updated Mar 28, 2020', 'Python', 'Updated Sep 7, 2020', 'Python', 'Updated Mar 23, 2020', 'Scala', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'Updated May 2, 2020', 'Updated Apr 6, 2020']}","{'location': 'Bandung', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['dark-sky-stream\nLinear regression\n'], 'url_profile': 'https://github.com/perfect-blue', 'info_list': ['Python', 'Updated Mar 28, 2020', 'Python', 'Updated Sep 7, 2020', 'Python', 'Updated Mar 23, 2020', 'Scala', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'Updated May 2, 2020', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': [""reglin\nregression linÃ©aire\nPar Mathis POTEAU, Ã§a donne la rÃ©gression linÃ©aire d'une fonction affine\n""], 'url_profile': 'https://github.com/delincta', 'info_list': ['Python', 'Updated Mar 28, 2020', 'Python', 'Updated Sep 7, 2020', 'Python', 'Updated Mar 23, 2020', 'Scala', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'Updated May 2, 2020', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mlnewbie1996', 'info_list': ['Python', 'Updated Mar 28, 2020', 'Python', 'Updated Sep 7, 2020', 'Python', 'Updated Mar 23, 2020', 'Scala', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'Updated May 2, 2020', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aniqairfan998', 'info_list': ['Python', 'Updated Mar 28, 2020', 'Python', 'Updated Sep 7, 2020', 'Python', 'Updated Mar 23, 2020', 'Scala', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'Updated May 2, 2020', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression\n'], 'url_profile': 'https://github.com/Narendar145', 'info_list': ['Python', 'Updated Mar 28, 2020', 'Python', 'Updated Sep 7, 2020', 'Python', 'Updated Mar 23, 2020', 'Scala', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'Updated May 2, 2020', 'Updated Apr 6, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['Linear Regression using Gradient Decent by Shubham\nThis is an animation of Linear Regression, which uses gradient decent to make sure that you understand the underlying proncipals of Linear Regression.\nYou can see the website on https://linear-regression-visualization.netlify.app/\nFor YouTube Video, go to https://www.youtube.com/channel/UCErV65oPNiYu-uLNs_AmwaA\nMake sure to Credit:\nShubham Gupta, ( https://www.learningdrop.com )\n'], 'url_profile': 'https://github.com/ShubhamGupta-tch', 'info_list': ['Python', 'Updated Mar 28, 2020', 'Python', 'Updated Sep 7, 2020', 'Python', 'Updated Mar 23, 2020', 'Scala', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'Updated May 2, 2020', 'Updated Apr 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['COVID-dataset\nPolynomial Regression\n'], 'url_profile': 'https://github.com/saravanan8015', 'info_list': ['Python', 'Updated Mar 28, 2020', 'Python', 'Updated Sep 7, 2020', 'Python', 'Updated Mar 23, 2020', 'Scala', 'Updated Apr 5, 2020', 'Python', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'Updated May 2, 2020', 'Updated Apr 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Stat-style\nLinear Regression\n'], 'url_profile': 'https://github.com/DataRohanScientist', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Dec 28, 2020']}","{'location': 'Pakistan ', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['Linear-Regression\na data visualization of a linear regression model with housing dataset\n'], 'url_profile': 'https://github.com/munizaimran', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Dec 28, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['Machine-Learning-in-python\nLinear Regression, Logistic regression,Neural network from scratch in python\n'], 'url_profile': 'https://github.com/ArjunwadkarAjay', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Dec 28, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sjshilpa27', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Dec 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '175 contributions\n        in the last year', 'description': ['DTR\nDecision Tree regression\n'], 'url_profile': 'https://github.com/Prerna99-star', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Dec 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Vrajpatel18', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Dec 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '189 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prashan1', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Dec 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '113 contributions\n        in the last year', 'description': ['Regression-Algorithm\nLineer&Polynomic Regression algorithm\n'], 'url_profile': 'https://github.com/BurhanCabiroglu', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Dec 28, 2020']}","{'location': 'Amsterdam ', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nickrood', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Dec 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['#Avocado Prices\nHistorical data on avocado prices and sales volume in multiple US markets.\nThis data was downloaded from the Hass Avocado Board website in May of 2018 & compiled into a single CSV.\nI have data where temporal ordering matters ,for that reason recurrent networks are a great fit and easily outperform models that first flatten the temporal data.\nIn this notebook, I will predict the average price of avocado using time-series modeling techniques\nI wanted to calculate root mean squared error and  error distribution.\nTo use dropout with recurrent networks, I should use a time-constant dropout mask and recurrent dropout mask. These are built into Keras recurrent layers, so all I have to do is use the dropout and recurrent_dropout arguments of recurrent layers.Stacked RNNs provide more representational power than a single RNN layer.\n'], 'url_profile': 'https://github.com/lauracarpaciu', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 28, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Dec 28, 2020']}"
"{'location': 'Hyderabad', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vishal1408', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'JavaScript', 'Updated Mar 24, 2020', 'Java', 'Updated Apr 15, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MohammedRaheemP', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'JavaScript', 'Updated Mar 24, 2020', 'Java', 'Updated Apr 15, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '175 contributions\n        in the last year', 'description': ['Support-Vector-Machine\nSupport Vector Regression\n'], 'url_profile': 'https://github.com/Prerna99-star', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'JavaScript', 'Updated Mar 24, 2020', 'Java', 'Updated Apr 15, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['CA05: Logistic Regression and kNN\nA: Logistic Regression\nData Source: Cardiovascular Disease (CVD) kills more people than cancer flobally. A dataset of real heart patients collected from a 15 year heart study cohot is made available for this assignment. The dataset has 16 patient features. Note that none of the features include any Blood Test information.\nFeature definitions can be found at the following site: https://sleepdata.org/datasets/shhs/variables\nB: kNN\nData Source: The data is a subset of the IMDb dataset from the UCI Machine Learning Repository. The data contains 30 movies, including data for each movie across seven genres and their IMDB ratings.\nObjective: Replicate a recommender system on a small scare. Given the movies dataset, what are the 5 most similar movies to a movie query?\n'], 'url_profile': 'https://github.com/lizhyde5', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'JavaScript', 'Updated Mar 24, 2020', 'Java', 'Updated Apr 15, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sai2119', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'JavaScript', 'Updated Mar 24, 2020', 'Java', 'Updated Apr 15, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Finland', 'stats_list': [], 'contributions': '182 contributions\n        in the last year', 'description': ['This is a Next.js project bootstrapped with create-next-app.\nGetting Started\nFirst, run the development server:\nnpm run dev\n# or\nyarn dev\nOpen http://localhost:3000 with your browser to see the result.\nYou can start editing the page by modifying pages/index.js. The page auto-updates as you edit the file.\nLearn More\nTo learn more about Next.js, take a look at the following resources:\n\nNext.js Documentation - learn about Next.js features and API.\nLearn Next.js - an interactive Next.js tutorial.\n\nYou can check out the Next.js GitHub repository - your feedback and contributions are welcome!\nDeploy on ZEIT Now\nThe easiest way to deploy your Next.js app is to use the ZEIT Now Platform from the creators of Next.js.\nCheck out our Next.js deployment documentation for more details.\n'], 'url_profile': 'https://github.com/msand', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'JavaScript', 'Updated Mar 24, 2020', 'Java', 'Updated Apr 15, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['testapp\nCD - Regression Testing\nHow to run this application :\njava -jar ""jar_name"" ""UI_IP"" ""UI_PORT"" ""API_IP"" ""API_PORT""\njava -jar testapp.jar localhost 4200 localhost 8080\njava -cp testapp.jar com.org.testapp.StartHere localhost 4200 localhost 8080\n'], 'url_profile': 'https://github.com/D3OP', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'JavaScript', 'Updated Mar 24, 2020', 'Java', 'Updated Apr 15, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Tbilisi , Georgia', 'stats_list': [], 'contributions': '506 contributions\n        in the last year', 'description': ['Train/Load model\npython run.py --train True --lr 0.009 --state True\n\n'], 'url_profile': 'https://github.com/mysterio42', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'JavaScript', 'Updated Mar 24, 2020', 'Java', 'Updated Apr 15, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Simple Regression: OLS\n'], 'url_profile': 'https://github.com/dipinpdinesh', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'JavaScript', 'Updated Mar 24, 2020', 'Java', 'Updated Apr 15, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aghiles98', 'info_list': ['Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'JavaScript', 'Updated Mar 24, 2020', 'Java', 'Updated Apr 15, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jenny31094', 'info_list': ['Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'C++', 'MIT license', 'Updated May 13, 2020', 'R', 'Updated Mar 27, 2020', '1', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Pune,India', 'stats_list': [], 'contributions': '420 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sidrakshe28', 'info_list': ['Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'C++', 'MIT license', 'Updated May 13, 2020', 'R', 'Updated Mar 27, 2020', '1', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saeedfalowo', 'info_list': ['Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'C++', 'MIT license', 'Updated May 13, 2020', 'R', 'Updated Mar 27, 2020', '1', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['ML-regression\n'], 'url_profile': 'https://github.com/YiYuChen227', 'info_list': ['Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'C++', 'MIT license', 'Updated May 13, 2020', 'R', 'Updated Mar 27, 2020', '1', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Oleksiy05', 'info_list': ['Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'C++', 'MIT license', 'Updated May 13, 2020', 'R', 'Updated Mar 27, 2020', '1', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '179 contributions\n        in the last year', 'description': ['GP_SR\n'], 'url_profile': 'https://github.com/LDNN97', 'info_list': ['Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'C++', 'MIT license', 'Updated May 13, 2020', 'R', 'Updated Mar 27, 2020', '1', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['data_analytics\nCredit Risk using Logistic Regression\n'], 'url_profile': 'https://github.com/milanpandey108', 'info_list': ['Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'C++', 'MIT license', 'Updated May 13, 2020', 'R', 'Updated Mar 27, 2020', '1', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Indonesia', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': [""Support Vector Regression Package\nMySVR is a Support Vector Regression (SVR) package with multi-kernel feature. Written with a simple style, this package is suitable for anyone who wish to learn about SVR implementation in Python.\nTable of contents\n\nExample\nInstallation\nDependencies\nReferences\n\nExample\nExample of the package usage can be found in examples folder. Here an example of 1 dimensional case found in examples/1d_svr is given.\nStart by importing all required package:\nimport numpy as np\nfrom svr.SV import SVR\nimport matplotlib.pyplot as plt\nThe SVR package requires input variables X and its corresponding response y, therefore we define the inputs as:\nXsamp = np.array([0,0.5,0.25,0.75,0.125,0.625,0.375,0.875,0.0625,0.5625]).reshape(-1,1)  # The input should be nsamp x nvar\nYsamp = Xsamp * np.sin(Xsamp*np.pi)  # The response should be nsamp x 1\nmaxY = max(abs(Ysamp))  # For normalizing Y\nThe next step is to define the parameter dictionary details of the dictionary key is available in help(SVR):\nsvrinfo = dict()\nsvrinfo['x'] = Xsamp  # Input variables X\nsvrinfo['y'] = Ysamp/maxY  # Corresponding input response Y (normalized)\nsvrinfo['epsilon'] = 0.05  # Define the epsilon tube, this parameter is optional \nsvrinfo['optimizer'] = 'lbfgsb'  # Define optimizer, this parameter is optional\nsvrinfo['errtype'] = 'L2'  # Define metric for model training, this parameter is optional\nsvrinfo['kerneltype'] = ['gaussian','matern52']  # Define kernel type, in this case we use multiple kernel for demo. This parameter is optional \nTo create and train the model, simply feed the dictionary into the SVR:\nmodel = SVR(svrinfo, normalize=False)\nmodel.train()\nTo predict values, feed your input to the .predict() method:\nxplot = np.linspace(0,1,100).reshape(-1,1) # Create a set of prediction input\nypred = model.predict(xplot)\nFinally, plotting:\nplt.plot(xplot, ypred * maxY, 'k', label='Prediction')\nplt.plot(xplot, (ypred - model.svrinfo.epsilon) * maxY, 'r--', label='Epsilon -')\nplt.plot(xplot, (ypred + model.svrinfo.epsilon) * maxY, 'r--', label='Epsilon +')\nplt.scatter(Xsamp, Ysamp, c='b', marker='+', label='Samples')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\nThe result might looks like the following:\n\nInstallation\nMySVR works with Python 3, to install write:\npip install mysvr\n\nDependencies\nMySVR has the following dependencies:\n\nnumpy\nscipy\nmatplotlib\nsobolsampling\ncvxopt\n\nReference\n\nChang, M., & Lin, C. (2005). Leave-One-Out Bounds for Support Vector Regression Model Selection. Neural Computation, 17(5), 1188 1222. https://doi.org/10.1162/0899766053491869\nUQLab User manual for Support Vector Regression\nForrester, A., Sobester, A., & Keane, A. (2008). Engineering Design via Surrogate Modelling: A Practical Guide. John Wiley & Sons.\n\nAuthor\nKemas Zakaria and Ghifari Adam F\n""], 'url_profile': 'https://github.com/fazaghifari', 'info_list': ['Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'C++', 'MIT license', 'Updated May 13, 2020', 'R', 'Updated Mar 27, 2020', '1', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Bhubhaneshwar', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['Stock-Prediction\nStock prediction using Linear regression\nYet another short and crisp notebook showing the basics of how to use ML Algo on dataset to predict stock prices in near future.\n'], 'url_profile': 'https://github.com/Akeel3105', 'info_list': ['Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'C++', 'MIT license', 'Updated May 13, 2020', 'R', 'Updated Mar 27, 2020', '1', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Gurgaon, Haryana', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Puja115066', 'info_list': ['Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'C++', 'MIT license', 'Updated May 13, 2020', 'R', 'Updated Mar 27, 2020', '1', 'Python', 'MIT license', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '307 contributions\n        in the last year', 'description': [""I implement the linear regression from scratch in Octave.\nIt includes all the mathematics to understand the working of gradient descent.\nBy implementing this I clearly understand all the concepts of linear regression,\nwhich is the basis of machine learning.\nMOTIVATION.\nIn today's machine learning world there are a lot of libraries. Using them is very easy and very helpful to the industry level.\nBut, by using them you'll never understand the concepts behind the learning algorithms. That how they work actually. Because, if you don't go deeper then there is no catch in that, you eventually get bored and start unliking it as I do.\nSo, building from scratch is always fun and interesting to know the things behind it. Unless you don't like mathematics, then you are in the wrong field.\nGNU OCTAVE.\nOctave is an open-source programming language.\nIt turns out building machine learning algorithms from scratch is easy in this language. Because it is more math friendly and fast.\nIf you want to scale up. First, build in octave then go to other programming languages like Python.\nLINEAR REGRESSION.\nJust to make it simple, I categories the building of the machine learning model into 5 steps. This is a simple model of house price prediction.\nSTEP 1: Getting data.\nFirst, we collect data on a real-world example. In this problem, we have 27 examples of houses. The area in sqft., the number of rooms as features and prices as the labels.\nFeatures: The independent variables. (Input)\nLabels: The dependent variables. (Output)\nSTEP 2: Preprocessing.\nGenerally, there are various processes. But, in this particular problem, we only need feature scaling. In this process, we scale the value of the features in a finite range (-1 and 1).\nFORMULA:\nmu = mean(X);\ns = max(X) - min(X);\nX := (X - mu)./s;\n\nSTEP 3: Cost Function.\nThe cost function is the measure of how much our model is wrong. We use MSE (Mean squared error). Use square to ignoring negative values. Divide by 2m instead of m (m is the number of training examples). Because when calculating the derivative of error 2 cancels out.\nFORMULA:\nprediction = X*theta;\nerror = (prediction - y).^2;\nJ = (1/(2*m))*sum(error);\n\nTheta is an array of numbers of the size one greater than the number of features.\nSTEP 4: Gradient Descent.\nGradient descent is a legend algorithm for optimizing the cost function. It decreases the value of theta in accordance with minimizing the cost function.\nFORMULA:\nfor i=0:epochs\n\tprediction = X*theta;\n\ttheta = theta - alpha*(1/m)*sum((prediction - y).*X);\n\tj_history(i) = J(X,y,theta);\nend\n\nepochs are the num of times we want to repeat the training.\nj_history is the record of cost function over the iterations.\nPlotting j_history with the number of iterations gives a decreasing curve.\nSTEP 5: Prediction.\nOur theta is now modified by using it we can predict now. But first, we also scale the new features and then add 1 at the start of the new features array.\nFORMULA:\nprediction = X*theta;\n\nX here is array of new features including 1 at start.\nCONCLUSION.\nNow you know what to do in general ML problems. Let's get going with scratch. This post not explaining the exact mathematics. You may be confused. It is the only generalization of the process of making a machine learning model.\nTo better understanding go through the course of machine learning on Coursera.\nRESOURCE.\nMachine Learning course: https://www.coursera.org/learn/machine-learning\nTHANKS, AND KEEP CODING\n\n""], 'url_profile': 'https://github.com/AnujCodeZ', 'info_list': ['1', 'MATLAB', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/adia4', 'info_list': ['1', 'MATLAB', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pradepkaushik', 'info_list': ['1', 'MATLAB', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shreyshah02', 'info_list': ['1', 'MATLAB', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Blojak', 'info_list': ['1', 'MATLAB', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2021']}","{'location': 'Haryana', 'stats_list': [], 'contributions': '657 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nThis is Simple Regression Model\n'], 'url_profile': 'https://github.com/vikasdhiman0635', 'info_list': ['1', 'MATLAB', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['shafayazeem12-gmail.com\nAssignment 2 (simple linear regression)\n'], 'url_profile': 'https://github.com/shafay-azeem', 'info_list': ['1', 'MATLAB', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nishan191', 'info_list': ['1', 'MATLAB', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Sharing-Github\nI will be sharing small ML tutorials on basic datasets.\nThe tutorials might contain grammatical errors. However, the overall idea/intuition is there.\n'], 'url_profile': 'https://github.com/omarbak', 'info_list': ['1', 'MATLAB', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3,461 contributions\n        in the last year', 'description': ['\nnumpy-linreg\nLinear Regression with numpy only.\nInstallation\nThe numpy-linreg git repo is available as PyPi package\npip install numpy-linreg\npip install git+ssh://git@github.com/ulf1/numpy-linreg.git\n\nUsage\nRidge Regression\nimport numpy_linreg.ridge as ridge\nimport numpy_linreg.metrics as metrics\nbeta = ridge.lu(y, X)\nrmse = metrics.rmse(y, X, beta)\n\nOLS Regression\nimport numpy_linreg.ols as ols\nbeta = ols.lu(y, X)\nbeta = ols.pinv(y, X)\nbeta = ols.qr(y, X)\nbeta = ols.svd(y, X)\n\nCheck the examples folder for notebooks.\nCommands\nInstall a virtual environment\npython3.6 -m venv .venv\nsource .venv/bin/activate\npip3 install --upgrade pip\npip3 install -r requirements.txt\n\n(If your git repo is stored in a folder with whitespaces, then don\'t use the subfolder .venv. Use an absolute path without whitespaces.)\nPython commands\n\nJupyter for the examples: jupyter lab\nCheck syntax: flake8 --ignore=F401 --exclude=$(grep -v \'^#\' .gitignore | xargs | sed -e \'s/ /,/g\')\nUpload to PyPi with twine: python setup.py sdist && twine upload -r pypi dist/*\n\nClean up\nfind . -type f -name ""*.pyc"" | xargs rm\nfind . -type d -name ""__pycache__"" | xargs rm -r\nrm -r .pytest_cache\nrm -r .venv\n\nSupport\nPlease open an issue for support.\nContributing\nPlease contribute using Github Flow. Create a branch, add commits, and open a pull request.\n'], 'url_profile': 'https://github.com/ulf1', 'info_list': ['1', 'MATLAB', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Python', 'MIT license', 'Updated Feb 26, 2021']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/souravramos', 'info_list': ['Python', 'Updated Mar 28, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Oct 2, 2020', '2', 'Python', 'Updated May 2, 2020', '2', 'R', 'Updated Mar 24, 2020', '2', 'Shell', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'C', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/linhnguyen215538', 'info_list': ['Python', 'Updated Mar 28, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Oct 2, 2020', '2', 'Python', 'Updated May 2, 2020', '2', 'R', 'Updated Mar 24, 2020', '2', 'Shell', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'C', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Upvote\nPredicting Upvotes\nAn online question and answer platform has hired you as a data scientist to identify the best question authors on the platform. This identification will bring more insight into increasing the user engagement. Given the tag of the question, number of views received, number of answers, username and reputation of the question author, the problem requires you to predict the upvote count that the question will receive.\nEvaluation Metric\nThe evaluation metric for this competition is RMSE (root mean squared error)\n'], 'url_profile': 'https://github.com/PROFESSOR-PENGUIN', 'info_list': ['Python', 'Updated Mar 28, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Oct 2, 2020', '2', 'Python', 'Updated May 2, 2020', '2', 'R', 'Updated Mar 24, 2020', '2', 'Shell', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'C', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['CORONAVIRUS-PROBABILITY-DETECTION\nWorking on Coronavirus probability detection model from infected patients data (such as fever, bodypain,runny  nose, age , difficulty in breathing) using Logistic regression(Machine Learning) with PYTHON\n'], 'url_profile': 'https://github.com/amirshaikh19', 'info_list': ['Python', 'Updated Mar 28, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Oct 2, 2020', '2', 'Python', 'Updated May 2, 2020', '2', 'R', 'Updated Mar 24, 2020', '2', 'Shell', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'C', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'Lahore, Pakistan', 'stats_list': [], 'contributions': '229 contributions\n        in the last year', 'description': ['Linear Regression\nHere is my project of Employee - Salaries Prediction using Linear Regression (Machine Learning).\nAuthor\nYou can get in touch with me on my LinkedIn Profile:\nMuhammad Junaid Iqbal\n\n'], 'url_profile': 'https://github.com/thejunaidiqbal', 'info_list': ['Python', 'Updated Mar 28, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Oct 2, 2020', '2', 'Python', 'Updated May 2, 2020', '2', 'R', 'Updated Mar 24, 2020', '2', 'Shell', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'C', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['ISLET\nThis is the package for paper: ""ISLET: Fast and Optimal Low-Rank Tensor Regression via Importance Sketching"" by Anru Zhang, Yuetian Luo, Garvesh Raskutti and Ming Yuan (2020) SIAM Journal on Mathematics of Data Science.\nIntall the package\nYou can install it directly from GitHub through devtools:\nlibrary(devtools)\ndevtools::install_github(""yuetianluo/ISLET"")\nlibrary(ISLET)\nHow to use\nSee ""manual.pdf"" and ""ISLET-example.R"" for reference.\n'], 'url_profile': 'https://github.com/yuetianluo', 'info_list': ['Python', 'Updated Mar 28, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Oct 2, 2020', '2', 'Python', 'Updated May 2, 2020', '2', 'R', 'Updated Mar 24, 2020', '2', 'Shell', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'C', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': [""ALEX AutoML\nAutomated Learning Engine X (ALEX) by Datastreams AI facilitates automated machine learning for tabular data (classification and regression)\nThis package includes trial version of ALEX that supports batch CSV upload only. Please refer to the corresponding chapter for a complete list of limitations.\nPlease visit our website https://dstreams.ai/ or send us a message at contact@dstreams.ai if you would like to learn more or try the full version of ALEX.\nInstallation of this version of ALEX will require downloading of docker containers from hub.docker.com with a total size of  ~ 6GB. This might take some time with a slow internet connection. Please refer to the Hardware Requirements section for other system requirements.\nHow to install AutoML Engine from Docker images\nInstall Docker environment\nWindows\nDocker Desktop for Windows is the Community version of Docker for Microsoft Windows. Please see details at\nhttps://docs.docker.com/docker-for-windows/install/\nDocker Toolbox provides a way to use Docker on Windows systems that do not meet minimal system requirements for the Docker Desktop for Windows app. Please see details at https://docs.docker.com/toolbox/toolbox_install_windows/\nMacOS\nDocker Desktop for Mac is the Community version of Docker for Mac. You can download Docker Desktop for Mac from Docker Hub. Please see details at https://docs.docker.com/docker-for-mac/install/\nDocker Toolbox provides a way to use Docker on older Macs that do not meet minimal system requirements for Docker Desktop for Mac. Please see details at https://docs.docker.com/toolbox/toolbox_install_mac/\nLinux\nDocker Engine is available on a variety of Linux platforms. Please see details at https://docs.docker.com/engine/install/#server\nInstall Docker Compose tool. Please see details at https://docs.docker.com/compose/install/\nGet ALEX\nClone repository with Alex or download a zip archive and then unzip it to a local folder. A path to this folder will be used in the next steps.\nUsing git tool:\ngit clone https://github.com/dstreamsai/AutoML.git\n\nZip archive can be downloaded from the menu above or using the direct link below:\nhttps://github.com/dstreamsai/AutoML/archive/master.zip\n\nInstall ALEX\nWindows steps\nIn case of Docker Desktop installed run Docker Desktop application. When the whale icon in the status bar stays steady, Docker Desktop is up-and-running.\nIn case of Docker Toolbox installed run Docker Quickstart Terminal as Administrator. The terminal does several things to set up Docker Toolbox for you. When it is done, the terminal displays the $ prompt.\nNote:\nRunning of the PowerShell scripts is deprecated by Windows Security Policy by default. So run PowerShell.exe as Administrator and then execute the scripts.\nOpen PowerShell.exe as Administrator. Change directory to the directory with Alex (e.g. C:\\Docker\\alex):\ncd C:\\Docker\\alex\\AutoML\n\nTo start the cluster run 'start.ps1' script from Windows folder using the command below:\npowershell.exe -noprofile -executionpolicy bypass -file .\\Windows\\start.ps1\n\nWait for this output:\nCluster is up. You can access Alex GUI from the link below:\nhttps://<ip-address>/LinkWorkbench\n\nTo stop the cluster run 'stop.ps1' script from Windows folder:\npowershell.exe -noprofile -executionpolicy bypass -file .\\Windows\\stop.ps1\n\nWait for this output:\nCluster is stopped\n\nLinux steps\nChange directory to the directory with Alex (e.g. /opt/alex)\ncd /opt/alex/AutoML\n\nTo start the cluster run 'start.sh' script from Linux folder:\nchmod +x Linux/start.sh\nsudo ./Linux/start.sh\n\nWait for this output:\nCluster is up. You can access Alex GUI from the link below:\nhttps://<ip-address>/LinkWorkbench\n\nTo stop the cluster run 'stop.sh' script from Linux folder:\nchmod +x Linux/stop.sh\nsudo ./Linux/stop.sh\n\nWait for this output:\nCluster is stopped\n\nMacOS steps\nChange directory to the directory with Alex (e.g. /Users/<user>/Desktop/alex/AutoML)\ncd /Users/<user>/Desktop/alex/AutoML\n\nTo start the cluster run 'start.sh' script from MacOS folder:\nchmod +x MacOS/start.sh\n./MacOS/start.sh\n\nWait for this output:\nCluster is up. You can access Alex GUI from the link below:\nhttps://<ip-address>/LinkWorkbench\n\nTo stop the cluster run 'stop.sh' script from MacOS folder:\nchmod +x MacOS/stop.sh\n./MacOS/stop.sh\n\nWait for this output:\nCluster is stopped\n\nHow to use ALEX AutoML Engine\nUse the following credentials to access the Web interface of ALEX.\n\nLogin: alex\nPassword: alex\n\nPlease find short instruction at ALEX AI Project How To.docx\nHardware Requirements\nMinimal\n\n64-bit dual core CPU\n4 GB RAM\nAt least 15 GB of free hard disk space\n\n\nRecommended\n\n64-bit quad core CPU\n8 GB RAM\nAt least 15 GB of free hard disk space\n\nTrial copy limitations\nIn this trial version of the AutoML Engine it is possible to upload CSV file with the following limitations: the file size must be less than 100 MBytes, the number of rows must be less than 100000, the number of columns must be less than 4500.\nSingle CPU core is used during hyperparameters optimization phase.\nA user should also consider the following:\n\nIt is not recommended to restart/shutdown the containers when training process is ongoing.  A DB data may be corrupted.\nImproper shutdown procedure may corrupt data in the containers. Please use the scripts provided for containers' start/stop.\n\nPlease see Troubleshooting section for resolution steps.\nTroubleshooting\nThere are rare situations when containers cannot be restored correctly after hard shutdown or another type of container hard stop.\nIn case a container entered undetermined state and restart doesn't help it is recommended to re-create the setup using the commands below.\nWARNING\nAll created projects will be removed after the steps below\n\nFor Linux\\MacOS:\ncd /opt/alex/AutoML (for Linux)\ncd /Users/<user>/Desktop/alex/AutoML (for MacOS)\ndocker-compose kill\ndocker-compose rm\n\nConfirm removing of the containers and when containers are removed run the 'start.sh' script for your OS (from ./Linux or ./MacOS folders).\nFor Windows:\ncd C:\\Docker\\alex\\AutoML\ndocker-compose.exe stop\ndocker-compose.exe rm\n\nConfirm removing of the containers and when containers are removed run the 'start.ps1' script (use the full command from Windows steps above).\n""], 'url_profile': 'https://github.com/dstreamsai', 'info_list': ['Python', 'Updated Mar 28, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Oct 2, 2020', '2', 'Python', 'Updated May 2, 2020', '2', 'R', 'Updated Mar 24, 2020', '2', 'Shell', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'C', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['regressionLab\n'], 'url_profile': 'https://github.com/g8r-b8', 'info_list': ['Python', 'Updated Mar 28, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Oct 2, 2020', '2', 'Python', 'Updated May 2, 2020', '2', 'R', 'Updated Mar 24, 2020', '2', 'Shell', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'C', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'Berlin', 'stats_list': [], 'contributions': '1,701 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NurKeinNeid', 'info_list': ['Python', 'Updated Mar 28, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Oct 2, 2020', '2', 'Python', 'Updated May 2, 2020', '2', 'R', 'Updated Mar 24, 2020', '2', 'Shell', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'C', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['House_price-prediction\nPredicting House prices using linear regression and GBR(gradient boosting Regression) in Python\n'], 'url_profile': 'https://github.com/tarunkarthick', 'info_list': ['Python', 'Updated Mar 28, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'HTML', 'Updated Oct 2, 2020', '2', 'Python', 'Updated May 2, 2020', '2', 'R', 'Updated Mar 24, 2020', '2', 'Shell', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'C', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}"
"{'location': 'Staten Island, NY', 'stats_list': [], 'contributions': '199 contributions\n        in the last year', 'description': ['Pattern-Rec-Neural-Networks-Regression\nRegression (Linear, Multiple, Quadratic, Cubic, etc) Using Logistic Regression for Prediction\nRequirements:\n\nNumber of attribtues in dataset should not be less than 5 and the number of instances(rows) be no less than 300. Pick dependent variable\nDataset must be split into 3 subsets: approx training(70%), validation(20%), testing(10%).\nFor data preprocessing (onlt training and validation sets) you can use StandardScaler, MinMaxScaler, RobustScalar, Normalizer.\nUse gradient decent and sinusoidal activation function.\nUse fitting measure R-squared and normalized confusion matrix for testing set.\nVisualize all results with matplotlib and/or seaborn.\n\n'], 'url_profile': 'https://github.com/Gallo13', 'info_list': ['Jupyter Notebook', 'Updated May 22, 2020', 'MATLAB', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Mar 26, 2020', 'Updated Mar 29, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 29, 2020']}","{'location': 'Tunisia, Tunis', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Matlab-ML-basics\nA basic implementation of linear regression and logistic regression on Matlab.\nIntoductory exercices for machine learning.\nImplemented in matlab and inspired by the course of machine learning by Andrew Ng (Coursera).\nEvery exercice is explained step-by-step via comments.\n'], 'url_profile': 'https://github.com/Hmzbo', 'info_list': ['Jupyter Notebook', 'Updated May 22, 2020', 'MATLAB', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Mar 26, 2020', 'Updated Mar 29, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['Housing_model\nOptimised regression model for predicting housing prices\nThis is my first machine learning model.\nI used the California Housing Prices dataset from the STATlib repoistory as test and training data.\n'], 'url_profile': 'https://github.com/Saahil2000', 'info_list': ['Jupyter Notebook', 'Updated May 22, 2020', 'MATLAB', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Mar 26, 2020', 'Updated Mar 29, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '113 contributions\n        in the last year', 'description': ['Regression-elemental-sets\n'], 'url_profile': 'https://github.com/DrewNow', 'info_list': ['Jupyter Notebook', 'Updated May 22, 2020', 'MATLAB', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Mar 26, 2020', 'Updated Mar 29, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vibhagautam', 'info_list': ['Jupyter Notebook', 'Updated May 22, 2020', 'MATLAB', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Mar 26, 2020', 'Updated Mar 29, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bharathc346', 'info_list': ['Jupyter Notebook', 'Updated May 22, 2020', 'MATLAB', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Mar 26, 2020', 'Updated Mar 29, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '180 contributions\n        in the last year', 'description': [""House_Prices\nKaggle /  House Prices: Advanced Regression Techniques\nData fields\n\n\nSalePrice : The target variable that you're trying to predict\n\n\nMSSubClass : The building class / Identifies the type of dwelling involved in the sale\n20 : 1-STORY 1946 & NEWER ALL STYLES\n30 : 1-STORY 1945 & OLDER\n40 : 1-STORY W/FINISHED ATTIC ALL AGES\n45 : 1-1/2 STORY - UNFINISHED ALL AGES\n50 : 1-1/2 STORY FINISHED ALL AGES\n60 : 2-STORY 1946 & NEWER\n70 : 2-STORY 1945 & OLDER\n75 : 2-1/2 STORY ALL AGES\n80 : SPLIT OR MULTI-LEVEL\n85 : SPLIT FOYER\n90 : DUPLEX - ALL STYLES AND AGES\n120 : 1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n150 : 1-1/2 STORY PUD - ALL AGES\n160 : 2-STORY PUD - 1946 & NEWER\n180 : PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n190 : 2 FAMILY CONVERSION - ALL STYLES AND AGES\n\n\nMSZoning : The general zoning classification / MSZoning: Identifies the general zoning classification of the sale\nA\t: Agriculture\nC\t: Commercial\nFV\t: Floating Village Residential\nI\t: Industrial\nRH\t: Residential High Density\nRL\t: Residential Low Density\nRP\t: Residential Low Density Park\nRM\t: Residential Medium Density\n\n\nLotFrontage : Linear feet of street connected to property\n\n\nLotArea : Lot size in square feet\n\n\nStreet : Type of road access to property\nGrvl :\tGravel\nPave\t: Paved\n\n\nAlley : Type of alley access / Type of alley access to property\nGrvl\t: Gravel\nPave\t: Paved\nNA : No alley access\n\n\nLotShape : General shape of property\nReg : Regular\nIR1 : Slightly irregular\nIR2 : Moderately Irregular\nIR3 : Irregular\n\n\nLandContour : Flatness of the property\nLvl : Near Flat/Level\nBnk : Banked - Quick and significant rise from street grade to building\nHLS : Hillside - Significant slope from side to side\nLow : Depression\n\n\nUtilities : Type of utilities available\nAllPub : All public Utilities (E,G,W,& S)\nNoSewr : Electricity, Gas, and Water (Septic Tank)\nNoSeWa : Electricity and Gas Only\nELO : Electricity only\n\n\nLotConfig : Lot configuration\nInside\t: Inside lot\nCorner\t: Corner lot\nCulDSac : Cul-de-sac\nFR2 : Frontage on 2 sides of property\nFR3 : Frontage on 3 sides of property\n\n\nLandSlope : Slope of property\nGtl : Gentle slope\nMod : Moderate Slope\nSev : Severe Slope\n\n\nNeighborhood : Physical locations within Ames city limits\nBlmngtn : Bloomington Heights\nBlueste : Bluestem\nBrDale : Briardale\nBrkSide : Brookside\nClearCr : Clear Creek\nCollgCr : College Creek\nCrawfor : Crawford\nEdwards : Edwards\nGilbert : Gilbert\nIDOTRR : Iowa DOT and Rail Road\nMeadowV : Meadow Village\nMitchel : Mitchell\nNames : North Ames\nNoRidge : Northridge\nNPkVill : Northpark Villa\nNridgHt : Northridge Heights\nNWAmes : Northwest Ames\nOldTown : Old Town\nSWISU : South & West of Iowa State University\nSawyer : Sawyer\nSawyerW : Sawyer West\nSomerst : Somerset\nStoneBr : Stone Brook\nTimber : Timberland\nVeenker : Veenker\n\n\nCondition1 : Proximity to main road or railroad(various conditions)\nArtery :\tAdjacent to arterial street\nFeedr : Adjacent to feeder street\nNorm : Normal\nRRNn : Within 200' of North-South Railroad\nRRAn : Adjacent to North-South Railroad\nPosN : Near positive off-site feature--park, greenbelt, etc.\nPosA : Adjacent to postive off-site feature\nRRNe : Within 200' of East-West Railroad\nRRAe : Adjacent to East-West Railroad\n\n\nCondition2 : Proximity to main road or railroad(various conditions / if a second is present)\nArtery : Adjacent to arterial street\nFeedr : Adjacent to feeder street\nNorm : Normal\nRRNn : Within 200' of North-South Railroad\nRRAn : Adjacent to North-South Railroad\nPosN : Near positive off-site feature--park, greenbelt, etc.\nPosA : Adjacent to postive off-site feature\nRRNe : Within 200' of East-West Railroad\nRRAe : Adjacent to East-West Railroad\n\n\nBldgType : Type of dwelling\n1Fam : Single-family Detached\n2FmCon : Two-family Conversion; originally built as one-family dwelling\nDuplx : Duplex\nTwnhsE : Townhouse End Unit\nTwnhsI : Townhouse Inside Unit\n\n\nHouseStyle : Style of dwelling\n1Story : One story\n1.5Fin : One and one-half story: 2nd level finished\n1.5Unf : One and one-half story: 2nd level unfinished\n2Story : Two story\n2.5Fin : Two and one-half story: 2nd level finished\n2.5Unf : Two and one-half story: 2nd level unfinished\nSFoyer : Split Foyer\nSLvl : Split Level\n\n\nOverallQual : Rates the overall material and finish of the house quality\n10\t: Very Excellent\n9 : Excellent\n8 : Very Good\n7 : Good\n6 : Above Average\n5 : Average\n4 : Below Average\n3 : Fair\n2 : Poor\n1 : Very Poor\n\n\nOverallCond : Rates the overall condition of the house\n10 : Very Excellent\n9 : Excellent\n8 : Very Good\n7 : Good\n6 : Above Average\n5 : Average\n4 : Below Average\n3 : Fair\n2 : Poor\n1 : Very Poor\n\n\nYearBuilt : Original construction date\n\n\nYearRemodAdd : Remodel date (same as construction date if no remodeling or additions)\n\n\nRoofStyle : Type of roof\nFlat :\tFlat\nGable :\tGable\nGambrel :\tGabrel (Barn)\nHip :\tHip\nMansard :\tMansard\nShed :\tShed\n\n\nRoofMatl : Roof material\nClyTile : Clay or Tile\nCompShg : Standard (Composite) Shingle\nMembran : Membrane\nMetal : Metal\nRoll :\tRoll\nTar&Grv : Gravel & Tar\nWdShake : Wood Shakes\nWdShngl : Wood Shingles\n\n\nExterior1st : Exterior covering on house\nAsbShng : Asbestos Shingles\nAsphShn : Asphalt Shingles\nBrkComm : Brick Common\nBrkFace : Brick Face\nCBlock : Cinder Block\nCemntBd : Cement Board\nHdBoard : Hard Board\nImStucc : Imitation Stucco\nMetalSd : Metal Siding\nOther : Other\nPlywood : Plywood\nPreCast : PreCast\nStone : Stone\nStucco : Stucco\nVinylSd : Vinyl Siding\nWd Sdng : Wood Siding\nWdShing : Wood Shingles\n\n\nExterior2nd : Exterior covering on house (if more than one material)\nAsbShng : Asbestos Shingles\nAsphShn : Asphalt Shingles\nBrkComm : Brick Common\nBrkFace : Brick Face\nCBlock : Cinder Block\nCemntBd : Cement Board\nHdBoard : Hard Board\nImStucc : Imitation Stucco\nMetalSd : Metal Siding\nOther : Other\nPlywood : Plywood\nPreCast : PreCast\nStone : Stone\nStucco : Stucco\nVinylSd : Vinyl Siding\nWd Sdng : Wood Siding\nWdShing : Wood Shingles\n\n\nMasVnrType : Masonry veneer type\nBrkCmn : Brick Common\nBrkFace : Brick Face\nCBlock : Cinder Block\nNone :\tNone\nStone: Stone\n\n\nMasVnrArea: Masonry veneer area in square feet\n\n\nExterQual : Exterior material quality / Evaluates the quality of the material on the exterior\nEx : Excellent\nGd : Good\nTA : Average/Typical\nFa : Fair\nPo : Poor\n\n\nExterCond : Present condition of the material on the exterior / Evaluates the present condition of the material on the exterior\nEx : Excellent\nGd : Good\nTA : Average/Typical\nFa : Fair\nPo : Poor\n\n\nFoundation : Type of foundation\nBrkTil : Brick & Tile\nCBlock : Cinder Block\nPConc : Poured Contrete\nSlab : Slab\nStone : Stone\nWood :\tWood\n\n\nBsmtQual : Height of the basement / Evaluates the height of the basement\nEx : Excellent (100+ inches)\nGd : Good (90-99 inches)\nTA : Typical (80-89 inches)\nFa : Fair (70-79 inches)\nPo : Poor (<70 inches)\nNA :\tNo Basement\n\n\nBsmtCond : General condition of the basement / Evaluates the general condition of the basement\nEx : Excellent\nGd : Good\nTA : Typical - slight dampness allowed\nFa : Fair - dampness or some cracking or settling\nPo : Poor - Severe cracking, settling, or wetness\nNA : No Basement\n\n\nBsmtExposure: Walkout or garden level basement walls  /Refers to walkout or garden level walls\nGd :\tGood Exposure\nAv : Average Exposure (split levels or foyers typically score average or above)\nMn : Mimimum Exposure\nNo : No Exposure\nNA : No Basement\n\n\nBsmtFinType1 : Quality of basement finished area / Rating of basement finished area\nGLQ : Good Living Quarters\nALQ : Average Living Quarters\nBLQ : Below Average Living Quarters\nRec : Average Rec Room\nLwQ : Low Quality\nUnf : Unfinshed\nNA : No Basement\n\n\nBsmtFinSF1 : Type 1 finished square feet\n\n\nBsmtFinType2 : Quality of second finished area (if present) / Rating of basement finished area (if multiple types)\nGLQ : Good Living Quarters\nALQ : Average Living Quarters\nBLQ : Below Average Living Quarters\nRec : Average Rec Room\nLwQ : Low Quality\nUnf : Unfinshed\nNA : No Basement\n\n\nBsmtFinSF2 : Type 2 finished square feet\n\n\nBsmtUnfSF : Unfinished square feet of basement area\n\n\nTotalBsmtSF : Total square feet of basement area\n\n\nHeating : Type of heating\nFloor : Floor Furnace\nGasA\t: Gas forced warm air furnace\nGasW\t: Gas hot water or steam heat\nGrav\t: Gravity furnace\nOthW\t: Hot water or steam heat other than gas\nWall\t: Wall furnace\n\n\nHeatingQC: Heating quality and condition\nEx :\tExcellent\nGd\t: Good\nTA\t: Average/Typical\nFa\t: Fair\nPo\t: Poor\n\n\nCentralAir : Central air conditioning\nN\tNo\nY\tYes\n\n\nElectrical : Electrical system\nSBrkr : Standard Circuit Breakers & Romex\nFuseA : Fuse Box over 60 AMP and all Romex wiring (Average)\nFuseF : 60 AMP Fuse Box and mostly Romex wiring (Fair)\nFuseP : 60 AMP Fuse Box and mostly knob & tube wiring (poor)\nMix : Mixed\n\n\n1stFlrSF : First Floor square feet\n\n\n2ndFlrSF : Second floor square feet\n\n\nLowQualFinSF : Low quality finished square feet (all floors)\n\n\nGrLivArea : Above grade (ground) living area square feet\n\n\nBsmtFullBath : Basement full bathrooms\n\n\nBsmtHalfBath : Basement half bathrooms\n\n\nFullBath : Full bathrooms above grade\n\n\nHalfBath : Half baths above grade\n\n\nBedroom : Number of bedrooms above basement level / Bedrooms above grade (does NOT include basement bedrooms)\n\n\nKitchen : Number of kitchens / Kitchens above grade\n\n\nKitchenQual : Kitchen quality\nEx : Excellent\nGd : Good\nTA : Typical/Average\nFa : Fair\nPo : Poor\n\n\nTotRmsAbvGrd : Total rooms above grade (does not include bathrooms)\n\n\nFunctional : Home functionality rating (Assume typical unless deductions are warranted)\nTyp : Typical Functionality\nMin1 : Minor Deductions 1\nMin2 : Minor Deductions 2\nMod : Moderate Deductions\nMaj1 : Major Deductions 1\nMaj2 : Major Deductions 2\nSev : Severely Damaged\nSal : Salvage only\n\n\nFireplaces : Number of fireplaces\n\n\nFireplaceQu : Fireplace quality\nEx : Excellent - Exceptional Masonry Fireplace\nGd : Good - Masonry Fireplace in main level\nTA : Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\nFa : Fair - Prefabricated Fireplace in basement\nPo : Poor - Ben Franklin Stove\nNA : No Fireplace\n\n\nGarageType : Garage location\n2Types : More than one type of garage\nAttchd : Attached to home\nBasment : Basement Garage\nBuiltIn : Built-In (Garage part of house - typically has room above garage)\nCarPort : Car Port\nDetchd : Detached from home\nNA : No Garage\n\n\nGarageYrBlt : Year garage was built\n\n\nGarageFinish : Interior finish of the garage\nFin : Finished\nRFn : Rough Finished\nUnf : Unfinished\nNA : No Garage\n\n\nGarageCars : Size of garage in car capacity\n\n\nGarageArea : Size of garage in square feet\n\n\nGarageQual : Garage quality\nEx : Excellent\nGd : Good\nTA : Typical/Average\nFa : Fair\nPo : Poor\nNA : No Garage\n\n\nGarageCond : Garage condition\nEx : Excellent\nGd : Good\nTA : Typical/Average\nFa : Fair\nPo : Poor\nNA : No Garage\n\n\nPavedDrive : Paved driveway\nY : Paved\nP : Partial Pavement\nN : Dirt/Gravel\n\n\nWoodDeckSF : Wood deck area in square feet\n\n\nOpenPorchSF : Open porch area in square feet\n\n\nEnclosedPorch : Enclosed porch area in square feet\n\n\n3SsnPorch : Three season porch area in square feet\n\n\nScreenPorch : Screen porch area in square feet\n\n\nPoolArea : Pool area in square feet\n\n\nPoolQC : Pool quality\nEx\t: Excellent\nGd\t: Good\nTA : Average/Typical\nFa : Fair\nNA : No Pool\n\n\nFence : Fence quality\nGdPrv : Good Privacy\nMnPrv : Minimum Privacy\nGdWo : Good Wood\nMnWw : Minimum Wood/Wire\nNA\t: No Fence\n\n\nMiscFeature : Miscellaneous feature not covered in other categories\nElev : Elevator\nGar2 : 2nd Garage (if not described in garage section)\nOthr : Other\nShed : Shed (over 100 SF)\nTenC : Tennis Court\nNA : None\n\n\nMiscVal : $Value of miscellaneous feature\n\n\nMoSold : Month Sold (MM)\n\n\nYrSold : Year Sold (YYYY)\n\n\nSaleType : Type of sale\nWD : Warranty Deed - Conventional\nCWD : Warranty Deed - Cash\nVWD : Warranty Deed - VA Loan\nNew : Home just constructed and sold\nCOD : Court Officer Deed/Estate\nCon : Contract 15% Down payment regular terms\nConLw : Contract Low Down payment and low interest\nConLI : Contract Low Interest\nConLD : Contract Low Down\n\n\nSaleCondition : Condition of sale\nNormal : Normal Sale\nAbnorml : Abnormal Sale -  trade, foreclosure, short sale\nAdjLand : Adjoining Land Purchase\nAlloca : Allocation - two linked properties with separate deeds, typically condo with a garage unit\nFamily : Sale between family members\nPartial : Home was not completed when last assessed (associated with New Homes)\n\n\n""], 'url_profile': 'https://github.com/mizykk', 'info_list': ['Jupyter Notebook', 'Updated May 22, 2020', 'MATLAB', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Mar 26, 2020', 'Updated Mar 29, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 29, 2020']}","{'location': 'Seoul, South Korea', 'stats_list': [], 'contributions': '329 contributions\n        in the last year', 'description': ['dss12_reg_ops\nKBL OPS modeling - EDA, Linear Regression\n'], 'url_profile': 'https://github.com/loveactualry', 'info_list': ['Jupyter Notebook', 'Updated May 22, 2020', 'MATLAB', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Mar 26, 2020', 'Updated Mar 29, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Logistic-Regression\nMachine Learning Project 3: Logistic Regression\n'], 'url_profile': 'https://github.com/parameswar-kotari', 'info_list': ['Jupyter Notebook', 'Updated May 22, 2020', 'MATLAB', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Mar 26, 2020', 'Updated Mar 29, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 29, 2020']}","{'location': 'Mysore', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shiveshsrivastava', 'info_list': ['Jupyter Notebook', 'Updated May 22, 2020', 'MATLAB', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Mar 26, 2020', 'Updated Mar 29, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['King-County-House-Prices\nregression problem for house prices prediction. Look at The House_sales.ipynb\nIBM data analysis and Regression.\n'], 'url_profile': 'https://github.com/Masha-M-Stephen', 'info_list': ['Jupyter Notebook', 'Updated May 31, 2020', 'Python', 'Updated Mar 25, 2020', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'TypeScript', 'Updated Jul 20, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Stata', 'Updated Mar 29, 2020', '1', 'C++', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Srishti325', 'info_list': ['Jupyter Notebook', 'Updated May 31, 2020', 'Python', 'Updated Mar 25, 2020', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'TypeScript', 'Updated Jul 20, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Stata', 'Updated Mar 29, 2020', '1', 'C++', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['ThaiCovidForecasting\nTF2 regression of Covid-19 in Thailand\n'], 'url_profile': 'https://github.com/JackDeBuff', 'info_list': ['Jupyter Notebook', 'Updated May 31, 2020', 'Python', 'Updated Mar 25, 2020', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'TypeScript', 'Updated Jul 20, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Stata', 'Updated Mar 29, 2020', '1', 'C++', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '155 contributions\n        in the last year', 'description': ['Temperature-Predictor\nRegression model for maximum temperature prediction\n'], 'url_profile': 'https://github.com/HJ1X', 'info_list': ['Jupyter Notebook', 'Updated May 31, 2020', 'Python', 'Updated Mar 25, 2020', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'TypeScript', 'Updated Jul 20, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Stata', 'Updated Mar 29, 2020', '1', 'C++', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/mondata-dev', 'info_list': ['Jupyter Notebook', 'Updated May 31, 2020', 'Python', 'Updated Mar 25, 2020', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'TypeScript', 'Updated Jul 20, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Stata', 'Updated Mar 29, 2020', '1', 'C++', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '162 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Madhu2511995', 'info_list': ['Jupyter Notebook', 'Updated May 31, 2020', 'Python', 'Updated Mar 25, 2020', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'TypeScript', 'Updated Jul 20, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Stata', 'Updated Mar 29, 2020', '1', 'C++', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': [""employees-modeling\nmodeling employees' attrition using binary logistic regression\n""], 'url_profile': 'https://github.com/enoo24', 'info_list': ['Jupyter Notebook', 'Updated May 31, 2020', 'Python', 'Updated Mar 25, 2020', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'TypeScript', 'Updated Jul 20, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Stata', 'Updated Mar 29, 2020', '1', 'C++', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'Bangkok', 'stats_list': [], 'contributions': '687 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/finerbrighterlighter', 'info_list': ['Jupyter Notebook', 'Updated May 31, 2020', 'Python', 'Updated Mar 25, 2020', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'TypeScript', 'Updated Jul 20, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Stata', 'Updated Mar 29, 2020', '1', 'C++', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'London UK', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['MARS\nA C++ implementation of Multivariate Adaptive Regression Splines. This is\na semi-brute force search for interactions and non-linearities. It will give almost as good\nperformance as a neural network, but with much faster model evaluation runtimes.\nSome references:\n\nThere is a nice writeup here describing the method.\nThere is also a commercial package here.\nThe documentation for the R ""earth"" package is here.\nStephen Milborrow maintains an excellent resource here.\nAdditionally there is an module for to scikit-learn here.\n\nBuild Requirements\nEigen - The code has been tested with version 3.3.4.\n    sudo apt install -y libeigen3-dev\nGoogleTest - Unfortunately, the library is no longer\navailable pre-compiled on Ubuntu.\n    sudo apt install -y libgtest-dev cmake\n    cd /usr/src/gtest\n    sudo cmake CMakeLists.txt && sudo make\n    sudo cp *.a /usr/lib\npybind11 - Install using your python package manager\n    conda install -y pybind11\nBuild Instructions\nFor now the package simply uses a Makefile:\n    cd mars\n    make\n    make test # optional - build and run the unit tests\nEventually the plan is to provide a full installer with setup.py or similar.\nAn Example\nHere we train a linear model with a categorical interaction.\nimport numpy as np\nX      = np.random.randn(10000,2)\nX[:,1] = np.random.binomial(1, .5, size=len(X))\ny      = 2*X[:,0] + 3*X[:,1] + X[:,0]*X[:,1] + np.random.normal(size=len(X))\n\n# convert to column-major float\nX = np.array(X,order=\'F\',dtype=\'f\')\ny = np.array(y,dtype=\'f\')\n\n# Fit the earth model\nimport mars\nmodel = mars.fit(X, y, max_epochs=8, tail_span=0)\nB     = mars.expand(X, model) # expand the basis\nbeta  = np.linalg.lstsq(B,y,rcond=None)[0]\ny_hat = B @ beta\nDepending on the random seed, the result should look similar to this:\nmars.pprint(model,beta)\n    -0.003\n    +1.972 * X[0]\n    +3.001 * X[1]\n    +1.048 * X[0] * X[1]\n'], 'url_profile': 'https://github.com/aleon1138', 'info_list': ['Jupyter Notebook', 'Updated May 31, 2020', 'Python', 'Updated Mar 25, 2020', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'TypeScript', 'Updated Jul 20, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Stata', 'Updated Mar 29, 2020', '1', 'C++', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}","{'location': 'Pune India', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Housing-Price-Prediction\nPredicting housing prices using regularized regression\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them at a higher price. For the same purpose, the company has collected a data set  from the sale of houses in Australia. The data is provided in the CSV file below.\nThe goal of this case study is to identify the factors which are most significant in predicting house prices and how well are they able to describe the price of the house.\nUsing Ridge and Lasso regressions models were built and accuracy and other metrics were analysed and we finally concluded that Lasso despite giving slightly less accuracy gives us 43% less number of features which is a added advantage because we have the list of top 20 factors / features to be accounted for predicting house prices.\nPython was used to perform the analysis\n'], 'url_profile': 'https://github.com/surajbhala', 'info_list': ['Jupyter Notebook', 'Updated May 31, 2020', 'Python', 'Updated Mar 25, 2020', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'TypeScript', 'Updated Jul 20, 2020', 'Python', 'Updated Apr 16, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Stata', 'Updated Mar 29, 2020', '1', 'C++', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['kaggle-covid2020-regression\nrepository for covid2020 regression challenge at Kaggle\n'], 'url_profile': 'https://github.com/Shogo1030', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'Updated Jun 19, 2020', 'Python', 'GPL-3.0 license', 'Updated May 30, 2020', 'HTML', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/CarlosCusiter', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'Updated Jun 19, 2020', 'Python', 'GPL-3.0 license', 'Updated May 30, 2020', 'HTML', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020']}","{'location': 'United States of America', 'stats_list': [], 'contributions': '235 contributions\n        in the last year', 'description': ['Gaussian-Process-Regression\nImplemented GP Regression with Various Kernel Functions\n=================================\nFILE DESCRIPTIONS\n\nDATASET folder - 5 input csv files\nJUPYTER FILES folder - 2 ipynb files, 2 pdf files of the code\nProblem4a_sol.py - Solution for problem 4a\nProblem4b_sol.py - Solution for problem 4b\nReport - final report\nHW Problems.pdf - Other Homework Problems\nProblem4b_output.csv - csv file for output from problem 4b\n=================================\nLibraries Used\n=================================\nNumpy\nmatplotlib\npandas\nscikit-learn\npickle\nseaborn\nmath\ntime\n\n=================================\nRUN INSTRUCTION\n""BEFORE RUNNING ANY CODES, PLEASE ENSURE THEY ARE NOT ENABLED TO OVERWRITE EXISTING OUTPUT FILES ELSE CHANGE DIRECTORY OF CURRENT OUTPUT FILES TO MAKE SURE""\n\nMake sure directory structure is maintained\nMake sure all the libraries are installed\nRUN Problem4a_sol.py for problem 4a\nRUN Problem4b_sol.py for problem 4b\n\n#######\nIDEALLY RUN the 2 JUPYTER NOTEBOOK files as the code has been done on jupyter before exporting as .py file\n#######\n'], 'url_profile': 'https://github.com/adheeshc', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'Updated Jun 19, 2020', 'Python', 'GPL-3.0 license', 'Updated May 30, 2020', 'HTML', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '337 contributions\n        in the last year', 'description': ['Predicting the popularity of online news with article attributes\nAbstract\nDriven by the interest of understanding what makes online news popular, we explored regression and classification methods to predict shares (log-transformed) and popularity using a wide variety of features related to these articles. For regression, linear model (stepwise selected), shrinkage methods (Ridge and Lasso), dimension reduction methods (PCR and PLS), as well as nonlinear models (GAMs with natural or smoothing splines) were attempted. For classification, logistic model (stepwise selected), Lasso-penalized logistic model, GAM logistic regression (with smoothing splines), LDA, QDA, KNN, and tree-based methods (such as classification tree, random forests, and boosting) were evaluated. The most noticeable feature identified by all methods is the average number of shares of an average keyword (kw_avg_avg). The time of publication also plays a key role: articles published on the weekends tend to generate more shares thus become more popular. This is consistent with the entertainment and leisure positioning of the news agency (Mashable) whose articles were examined in this study.\nMethods\n\nRegression\n\nLinear regression using stepwise selection\nDimention reduction\n\nPrincipal Components Regression (PCR)\nPartial Least Squares (PLS)\n\n\nShrinkage\n\nRidge\nLasso\n\n\nNonlinear\n\nGeneralized Additive Model (GAM) with natural splines\nGAM with smoothing splines\n\n\n\n\nClassification\n\nLogistic regression\n\nLogistic regression using stepwise selection\nLasso-penalized logistic regression\nGAM logistic regression with smoothing splines\n\n\nLinear Discriminant Analysis (LDA)\nQuadratic Discriminant Analysis (QDA)\nK-Nearest Neighbors (KNN)\n\n\nTree-based methods\n\nDecision Tree\nRandom Forest\nBoosting\n\n\n\nReference\nK. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal.\n'], 'url_profile': 'https://github.com/seaireal', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'Updated Jun 19, 2020', 'Python', 'GPL-3.0 license', 'Updated May 30, 2020', 'HTML', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020']}","{'location': 'Princeton, NJ', 'stats_list': [], 'contributions': '335 contributions\n        in the last year', 'description': ['Confound regression models for ISC analysis\n\n\n\nThis repository accompanies a manuscript in preparation by Samuel A. Nastase, Asieh Zadbood, Meir Meshulam, Kenneth A. Norman, and Uri Hasson titled ""Confound regression models for intersubject correlation analysis."" We test the effects of a variety of confound models inspired by similar work in the resting-state fMRI literature (Ciric et al., 2017; Parkes et al., 2018) on intersubject correlation (ISC) analyses (Hasson et al,. 2004; Nastase et al., 2019a). We evaluate these models on naturalistic story-listening fMRI datasets obtained from the publicly-available Narratives collection (Nastase et al., 2019b).\nReferences\n\n\nCiric, R., Wolf, D. H., Power, J. D., Roalf, D. R., Baum, G. L., Ruparel, K., Shinohara, R. T., Elliott, M. A., Eickhoff, S. B., Davatzikos, C., Gur, R. C., Gur, R. E., Bassett, D. S., & Satterthwaite, T. D. (2017). Benchmarking of participant-level confound regression strategies for the control of motion artifact in studies of functional connectivity. NeuroImage, 154, 174-187. https://doi.org/10.1016/j.neuroimage.2017.03.020\n\n\nHasson, U., Nir, Y., Levy, I., Fuhrmann, G., & Malach, R. (2004). Intersubject synchronization of cortical activity during natural vision. Science, 303(5664), 1634-1640. https://doi.org/10.1126/science.1089506\n\n\nNastase, S. A., Gazzola, V., Hasson, U., & Keysers, C. (2019a). Measuring shared responses across subjects using intersubject correlation. Social Cognitive and Affective Neuroscience, 14(6), 667â€“685. https://doi.org/10.1093/scan/nsz037\n\n\nNastase, S. A., Liu, Y.-F., Hillman, H., Zadbood, A., Hasenfratz, L., Keshavarzian, N., Chen, J., Honey, C. J., Yeshurun, Y., Regev, M., Nguyen, M., Chang, C. H. C., Baldassano, C. B., Lositsky, O., Simony, E., Chow, M. A., Leong, Y. C., Brooks, P. P., Micciche, E., Choe, G., Goldstein, A., Halchenko, Y. O., Norman, K. A., & Hasson, U. (2019b). Narratives: fMRI data for evaluating models of naturalistic language comprehension. OpenNeuro, ds002345. https://doi.org/10.18112/openneuro.ds002345.v1.0.1\n\n\nParkes, L., Fulcher, B., YÃ¼cel, M., & Fornito, A. (2018). An evaluation of the efficacy, reliability, and sensitivity of motion correction strategies for resting-state functional MRI. NeuroImage, 171, 415-436. https://doi.org/10.1016/j.neuroimage.2017.12.073\n\n\n'], 'url_profile': 'https://github.com/snastase', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'Updated Jun 19, 2020', 'Python', 'GPL-3.0 license', 'Updated May 30, 2020', 'HTML', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020']}","{'location': 'Seattle, WA', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['breast-cancer-detection\nUsing logistic regression to detect breast cancer\n'], 'url_profile': 'https://github.com/divitjawa', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'Updated Jun 19, 2020', 'Python', 'GPL-3.0 license', 'Updated May 30, 2020', 'HTML', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Linear-Regression_Used-Car-Price-Prediction\nUsed Car Price Prediction using Linear regression\n'], 'url_profile': 'https://github.com/dipinpdinesh', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'Updated Jun 19, 2020', 'Python', 'GPL-3.0 license', 'Updated May 30, 2020', 'HTML', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['House-Price-Prediction\n'], 'url_profile': 'https://github.com/deepakmarichi', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'Updated Jun 19, 2020', 'Python', 'GPL-3.0 license', 'Updated May 30, 2020', 'HTML', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['This repository contains R scripts testing various regression models using Bayesian inference. Most modeling fit using Stan.\nRaw data available in ""data.csv""\nR script used available in ""marketing_example.R""\n'], 'url_profile': 'https://github.com/peifern', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'Updated Jun 19, 2020', 'Python', 'GPL-3.0 license', 'Updated May 30, 2020', 'HTML', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020']}","{'location': 'Bay Area, CA', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['Machine-Learning-Matlab-Ex05-RegularizedLinearRegression\n'], 'url_profile': 'https://github.com/marioOrtegaGarcia', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'Updated Jun 19, 2020', 'Python', 'GPL-3.0 license', 'Updated May 30, 2020', 'HTML', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['House-Price-Prediction\n'], 'url_profile': 'https://github.com/deepakmarichi', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['This repository contains R scripts testing various regression models using Bayesian inference. Most modeling fit using Stan.\nRaw data available in ""data.csv""\nR script used available in ""marketing_example.R""\n'], 'url_profile': 'https://github.com/peifern', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Bay Area, CA', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['Machine-Learning-Matlab-Ex05-RegularizedLinearRegression\n'], 'url_profile': 'https://github.com/marioOrtegaGarcia', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Amherst, MA', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HowardKim', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Brasil', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['COVID19 predictions\nThis project uses learning machine algorithms, and regression models, to predict the covid-19 pandemic effects around the word.\nTo use this, you need the installed Anaconda environment, python 3.6 and some libraries specified in the header of jupyter notebook.\nThe original dataset is especified on the jupyter file.\nWe also added a apresentation to describe the construction and process analisys model.\nGood Luck!\n'], 'url_profile': 'https://github.com/invaderZi', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Amazon-Food-review\nNLP, Regression, Classification, unsupervised learning, Text visualization\n'], 'url_profile': 'https://github.com/halidkarim', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Bay Area, CA', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': [""Linear Regression & Gradient Descent Algorithm\nLinear Regression and Gradient Descent Algorithms are implemented for single and multiple variables. Using linear regression to approximate the profit a food truck will make based on the city's population.\nCost Function\nThe cost function allows us to test the accuracy of our hypothesis by taking calculating the mean squared error or our hypothesis.\nOur main goal in Linear Regression is to minimize the value of our cost function.\n\nHypothesis\nOur hypothesis is our predicted function that will approximate our data.\n\nBatch Gradient Descent\nGradient descent allows us to estimate the parameters in our hypothesis function. This is done by taking the derivative of the cost function (gives us tangent line) which returns a direction to move down the cost function.\n\n""], 'url_profile': 'https://github.com/marioOrtegaGarcia', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['House-Prices-Advanced-Regression-Techniques\n'], 'url_profile': 'https://github.com/PakholkovArtem', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Rio de Janeiro', 'stats_list': [], 'contributions': '179 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lucasvascrocha', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Gurgaon, Haryana', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Puja115066', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Jun 2, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jul 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': [""SN Type Classifier and Redshift Regression\nA short description of the project.\nProject Organization\nâ”œâ”€â”€ README.md          <- The top-level README for developers using this project.\nâ”œâ”€â”€ data\nâ”‚\xa0\xa0 â”œâ”€â”€ external       <- Data from third party sources.\nâ”‚\xa0\xa0 â”œâ”€â”€ interim        <- Intermediate data.\nâ”‚\xa0\xa0 â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.\nâ”‚\xa0\xa0 â””â”€â”€ raw            <- The original, immutable data dump.\nâ”‚\nâ”œâ”€â”€ docs               <- A default Sphinx project; see sphinx-doc.org for details.\nâ”‚\nâ”œâ”€â”€ models             <- Trained and serialized models, model predictions, and model summaries.\nâ”‚\nâ”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\nâ”‚                         the creator's initials, and a short `-` delimited description, e.g.\nâ”‚                         `1.0-jqp-initial-data-exploration`.\nâ”‚\nâ”œâ”€â”€ references         <- Data dictionaries, manuals, and other explanatory materials.\nâ”‚\nâ”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc..\nâ”‚\xa0\xa0 â””â”€â”€ figures        <- Generated graphics and figures.\nâ”‚\nâ”œâ”€â”€ requirements.txt   <- The requirements for reproducing the analysis environment, e.g.\nâ”‚                         generated with `pip freeze > requirements.txt.`\nâ”‚\nâ”œâ”€â”€ src                <- Source code for this project.\nâ”‚\xa0\xa0 â”œâ”€â”€ __init__.py    <- Makes src a Python module.\nâ”‚   â”‚\nâ”‚\xa0\xa0 â”œâ”€â”€ data           <- Scripts to make datasets.\nâ”‚\xa0\xa0 â”‚\xa0\xa0 â”œâ”€â”€ make_dataset.py  <- Script to make input dataset to learn.\nâ”‚\xa0\xa0 â”‚\xa0\xa0 â””â”€â”€ cosmology.py     <- Script to compute distmod.\nâ”‚   â”‚\nâ”‚\xa0\xa0 â”œâ”€â”€ hsc             <- Scripts to load input data or model.\nâ”‚\xa0\xa0 â”‚\xa0\xa0 â”œâ”€â”€ dataset.py  <- Script to format input values.\nâ”‚   â”‚\xa0\xa0 â”œâ”€â”€ loader.py   <- Script to load dataset.\nâ”‚   â”‚\xa0\xa0 â””â”€â”€ model.py    <- Script to define DNN architecture.\nâ”‚   â”œâ”€â”€ hsc_redshift.py <- Script to train a redshift regression model, to predict redshift values,\nâ”‚   â”‚                      and to search hyper parameters of the model.\nâ”‚   â”œâ”€â”€ hsc_search.py   <- Script to search hyper parameters of SN classifier.\nâ”‚   â”œâ”€â”€ hsc_sn_type.py  <- Script to train sn classifier and predict for sn types.\n\n\nProject based on the cookiecutter data science project template. #cookiecutterdatascience\n\nRequirements\n\npython\ntensorflow-gpu == 1.13\ndm-sonnet == 1.23\nclick\nnumpy\nscipy\nscikit-learn == 0.21\npandas\npytables\njoblib\ntqdm\nmatplotlib\nseaborn\nmlflow\noptuna == 0.14.0\nAdaBound-Tensorflow (github repository)\n\nWe recommend anaconda to\nset up an environment.\nYou can install the requirements with the following commands.\nconda create -n crest python click numpy scipy scikit-learn=0.21 pandas \\\n    pytables joblib tqdm matplotlib seaborn tensorflow-gpu=1.13\nconda activate crest\npip install dm-sonnet==1.23 mlflow optuna==0.14.0\n\n# AdaBound-Tensorflow is registered as a submodule of our program.\ngit submodule update -i\n\nor\n\ncd references\ngit clone https://github.com/taki0112/AdaBound-Tensorflow.git\nData Preparation\nYou need to make input data (HDF5 format) to learn a classifier/regression model.\nThe input data (HDF5 format) are made from the two tables (csv files);\none is flux data table and the other is meta data table. The structures of the tables are as follows:\nFlux data table\n\n\n\nobject_id\nmjd\npassband\nindex\nflux\nflux_err\n\n\n\n\nname1\n59751\nu\n0\n38.649\n0.4791\n\n\nname1\n59758\nu\n1\n46.387\n1.0287\n\n\n:\n:\n:\n:\n:\n:\n\n\nname100\n59749\nY\n6\n0.3974\n0.6411\n\n\n:\n:\n:\n:\n:\n:\n\n\n\nMeta data table\n\n\n\nobject_id\nsn_type\nredshift\n\n\n\n\nname1\nIa\n1.23\n\n\nname2\nIb\n0.89\n\n\n:\n:\n:\n\n\nname100\nIb\n1.11\n\n\n:\n:\n:\n\n\n\nYou need to save the tables as csv (comma separated values) format.\nA compressed file of sample dataset (Simdataset_HSC_sample.tar.gz) is available. It contains a flux data table and meta data table of 100,000 simulated supernovae for the classification of the HSC survey data.\nThe SN types are Ia, Ib, Ic, Ibc, II, IIL, IIN, or IIP.\nCommand\nYou convert the csv files into hdf5 files by the following command.\n# Making training dataset of HSC\npython data/convert_dataset.py hsc \\\n    --data=../data/raw/data.csv --metadata=../data/raw/metadata.csv \\\n    --output-path=../data/processed/sim_sn_data.h5 --data-type=train\n\n# Making test dataset of HSC\npython data/convert_dataset.py hsc \\\n    --data=../data/raw/data_test.csv --metadata=../data/raw/metadata_test.csv \\\n    --output-path=../data/processed/hsc_data.h5 --data-type=test\n\nThe names of the csv files and the hdf5 files are chosen arbitrarily.\nPLAsTiCC\nThe input data dimensions of all samples must be same.\nYou need to extract the data properly.\nThe data format of the csv files are same as the original data.\n(e.g. the values of passband are digits.)\nThe true labels of the test data are needed to evaluate the test accuracy.\n(You can download the dataset from here.)\n# Making training dataset of PLAsTiCC\npython data/convert_dataset.py plasticc \\\n    --data=../data/raw/training_set_extracted.csv --metadata=../data/raw/training_set_metadata.csv \\\n    --output-path=../data/processed/training_cosmos.h5 --data-type=train\n\n# Making test dataset of PLAsTiCC\npython data/convert_dataset.py plasticc \\\n    --data=../data/raw/test_set_extracted.csv --metadata=../data/raw/plasticc_test_metadata.csv \\\n    --output-path=../data/processed/test_cosmos.h5 --data-type=test\n\nSN type classifier\nTraining (HSC)\nThis classifier supports two classification tasks.\nOne is binary classification and the other is 3-class classification.\nThe binary classification task classifies the samples as:\n\nclass 0: Ia,\nclass 1: others (Ib, Ic, Ibc, II, IIL, IIN, IIP).\n\nThe 3-class classification task classifies the samples as:\n\nclass 0: Ia,\nclass 1: Ib Ic Ibc,\nclass 2: II IIL IIN IIP.\n\nUsage\npython hsc_sn_type.py fit-hsc \\\n    --sim-sn-path=../data/processed/sim_sn_data.h5 \\\n    --hsc-path=../data/processed/hsc_data.h5 \\\n    --model-dir=../models/result1 --seed=0 \\\n    --n-highways=3 --hidden-size=500 --drop-rate=1e-2 \\\n    --patience=200 --batch-size=1000 --norm \\\n    --activation=sigmoid \\\n    --input1=absolute-magnitude --input2=scaled-flux \\\n    --optimizer=adam --adabound-final-lr=1e-1 --lr=1e-3 \\\n    --eval-frequency=20 --mixup=mixup --fold=-1 --cv=5 --threads=4 \\\n    --binary --remove-y --use-batch-norm --use-dropout\n\n\n\n\nOption name\nDescription\nValue type / Choices\n\n\n\n\nsim-sn-path\nFile path of the training dataset\nstring\n\n\nhsc-path\nFile path of the test dataset\nstring\n\n\nmodel-dir\nOutput directory to save the learned models and prediction results\nstring\n\n\nseed\nRandom seed\nint\n\n\nn-highways\nThe number of Highway layers\nint\n\n\ndrop-rate\nThe ratio of dropout layer\nfloat\n\n\npatience\nThe model decides to finish to learn every patience epochs.\nint\n\n\nnorm\nThe flag to normalize the input values\n-\n\n\nactivation\nThe types of activation layer\nrelu, sigmoid, tanh, or identical\n\n\ninput1\nThe type of the main input values\nmagnitude or absolute-magnitude\n\n\ninput2\nThe type of the sub input values\nnone or scaled-flux\n\n\noptimizer\nThe type of optimizer\nmomentum, adam, amsbound, or adabound\n\n\nadabound-final-lr\nThe final learning rate. This is available when optimizer is adabound or amsbound\nfloat\n\n\nlr\nLearning rate\nfloat\n\n\neval-frequency\nEvaluate test dataset every eval-frequency epochs.\nint\n\n\nmixup\nUse mixup or not\nmixup or none\n\n\nfold\nThe index of the folds of cross validation. If 'fold' is -1, all folds are target\nint\n\n\nthreads\nThe number of threads\nint\n\n\ncv\nThe number of folds of cross validation\nint\n\n\nbinary / multi\nbinary for binary classification, multi for 3-class classification\n-\n\n\nremove-y\nThe flag to remove Y band or not\n-\n\n\nuse-batch-norm\nThe flag to use batch normalization layer or not\n-\n\n\nuse-dropout\nThe flag to use dropout layer or not\n-\n\n\n\nTraining (PLAsTiCC)\nUsage\npython hsc_sn_type.py fit-plasticc \\\n    --sim-sn-path=../data/processed/sim_sn_data.h5 \\\n    --training-cosmos-path=../data/processed/training_cosmos.h5 \\\n    --test-cosmos-path=../data/processed/test_cosmos.h5  \\\n    --model-dir=../models/result1 --seed=0 \\\n    --n-highways=3 --hidden-size=500 --drop-rate=1e-2 \\\n    --patience=200 --batch-size=1000 --norm \\\n    --activation=sigmoid \\\n    --input1=absolute-magnitude --input2=scaled-flux \\\n    --optimizer=adam --adabound-final-lr=1e-1 --lr=1e-3 \\\n    --eval-frequency=20 --mixup=mixup --fold=-1 --cv=5 --threads=4 \\\n    --binary --use-batch-norm --use-dropout\n\nThe options are almost same with the case in training HSC dataset.\nhsc-path and remove-y are unavailable.\ntraining-cosmos-path and test-cosmos-path are newly added.\n\n\n\nOption name\nDescription\nValue type / Choices\n\n\n\n\ntraining-cosmos-path\nFile path of the training dataset derived from PLAsTiCC\nstring\n\n\ntest-cosmos-path\nFile path of the test dataset derived from PLAsTiCC\nstring\n\n\n\nPrediction\nThe classifier predicts the class of input data with the trained model.\nUsage\npython hsc_sn_type.py predict \\\n    --data-path=../data/processed/hsc_data.h5 \\\n    --model-dir=../models/result1/0 \\\n    --data-type=HSC --output-name=prediction.csv\n\n\n\n\nOption name\nDescription\nValue type / Choices\n\n\n\n\ndata-path\nFile path of the dataset to predict\nstr\n\n\nmodel-dir\nDirectory that the trained model is in\nstr\n\n\ndata-type\nSimSN for training dataset, HSC for test dataset\nSimSN, HSC, or PLAsTiCC\n\n\noutput-name\nFile name to output the predicted results (the file is created in model-dir)\nstr\n\n\n\nThe first line of the output file show the class IDs.\nIn binary classification case (this means you used the option --binary to train the model),\nthe relations between the class IDs and sn types are as follows,\n\nclass 0: Ia,\nclass 1: the other sn types.\n\nIn multi classification case (the option is --multi),\nthe relationship is as follows,\n\nclass 0: Ia,\nclass 1: Ib Ic Ibc,\nclass 2: II IIL IIN IIP.\n\nThe predicted values are logits.\nA larger value means a higher probability to belong to the class.\nIf you apply the softmax function, you can interpret the output values as probabilities.\nHyper-parameter search of classifier model\nNote that it takes several days to optimize the hyper-parameters.\npython hsc_search.py search-hsc \\\n    --sim-sn-path=../data/processed/sim_sn_data.h5 \\\n    --hsc-path=../data/processed/hsc_data.h5 \\\n    --model-dir=../models/result1 --seed=0 \\\n    --patience=200 --batch-size=1000 --norm \\\n    --input1=absolute-magnitude --input2=scaled-flux \\\n    --optimizer=adam --adabound-final-lr=1e-1 --lr=1e-3 \\\n    --eval-frequency=20 --mixup=mixup --threads=4 \\\n    --binary --remove-y --n-trials=100\n\n\n\n\nOption name\nDescription\nValue type / Choices\n\n\n\n\nn-trials\nThe number of search\nint\n\n\n\nRedshift regression\nThis is a regression task to predict the redshift from the flux\nThis regression task is more difficult than the classification task.\nTherefore, it is recommended to use a larger model.\nUsage\npython hsc_redshift.py learn \\\n    --sim-sn-path=../data/processed/sim_sn_data.h5 \\\n    --hsc-path=../data/processed/hsc_data.h5 \\\n    --model-dir=../models/result1 --seed=0 \\\n    --n-highways=3 --hidden-size=500 --drop-rate=1e-2 \\\n    --patience=200 --batch-size=1000 --norm \\\n    --activation=sigmoid \\\n    --input1=magnitude --input2=scaled-flux \\\n    --optimizer=adam --adabound-final-lr=1e-1 --lr=1e-3 \\\n    --eval-frequency=20 --fold=-1 --cv=5 --threads=4 \\\n    --remove-y --use-batch-norm --target-redshift\n\n\n\n\nOption name\nDescription\nValue type / Choices\n\n\n\n\nsim-sn-path\nFile path of the training dataset\nstring\n\n\nhsc-path\nFile path of the test dataset\nstring\n\n\nmodel-dir\nOutput directory to save the learned models and prediction results\nstring\n\n\nseed\nRandom seed\nint\n\n\nn-highways\nThe number of Highway layers\nint\n\n\ndrop-rate\nThe ratio of dropout layer\nfloat\n\n\npatience\nThe model decides to finish to learn every patience epochs.\nint\n\n\nnorm\nThe flag to normalize the input values\n-\n\n\nactivation\nThe types of activation layer\nrelu, sigmoid, tanh, or identical\n\n\ninput1\nThe type of the main input values\nmagnitude or flux\n\n\ninput2\nThe type of the sub input values\nnone or scaled-flux\n\n\noptimizer\nThe type of optimizer\nmomentum, adam, amsbound, or adabound\n\n\nadabound-final-lr\nThe final learning rate, it is available when optimizer is adabound or amsbound\nfloat\n\n\nlr\nLearning rate\nfloat\n\n\neval-frequency\nEvaluate test dataset every eval-frequency epochs.\nint\n\n\nfold\nThe index of the folds of cross validation, if fold is -1 then all folds are target\nint\n\n\nthreads\nThe number of threads\nint\n\n\ncv\nThe number of folds of cross validation\nint\n\n\nremove-y\nThe flag to remove Y band or not\n-\n\n\nuse-batch-norm\nThe flag to use batch normalization layer or not\n-\n\n\ntarget-distmod / target-redshift\nThe distmod is the target value if the flag target-distmod is set. the redshift value is the target if target-redshift is set.\n-\n\n\n\nPrediction\nThis script predicts the redshift of input data with the trained model.\nUsage\npython hsc_redshift.py predict \\\n    --data-path=../data/processed/hsc_data.h5 \\\n    --model-dir=../models/result1/0 \\\n    --data-type=HSC --output-name=prediction.csv\n\n\n\n\nOption name\nDescription\nValue type / Choices\n\n\n\n\ndata-path\nFile path of the dataset to predict\nstr\n\n\nmodel-dir\nDirectory that the trained model is in\nstr\n\n\ndata-type\nSimSN for training dataset, HSC for test dataset\nSimSN or HSC\n\n\noutput-name\nFile name to output the predicted results (the file is created in model-dir)\nstr\n\n\n\nHyper-parameter search of regression model\npython hsc_redshift.py search \\\n    --sim-sn-path=../data/processed/sim_sn_data.h5 \\\n    --hsc-path=../data/processed/hsc_data.h5 \\\n    --model-dir=../models/search2 --seed=0 \\\n    --patience=200 --batch-size=1000 --norm \\\n    --input1=absolute-magnitude --input2=scaled-flux \\\n    --optimizer=adam --adabound-final-lr=1e-1 --lr=1e-3 \\\n    --eval-frequency=20 --threads=4 \\\n    --remove-y --target-redshift --n-trials=100\n\n\n\n\nOption name\nDescription\nValue type / Choices\n\n\n\n\nn-trials\nThe number of search\nint\n\n\n\nAcknowledgments\nThis project is supported by JST CREST Grant NumberJPMHCR1414, JST AIP Acceleration Research Grant NumberJP20317829, MEXT KAKENHI Grant Numbers 17H06363, 18H04345, and JSPS KAKENHI GrantNumbers 16H02183, 18K03696, 19H00694, 20H00179, 20H04730, 20HT0063.\n""], 'url_profile': 'https://github.com/ichiro-takahashi', 'info_list': ['Python', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Python', 'Updated Mar 25, 2020', '2', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dpelezo', 'info_list': ['Python', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Python', 'Updated Mar 25, 2020', '2', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fajaralhanief', 'info_list': ['Python', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Python', 'Updated Mar 25, 2020', '2', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['ML-HW2\nLogistic Regression / Multi-Layered Neural Network / Gradients\n'], 'url_profile': 'https://github.com/vincentdmai', 'info_list': ['Python', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Python', 'Updated Mar 25, 2020', '2', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Joinville SC', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['House Prices\nprevendo preÃ§os de imÃ³veis em Joiville usando python.\n'], 'url_profile': 'https://github.com/wilsonazer', 'info_list': ['Python', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Python', 'Updated Mar 25, 2020', '2', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['ML_LinReg_Petal_Length_Predictor\nA Petal length predictor using Linear Regression demonstrated using a Python Flask application.\nDataset Used: Seaborn Iris dataset\nML Algorithm Used: Linear Regression\nApplication Stack:\n\nPython (sklearn, flask)\n\n#pip packages\npip install seaborn\npip install flask\npip install flask-cors\npip install scikit-learn\n'], 'url_profile': 'https://github.com/arindomjit', 'info_list': ['Python', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Python', 'Updated Mar 25, 2020', '2', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Blojak', 'info_list': ['Python', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Python', 'Updated Mar 25, 2020', '2', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'UK', 'stats_list': [], 'contributions': '189 contributions\n        in the last year', 'description': [""Emulator\nScripts\n\nMerge shapefiles into a single multi-polygon (create_merged_shapefile.ipynb).\nCrop emissions data to China shapefile training (emis_apply_mask_train.py) and test (emis_apply_mask_test.py) data.\nDesign of Latin hypercube designs (latin_hypercube.ipynb).\nOutput of Latin hypercube designs for training (latin_hypercube_inputs_training.csv) and test (latin_hypercube_inputs_test.csv) data.\nAutomatic machine learning tool (TPOT) using genetic programming to optimise model pipeline on 50 random grid cells (tpot_optimiser.py), with outputs in the tpot_gridcells folder (tpot_emulator_pipeline_PM2_5_DRY_*.py).\nConfiguration for TPOT based on Gaussian process regresor (config_regressor_gaussian.py).\nEmulator cross-validation and sensitivity analysis (emulator.ipynb). Interactively computed on a HPC using Dask and Jupyter Lab following instructions here.\nEmulator predictions for custom inputs (emulator_predictions.py). Submitted to HPC (emulator_predictions.bash) using Dask for workers viewing worker status on Jupyter Lab. Can submit in batch mode (emulator_predictions_batch.bash).\nRegrid custom outputs to population grid of the world (regrid_to_popgrid.py). Submitted to HPC (regrid_to_popgrid.bash) using Dask for workers viewing worker status on Jupyter Lab. Can submit in batch mode (regrid_to_popgrid_batch.bash).\nCrop population-weighted output predictions to region's shapefile (popweighted_region.py). Submitted to HPC (popweighted_region.bash) using Dask for workers viewing worker status on Jupyter Lab. Uses cropping functions (cutshapefile.py).\nVarious emulator plots including emulator evaluation, sensitivity maps, prediction maps, and 2D contour pairs, (emulator_plots.ipynb).\n\nSetup Python environment\n\nCreate a conda environment with the required libraries from the config file (.yml) in the repository:\n\nconda env create --name pangeo --file=pangeo.yml  \npip install salib dask_labextension pyarrow  \njupyter labextension install dask-labextension  \njupyter labextension install @jupyter-widgets/jupyterlab-manager  \n\nLicense\nThis code is currently licensed under the GPLv3 License, free of charge for non-commercial use.\n\n""], 'url_profile': 'https://github.com/lukeconibear', 'info_list': ['Python', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Python', 'Updated Mar 25, 2020', '2', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Dhaka,Bangladesh', 'stats_list': [], 'contributions': '1,026 contributions\n        in the last year', 'description': ['LinearRegressionPolynomialRegressionAndRandomForestRegression\n'], 'url_profile': 'https://github.com/MdShahadatHossainbd', 'info_list': ['Python', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Python', 'Updated Mar 25, 2020', '2', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'New York, New York', 'stats_list': [], 'contributions': '285 contributions\n        in the last year', 'description': [""Module 2 Final Project\nGabriela Lopez & Mendel Oster\nIntroduction\nFor this project, we found a dataset and attempted to create a linear regression model for it.\nSource\nWe took our data from a MIT course assignment linked below\nhttps://ocw.mit.edu/courses/sloan-school-of-management/15-071-the-analytics-edge-spring-2017/linear-regression/assignment-2/reading-test-scores/\nOur Data at a glance\nOur subset contains information about the demographics and schools for American students taking the exam, derived from 2009 PISA Public-Use Data Files http://nces.ed.gov/pubsearch/pubsinfo.asp?pubid=2011038\n(PISA) The Programme for International Student Assessment is a test given every three years to 15-year-old students from around the world to evaluate their performance in mathematics, reading, and science.\nFeatures of our data\n\ngrade: The grade in school of the student (most 15-year-olds in America are in 10th grade)\nmale: Whether the student is male (1/0)\nraceeth: The race/ethnicity composite of the student\npreschool: Whether the student attended preschool (1/0)\nexpectBachelors: Whether the student expects to obtain a bachelor's degree (1/0)\nmotherHS: Whether the student's mother completed high school (1/0)\nmotherBachelors: Whether the student's mother obtained a bachelor's degree (1/0)\nmotherWork: Whether the student's mother has part-time or full-time work (1/0)\nfatherHS: Whether the student's father completed high school (1/0)\nfatherBachelors: Whether the student's father obtained a bachelor's degree (1/0)\nfatherWork: Whether the student's father has part-time or full-time work (1/0)\nselfBornUS: Whether the student was born in the United States of America (1/0)\nmotherBornUS: Whether the student's mother was born in the United States of America (1/0)\nfatherBornUS: Whether the student's father was born in the United States of America (1/0)\nenglishAtHome: Whether the student speaks English at home (1/0)\ncomputerForSchoolwork: Whether the student has access to a computer for schoolwork (1/0)\nread30MinsADay: Whether the student reads for pleasure for 30 minutes/day (1/0)\nminutesPerWeekEnglish: The number of minutes per week the student spend in English class\nstudentsInEnglish: The number of students in this student's English class at school\nschoolHasLibrary: Whether this student's school has a library (1/0)\npublicSchool: Whether this student attends a public school (1/0)\nurban: Whether this student's school is in an urban area (1/0)\nschoolSize: The number of students in this student's school\nreadingScore: The student's reading score, on a 1000-point scale\n\nGoal\nTo predict the reading scores of students from the US on the 2009 PISA exam using Linear Regression models such as OLS (ordinary least squares), Polynomials, Ridge, and Lasso. Additionally, we'll run some statistical tests on the provided data.\nLibraries Used\n**Data Cleaning**\nPandas\nSeaborn\nMatplotlib\n\n**Hypothesis Testing**\nPandas\nMatplotlib\nScipy\nStatsmodels\n\n**Regression Model**\nPandas\nSklearn\nMatplotlib\nNumpy\nStatsmodels\nRe\nMath\n\nPreprocessing\nSee: Data Cleaning.ipynb\n\nNull values\nIn the image below you can see the columns with the count of null values and what percentage of that column is null.\n\nAs you can see the columns about if the parents have a bachelor degree has more than 10% null values. There is a risk that these parents were simply embarassed and therefore did not provide their information, therefore, if we simply drop these nulls we may be introducing some bias to our datasets. We concluded it was best to drop these two columns.\nFor the rest of the dataset, we simply dropped the rows with null values.\nCategorical Features\nBesides all of our dummy variables, there were two categorical variables. Of these, raceeth was made up of string values. To be able to work with this data, we created dummy variables for this column.\nOutliers\nSince our dataset included some extreme outliers we went with a conventional route of removing them.\nWe found our Upper fence and Lower fence and removed everything outside of them.\nThe formula is:\nUpper fence = Q3 + (1.5 * IQR)\nLower fence = Q1 â€“ (1.5 * IQR)\nWhere IQR is the interquartile range\nExploratory Data Analysis\nAfter cleaning, our data has 3,109 rows, 27 feature columns, and one target column. It's made up of float and integer values.\nOf the feature columns, there are 3 quantitative columns, which we are treating as continuous values for our model. All others are categorical with values being either 0 or 1.\nWe will look at the distribution of these columns and their correlation more when starting to build our model.\nHypothesis Testing\nSee: Hypothesis Testing.ipynb\n\nNext, we made observations and asked questions of our data. We checked various groups for a statistical difference in their reading scores.\nAlternative Hypothesis: There is a statistically significant difference in the reading scores of these groups.\nNull Hypothesis: There is no statistically significant difference between the reading scores of these groups.\nFor all of the following groups we were able to reject the null hypothesis. This means we found the difference between the means of these groups is indeed statistically significant.\nStudents with working parents vs non working parents\nAverage score for students with at least one working parent: 524.81\nAverage score for students with parents who don't work: 495.77\nStudents who speak english at home vs those who do not\nAverage score for students who speak english at home: 527.25\nAverage score for students who don't speak english at home: 494.63\nStudents with a parent that graduated high school vs those without one\nAverage score for students with a parent who graduated High school: 526.37\nAverage score for students with parents who didn't graduate High school: 478.93\nStudents who attend public school vs those who don't\nAverage score for students from Public schools: 520.63\nAverage score for students from Private schools: 551.61\nStudents with a parent from the US vs those without one\nAverage score for students with a parent born in the US: 526.95\nAverage score for students with parents born outside of the US: 507.19\nAverage reading score for students of different races\nWe were unable to perform Anova testing for this since some of the races in our dataset are not presented with enough datapoints. However, to visualize the difference between these in our data, we've included the plot below.\n\n95% Confidence Interval for Mean of Population Reading Score\nWe calculated a 95% confidence interval of 350.98 - 696.12.\nRegression Model\nSee: Regression Model.ipynb\n\nOur first step in model building was looking for multicollinearity. We found motherBornUS and fatherBornUS to be highly correlated. To pick which one to remove, we looked for which has a smaller correlation with our target variable.\nNext, we needed to choose which one of our dummy variable columns to drop to avoid redundancy. To pick which one to drop we followed the same step of looking for lowest correlation with target variable.\nAs mentioned in our EDA, we had 3 continuous features to work with. We plotted these against our target, and saw that no linear relationship exists.\n\nWe also saw they had non-normal distributions with very differnt ranges.\nOur next step was to split the data into a training, testing, and validation set. We then created a scaler from the training set and used it to scale all three sets. This standardized our continuous variables and helped make our results more trustworthy.\nOLS\nFor our first model, we used all of the features in our dataset.\nThe results of our first model showed many features with high p-values. For our second model, we removed any variable which had a p-value higher than 0.5 in our first model. These removed variables were: preschool, motherWork, selfBornUS, fatherBornUS, Morethannorace, and minutesPerWeekEnglishscaled.\nNext, we thought it might be useful to split up the categorical and the continuous variables into two models.\nBelow is a summary of the results we got from these models.\n\nPolynomial\nWe decided to try out a polynomial model for our continuous variables and once again confirmed that these features are not good predictors.\n\nAs you can see, we could get better (albeit not good) results by choosing a higher order polynomial model. However our testing results only got worse, meaning we are overfitting our data.\nRidge and Lasso\n\nWe got very similar values for our regularization models.\nAdditionally, we used the Lasso model to identify which features were unimportant by seeing which had a coefficient of zero. These were: preschool, motherWork, selfBornUS, fatherBornUS, Morethannorace, and minutesPerWeekEnglishscaled. We are happy to see these were the values we deemed less important in our second OLS model.\nConclusion & Next Steps\nIn the end, we weren't able to find a good linear regression model for this dataset. This is probably because most of the collected data is categorical, and our continuous variables did not have a linear relationship with the target variables. As such, it seems that a linear model won't be sufficient to predict student test scores with this data.\nOur model with the smallest Root Mean Squared Error (RMSE) is our second OLS Model. From this model's results, we can see that the variables with the biggest impact (lowest p-value, highest coefficient) are:\n\nexpectBachelors\nread30MinsADay\nBlack\n\nPossible ways to continue this project:\n\nComplete hypothesis tests on the rest of the groups\nComplete an ANOVA test for the different grades\nImprove OLS model by developing new continuous features\n\n""], 'url_profile': 'https://github.com/gabbyflabby', 'info_list': ['Python', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Python', 'Updated Mar 25, 2020', '2', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Nov 6, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Logistic_regression-with-Netwon-DFP-BFGS\nUsing Netwon,DFP and BDGS method to solve Logistic_regression problom\nMore details can be read in my CSDN blog.\nhttps://blog.csdn.net/weixin_45424997/article/details/104950049\nP.S ""answer.txt"" is the label of the ""text_data.txt""\n'], 'url_profile': 'https://github.com/Woshiwzl1997', 'info_list': ['2', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'MATLAB', 'Updated Mar 25, 2020', 'Updated Mar 24, 2020', '2', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Linear Regression\nIn simple linear regression a single independent variable is used to predict the value of a dependent variable.\nThe most common method for fitting a regression line is the method of least-squares. This method calculates the best-fitting line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line (if a point lies on the fitted line exactly, then its vertical deviation is 0). Because the deviations are first squared, then summed, there are no cancellations between positive and negative values.\nIn this project, I made a very basic LR program in Python on study hours and marks data and with a regression line we predict other data too.\nBy - Rishi Mahajan\nInstagram - @bloodshed_developer\n'], 'url_profile': 'https://github.com/bloodsheddev', 'info_list': ['2', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'MATLAB', 'Updated Mar 25, 2020', 'Updated Mar 24, 2020', '2', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/smmangla', 'info_list': ['2', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'MATLAB', 'Updated Mar 25, 2020', 'Updated Mar 24, 2020', '2', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'Vancouver, BC, Canada', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Comparison of PCA, Linear Regression and Logistic Regression method in terms of accuracy and error rate for breast cancer dataset ðŸ¤˜\nIn this project, I will be analyzing the efficiency of Principal Component Analysis, Linear Regression and Logistic Regression Method and comparing it in terms of error rate and number of misclassifications. I will be considering WDBC (Wisconsin Diagnostic Breast Cancer) dataset. The dataset has 30 features selected from each of 569 patients of which 357 patients were diagnosed as benign and 212 patients were diagnosed as malignant.\nData â˜ï¸\nWDBC (Wisconsin Diagnostic Breast Cancer) dataset\nMethods and Software used ðŸ’»\nMATLAB \nPrincipal Component Analysis \nLinear Regression \nLogistic Regression\nInsights ðŸ“\n\n\n\n'], 'url_profile': 'https://github.com/nikitakpr', 'info_list': ['2', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'MATLAB', 'Updated Mar 25, 2020', 'Updated Mar 24, 2020', '2', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/priyankasahu200187', 'info_list': ['2', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'MATLAB', 'Updated Mar 25, 2020', 'Updated Mar 24, 2020', '2', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '213 contributions\n        in the last year', 'description': ['Covid-19\nHello, this is a simple data science project to show linear regression of covid-19 in IRAN.\n'], 'url_profile': 'https://github.com/sinarazi', 'info_list': ['2', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'MATLAB', 'Updated Mar 25, 2020', 'Updated Mar 24, 2020', '2', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'Turkey', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': ['regressions\n'], 'url_profile': 'https://github.com/emrekasg01', 'info_list': ['2', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'MATLAB', 'Updated Mar 25, 2020', 'Updated Mar 24, 2020', '2', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/K2587', 'info_list': ['2', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'MATLAB', 'Updated Mar 25, 2020', 'Updated Mar 24, 2020', '2', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['3Dvision\nGravitation vector regression from single-images using UprightNet.\n'], 'url_profile': 'https://github.com/oskargh', 'info_list': ['2', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'MATLAB', 'Updated Mar 25, 2020', 'Updated Mar 24, 2020', '2', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'Ä°zmir', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': [""Breath_Cancer_Logistic_Regression\nI analyse the breath cancer thanks to logistic regression\nIn this repository i used dataset thanks to : https://www.kaggle.com/merishnasuwal/breast-cancer-prediction-dataset\nAnd When i working logistic regression i make advantage of :https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\nFor any thing which you want to ask, don't hesitate\nEnjoy your Coding!!\n-ugeure\n""], 'url_profile': 'https://github.com/ugeure', 'info_list': ['2', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'MATLAB', 'Updated Mar 25, 2020', 'Updated Mar 24, 2020', '2', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '167 contributions\n        in the last year', 'description': ['Linear-Regression\nFundamentals of linear, polynomial and multi-variable linear regression.\n'], 'url_profile': 'https://github.com/OrionMat', 'info_list': ['MATLAB', 'Updated Mar 27, 2020', 'MATLAB', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Aug 29, 2020', 'MATLAB', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Scala', 'Updated Mar 24, 2020', 'R', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'Toronto, ON', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['linearRegression\nLinear Regression Matrix and Normal Equetion model (Octave), could be used in Matlab as well\nTask has been done in Octave 5.2 (in minor version should work as well)\nFunctions:\ncomputecost.m -  Cost function for linear regression\ncomputecostMulti - Cost function for polinomial regression\nfeatureNormalize.m - feature normalization function using Mean and Standard Deviation\ngradientDescent.m - gradient descent algorithm to find Theta\ngradientDescentMulti.m - gradient descent algorithm to find Theta for n-features\nnormalEqn.m -  Normal Equation algorithm (good to use on data sets up to 10,000 samples)\nFor number of samples more than 10,000 is better to use gradient descent as it is more time saving way.\n'], 'url_profile': 'https://github.com/maxdatascience', 'info_list': ['MATLAB', 'Updated Mar 27, 2020', 'MATLAB', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Aug 29, 2020', 'MATLAB', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Scala', 'Updated Mar 24, 2020', 'R', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'Istanbul, Turkey', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['DeepLearning\nSome algorithms about Deeplearning. Genetic algorithm, Logistic and Linear Regression etc.\n'], 'url_profile': 'https://github.com/HakanAcundas', 'info_list': ['MATLAB', 'Updated Mar 27, 2020', 'MATLAB', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Aug 29, 2020', 'MATLAB', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Scala', 'Updated Mar 24, 2020', 'R', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rebear217', 'info_list': ['MATLAB', 'Updated Mar 27, 2020', 'MATLAB', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Aug 29, 2020', 'MATLAB', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Scala', 'Updated Mar 24, 2020', 'R', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'Madrid', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['food-images\nConvolutional Neural Network for a image regression problem\n'], 'url_profile': 'https://github.com/caumente', 'info_list': ['MATLAB', 'Updated Mar 27, 2020', 'MATLAB', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Aug 29, 2020', 'MATLAB', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Scala', 'Updated Mar 24, 2020', 'R', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': [""Classifying Iris Species Using Logistic Regression\nThis project is just for me to understand more about logistic regression by following some tutorials available in the internet. The main website that I referred to is Analytics Vidhya\nGetting Started\nThis project uses the iris data set which can be downloaded at UC Irvine Machine Learning Repository\nPrerequisites\nImport all the required libraries. I use the pandas' DataFrame to have a better understanding of the data set by looking at the descriptive statistics of the data set. Later, I use the LabelEncoder to change the three classes of the species to a dummy variable such that 0 represents Iris-Sentosa, 1 represents Iris-Versicolor, and 2 represents Iris-Virginica. The train_test_split is used to separate the data into two parts. One part is the train set, and the orher is the test set. Logistic regression model is trained by the train set and later verified for the accuracy score using the test set.\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nIn the last part of the coding, this simple program will ask for input from the user. For better understanding, I use the mode value of each column for each species. For instance, majority of flowers from Iris-Virginica species have the following value:\n\n\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\n\n\n6.3\n3.0\n5.1\n1.8\n\n\nIf we take these values as input, then it is expected that the algorithm will return Iris-Virginica as the prediction.\nBuilt With\n\nPandas\nSci-kit Learn\n\nAcknowledgments\n\nThanks to Analytics Vidhya for sharing so many useful information and tutorial which can be easily followed by a novice like me.\nThanks to UC Irvine Machine Learning Repository for providing this nice Iris data set publicly.\n\n""], 'url_profile': 'https://github.com/muhdamir', 'info_list': ['MATLAB', 'Updated Mar 27, 2020', 'MATLAB', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Aug 29, 2020', 'MATLAB', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Scala', 'Updated Mar 24, 2020', 'R', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '222 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anastasiia01', 'info_list': ['MATLAB', 'Updated Mar 27, 2020', 'MATLAB', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Aug 29, 2020', 'MATLAB', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Scala', 'Updated Mar 24, 2020', 'R', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '281 contributions\n        in the last year', 'description': ['Example regression in chimney library 0.5.0:\nWorked previously in 0.4.2:\nCHIMNEY_VERSION=""0.4.2"" sbt example/run fails/run\n...\nUsing chimney version 0.4.2\n...\n[info] running net.flicken.ChimneyExample\nVenue(Venue Name)\n...\n[info] running net.flicken.ChimneyExample\nEvent(ManuallyFilled(Venue Name))\nVenue(Venue Name)\n[success] Total time: 1 s, completed Mar 24, 2020, 12:17:12 PM\n\nNo longer works in 0.5.0:\n CHIMNEY_VERSION=""0.5.0"" sbt example/run fails/run\n...\n Using chimney version 0.5.0\n...\n [info] running net.flicken.ChimneyExample\n Venue(Venue Name)\n [success] Total time: 3 s, completed Mar 24, 2020, 12:17:52 PM\n....\n [error] .../fails/src/main/scala/net/flicken/ChimneyExample.scala:16:35: Chimney can\'t derive transformation from net.flicken.internal.Venue to net.flicken.dto.Venue\n [error]\n [error] net.flicken.dto.Venue\n [error]   name: java.lang.String - no accessor named name in source type net.flicken.internal.Venue\n [error]\n [error] Consult https://scalalandio.github.io/chimney for usage examples.\n [error]\n [error]     println(venue.into[dto.Venue].transform)\n [error]                                   ^\n [error] one error found\n [error] (fails / Compile / compileIncremental) Compilation failed\n [error] Total time: 0 s, completed Mar 24, 2020, 12:17:52 PM\n\n'], 'url_profile': 'https://github.com/flicken', 'info_list': ['MATLAB', 'Updated Mar 27, 2020', 'MATLAB', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Aug 29, 2020', 'MATLAB', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Scala', 'Updated Mar 24, 2020', 'R', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['https://www.kaggle.com/c/bike-sharing-demand/data\nLinearRegressionWithR\nLinear Regression On Bike Sharing Demand Kaggle Dataset.\nFor this project you will be doing the Bike Sharing Demand Kaggle challenge!\nInstructions\nJust complete the tasks outlined below.\nGet the Data\nYou can download the data or just use the supplied csv in the repository. The data has the following features:\ndatetime - hourly date + timestamp\nseason - 1 = spring, 2 = summer, 3 = fall, 4 = winter\nholiday - whether the day is considered a holiday\nworkingday - whether the day is neither a weekend nor holiday\nweather -\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\ntemp - temperature in Celsius\natemp - ""feels like"" temperature in Celsius\nhumidity - relative humidity\nwindspeed - wind speed\ncasual - number of non-registered user rentals initiated\nregistered - number of registered user rentals initiated\ncount - number of total rentals\nRead in bikeshare.csv file and set it to a dataframe called bike.\nCheck the head of df\ndatetime\tseason\tholiday\tworkingday\tweather\ttemp\tatemp\thumidity\twindspeed\tcasual\tregistered\tcount\n1\t2011-01-01 00:00:00\t1\t0\t0\t1\t9.84\t14.395\t81\t0\t3\t13\t16\n2\t2011-01-01 01:00:00\t1\t0\t0\t1\t9.02\t13.635\t80\t0\t8\t32\t40\n3\t2011-01-01 02:00:00\t1\t0\t0\t1\t9.02\t13.635\t80\t0\t5\t27\t32\n4\t2011-01-01 03:00:00\t1\t0\t0\t1\t9.84\t14.395\t75\t0\t3\t10\t13\n5\t2011-01-01 04:00:00\t1\t0\t0\t1\t9.84\t14.395\t75\t0\t0\t1\t1\n6\t2011-01-01 05:00:00\t1\t0\t0\t2\t9.84\t12.88\t75\t6.0032\t0\t1\t1\nCan you figure out what is the target we are trying to predict? Check the Kaggle Link above if you are confused on this.\nExploratory Data Analysis\nCreate a scatter plot of count vs temp. Set a good alpha value.\nPlot count versus datetime as a scatterplot with a color gradient based on temperature. You\'ll need to convert the datetime column into POSIXct before plotting.\nHopefully you noticed two things: A seasonality to the data, for winter and summer. Also that bike rental counts are increasing in general. This may present a problem with using a linear regression model if the data is non-linear. Let\'s have a quick overview of pros and cons right now of Linear Regression:\nPros:\nSimple to explain\nHighly interpretable\nModel training and prediction are fast\nNo tuning is required (excluding regularization)\nFeatures don\'t need scaling\nCan perform well with a small number of observations\nWell-understood\nCons:\nAssumes a linear relationship between the features and the response\nPerformance is (generally) not competitive with the best supervised learning methods due to high bias\nCan\'t automatically learn feature interactions\nWe\'ll keep this in mind as we continue on. Maybe when we learn more algorithms we can come back to this with some new tools, for now we\'ll stick to Linear Regression.\nWhat is the correlation between temp and count?\ntemp\tcount\ntemp\t1.0000000\t0.3944536\ncount\t0.3944536\t1.0000000\nLet\'s explore the season data. Create a boxplot, with the y axis indicating count and the x axis begin a box for each season.\nNotice what this says:\nA line can\'t capture a non-linear relationship.\nThere are more rentals in winter than in spring\nWe know of these issues because of the growth of rental count, this isn\'t due to the actual season!\nFeature Engineering\nA lot of times you\'ll need to use domain knowledge and experience to engineer and create new features. Let\'s go ahead and engineer some new features from the datetime column.\nCreate an ""hour"" column that takes the hour from the datetime column. You\'ll probably need to apply some function to the entire datetime column and reassign it. Hint:\ntime.stamp <- bike$datetime[4]\nformat(time.stamp, ""%H"")\ndatetime\tseason\tholiday\tworkingday\tweather\ttemp\tatemp\thumidity\twindspeed\tcasual\tregistered\tcount\thour\n1\t2011-01-01\t1\t0\t0\t1\t9.84\t14.395\t81\t0\t3\t13\t16\t00\n2\t2011-01-01 01:00:00\t1\t0\t0\t1\t9.02\t13.635\t80\t0\t8\t32\t40\t01\n3\t2011-01-01 02:00:00\t1\t0\t0\t1\t9.02\t13.635\t80\t0\t5\t27\t32\t02\n4\t2011-01-01 03:00:00\t1\t0\t0\t1\t9.84\t14.395\t75\t0\t3\t10\t13\t03\n5\t2011-01-01 04:00:00\t1\t0\t0\t1\t9.84\t14.395\t75\t0\t0\t1\t1\t04\n6\t2011-01-01 05:00:00\t1\t0\t0\t2\t9.84\t12.88\t75\t6.0032\t0\t1\t1\t05\nNow create a scatterplot of count versus hour, with color scale based on temp. Only use bike data where workingday==1.\nOptional Additions:\nNow create the same plot for non working days:\nYou should have noticed that working days have peak activity during the morning (~8am) and right after work gets out (~5pm), with some lunchtime activity. While the non-work days have a steady rise and fall for the afternoon\nNow let\'s continue by trying to build a model, we\'ll begin by just looking at a single feature.\nBuilding the Model\nUse lm() to build a model that predicts count based solely on the temp feature, name it temp.model\nGet the summary of the temp.model\nCall:\nlm(formula = count ~ temp, data = bike)\nResiduals:\nMin      1Q  Median      3Q     Max\n-293.32 -112.36  -33.36   78.98  741.44\nCoefficients:\nEstimate Std. Error t value Pr(>|t|)\n(Intercept)   6.0462     4.4394   1.362    0.173\ntemp          9.1705     0.2048  44.783   <2e-16 ***\nSignif. codes:  0 â€˜â€™ 0.001 â€˜â€™ 0.01 â€˜â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1\nResidual standard error: 166.5 on 10884 degrees of freedom\nMultiple R-squared:  0.1556,\tAdjusted R-squared:  0.1555\nF-statistic:  2006 on 1 and 10884 DF,  p-value: < 2.2e-16\nYou should have gotten 6.0462 as the intercept and 9.17 as the temp coeffecient. How can you interpret these values? Do some wikipedia research, re-read ISLR, or revisit the Linear Regression lecture notebook for more on this.\nHow many bike rentals would we predict if the temperature was 25 degrees Celsius? Calculate this two ways:\nUsing the values we just got above\nUsing the predict() function\nYou should get around 235.3 bikes.\n1: 235.309724995272\nUse sapply() and as.numeric to change the hour column to a column of numeric values.\nFinally build a model that attempts to predict count based off of the following features. Figure out if theres a way to not have to pass/write all these variables into the lm() function. Hint: StackOverflow or Google may be quicker than the documentation.\nseason\nholiday\nworkingday\nweather\ntemp\nhumidity\nwindspeed\nhour (factor)\nGet the summary of the model\nCall:\nlm(formula = count ~ . - casual - registered - datetime - atemp,\ndata = bike)\nResiduals:\nMin      1Q  Median      3Q     Max\n-324.61  -96.88  -31.01   55.27  688.83\nCoefficients:\nEstimate Std. Error t value Pr(>|t|)\n(Intercept)  46.91369    8.45147   5.551 2.91e-08 ***\nseason       21.70333    1.35409  16.028  < 2e-16 ***\nholiday     -10.29914    8.79069  -1.172    0.241\nworkingday   -0.71781    3.14463  -0.228    0.819\nweather      -3.20909    2.49731  -1.285    0.199\ntemp          7.01953    0.19135  36.684  < 2e-16 ***\nhumidity     -2.21174    0.09083 -24.349  < 2e-16 ***\nwindspeed     0.20271    0.18639   1.088    0.277\nhour          7.61283    0.21688  35.102  < 2e-16 ***\nSignif. codes:  0 â€˜â€™ 0.001 â€˜â€™ 0.01 â€˜â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1\nResidual standard error: 147.8 on 10877 degrees of freedom\nMultiple R-squared:  0.3344,\tAdjusted R-squared:  0.3339\nF-statistic:   683 on 8 and 10877 DF,  p-value: < 2.2e-16\nDid the model perform well on the training data? What do you think about using a Linear Model on this data?\nYou should have noticed that this sort of model doesn\'t work well given our seasonal and time series data. We need a model that can account for this type of trend.\n'], 'url_profile': 'https://github.com/AakashPahuja', 'info_list': ['MATLAB', 'Updated Mar 27, 2020', 'MATLAB', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Aug 29, 2020', 'MATLAB', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Scala', 'Updated Mar 24, 2020', 'R', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'San Diego, CA', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Titanic Logistic Regressor Analysis\nLogistic Regression to predict Survivors on the Titanic\n'], 'url_profile': 'https://github.com/FreshOats', 'info_list': ['MATLAB', 'Updated Mar 27, 2020', 'MATLAB', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Aug 29, 2020', 'MATLAB', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Apr 11, 2020', 'Python', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 27, 2020', 'Scala', 'Updated Mar 24, 2020', 'R', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '213 contributions\n        in the last year', 'description': ['Machine Learning Mastery\nMulti-Otput Regression Models with Python\nblog\n'], 'url_profile': 'https://github.com/youngsoul', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020']}","{'location': 'Bay Area', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/trdelgado', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020']}","{'location': 'Warsaw, Poland', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mateuszwosinski', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Multiple_Linear_Regression_by_python\nPython code for Multiple Linear Regression for Machine Learning\n'], 'url_profile': 'https://github.com/amitdivekar30', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['Analyst-Meets-Soccer_Data\nComparing Simple Regression, Simple Regression with CV & Lasso Regression\nChoosing a sample\nOne way to choose a sample out of a significantly heterogeneous dataset for model training is to\nuse â€œLeave One-Out Cross Validationâ€. An example of this method is to randomly order the\ndataset, and then divide the data set into 5 parts. Then the model is trained on the data from 4\nparts and is tested on the 5th part. Then this process is repeated with a different combination of 4\ntraining parts and 5th testing part.\nThe benefit of this method specifically regarding heterogenous data is that some of the\nâ€œheterogeneousâ€ observations (AKA, â€œextreme outliersâ€) will at one point be left out of the\ntraining data. This will avoid overfitting a model to all the outlier observations. Other benefits\nare that each observation will be used for both training and validation, and the sampling variance\nof cross validation model selection will be lower.\nFitting a simple regression model\nIn the first exercise, we fit a simple linear regression model to predict the overall score of players\nwithin the FIFA Dataset. This was done after randomly splitting the data into test and training\nsets with only 10% of the records in the training set. One way to measure the accuracy of the\nmodel is by measuring how close the data is to the fitted regression line through the R-squared\nstatistic measure.\nTo perform this simple linear regression model, we first cleaned our data by removing null\nvalues. We then created indicator variables for categorical data after determining if there was a\ncorrelation between them and the dependent variable using the Spearman correlation test. We\neliminated the features that were highly correlated with one another to reduce multicollinearity\neffect. Although multicollinearity is not an issue when the model is used for prediction, it helped\nus in improving the model stability and a higher R2 value with high adjusted R2 as well. We had\na total of 33 features. After fitting our linear regression model, we obtained an R2 value of\napproximately 0.921.\nFitting a simple regression model with 5-fold cross validation\nNext, we tried tweaking our model by applying a different technique (cross validation using kfold). We trained the model using 10% of the data. Then by choosing 5-fold cross validation, the\ntest set was split into 5 parts with the same ratio of training / test sets which alternated until all\npossible combinations were made.\nAfter fitting our simple regression model with a 5-fold cross validation, we obtained a prediction\nwith an R2 of approximately 0.922 which is the average of the accuracy of each fold. In both\ncases (Part 1 and 2), the R2 was quite high, but the accuracy (measured using R2) was slightly\nhigher using a 5-fold cross validation. This may be due to the cross-validationâ€™s better\nperformance on unseen data. The higher R2 values explains that the model is good at predicting\nthe data that it hasnâ€™t seen already.\nFitting a Lasso Regression\nFor this part, we used Lasso regression to predict the overall scores of players in the test set\nusing the default value of alpha = 1. At this value of alpha, the number of features used in the\nmodel was 20 with an R2 of 0.915. While the value is still relatively high, it is smaller than the\nR2 obtained from the simple linear regression model fitted in part 1.\nEssentially, Lasso regression operates using a shrinkage technique which focuses on reducing\nhigh levels of multicollinearity rather than trying to maximize the prediction accuracy. Hence, in\nthe case of using alpha = 1, the number of features used in the model was reduced to 20. If the\nfocus is to improve prediction error, then perhaps using a simple linear regression model is more\neffective; however, Lasso regression may be a better tool for interpretability. Since we had a\nhigh value of 1 for the alpha, the lasso optimized by reducing the number of features in the\nmodel which explains the slight reduction in the R2 value over the one obtained from the linear\nregression model.\nComparing Ridge & Lasso\nRidge and Lasso Regressions operate similarly in that they try to decrease the model complexity\nby minimizing the number of predictors. Instead of removing them using backward or forward\nselection, we can examine the variableâ€™s effect on the response by penalizing those predictors\nthat have coefficients far away from zero. The theoretical difference between Ridge and Lasso\nregression is that Ridgeâ€™s penalty term uses a squared magnitude of coefficient whereas, Lassoâ€™s\npenalty term uses an absolute magnitude of the coefficient.\nAs a result, Lasso shrinks the less important featuresâ€™ coefficients to 0 and eliminates them from\nthe model. Accordingly, Lasso will have fewer features. While this may be better for feature\nselection, it would lead to yielding a less accurate prediction. This was re-validated when testing\nour model where we observed a higher R2 value (0.921) with a higher number of features in the\nmodel.\nFitting a Lasso Regression with Optimal Alpha\nAccording to our code, the ideal value of alpha is 0.001 where 32 features are utilized. Using\nthese 32 features, an R2 value of 0.924 was deduced which is higher than the value obtained\nfrom Part 1 (0.921). The main issue with Lasso is that when the model has correlated variables, it\nwill keep one variable while setting other variables to zero thus sacrificing the accuracy of the\nmodel to avoid multicollinearity. Nonetheless, by looking for an ideal value for alpha, we are\ntargeting the best estimator which would yield the highest model accuracy rather than focus on\nminimizing the number of features that may contribute to multicollinearity.\nAssessing the best with Information Criterion\nAccording to the results obtained, we notice that the values of AIC / BIC are lower in the case of\nour Lasso regression model which we built in Part 5. Having a lower AIC or BIC value indicates a\nbetter fit; hence, the Lasso model built in Part 5 was a better model. The only difference between\nAIC and BIC is that BIC considers the number of observations as shown in the formula below:\nAIC = -2ln(L) + 2k\nBIC = -2ln(L) + 2ln(N)*k\nAccordingly, at any number of observations above 2, the value of BIC would always be\nhigher. After deducing the corrected AIC for both models, we notice that the values are\nextremely close (21849.14 vs. 21848.99) for the simple linear regression and (21838.44 vs.\n21838.30) for the Lasso regression. This is usually the case when the number of observations is\nlarge, so the corrected AIC converges to AIC.\nICs vs. CVs\nInformation Criteria (IC) are alternatives to Cross Validation. ICs should not be trusted entirely\nthe same as Cross Validation, especially depending on the situation. In general, an Information\nCriterion such as AIC only estimates a modelâ€™s out-of-sample deviance. By contrast, k-fold\nCross Validation, for example, uses k-1 parts of the data to train a model, and then uses the kth\npart to test the model. So this method actually tests the model against out-of-sample data; it does\nnot just estimate the modelâ€™s deviation on out-of-sample data like AIC.\nIn addition, in the specific situation where the data is very large such that the number of\nparameters (which is also the number of degrees of freedom, â€œdfâ€) equals or nearly equals the\nnumber of observations (n), then AIC will give a bad approximation and will overfit the data. (In\nthis case the corrected AIC (AICc) should be used.) But when comparing the AIC to the Cross\nValidation in this case, AIC does not perform as well.\n'], 'url_profile': 'https://github.com/khaledimad', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020']}","{'location': 'Islamabad,Pakistan', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['Basic-Ham-and-Spam-Classifier-using-Logistic-Regression-Model\nBasic Ham and Spam Classifier using Logistic Regression Model\n'], 'url_profile': 'https://github.com/Huzaifa-Sajjad', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Covid-19 Screening using Logistic Regression and Web Interface\nThis model is based on a Randomly Generated Dataset so Accuracy will change.\nWeb Interface is made using Flask package in Python.\nInstallations\nRequirements can be installed using pip python package manager\n  pip install flask\n  pip install pickle\n  pip install Jinja2\n\nRequirements for Training.py\n  pip install pandas\n  pip install numpy\n  pip install sklearn\n  pip install pickle\n\n***NOTE: Model has not been trained after adding feature scaling\nRetrain the model. Then start using.\n'], 'url_profile': 'https://github.com/Saarge-z', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Benniah', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/szafrix', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020']}","{'location': 'Beijing', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['Logistic Regression\næ•°æ®\nä½¿ç”¨çš„æ•°æ®é›†æ˜¯UCI a9aã€‚\nLink: http://ml.cs.tsinghua.edu.cn/~wenbo/data/a9a.zip\næ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆ32,561ï¼‰å’Œæµ‹è¯•é›†ï¼ˆ16,281ï¼‰ï¼Œæ¯ä¸€è¡Œçš„æ ¼å¼ä¸º\nLabel [feature id]:[feature] [feature id]:[feature] [feature id]:[feature] [feature id]:[feature]\n\nå…¶ä¸­é»˜è®¤åªåŒ…å«ä¸º1çš„åˆ—ï¼Œfeatureæ€»åˆ—æ•°ä¸º123ï¼Œå¦‚ä¸‹ä¾‹æ‰€ç¤ºï¼š\n-1 3:1 11:1 14:1 19:1 39:1 42:1 55:1 64:1 67:1 73:1 75:1 76:1 80:1 83:1\n-1 5:1 7:1 14:1 19:1 39:1 40:1 51:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n-1 3:1 6:1 17:1 22:1 36:1 41:1 53:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n...\n\ndataloader.pyåŒ…å«äº†è¯»å–æ•°æ®çš„load(filename)æ–¹æ³•ï¼Œå¯ä»¥è¯»å–ä¸€ä¸ªè¿™ç§æ ¼å¼çš„a9aæ•°æ®æ–‡ä»¶ã€‚\næ¢¯åº¦ä¸Šå‡æ³•\ngd.pyå®žçŽ°äº†æ¢¯åº¦ä¸Šå‡æ³•æ±‚è§£Logistic Regressionã€‚\nä¾èµ–åº“ï¼š\n\nnumpy\n\nä»£ç æ–‡ä»¶çš„ä½¿ç”¨æ–¹æ³•ï¼š\n$ python gd.py -h\nusage: gd.py [-h] [--data DATA] [--lr LR] [--patience PATIENCE]\n             [--output OUTPUT]\n\nUsing gradient descent to solve logistic regression.\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --data DATA          Directory of the a9a data.\n  --lr LR              Learning rate.\n  --patience PATIENCE  Patience to stop.\n  --output OUTPUT      Output log-likelihood and accuracy during training to\n                       pickle file.\nä»£ç è¿è¡Œå®ŒæˆåŽï¼Œä¼šè¾“å‡ºä¸€ä¸ªpickleæ–‡ä»¶ï¼ŒåŒ…å«è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ•°ä¼¼ç„¶å’Œæµ‹è¯•é›†ä¸Šæ­£ç¡®çŽ‡çš„å˜åŒ–ï¼Œç”¨äºŽç”»å›¾ã€‚\næµ‹è¯•äº†lr=0.001, 0.01, 0.05, 0.1å››ç§æƒ…å†µï¼Œæœ€ä¼˜æ­£ç¡®çŽ‡å¦‚ä¸‹è¡¨ï¼š\n\n\n\nlr\ntest acc\n\n\n\n\n0.001\n0.999386\n\n\n0.01\n0.999447\n\n\n0.05\n0.999324\n\n\n0.1\n0.999447\n\n\n\nå››ç§å­¦ä¹ çŽ‡çš„æµ‹è¯•é›†æ­£ç¡®çŽ‡å’Œå¯¹æ•°ä¼¼ç„¶å˜åŒ–å¦‚ä¸‹å›¾ï¼š\n\nå¯ä»¥çœ‹å‡ºï¼š\n\nå››ç§å­¦ä¹ çŽ‡çš„æœ€ä¼˜æ­£ç¡®çŽ‡ç›¸ä¼¼ï¼Œä½†å­¦ä¹ çŽ‡è¶Šå°ï¼Œæ”¶æ•›è¶Šå¿«\nå­¦ä¹ çŽ‡è¶Šå¤§ï¼Œè®¡ç®—å‡ºçš„å¯¹æ•°ä¼¼ç„¶è¶Šå°ï¼Œä»¥è‡³äºŽlr=0.1, 0.05æ—¶å¯¹æ•°ä¼¼ç„¶å‘ä¸‹æº¢å‡ºäº†ï¼›è¿™å¯èƒ½å’Œwæœ‰å¤šç§è¡¨ç¤ºç›¸å…³\n\næ¢¯åº¦ä¸Šå‡æ³•å¯¹wçš„åˆå€¼ä¸å¤ªæ•æ„Ÿã€‚\nIRLSæ³•\nirls.pyå®žçŽ°äº†IRLSæ³•æ±‚è§£Logistic Regressionã€‚\nä¾èµ–åº“ï¼š\n\nnumpy\nsklearn\n\nä»£ç æ–‡ä»¶çš„ä½¿ç”¨æ–¹æ³•ï¼š\nirls.py -h\nusage: irls.py [-h] [--data DATA] [--lamb LAMB] [--cross]\n               [--patience PATIENCE]\n\nUsing gradient descent to solve logistic regression.\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --data DATA          Directory of the a9a data.\n  --lamb LAMB          Regularization constant.\n  --cross              Do cross validation or not.\n  --patience PATIENCE  Patience to stop.\nåœ¨ä¸ä½¿ç”¨10æŠ˜éªŒè¯æ—¶ï¼Œä»£ç çš„è¿è¡Œç»“æžœå¦‚ä¸‹è¡¨ï¼ˆlambdaæ˜¯æ­£åˆ™åŒ–å¸¸æ•°ï¼‰ï¼š\n\n\n\nlambda\nstep\ntrain acc\ntest acc\nw norm\n\n\n\n\n0\n11\n1.0000\n1.0000\n34.7406\n\n\n0.0001\n11\n1.0000\n1.0000\n30.5984\n\n\n0.001\n11\n1.0000\n1.0000\n25.9729\n\n\n0.01\n11\n1.0000\n1.0000\n20.9657\n\n\n0.1\n12\n1.0000\n1.0000\n16.2047\n\n\n1\n12\n0.9999\n1.0000\n11.8451\n\n\n\nå¯ä»¥çœ‹å‡ºï¼Œåœ¨æ­£åˆ™åŒ–å¸¸æ•°ä¸åŒæ—¶ï¼Œæ¨¡åž‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®çŽ‡ç›¸å·®ä¸å¤§ï¼Œåªæœ‰wçš„å€¼æœ‰æ‰€ä¸åŒï¼Œæ­£åˆ™åŒ–å¸¸æ•°è¶Šå¤§ï¼Œwçš„L2-normè¶Šå°ã€‚\nåœ¨ä½¿ç”¨10æŠ˜éªŒè¯æ—¶ï¼Œä»£ç çš„è¿è¡Œç»“æžœå¦‚ä¸‹è¡¨ï¼š\n\n\n\nlambda\nave step\nave train acc\nave val acc\nave test acc\nfinal test acc\nfinal w norm\n\n\n\n\n0\n8.4\n1.0000\n1.0000\n1.0000\n1.0000\n26.1409\n\n\n0.0001\n8.5\n1.0000\n1.0000\n1.0000\n1.0000\n24.2089\n\n\n0.001\n7.3\n1.0000\n0.9999\n1.0000\n1.0000\n20.0847\n\n\n0.01\n8.0\n1.0000\n1.0000\n0.9999\n1.0000\n17.6599\n\n\n0.1\n6.5\n1.0000\n0.9998\n1.0000\n0.9999\n13.8871\n\n\n1\n6.0\n0.9999\n0.9998\n0.9999\n0.9999\n11.0648\n\n\n\nå¯ä»¥çœ‹å‡ºï¼Œæ­£åˆ™åŒ–å¸¸æ•°è¶Šå¤§ï¼Œwçš„L2-normè¶Šå°ï¼Œä¸”æ”¶æ•›é€Ÿåº¦è¶Šå¿«ï¼›å…¶ä»–å‡†ç¡®çŽ‡åˆ™æ²¡æœ‰æ˜Žæ˜¾å·®åˆ«ã€‚\nå€¼å¾—æ³¨æ„çš„æ˜¯ï¼š\n\nåœ¨ä¸åŠ æ­£åˆ™åŒ–é¡¹æ—¶ï¼ŒHessiançŸ©é˜µå¯èƒ½ä¸ºå¥‡å¼‚çŸ©é˜µï¼Œéœ€è¦ç”¨ä¼ªé€†ï¼ˆnp.linalg.pinvï¼‰ã€‚\nç‰›é¡¿æ³•å¯¹wçš„åˆå€¼æ•æ„Ÿï¼Œå¦‚æžœè®¾ç½®å¾—å¤ªå¤§ï¼Œä¼šå¯¼è‡´æ¢¯åº¦å¤±æ•ˆã€‚\n\nè™½ç„¶IRLSæ³•æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°è¾ƒå°‘ï¼Œè€Œä¸”æ¯”è¾ƒç¨³å®šï¼ˆåœ¨wçš„åˆå§‹å€¼è®¾ç½®è¾ƒå¥½çš„å‰æä¸‹ï¼‰ï¼Œä½†ç”±äºŽæ¯æ¬¡è¿­ä»£éƒ½éœ€è¦è®¡ç®—è¾ƒå¤§çš„çŸ©é˜µä¹˜æ³•å’Œæ±‚é€†ï¼Œå› æ­¤æ•´ä½“æ•ˆçŽ‡æ¯”æ¢¯åº¦ä¸Šå‡æ³•å·®ã€‚\n'], 'url_profile': 'https://github.com/zhanghuimeng', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/szafrix', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated May 21, 2020', 'Apache-2.0 license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Beijing', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['Logistic Regression\næ•°æ®\nä½¿ç”¨çš„æ•°æ®é›†æ˜¯UCI a9aã€‚\nLink: http://ml.cs.tsinghua.edu.cn/~wenbo/data/a9a.zip\næ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆ32,561ï¼‰å’Œæµ‹è¯•é›†ï¼ˆ16,281ï¼‰ï¼Œæ¯ä¸€è¡Œçš„æ ¼å¼ä¸º\nLabel [feature id]:[feature] [feature id]:[feature] [feature id]:[feature] [feature id]:[feature]\n\nå…¶ä¸­é»˜è®¤åªåŒ…å«ä¸º1çš„åˆ—ï¼Œfeatureæ€»åˆ—æ•°ä¸º123ï¼Œå¦‚ä¸‹ä¾‹æ‰€ç¤ºï¼š\n-1 3:1 11:1 14:1 19:1 39:1 42:1 55:1 64:1 67:1 73:1 75:1 76:1 80:1 83:1\n-1 5:1 7:1 14:1 19:1 39:1 40:1 51:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n-1 3:1 6:1 17:1 22:1 36:1 41:1 53:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n...\n\ndataloader.pyåŒ…å«äº†è¯»å–æ•°æ®çš„load(filename)æ–¹æ³•ï¼Œå¯ä»¥è¯»å–ä¸€ä¸ªè¿™ç§æ ¼å¼çš„a9aæ•°æ®æ–‡ä»¶ã€‚\næ¢¯åº¦ä¸Šå‡æ³•\ngd.pyå®žçŽ°äº†æ¢¯åº¦ä¸Šå‡æ³•æ±‚è§£Logistic Regressionã€‚\nä¾èµ–åº“ï¼š\n\nnumpy\n\nä»£ç æ–‡ä»¶çš„ä½¿ç”¨æ–¹æ³•ï¼š\n$ python gd.py -h\nusage: gd.py [-h] [--data DATA] [--lr LR] [--patience PATIENCE]\n             [--output OUTPUT]\n\nUsing gradient descent to solve logistic regression.\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --data DATA          Directory of the a9a data.\n  --lr LR              Learning rate.\n  --patience PATIENCE  Patience to stop.\n  --output OUTPUT      Output log-likelihood and accuracy during training to\n                       pickle file.\nä»£ç è¿è¡Œå®ŒæˆåŽï¼Œä¼šè¾“å‡ºä¸€ä¸ªpickleæ–‡ä»¶ï¼ŒåŒ…å«è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ•°ä¼¼ç„¶å’Œæµ‹è¯•é›†ä¸Šæ­£ç¡®çŽ‡çš„å˜åŒ–ï¼Œç”¨äºŽç”»å›¾ã€‚\næµ‹è¯•äº†lr=0.001, 0.01, 0.05, 0.1å››ç§æƒ…å†µï¼Œæœ€ä¼˜æ­£ç¡®çŽ‡å¦‚ä¸‹è¡¨ï¼š\n\n\n\nlr\ntest acc\n\n\n\n\n0.001\n0.999386\n\n\n0.01\n0.999447\n\n\n0.05\n0.999324\n\n\n0.1\n0.999447\n\n\n\nå››ç§å­¦ä¹ çŽ‡çš„æµ‹è¯•é›†æ­£ç¡®çŽ‡å’Œå¯¹æ•°ä¼¼ç„¶å˜åŒ–å¦‚ä¸‹å›¾ï¼š\n\nå¯ä»¥çœ‹å‡ºï¼š\n\nå››ç§å­¦ä¹ çŽ‡çš„æœ€ä¼˜æ­£ç¡®çŽ‡ç›¸ä¼¼ï¼Œä½†å­¦ä¹ çŽ‡è¶Šå°ï¼Œæ”¶æ•›è¶Šå¿«\nå­¦ä¹ çŽ‡è¶Šå¤§ï¼Œè®¡ç®—å‡ºçš„å¯¹æ•°ä¼¼ç„¶è¶Šå°ï¼Œä»¥è‡³äºŽlr=0.1, 0.05æ—¶å¯¹æ•°ä¼¼ç„¶å‘ä¸‹æº¢å‡ºäº†ï¼›è¿™å¯èƒ½å’Œwæœ‰å¤šç§è¡¨ç¤ºç›¸å…³\n\næ¢¯åº¦ä¸Šå‡æ³•å¯¹wçš„åˆå€¼ä¸å¤ªæ•æ„Ÿã€‚\nIRLSæ³•\nirls.pyå®žçŽ°äº†IRLSæ³•æ±‚è§£Logistic Regressionã€‚\nä¾èµ–åº“ï¼š\n\nnumpy\nsklearn\n\nä»£ç æ–‡ä»¶çš„ä½¿ç”¨æ–¹æ³•ï¼š\nirls.py -h\nusage: irls.py [-h] [--data DATA] [--lamb LAMB] [--cross]\n               [--patience PATIENCE]\n\nUsing gradient descent to solve logistic regression.\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --data DATA          Directory of the a9a data.\n  --lamb LAMB          Regularization constant.\n  --cross              Do cross validation or not.\n  --patience PATIENCE  Patience to stop.\nåœ¨ä¸ä½¿ç”¨10æŠ˜éªŒè¯æ—¶ï¼Œä»£ç çš„è¿è¡Œç»“æžœå¦‚ä¸‹è¡¨ï¼ˆlambdaæ˜¯æ­£åˆ™åŒ–å¸¸æ•°ï¼‰ï¼š\n\n\n\nlambda\nstep\ntrain acc\ntest acc\nw norm\n\n\n\n\n0\n11\n1.0000\n1.0000\n34.7406\n\n\n0.0001\n11\n1.0000\n1.0000\n30.5984\n\n\n0.001\n11\n1.0000\n1.0000\n25.9729\n\n\n0.01\n11\n1.0000\n1.0000\n20.9657\n\n\n0.1\n12\n1.0000\n1.0000\n16.2047\n\n\n1\n12\n0.9999\n1.0000\n11.8451\n\n\n\nå¯ä»¥çœ‹å‡ºï¼Œåœ¨æ­£åˆ™åŒ–å¸¸æ•°ä¸åŒæ—¶ï¼Œæ¨¡åž‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®çŽ‡ç›¸å·®ä¸å¤§ï¼Œåªæœ‰wçš„å€¼æœ‰æ‰€ä¸åŒï¼Œæ­£åˆ™åŒ–å¸¸æ•°è¶Šå¤§ï¼Œwçš„L2-normè¶Šå°ã€‚\nåœ¨ä½¿ç”¨10æŠ˜éªŒè¯æ—¶ï¼Œä»£ç çš„è¿è¡Œç»“æžœå¦‚ä¸‹è¡¨ï¼š\n\n\n\nlambda\nave step\nave train acc\nave val acc\nave test acc\nfinal test acc\nfinal w norm\n\n\n\n\n0\n8.4\n1.0000\n1.0000\n1.0000\n1.0000\n26.1409\n\n\n0.0001\n8.5\n1.0000\n1.0000\n1.0000\n1.0000\n24.2089\n\n\n0.001\n7.3\n1.0000\n0.9999\n1.0000\n1.0000\n20.0847\n\n\n0.01\n8.0\n1.0000\n1.0000\n0.9999\n1.0000\n17.6599\n\n\n0.1\n6.5\n1.0000\n0.9998\n1.0000\n0.9999\n13.8871\n\n\n1\n6.0\n0.9999\n0.9998\n0.9999\n0.9999\n11.0648\n\n\n\nå¯ä»¥çœ‹å‡ºï¼Œæ­£åˆ™åŒ–å¸¸æ•°è¶Šå¤§ï¼Œwçš„L2-normè¶Šå°ï¼Œä¸”æ”¶æ•›é€Ÿåº¦è¶Šå¿«ï¼›å…¶ä»–å‡†ç¡®çŽ‡åˆ™æ²¡æœ‰æ˜Žæ˜¾å·®åˆ«ã€‚\nå€¼å¾—æ³¨æ„çš„æ˜¯ï¼š\n\nåœ¨ä¸åŠ æ­£åˆ™åŒ–é¡¹æ—¶ï¼ŒHessiançŸ©é˜µå¯èƒ½ä¸ºå¥‡å¼‚çŸ©é˜µï¼Œéœ€è¦ç”¨ä¼ªé€†ï¼ˆnp.linalg.pinvï¼‰ã€‚\nç‰›é¡¿æ³•å¯¹wçš„åˆå€¼æ•æ„Ÿï¼Œå¦‚æžœè®¾ç½®å¾—å¤ªå¤§ï¼Œä¼šå¯¼è‡´æ¢¯åº¦å¤±æ•ˆã€‚\n\nè™½ç„¶IRLSæ³•æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°è¾ƒå°‘ï¼Œè€Œä¸”æ¯”è¾ƒç¨³å®šï¼ˆåœ¨wçš„åˆå§‹å€¼è®¾ç½®è¾ƒå¥½çš„å‰æä¸‹ï¼‰ï¼Œä½†ç”±äºŽæ¯æ¬¡è¿­ä»£éƒ½éœ€è¦è®¡ç®—è¾ƒå¤§çš„çŸ©é˜µä¹˜æ³•å’Œæ±‚é€†ï¼Œå› æ­¤æ•´ä½“æ•ˆçŽ‡æ¯”æ¢¯åº¦ä¸Šå‡æ³•å·®ã€‚\n'], 'url_profile': 'https://github.com/zhanghuimeng', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated May 21, 2020', 'Apache-2.0 license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Guadalajara, Jalisco Mexico.', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kiyosh31', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated May 21, 2020', 'Apache-2.0 license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shreyshah02', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated May 21, 2020', 'Apache-2.0 license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '189 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prashan1', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated May 21, 2020', 'Apache-2.0 license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '291 contributions\n        in the last year', 'description': ['Quantitative Trading Strategy\nFinal Project for Regression Analysis & Quantitative Trading Strategies\nStrategists: Robert Hatem (LinkedIn, Github) & Benjamin Morgan (LinkedIn, Github)\nMotivation Behind Exploring this Strategy:\nThe motivation behind exploring this strategy is to explore the investable opportunities in equities by analyzing their sentimental features. Quandl provided pre-computed sentiment features for equities that were easily accessible to analyze. Additionally, we wanted to explore basic machine learning algorithms for predicting next period returns and measure performance against the market as a benchmark.\nSkiena et. al. (2010) uses sentiment data to build simple long-short strategies. Skienaâ€™s dataset contains similar information to ours; it uses feature based on â€œpolarityâ€, how positive or negative the mention is, and volume of news mentions. However, one difference is that Skiena separates information from blogs and news, while our dataset contains all sources together. They find that patterns from opinion in blogs persists longer than from news, which makes sense since opinions are less likely to contain â€œtrueâ€ information that will change investorsâ€™ investment decisions. This paper provides evidence that sentiment-based investing, which we are analyzing in this write-up, is widespread.\nFundamental analysis and investing is a widely-known investing technique that is used in different approaches but is ultimately based on how the company is perfoming as represented by their fundamental financial data. Charles Lee, the Henrietta J. Louis Professor at Cornell University, strives to connect fundamental value and investor sentiment with what is known as ""fusion investing"" in his Fusion Investing paper (2003). Lee states that while ""researchers are finding that even though returns are difficult to predict, prediction is not an impossible task"" and that there are observable systematic patterns in price return series (Lee 2003). While Lee takes the position of observing sentiment value from an investor, our strategy takes the available news sentiment data of the company, and not an individual investor, and attempts to predict performance within those believed systematic patterns.\nCreating a Universe of Equities to Use:\nThe idea of using sentiment as a basis for our strategy requires a few different components. First, we would need a universe of equities to use. We also need specific sentiment data for the strategy, end of day data to measure performance, and fundamental financial ratios as a complement to the sentiment data. The fundamental data consists of a set of Zack\'s files from the Quandl database and both the sentiment data and the end of day data were queried from Quandl using their Quandl API.\nSince the fundamental data is from fixed csv files, which can be found in Quandl\'s Zacks Collection B database, that\'s where the universe of equities started. The fundamental dataset consists of 9107 unique tickers which established the foundation for our universe. The fundamental dataset contains the following columns as features: Weighted Average Shares Outstanding, Current Ratio, Gross Margin, Operating Profit Margin, EBIT Margin, Pretax Profit Margin, Profit Margin, Free Cash Flow, Asset Turnover, Inventory Turover, Receivables Turnover, Days Sales in Receivables, Return on Equity, Return on Tangible Equity, Return on Assets, Return on Investments, Free Cash Flow Per Share, Book Value Per Share, and Operating Cash Flow Per Share.\nWith the understanding that end of day data would be a bit easier to find than sentiment, we queried quandl\'s sentiment database with the 9107 tickers. The database consists of 5 different sentiment features: Sentiment, Sentiment High, Sentiment Low, News Volume, and News Buzz.\n\nThe Sentiment feature is a numeric measure of the bullishness / bearishness of news coverage of the stock.\nThe Sentiment High/Low feature is the highest and lowest intraday sentiment scores.\nThe News Volume feature is the absolute number of news articles covering the stock.\nThe News Buzz feature is a numeric measure of the change in coverage volume for the stock.\n\nWe found that after filtering out the tickers that were not present in the database and handling any errors and data quality issues, such as missing data, the sentiment dataset held 4753 unique tickers with the 5 different feature sets.\nAt that point, we took the smaller of the two ticker lists and scraped Quandl\'s EOD database. After handling any present data quality issues, the EOD database contained 2490 unique tickers.\nAfter examining the three unique datasets, the selection of tickers to use for the merging of the three datasets came from the database with the smallest number of tickers: the end of day data.\nRunning through the concatenating loop and adding a few more necessary filters to handle any remaining data quality issues during the concatenation, leaves a total of 1130 tickers in our universe with the necessary fundamental data, sentiment data, and end of day data.\nThe output of the ObtainDataFile.py script is a dictionary with two initial keys: Raw_Data and Factors. The Factors key contains a dictionary of monthly and daily Five Factor datasets from Fama and French. The Raw_Data value contains 1130 dictionaries with the ticker as the key and a dataframe from the concatenation as the value.\n# from ObtainDataFile import *\n#from PerformanceMetrics import *\n# from strategy import *\n\nBy running the start_running() function, you will start the process of gathering and combining the data streams together to return a single dictionary data structure with the universe of tickers.\n# all_data = start_running()\n# raw_data = pd.read_pickle(\'complete_dataset.pickle\')\n\nData compilation is complete. \n1130 tickers in universe.\nFile name is complete_dataset.pickle and is located in the local directory.\n\nThis .read_pickle() statement will read, instead of compile, the single dictionary data structure with the universe of tickers.\nStrategy Description:\nThis is a long-only equity strategy with monthly frequency. The equities for the given month are selected based on their predicted probability of having positive returns in the next month. The probability is predicted using three models averaged together:\n\nlogistic regression\ngradient boosting,\nLSTM (long short-term memory)\n\n\nTarget Variable\n\n0: returns in following month are negative\n1: returns in following month are positive\n\nIn other words, we predict the binary target variable of returns being positive or negative in the following month, using three sepearate models, then average their predictions (which are probabilities).\nEquity selection\n\naverage the three models\' predictions\nselect top 20%, go long a random half of them (10% total)\ngo short the market by an amount $\\beta$ times the amount you go long the equity. This is to be market neutral.\n\nBeta is calculated as the correalation of the equity\'s daily returns with the market\'s returns (Nasdaq) over the previous month. We do not go short any equities to avoid the complexities of shorting.\nThe models are selected using grid-search for logistic and gradient boosting, and using default values for the LSTM. The LSTM uses time windows of length (i.e. predicting next-month\'s return sign using the prior 3 months\' values). The LSTM could be futher optimized to possibly get better performance.\nInput features\n\nFeatures come as daily\nthen aggregated to monthly using quantiles\n50%, 75%, and 90% quantile of daily values over the previous month.\n\nFor example, five daily features would turn in to 15 monthly features (5 features * 3 quantiles). This aggregation method allows us to use information about the higher values from the previous month (90% quantile) as well as the more typical values (50% quantile).\nData\n\n1130 tickers\n76 months (January 2013 to April 2019)\n69 features\n\xa0\xa0* 23 base features, each with 3 quantiles\n\xa0\xa0\xa0\xa0* 5 sentiment\n\xa0\xa0\xa0\xa0* 17 fundamental\n\nGenerating Portfolios from our Randomly Selected Equity Positions in the top perfoming 20% based on Performance Metrics\nOne idea was to take a random selection of positions that the strategy lists in the top 20% -quantile. We observed 100 generated portfolios that all produced similar results, as you can see below. The figure of 100 histograms shows the distribution of their returns with a density line for each chart.\n\nEach portfolio performed similarly across the months as well. Below, you can see a single histogram with a density line describing the mean returns across all portfolios for each month.\n\nBy taking the standard deviation of the monthly PnL across all the portfolios, we can observe the values increasing as time continues, as the figure below shows.\n\nAnalysis on Generating Portfolios from Random Equity Selections:\nWhile the initial thought seemed like a good idea, in reality, the randomly selected equities to populate portfolios showed that there was no outstanding value in randomly selecting from our top performing equities in the top 20%-quantile. This can easily be viewed in the Distribution of Portfolio Returns... figure showing that the portfolios more or less behaved the same over the given time period.\nRunning the Strategy:\n# i = 0.7 #fraction that is training data\n# b_cap = 10 #beta cap\n# s = True #includes sentiment features\n# df_test, results = develop_all_three_models(frac_training=i, use_sentiment=s)\xa0\xa0\xa0\xa0\n# results_summary = run_strategy(df_test, ticker_dict=raw_data[\'Raw_Data\'], sort_column=\'y_pred_avg\', \n#                                seed=False, frac_long_tickers=0.01, mult_extra=2, beta_cap_floor=-b_cap, \n#                                beta_cap_ceil=b_cap, plot=False)\n\nStrategy Results and Performance Metrics:\n# take input of strategy. \n# total_dict = getStatsAll(results_summary) #using a beta cap of +/- 10.\n\n \nNotice: this analysis is in-sample inference only.\nThe longer testing periods are more reliable\n\nThe strategy is developed using progressivley larger training sets. The orange line was trained on only 10% of months (starting January 2013), and tested on the remaining 90% of months (up to April 2019).\nThe strategy which was tested the shortest is the brown, and its returns are not good. In its five months, it swings widely and finishes barely positive.\nHowever, this strategy (three models estimated on training data, then averaged) is the least reliable because it was tested on the shortest period. It doesn\'t get much time to reveal its performance on test data, so we do not rely on these short test period.\nHowever, using a shorter training set risks the model getting enough data to find the patterns in the data. With shorter training sets, the model could be less predictive, but we will more likely detect that it is unpredictive. With longer training sets, the model could be more predictive, but we are less likely to detect that it is unpredictive.\nThe brown line is the least reliable, due to its short testing period.\n\nThe trend is up, but with a large drop at the end\n\nBy contrast, the orange line was tested on 90% of the data, giving it a long testing period to reveal its performance out-of-sample. The orange line has an upward trend and a sharp dip at the end. This suggests some optimism, but so does the market returns. Also, there is concern from the large concern for the sudden drop.\n\nLarge drop comes from large a net short position in the market moving against us\n\nThe drop is due to large net short position in the market that happends to turn against us. This large short position comes from an unusually large individual beta. The large net short position (from one stock with unusually large beta) is not unique, but the adverse price change while holding the large position is unique. There are other periods where we hold large short market positions, but the price doesn\'t move so badly in those periods. In February and March, it does hurt us.\n\nMedium-range forecasts are similar, but with a slower start\n\nThe remaining three strategies (greed, red, purple) show similar patterns; general upward trends with a large drop near the end (February and March). However, they are begin flat for the first few months before starting their rise.\n\nLet\'s address the large drop at the end\n\nAgain, the large drop is due to large shorts market positions moving against us. It\'s difficult to predict when the market will move against us, so instead we will cap our short positions so that markets drops don\'t hurt as much (favorable market moves will also help us less under capping. It limits both downside and upside).\nWe now apply a hard cap to the short market positions that we are taking. The betas of any given stock in any prior month are generally less than 5, but sometimes larger 10 or even 30. The strategy runs above used a cap of 10, so now we tighten the cap to 5 (applied as absolute value of 5), and 1, to see if we can soften that large drop.\n\nAnalysis\n# take input of strategy. \n# total_dict = getStatsAll(results_summary, beta_plot = True, training_split_value_to_plot=0.3) #using a beta cap of +/- 1. \n\n  \nRegarding the Top Figure above:\n\nWith the tight cap on beta (between -1 and 1), we again see a general upward trend. You can see that the strategy using the training size of 30% (green line) in the tightest beta constraint configuration of 1.0, outperforms the other training sizes in the same beta cap grouping.\n\nRegarding the Middle Figure of 5 plots above:\n\nYou can see that when the beta cap group of training sizes is separated into individual charts, the split size of 30% and 50% outperform the market but only the 30% split results significantly outperform the market.\n\nRegarding the Bottom Figure above:\n\nWe choose training fraction of 0.3, then show the strategy using different caps for beta. There is mostly overlap, but the strictest beta cap (1.0) shows highest performance, especially by the end of the simulation.\nAlso, the sharp drops around November 2016 and March 2019 seem to be softened, as was expected from the capping. However, the small caps (5 & 10) didn\'t seem to curtail the drops; only cap of 1 seems to work.\nFor the first half of the strategy, capping seemed to have minimal effect. But by the end, the strict cap lead to the best performance.\nWhile capping could plausibly improve PnL here, keep in mind that it would also reduce the market-neutrality, which was the point of investing the index in the first place. We do not explore whether market-neutrality was affected here.\nThe optimal stratey identified is when the training size is only 30% of the entire dataset with a beta cap of +/- 1.0. You can see that in the bottom figure below, the Beta Cap: 1.0 line easily outperforms the others in the same training size group and outperforms the Market, the purple line. These results show that the strategy made smart decisions, did not take too many losses, and capitalized on an increasing Market.\n\n\nThe image above represents the dataframe of strategy results using a beta cap of 1.0. Our best performing strategy, using the training split of 30%, had an Annualized Return of 18.77%, Annualized Volatility of 27.73%, an Annualized Sharpe of 0.605, a Max Drawdown of 0.167, and an Alpha of 0.014. Below, you wcan see just how the strategy performs against the market since the start of 2015. The strategy looks promising but we need to stress test to be sure.\n\nStress Testing Against Simulated 2008 Performance Compared to Recent Performance Without Sentiment\nSince the sentiment data does not go past 2013, we are opting to stress test the strategy by observing the strategy\'s performance without the sentiment features from 2015 to 2019 and observe the strategy\'s performance between 2005 and 2010. Then we will compare the differential performance of the annualized market returns and annualized strategy returns between the two time periods.\n\n\nThe strategy can be observed against the market from 2015 to 2019 in the time period above. Take note that the Annualized Strategy Return is 2.74% against the Market\'s Annualized Return of 12.67%. We will be comparing these along with the other listed metrics to the strategy\'s performance from 2015 to 2010.\n\n\nStress Testing Analysis\nNow taking a look at the performance of the stressed strategy from 2007 through 2010. The strategy does produce a higher Annualized Strategy Return of 5.09% against the Market\'s Annualized Return of 0.58%. We do see a larger max drawdown and a larger portfolio beta on the stressed strategy and this can be expected. It is surprising to see that not only did the strategy perform positively during this period, it beat the market and beat the 2015-2019 period we were comparing it to.\nSome of the other metrics did perform better in the 2015-2019 dataset, such as a drawdown that is almost half at 0.259 and a much lower portfolio beta at 0.332. Volatility  was higher for both the strategy and the market during the stress period and this is expected since this was a purposeful stress run during a known financial crisis.\nConcluding Thoughts:\nWe conclude that there is evidence of proof of concept utilizing a strategy that leverages sentiment data. While these initial results seem profitable and optimistic, it would be wise to continue with our testing by continuing to fine-tune the machine learning models, refine the universe filtering, and stress test against a different variety of scenarios and not just a single crisis. Sentiment data proves to be an unique feature set and as new natural language processing techniques continue to develop and employed, sentiment data will continue to grow and offer new ways of exploring different investment opportunities.\nBibliography & Data Sources:\n\nFrench, Kenneth R. Kenneth R. French - Home Page. Accessed June 01, 2019.\n\xa0\xa0\xa0\xa0http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/index.html.\nLee, Charles M.C. 2003. Fusion Investing. Tech. Fusion Investing. AIMR.\n""Quandl."" Quandl.com. Accessed June 01, 2019.\n\xa0\xa0\xa0\xa0https://www.quandl.com/.\n""The Place For Learning Quant Finance."" Quantopian. Accessed June 01, 2019.\n\xa0\xa0\xa0\xa0https://www.quantopian.com/.\n""Where Developers Learn, Share, & Build Careers."" Stack Overflow. Accessed June 02, 2019.\n\xa0\xa0\xa0\xa0https://stackoverflow.com/.\nZhang, Wenbin, and Steven Skiena. 2010. Trading Strategies to Exploit Blog and News Sentiment. Tech.\n\xa0\xa0\xa0\xa0Trading Strategies to Exploit Blog and News Sentiment. Association for the Advancement of Artificial Intelligence.\n\n'], 'url_profile': 'https://github.com/bmorgan0921', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated May 21, 2020', 'Apache-2.0 license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Toronto, ON, Canada', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['housing-model\nA regression model to predict housing price in Toronto\n'], 'url_profile': 'https://github.com/mjlaali', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated May 21, 2020', 'Apache-2.0 license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Melbourne, VIC', 'stats_list': [], 'contributions': '387 contributions\n        in the last year', 'description': ['Advanced Regression Techniques to Forecast Bushfires\nIn this project we aim to create models that can predict the area burned in fires. We will be working with data from the UCI machine learning repository and is a collection curated by scientists Paulo Cortez and Anibal Morais from the Unviersity of Minho, Portugal.\nThe data includes meteorological measurements taken at the time the fire was reported, spatial/temporal measurements, and index metrics that take into a account weather data in the recent past. The index metrics are a part of the Canadian Fire Weather Index System (FWI). The data was collected from January 2000 to December 2003 from the northeast region of Portugal. Data was collected from a total of 517 fires.\nMany different ML models can be applied - in the basis paper the one who achieved the best performance was Support Vector Machines (SVM). However, in this project we will be testing advanced regression techniques only.\nProject Development\n\nData exploration and manipulation. Data reshaping and outlier removal to enhance the dataset.\nCreate models to predict the burned area according to different regression techniques.\nCompare the models performance and choose a winner.\n\nAdvanced Regression\nWe make use of advanced techniques such as subset selection, removal of influential points and so on.\n\n\nReference (Dataset)\nP. Cortez and A. Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data.\nIn J. Neves, M. F. Santos and J. Machado Eds., New Trends in Artificial Intelligence,\nProceedings of the 13th EPIA 2007 - Portuguese Conference on Artificial Intelligence, December,\nGuimaraes, Portugal, pp. 512-523, 2007. APPIA, ISBN-13 978-989-95618-0-9.\nAvailable at: http://www.dsi.uminho.pt/~pcortez/fires.pdf\n'], 'url_profile': 'https://github.com/Agewerc', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated May 21, 2020', 'Apache-2.0 license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'ghaziabad', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['Scratch-_losistic_nueral_model\nHard coded basic logistic regression nueral network model\n'], 'url_profile': 'https://github.com/pranjul6386', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated May 21, 2020', 'Apache-2.0 license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'Annapolis, MD', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['yelp_rating_predictor\nA Yelp Rating Linear Regression Model. Data obtained from Yelp at https://www.yelp.com/dataset\n'], 'url_profile': 'https://github.com/dbauer19', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Terra', 'Updated Apr 1, 2020', 'Python', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated May 21, 2020', 'Apache-2.0 license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['LinearRegression\nit consist of regression model built using python.\n'], 'url_profile': 'https://github.com/BinduPallaki', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated May 13, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Boston, USA', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['linear-regression-from-scratch\nLearning to build the linear regression predictor from scratch.\n'], 'url_profile': 'https://github.com/TheGreymanShow', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated May 13, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Predicted-cancer-probability\nThe probability of cancer predicted by logistic regression\n'], 'url_profile': 'https://github.com/yiwangde', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated May 13, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Assignment2\nREPOSITORY FOR ASSIGNMENT 2 THROUGH SIMPLE LINEAR REGRESSION\n'], 'url_profile': 'https://github.com/ahmer-cs', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated May 13, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Bay Area', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/trdelgado', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated May 13, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '561 contributions\n        in the last year', 'description': [""game-sales-FIDS\nThis project presents an analysis of video game sales, using a dataset of games sold after the year 2000. We (Bryan Santos and Tim Lee) attempt to conduct a regression analysis to unlock actionable business insights.\nIn this repository are two notebooks:\n\ntech_presentation_notebook - This notebook serves as a walkthrough through our initial data load and exploration, hypothesis testing, and feature engineering.\nregression_models - This notebook contains our regression anaysis, including the training process for 19 models using various combinations of features, regression techniques, and polynomial complexity.\n\nOur presentation deck is located here: https://docs.google.com/presentation/d/1I1X66QhRbQfrPFrWEN2oMRlhWjOwGluZ-PZPwMvtNwo/edit?usp=sharing\nEDA and Visualizations\nOur EDA and visualizations can be viewed in detail in the tech_presentation_notebook.\nFeature Engineering\nWe added additional data and engineered several features, mostly related to the games' platforms/consoles:\n\nA percentage of where in the console's lifespan a game was released (to identify launch titles)\nA flag to indicate whether a game was released on an obsolete console (such as a PS2 game released after the PS3 launch)\nA flag to indicate games that were re-released on a future generation of a console\nAggregations of Metacritic user and critic scores to serve as proxies for a game's publisher, developer, and genre\n\nModels\nWe generated 19 models, experimenting with various the inclusion of features, feature engineering, ridge regression, elastic regularization, gradient boosting, and polynomial complexity. We concluded that our 17th model was the best, with all features (and engineered features), and a third-order polynomial regression. Increasing the polynomial order or adding elastic regularization both risked overfitting the training data, as the spread between training and test R2 began to diverge.\nModel 17: RMSE: 0.931, R2 (train): 0.434, R2 (test): 0.414\n\nKey Features\nWe identified three key features with an effect on our model.\n\nConsole units sold. This information was added as reference data to account for this simple axiom: people won't buy games for consoles they don't own. The PS2 and Dreamcast were competitors, but the PS2 outsold the Dreamcast 16:1. PS2 games would logically sell more units than Dreamcast games. Though this is a rather self-explanatory feature, we didn't achieve satisfactory models until adding this feature.\nCurrent generation flag This engineered feature indicates when a game was released for an obsolete console, a console whose successor has launched. A two-sample t-test suggested that these games have lower sales.\nMetacritic score This is a representation of a game's critical reception. Better-quality games have higher sales. It's worth noting that we observed a negative correlation between Metacritic's user scores and a game's sales. It may be because popular games attract lots of negative attention online, and dissatisfied users will take to the web to complain.\n\n\n\nConclusions and Use-Case Recomendations\nWe identified three business-case recommendations as guidance when planning development of games.\n\nMake critically-accepted games, but ignore complaining from the masses Game quality and reception DOES matter, but only from the critics. Professional critics and journalists are qualified at evaluating games, and their clout does matter in shaping public perception of games. However, finding a negative correlation between user scores and game sales was surprising, and indicates it may not be best to try and pander to the loudest or the most vocally dissatisfied customers.\nDon't make games for obsolete consoles We observed a drop in sales in games that were released on obsolete consoles. Games have moved onto the next generation, and resources are better spent capturing customers on the latest console.\nDon't bother re-releasing games on successor consoles Conversely, if a game is released on a console near the end of its lifespan, it may not be worth it to spend development time and effort porting the game to the successor console. We saw higher sales of games on the predecessor console. Perhaps gamers interested in the game would have bought it at their first opportunity, when available on the predecessor console. We admit that there is one scenario where this suggestion loses validity: if fans are actually purchasing the game twice. However, to negate this suggestion, the increased sales from gamers purchasing the game twice would have to offset the development resources spent on porting the game to the new console.\n\n\n\n\n""], 'url_profile': 'https://github.com/JohnTheTripper', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated May 13, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': [""Housing-Price-Predictor\nHousing price predictor using Linear Regression model of sklearn\nExecute the 'HousingPrice.py' python file\nScreenshots -\n\n""], 'url_profile': 'https://github.com/adityanair239', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated May 13, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Simple_Linear_Regression_by_python\nSimple Linear Regression using Python for machine learning\n'], 'url_profile': 'https://github.com/amitdivekar30', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated May 13, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/K2587', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated May 13, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Africa', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dahnny', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated May 13, 2020', '1', 'Python', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TalpalaruBogdan', 'info_list': ['C#', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nntrongnghia', 'info_list': ['C#', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Boston, USA', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['logistic-regression-from-scratch\nLearning to build a logistic regression classifier from scratch.\n'], 'url_profile': 'https://github.com/TheGreymanShow', 'info_list': ['C#', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '268 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ernestng11', 'info_list': ['C#', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Malaysia', 'stats_list': [], 'contributions': '153 contributions\n        in the last year', 'description': [""credit_ml\n\nSimple Web App\nSimple Flask App as the backend model for Machine learning. This project is inspired by a Predictive Model that I've assigned to train during my Predict Model assignment.\nI thought that this would be a waste if I could deploy it online and let users to consume the Web App.\n""], 'url_profile': 'https://github.com/zernonia', 'info_list': ['C#', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shashiranjan55', 'info_list': ['C#', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '224 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MaripiPerea', 'info_list': ['C#', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Usamamalik11', 'info_list': ['C#', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/Fadhilah24', 'info_list': ['C#', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MohammedRaheemP', 'info_list': ['C#', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amitk2587', 'info_list': ['Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['This is an implementation of Logistic Regression.\n'], 'url_profile': 'https://github.com/ibrahimzafar', 'info_list': ['Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['logisticRegression\nusing logistic regression we predict the loan outcomes (to issue or reject the loan) from bank customer dataset.\n'], 'url_profile': 'https://github.com/ramo16', 'info_list': ['Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '276 contributions\n        in the last year', 'description': [""Linear-Regression\nMovie's Revenue Prediction using 5k+ movie's budget and revenue database.\n""], 'url_profile': 'https://github.com/pritom02bh', 'info_list': ['Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['Amazon Fine Food Reviews Analysis\nData Source: https://www.kaggle.com/snap/amazon-fine-food-reviews\nAbout Dataset:\nThe Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.\nNumber of reviews: 568,454\nNumber of users: 256,059\nNumber of products: 74,258\nTimespan: Oct 1999 - Oct 2012\nNumber of Attributes/Columns in data: 10\nAttribute Information:\n\nId\nProductId - unique identifier for the product\nUserId - unqiue identifier for the user\nProfileName\nHelpfulnessNumerator - number of users who found the review helpful\nHelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\nScore - rating between 1 and 5\nTime - timestamp for the review\nSummary - brief summary of the review\nText - text of the review\n\nObjective:\nGiven a review, determine whether the review is positive (rating of 4 or 5) or negative (rating of 1 or 2).\n[Q] How to determine if a review is positive or negative?\n[Ans] We could use Score/Rating. A rating of 4 or 5 can be cosnidered as a positive review. A rating of 1 or 2 can be considered as\nnegative one. A review of rating 3 is considered nuetral and such reviews are ignored from our analysis. This is an approximate and\nproxy way of determining the polarity (positivity/negativity) of a review.\n'], 'url_profile': 'https://github.com/sanjeevpalla', 'info_list': ['Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/storari04', 'info_list': ['Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Linear-regression\nHere we will be implementing the linear regression algorithm to predict students final grade based on a series of attributes.\n'], 'url_profile': 'https://github.com/prasanth-99', 'info_list': ['Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020']}","{'location': 'Houston, TX', 'stats_list': [], 'contributions': '167 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Yasir77788', 'info_list': ['Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Samarche92', 'info_list': ['Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '374 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/testAlm', 'info_list': ['Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/juhjoo', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Linear-Regression\nUsing Gradient Descent in Matlab\n'], 'url_profile': 'https://github.com/AswinVishnuA', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Logistic-Regression\nfor detail variable please see on Leads X Education Data Dictionary.xlsx\nfor see my coding please see on Lead Scoring.ipynb\nfor see the data set please open Leads X Education.csv\n'], 'url_profile': 'https://github.com/sel599', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Linear-Regression\nAssignment for Machine Learning course.\n'], 'url_profile': 'https://github.com/jamie22334', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Harshitasahni', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '261 contributions\n        in the last year', 'description': ['regression-notebooks\n'], 'url_profile': 'https://github.com/fergmack', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rubenlop', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Logistic-regression\nBuilt a model using a  telecommunication company dataset to predict when its customers will leave for a competitor, so that they can take some action to retain the customers.\n'], 'url_profile': 'https://github.com/santoshsahini19', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/davidcloser', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Regression_Methods\n'], 'url_profile': 'https://github.com/wassonxu', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'MATLAB', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 13, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DhusorAust', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Dec 4, 2020', 'Updated Oct 15, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['LinearRegression\nUsing Linear regression to check for the relationship between price of Lego set and number of pieces.\n'], 'url_profile': 'https://github.com/SurajSajjan', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Dec 4, 2020', 'Updated Oct 15, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '163 contributions\n        in the last year', 'description': ['king-county-housing\nHousing Price Prediction - by  paul williams\n'], 'url_profile': 'https://github.com/PaulWill92', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Dec 4, 2020', 'Updated Oct 15, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['regression-day\nGetting started\n\nDownload the selenium IDE browser addon from here\nOpen the regression-day.side file from this repo into the selenium IDE\nRun each test suite\n\n'], 'url_profile': 'https://github.com/im-fdavies', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Dec 4, 2020', 'Updated Oct 15, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Logistic-Regression\nthis linear regression code still eror.\ntrain-images-idx3-ubyte cannot be upload because the file size is larger than 25 mb\n'], 'url_profile': 'https://github.com/Fadhilah24', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Dec 4, 2020', 'Updated Oct 15, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'Cairo', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': ['All you need about Logistic Regression (Classification)\nsome of text and snapshots are related to coursera Machine Learning Course\nsome tasks also solved in Octave language\nsome of tasks implemented in different libraries and frameworks like Tensorflow and sklearn\nThe repo is cosisit of these things:\nMaterials\n\nMaterials in Arabic from my studying to different courses\nEnglish slides for Logistic Regression that Explain the different aspect of Classification\nExamples For more understanding\n\nTasks Implemented\n\nHello Logistic Regression\npython code for Binary Classification with Andrew Ng Course\nManual Logistic Regression Task from ex2 of week3 Ml By Andrew Ng in python.\nSklearn Logistic Regression Task from ex2 of week3 Ml By Andrew Ng in python.\nTensorflow Logistic Regression Task from ex2 of week3 Ml By Andrew Ng in python.\nTensorflow clothes Image classifcation\nTensorflow handwriting numbers Image classifcation\n\nDifferent Graphs that help get more intuition\n\nploting data\nSigmoid function\nCost function graph\n\nsome snapshot of different graph and slides and Examples\n\nploting data\n\n\n\nSigmoid function\n\n\n\ndifferent Cost function plots\n\n\n\n'], 'url_profile': 'https://github.com/Abdelrahmanrezk', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Dec 4, 2020', 'Updated Oct 15, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['logisticRegression\n'], 'url_profile': 'https://github.com/BinduPallaki', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Dec 4, 2020', 'Updated Oct 15, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'Corvallis, OR', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rongfang323', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Dec 4, 2020', 'Updated Oct 15, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sriramaraju423', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Dec 4, 2020', 'Updated Oct 15, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Oficina Logistic Regression\nObjetivo e escopo: Apresentar a teoria de modelo de regressÃ£o linear, e apllicar a um exemplo em python sobre classificaÃ§Ã£o do coronavÃ­rus (Covid-19). Oficina apresentada em reuniÃ£o LAMFO em 28 de marÃ§o de 2020.\nApresentadores:\n\nAlÃ­cia Macedo\nAlixandro Werneck\nLucas Moreira Gomes\n\nTeoria - Slides e explicaÃ§Ã£o\nOs slides utilizados na apresentaÃ§Ã£o estÃ£o disponÃ­veis em:\n\nhttps://pt.overleaf.com/project/5e7df851dc340200018380cf\n\ne o post blog estÃ¡ disponÃ­vel na pasta BLOG deste projeto.\nCÃ³digos\nO cÃ³digo utilizado para apresentaÃ§Ã£o da oficina aplicou o processo de aprendizado em um data-set de casos de coronavÃ­rus.\nImportando dados\nOs dados utilizados para os casos de cornavÃ­rus sÃ£o disponibilizados no Kaggle.\n\nhttps://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/version/47\n\nPrimeiramente importamos os dados e procuramos por valores ruins ...\nimport pandas as pd\nimport random\n\ndata = pd.read_csv(""COVID19.csv"",usecols=[""age"",""death"",""gender""])\nprint(len(data))\n\nprint(data) #checando dataframe\n\nprint(data.death.unique()) #checando por valores ruins\nprint(data.age.unique())\nprint(data.gender.unique())\nAgora corrigimos os valores onde precisamos\ndata = data.dropna(subset=[\'gender\']) #tirando linhas que nÃ£o tenham informaÃ§Ã£o sobre o gÃªnero ..\n\ndef gender(x): # funÃ§Ã£o para convertendo as variÃ¡vel categÃ³rica para valor numÃ©rico\n    if x == ""male"":\n        return 0\n    else:\n        return 1\n\ndata[\'gender\'] = data[\'gender\'].apply(lambda x: gender(x)) #aplica a funÃ§Ã£o gender no dataframe data coluna gender\nprint(data.gender.unique())\n\ndata = data[(data[""death""] == ""1"" ) | (data[""death""] == ""0"" )] # deixamos apenas os resultados para 1 e 0.\n\nprint(data.death.unique())\n\n\nprint(data.mean())\ndata = data.fillna(data.mean()) #completamos valores nÃ£o disponÃ­veis pela mÃ©dia de cada coluna.\nEm seguida preparamos os dados para serem processados. X sÃ£o nossas observaÃ§Ãµes sobre o output (variÃ¡veis) e Y nosso output. Nesse caso, Y Ã© a informaÃ§Ã£o sobre a morte (1) ou nÃ£o morte (0) de um paciente.\ny = data[""death""] \nX = data.drop(""death"",axis=1)\nSeparamos os dados em dois: Treino e teste. Aqui, usamos 10% para teste, e 90% para treinamento.\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)\n# print(X_test)\nAntes de converter nossos dados para um vetor que possa ser analisado pelo modelo, precisamos fazer com que todos os valores de X estejam na mesmo proporÃ§Ã£o. Para isso, fazemos com que eles fiquem entre -1 e 1, para qualquer variÃ¡vel em X usando a funÃ§Ã£o StandardScaler().\nfrom sklearn.preprocessing import StandardScaler \nsc_X = StandardScaler() \nX_train = sc_X.fit_transform(X_train) \nX_test = sc_X.transform(X_test)\nAgora treinamos o modelo, e predizemos os resultados para os valores de treinamento\nfrom sklearn.linear_model import LogisticRegression \nclassifier = LogisticRegression(random_state=0,class_weight = ""balanced"") \nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\nPara avaliar a precisÃ£o do modelo, utilizamos uma matriz de confusÃ£o.\nfrom sklearn.metrics import confusion_matrix \ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nPor Ãºltimo, plotamos os resulados para conferir visualmente nosso classificador (verde Ã© saudÃ¡vel e vermelho Ã© doente).\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap \nX_set, y_set = X_train, y_train \nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) \nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap((\'green\', \'red\')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max()) \nfor i, j in enumerate(np.unique(y_set)): \n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap((\'green\', \'red\'))(i), label = j) \nplt.title(\'(COVID) Logistic Regression (Training set)\') \nplt.xlabel(\'Sexo\') \nplt.ylabel(\'Idade\') \nplt.legend() \nplt.show()\n\nReferÃªncias\n\nhttps://www.marktechpost.com/2019/06/12/logistic-regression-with-a-real-world-example-in-python/\nhttps://towardsdatascience.com/introduction-to-logistic-regression-66248243c148\nhttps://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/\n\n'], 'url_profile': 'https://github.com/lamfo-unb', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Dec 4, 2020', 'Updated Oct 15, 2020', 'MATLAB', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogistic Regression is a supervised classification algorithm used to predict the probability of a target variable. It can be used for various classification problems that can be either binomial or multinomial.\nHere, I have taken few datasets from the internet and tried to fit a logistic regression model on them.\nFirst, the dataset used is Titanic that predicts which passenger the Titanic shipwreck, and was taken from Kaggle. The variable survived needs to be predicted and that is done using all other variables from the dataset. The survived column contains the values 0 and 1. The accuracy obtained is 79.85%.\nNext dataset used is Pima Indians Diabetes which is available in the package mlbench. The variable diabetes needs to be predicted and all the other variables are used as the independent variables. Diabetes column has values ""pos"" and ""neg"". The accuracy obtained is 73.91%.The ROC curve and AUC was drawn, the AUC value obtained was 0.82.\nNext dataset used is Seeds dataset from Kaggle. Here the variable to be predicted (dependent variable) is class and has 3 levels: 1, 2 and 3 and all the other independent variables were used for building the model. The accuracy obtained is 95.24%.\n'], 'url_profile': 'https://github.com/AnushreeChakraborty', 'info_list': ['R', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'R', 'Updated Mar 28, 2020', 'SAS', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'HTML', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 26, 2020']}","{'location': 'New Jersey, USA', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['Polynomial-Regression-\nPersonal Mini-projects (R and Python)\nTruth Detector\nIs you new Employee lying about how much he got paid on his previous job?\nA potential employee told his new company that his current salary is $250k, the human resource retrieved the salary amount\nassociated position levels in his previous company. The potential new employee has a position level 7.5, we would like to\nbuild a salary bluffing detector to see if he told the truth.\n'], 'url_profile': 'https://github.com/AdeloreSimiloluwa', 'info_list': ['R', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'R', 'Updated Mar 28, 2020', 'SAS', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'HTML', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogistic Regression on Titanic Dataset classifying survived people and unsurvived people with some basic Data Wrangling.\n'], 'url_profile': 'https://github.com/RajSalvi738', 'info_list': ['R', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'R', 'Updated Mar 28, 2020', 'SAS', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'HTML', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression is a Machine Learning algorithm based on supervised learning. It gives a linear approach to modeling a linear relationship between a dependent variable and one or more independent variables. It can be either simple or multiple depending on the number of independent variables. It performs the task of predicting the dependent variable on the basis of the independent variables and hence fits a linear relationship between the input and the output variables.\nHere I have taken few datasets from various repositories and tried to fit a Linear Regression model on them.\nFirst, I have taken the Student Performance dataset from UCI Machine Learning Repository. Here, a multiple linear regression model was fitted to the dataset where the variable G3 (i.e., the final score of the student) was predicted using all the other variables with accuracy 80.45%. Also, simple linear regression models were fitted with variables G1 and G2. For getting a better fit a combination of G1 and G2 was also used for the prediction. The final conclusion was: G2 or a combination of G1 and G2 gives more accuracy and hence gives a better fit to the model.\nThe next dataset used was Real Estate Price Prediction from Kaggle. Here, the price of the house needs to be predicted (dependent variable) on the basis of various other factors as the independent variable. A multiple linear regression model was fit using the independent variables distance to the nearest MRT station, number of convenience stores, latitude and longitude with an accuracy 79.29%. Then, simple linear regression model were fitted using each of these variables. It was clearly seen that no. of convenience stores gives a better fit for the model than latitude and longitude as it gives more accuracy.\nThe next dataset used was Medical Cost Personal Datasets from Kaggle. Here, a multiple linear regression model was fitted to the dataset where the variable charges  (dependent variable) was predicted using all the other variables with accuracy 91.83%. Also, a simple linear regression model was fitted with variable age (of the house) with accuracy 69.95%. It can be clearly seen that the multiple linear regression model gives a better fit for the data set as it gives more accuracy.\n'], 'url_profile': 'https://github.com/AnushreeChakraborty', 'info_list': ['R', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'R', 'Updated Mar 28, 2020', 'SAS', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'HTML', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 26, 2020']}","{'location': 'KrakÃ³w, Poland', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Logistic_Regression\nThis project was prepared for the subject ""223480-1528 Regresja logistyczna z wykorzystaniem narzÄ™dzi SAS"".\nThe subject of consideration is the analysis of the reasons for divorce, using logistic regression and SAS Enterprise software.\nThe conclusions were written in the PDF file (in Polish).\n'], 'url_profile': 'https://github.com/mic9410', 'info_list': ['R', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'R', 'Updated Mar 28, 2020', 'SAS', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'HTML', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sivakishor24', 'info_list': ['R', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'R', 'Updated Mar 28, 2020', 'SAS', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'HTML', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 26, 2020']}","{'location': 'CHANGA', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Understanding Linear regression using Ordinary Least Square method and Gradient Descent\n'], 'url_profile': 'https://github.com/hemantnyadav', 'info_list': ['R', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'R', 'Updated Mar 28, 2020', 'SAS', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'HTML', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 26, 2020']}","{'location': 'Navi Mumbai', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['Regression-Analysis\nIn this repository, there is a representation of various regression techniques like Linear Regression and Logistic Regression.\n'], 'url_profile': 'https://github.com/pragyy', 'info_list': ['R', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'R', 'Updated Mar 28, 2020', 'SAS', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'HTML', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Logistic_Regression\nThis is a example of Logistic Regression\n'], 'url_profile': 'https://github.com/TengLi931128', 'info_list': ['R', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'R', 'Updated Mar 28, 2020', 'SAS', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'HTML', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 26, 2020']}","{'location': 'Warangal', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Abstract:\nThe data set contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006â€“2011), when the plant was set to work with full load.\nData set information:\nThe data set contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006â€“2011), when the power plant was set to work with full load. Features consist of hourly average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict the net hourly electrical energy output (EP) of the plant. A combined cycle power plant (CCPP) is composed of gas turbines (GT), steam turbines (ST) and heat recovery steam generators. In a CCPP, the electricity is generated by gas and steam turbines, which are combined in one cycle, and is transferred from one turbine to another. While the Vacuum is collected from and has effect on the Steam Turbine, the other three of the ambient variables effect the GT performance.\nAttribute information:\nFeatures consist of hourly average ambient variables\xa0\n1. Temperature (T) in the range 1.81Â°C and 37.11Â°C,\n2.  Ambient Pressure (AP) in the range 992.89â€“1033.30 millibar,\n3. Relative Humidity (RH) in the range 25.56% to 100.16%\n4. Exhaust Vacuum (V) in the range 25.36â€“81.56 cm Hg\n5. Net hourly electrical energy output (EP) 420.26â€“495.76 MW\n\xa0The averages are taken from various sensors located around the plant that record the ambient variables every second. The variables are given without normalization.\n'], 'url_profile': 'https://github.com/Rakesh148', 'info_list': ['R', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 23, 2020', 'R', 'Updated Mar 28, 2020', 'SAS', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'HTML', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/1753036', 'info_list': ['R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Jan 29, 2021']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['logistic-regression-\npredict the chance of survival of people on titanic ship\n'], 'url_profile': 'https://github.com/chauhan539', 'info_list': ['R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Jan 29, 2021']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['linear-regression\nThis project uses linear regression to carry out predictions from a training dataset\n'], 'url_profile': 'https://github.com/tanushree-ghai0201', 'info_list': ['R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Jan 29, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Montreal Live TV Channels Prediction\nThese data belong to the Montreal Local TV channels, which contain some information. The goal is to develop a model that predicts outcomes within a specified error threshold (to be determined), and it should predict the Market Share_total column for the next season based on the features available at the data set.\n\n\nDownload and put these files into the ""data"" directory.\nData.csv ,\nTest.csv\n\n\nInstall required packages : conda install --file requirements.txt\n\n\nOpen main.ipynb notebook and enjoy :)\n\n\n'], 'url_profile': 'https://github.com/omidrshi', 'info_list': ['R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Jan 29, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dhivyabharathi001', 'info_list': ['R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Jan 29, 2021']}","{'location': 'Tbilisi , Georgia', 'stats_list': [], 'contributions': '506 contributions\n        in the last year', 'description': ['python run.py\n  --batch BATCH  Training Batch size default: 100\n  --iters ITERS  Number of training iterations default: 3000\n  --lr LR        Model learning rate default: 0.001\n  --load LOAD    True: Load trained model False: Train model default: True\n\nLinear Model\nIteration: 500 Loss: 1.8009570837020874 Accuracy 69.87\nIteration: 1000 Loss: 1.566989541053772 Accuracy 77.83\nIteration: 1500 Loss: 1.3989052772521973 Accuracy 80.26\nIteration: 2000 Loss: 1.2525714635849 Accuracy 81.68\nIteration: 2500 Loss: 1.1943385601043701 Accuracy 82.72\nIteration: 3000 Loss: 0.9942864179611206 Accuracy 83.37\n\n \n \n \n \n \n'], 'url_profile': 'https://github.com/mysterio42', 'info_list': ['R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Jan 29, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/julialisz', 'info_list': ['R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Jan 29, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/julialisz', 'info_list': ['R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Jan 29, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['linearRegression\nWe have used linear regression to predict the impact in sales depending on advertisements via tv, radio and newspaper\n'], 'url_profile': 'https://github.com/ramo16', 'info_list': ['R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Jan 29, 2021']}","{'location': 'Paris', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['Datascience X Logistic Regression\nHarry Potter and a Data Scientist\nRecreation of the Hogwarts Sorting Hat (classifier) using muggle tools (logistic regression). Plus, data analysis and visualization in different ways.\nV.1 Data Analysis\n./describe.py datasets/dataset_train.csv\n\nThe program that takes a dataset as a parameter and displays information\nfor all numerical features, like so:\n               Index    Hogwarts House     First Name      Last Name\nCount    1600.000000              1600           1600           1600\nMean      799.500000               NaN            NaN            NaN\nStd       462.024530               NaN            NaN            NaN\nMin         0.000000               NaN            NaN            NaN\n25%       399.750000               NaN            NaN            NaN\n50%       799.500000               NaN            NaN            NaN\n75%      1199.250000               NaN            NaN            NaN\nMax      1599.000000               NaN            NaN            NaN\n\n            Birthday      Best Hand       Arithmancy      Astronomy\nCount           1600           1600      1566.000000    1568.000000\nMean             NaN            NaN     49634.570243      39.797131\nStd              NaN            NaN     16679.806036     520.298268\nMin              NaN            NaN    -24370.000000    -966.740546\n25%              NaN            NaN     38511.500000    -489.551387\n50%              NaN            NaN     49013.500000     260.289446\n75%              NaN            NaN     60811.250000     524.771949\nMax              NaN            NaN    104956.000000    1016.211940\n\n           Herbology    Defense Against the Dark Arts     Divination\nCount    1567.000000                      1569.000000    1561.000000\nMean        1.141020                        -0.387863       3.153910\nStd         5.219682                         5.212794       4.155301\nMin       -10.295663                       -10.162119      -8.727000\n25%        -4.308182                        -5.259095       3.099000\n50%         3.469012                        -2.589342       4.624000\n75%         5.419183                         4.904680       5.667000\nMax        11.612895                         9.667405      10.032000\n\n         Muggle Studies    Ancient Runes    History of Magic    Transfiguration\nCount       1565.000000      1565.000000         1557.000000        1566.000000\nMean        -224.589915       495.747970            2.963095        1030.096946\nStd          486.344840       106.285165            4.425775          44.125116\nMin        -1086.496835       283.869609           -8.858993         906.627320\n25%         -577.580096       397.511047            2.218653        1026.209993\n50%         -419.164294       463.918305            4.378176        1045.506996\n75%          254.994857       597.492230            5.825242        1058.436410\nMax         1092.388611       745.396220           11.889713        1098.958201\n\n             Potions    Care of Magical Creatures         Charms         Flying\nCount    1570.000000                  1560.000000    1600.000000    1600.000000\nMean        5.950373                    -0.053427    -243.374409      21.958012\nStd         3.147854                     0.971457       8.783640      97.631602\nMin        -4.697484                    -3.313676    -261.048920    -181.470000\n25%         3.646785                    -0.671606    -250.652600     -41.870000\n50%         5.874837                    -0.044811    -244.867765      -2.515000\n75%         8.248173                     0.589919    -232.552305      50.560000\nMax        13.536762                     3.056546    -225.428140     279.070000\n\nV.2 Data Visualization\nV.2.1 Histogram\n./histogram.py datasets/dataset_train.csv\n\nScript which displays a histogram answering the next question:\n\nWhich Hogwarts course has a homogeneous score distribution between all four houses?\n\n\n\nV.2.2 Scatter plot\n./scatter_plot.py datasets/dataset_train.csv\n\nScript which displays a scatter plot answering the next question:\n\nWhat are the two features that are similar?\n\n\n\nV.2.3 Pair plot\n./pair_plot.py datasets/dataset_train.csv\n\nScript which displays a pair plot that might help you decide on:\n\nWhat features are you going to use for your logistic regression?\n\n\nV.3 Logistic Regression\nMagic Hat that performs a multi-classifier using a logistic regression one-vs-all\nTrain\n./logreg_train.py datasets/dataset_train.csv [-f]\n\n\n-f -- to find 5 courses that would make the best combination for model training\n\nProgam that trains through gradient descent multiple sets of parameters theta for one-vs-all logistic regression based on grades from the best combination of 5 courses\n\ngenerates weights.csv file\n\nPredict\n./logreg_predict.py datasets/dataset_test.csv weights.csv\n\nProgram that sorts students into Hogwarts houses\n\ngenerates houses.csv file\n\n$> cat houses.csv\nIndex,Hogwarts House\n0,Gryffindor\n1,Hufflepuff\n2,Ravenclaw\n3,Hufflepuff\n4,Slytherin\n5,Ravenclaw\n6,Hufflepuff\n        [...]\n\nAnnex - Mathematics\nHypothesis\n\nLogistic/Sigmoid Function\n\nVectorized implementation of Gradient Descent\n\n'], 'url_profile': 'https://github.com/dishults', 'info_list': ['R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Jan 29, 2021']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Emmateetee', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sriramaraju423', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020']}","{'location': 'Dhaka,Bangladesh', 'stats_list': [], 'contributions': '548 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShanjinurIslam', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020']}","{'location': 'Karnataka, India', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BlackFlair', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020']}","{'location': 'Mexico', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Regression_Hessian\nTable of content:\n'], 'url_profile': 'https://github.com/ieliasbar', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020']}","{'location': 'visakhapatnam', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Trade_Regression\n'], 'url_profile': 'https://github.com/buddhagandhi', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kvshah93', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amitjha1994', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Logistic_Regression\n'], 'url_profile': 'https://github.com/hvgollar', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Linear-Regression\nsimple linear regression : y = m * x + c , where x and y is independent and dependent variable respectively, m is slope and c is coefficient.\nmultiple linear regression : y = m1 * x1 + m2 * x2 + c1 + c2 , where x1,x2 and y1,y2 are independent and dependent variables respectively, m1 and m2 are slopes and c1 and c2 are coefficients.\n'], 'url_profile': 'https://github.com/premparmar11', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['Linear-regression\nApplying linear regression with SGD implementation on Boston House Prices Dataset\n'], 'url_profile': 'https://github.com/Monika727', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'BSD-2-Clause license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020', '9', 'JavaScript', 'Updated Jul 19, 2020', '1', 'JavaScript', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Shell', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sivakishor24', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'BSD-2-Clause license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020', '9', 'JavaScript', 'Updated Jul 19, 2020', '1', 'JavaScript', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Shell', 'Updated Mar 26, 2020']}","{'location': 'Lahore', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Simple Regression Occupany Detection\nSimple regression (very basic) that has ANN , Linear Regression and SVR on a data that contains electricity usage by adults and childern in a particular house.\nData set contains a timestamp data of 30 days from a certain area in Pakistan. The data contains usage of energy in kWh with number of adults and number of childern present at that particular timestamp.\nThe attempts contains data cleaning and extracting subset of data of one house to train following models\n\nLinear Model for linear regression\nSVR\nANN\n\n----Attempt doesnt mean necessary correct----\n'], 'url_profile': 'https://github.com/syedtumair', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'BSD-2-Clause license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020', '9', 'JavaScript', 'Updated Jul 19, 2020', '1', 'JavaScript', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Shell', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Graph-Regression\nThesis work\n""Code"" folder contains code while ""Latex"" folder contains reports in latex and pdf versions.\n'], 'url_profile': 'https://github.com/eugeniobonifazi', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'BSD-2-Clause license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020', '9', 'JavaScript', 'Updated Jul 19, 2020', '1', 'JavaScript', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Shell', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['clother_regression\n'], 'url_profile': 'https://github.com/dinhhung1598753', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'BSD-2-Clause license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020', '9', 'JavaScript', 'Updated Jul 19, 2020', '1', 'JavaScript', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Shell', 'Updated Mar 26, 2020']}","{'location': 'Montreal', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['ðŸš§This project is currently under development and may undergo changes ðŸš§\ncypress-screenshot-diff\nðŸ“¸Cypress screenshot diffing commands with multiple screenshot folders ability\nInstallation\nThis module is distributed via npm which is bundled with node and should be installed as one of your project\'s devDependencies:\nnpm install --save-dev cypress-screenshot-diff\nUsage\ncypress-screenshot-diff extends Cypress\' cy command and adds matchScreenshot().\n\n\nAdd this line to your project\'s cypress/support/commands.js:\n const { addCommands } = require(""cypress-screenshot-diff"");\n \n addCommands();\n\n\nAdd/Update these tasks to your project\'s cypress/plugins/index.js\nconst { addScreenshotDiffPlugin } = require(""cypress-screenshot-diff"");\n\nmodule.exports = (on, config) => {\n  addScreenshotDiffPlugin(on,config);\n};\n\n\nConfiguration\nTo configure cypress-screenshot-diff, use the following custom command:\ncy.configureCypressScreenshotDiff(config)\nExample\ncy.matchScreenshot();\ncy.get(""h1"").matchScreenshot();\nWhy\nIn Cypress\' infancy, before visual regression plugins, I made my own for personal use using jimp\'s diff utility and Cypress. However, as I started working with bigger monorepos, keeping the screenshots in a single folder, which is where Cypress takes screenshots, was getting pretty hefty for devs. Unfortunately, Cypress does not allow for dynamic screenshot folder roots either, and I didn\'t find any that did what I wanted structure wise. So I reworked my existing implementation to use pixelmatch and allow for different screenshot folders. If you come accross the same problem, hopefully this will help!\nInspired By\n\njest-image-snapshot\ncypress-image-snapshot\n\n'], 'url_profile': 'https://github.com/tiffanosaurus', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'BSD-2-Clause license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020', '9', 'JavaScript', 'Updated Jul 19, 2020', '1', 'JavaScript', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Shell', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ashkin37', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'BSD-2-Clause license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020', '9', 'JavaScript', 'Updated Jul 19, 2020', '1', 'JavaScript', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Shell', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aliaguilar', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'BSD-2-Clause license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020', '9', 'JavaScript', 'Updated Jul 19, 2020', '1', 'JavaScript', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Shell', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Oleksiy05', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'BSD-2-Clause license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020', '9', 'JavaScript', 'Updated Jul 19, 2020', '1', 'JavaScript', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Shell', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/repairnator', 'info_list': ['Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'R', 'BSD-2-Clause license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 27, 2020', '9', 'JavaScript', 'Updated Jul 19, 2020', '1', 'JavaScript', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Shell', 'Updated Mar 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prawizard', 'info_list': ['Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Oct 25, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'R', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jjrico', 'info_list': ['Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Oct 25, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'R', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/erinata', 'info_list': ['Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Oct 25, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'R', 'Updated Mar 24, 2020']}","{'location': 'Edinburg, Texas', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['Linear_Regression_Iris_Dataset\nThis repository contains an implementation of a linear regression algorithm using gradient descent to update the weights and biases while calculating the loss/cost using mean squared loss, $MSE (X,\\theta)= \\frac{1}{2N}\\sum_{i = 1}^{N}\\left (\\hat{y_{i}} - y_{i} \\right)^{2}$.\nAll the costs and weights after each iteration are logged inside a dataframe outputdf which can be used to visualize the variation of cost and weights against training iterations. An early stopping criterion ($\\epsilon$) has been incorporated which stops the training process at the iteration i when $cost_{i} - cost_{i-1} < \\epsilon$ becomes true.\nFunction gradient_descent is tested by training for the given attribute sepal length with label petal width for two classes of flowers Iris Versicolor and Iris Virginica collected from Iris Dataset.\nYou will need to install python, numpy, pandas, matplotlib, and scikit-learn. Scikit-learn is used for importing the Iris Dataset. If you have anaconda installed, run the following:\nconda create -n envName python numpy pandas matplotlib scikit-learn\nThis will create a conda environment with python, numpy, pandas, matplotlib, and scikit-learn installed in it. Run conda activate envName to activate or conda deactivate to deactivate the environment.\nIf you are not seeing the equations above, please install the MathJax plugin for your chrome browser.\n'], 'url_profile': 'https://github.com/Rysul119', 'info_list': ['Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Oct 25, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'R', 'Updated Mar 24, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Logistic-_Regression_by_python\nPython Code for Logistic regression for Machine learning of Classification problem\n'], 'url_profile': 'https://github.com/amitdivekar30', 'info_list': ['Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Oct 25, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'R', 'Updated Mar 24, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mswang1984', 'info_list': ['Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Oct 25, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'R', 'Updated Mar 24, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Heroku-SalaryRegression\nPredict the salary of the employee based on input feature. Used Boosting Regressor algorithm.\n'], 'url_profile': 'https://github.com/kashten', 'info_list': ['Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Oct 25, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'R', 'Updated Mar 24, 2020']}","{'location': 'Greece', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': [""aiverse-linear-regression\nImplementation of linear regression's data preprocessing and feature engineering steps.\nDataset grabbed from here\n""], 'url_profile': 'https://github.com/christk1', 'info_list': ['Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Oct 25, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'R', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Linear-regression-task\n'], 'url_profile': 'https://github.com/Kemar73', 'info_list': ['Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Oct 25, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'R', 'Updated Mar 24, 2020']}","{'location': 'London, United Kingdom', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/deegandeegan', 'info_list': ['Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Oct 25, 2020', '1', 'Python', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'R', 'Updated Mar 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Oleksiy05', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Kotlin', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '121 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vlytvyne', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Kotlin', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'Jodhpur, Rajasthan .', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Linear-Regression-Model\n'], 'url_profile': 'https://github.com/arora1209', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Kotlin', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'Winter Springs, FL', 'stats_list': [], 'contributions': '486 contributions\n        in the last year', 'description': ['BaggingInRegression\nThis is for assignment 06 of Data Mining at UCF.\n'], 'url_profile': 'https://github.com/darkhark', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Kotlin', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Logistic-Regression-Course-Udemy\nCourse Udemy Learning Python for Data Analysis and Visualisation\n'], 'url_profile': 'https://github.com/sservaes', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Kotlin', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '165 contributions\n        in the last year', 'description': ['RandomForest_regression\n'], 'url_profile': 'https://github.com/Tabrez911', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Kotlin', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['Part-2-Regression\n'], 'url_profile': 'https://github.com/riyagoel192', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Kotlin', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['linear_regression.m\n'], 'url_profile': 'https://github.com/FandyAlifian', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Kotlin', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['Prediction with Logistic Regression\nThis model predicts whether or not an internet user will click an ad based off the features of that user\n'], 'url_profile': 'https://github.com/KizitoNaanma', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Kotlin', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/crankbot0118', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Kotlin', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Josipmrden', 'info_list': ['Roff', 'Updated Sep 26, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 29, 2020', 'JavaScript', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['Logistic regression on Donors Choose Dataset\nThe goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school. DonorsChoose.org can then use this information to identify projects most likely to need further review before approval.\n'], 'url_profile': 'https://github.com/Monika727', 'info_list': ['Roff', 'Updated Sep 26, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 29, 2020', 'JavaScript', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NavaneethakrishnanL', 'info_list': ['Roff', 'Updated Sep 26, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 29, 2020', 'JavaScript', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['House-Prices-Regression-Technique\nLearning from the Comprehensive data exploration with Python on Kaggle\n'], 'url_profile': 'https://github.com/bmd6', 'info_list': ['Roff', 'Updated Sep 26, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 29, 2020', 'JavaScript', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020']}","{'location': 'Pennsburg Pa', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['FuelConsumption_Regression\nIn this notebook, we learn how to use scikit-learn to implement Multiple linear regression. We download a dataset that is related to fuel consumption and Carbon dioxide emission of cars. Then, we split our data into training and test sets, create a model using training set, Evaluate your model using test set, and finally use model to predict unknown value\n'], 'url_profile': 'https://github.com/larex201', 'info_list': ['Roff', 'Updated Sep 26, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 29, 2020', 'JavaScript', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MohammedRaheemP', 'info_list': ['Roff', 'Updated Sep 26, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 29, 2020', 'JavaScript', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['simple-logistic-regression\n'], 'url_profile': 'https://github.com/dhivyabharathi001', 'info_list': ['Roff', 'Updated Sep 26, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 29, 2020', 'JavaScript', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020']}","{'location': 'egypt', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['-Amrsaadmahmoud-Linear_Regression_Skeleton-\n'], 'url_profile': 'https://github.com/Amrsaadmahmoud', 'info_list': ['Roff', 'Updated Sep 26, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 29, 2020', 'JavaScript', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/comet-hale', 'info_list': ['Roff', 'Updated Sep 26, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 29, 2020', 'JavaScript', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Logistic-Regression-from-scratch\n'], 'url_profile': 'https://github.com/SmritiAgrawal04', 'info_list': ['Roff', 'Updated Sep 26, 2020', 'Jupyter Notebook', 'Updated Oct 13, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 29, 2020', 'JavaScript', 'Updated Mar 26, 2020', 'Updated Mar 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Speed Testing regression\n25 million rows, 10 variables.\nResults:\n\nmultivariate cox regression (3 variable). Best time: 2.56 minutes, average 3.5 minutes (Macbook air run locally)\n\nNotes:\nPossible package for parallel processing regression\n\nhttps://bioconductor.org/packages/release/data/experiment/vignettes/RegParallel/inst/doc/RegParallel.html#speed-up-processing\n\n'], 'url_profile': 'https://github.com/ebmdatalab', 'info_list': ['R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', 'R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Regression-MLP-Linear-\n'], 'url_profile': 'https://github.com/SmritiAgrawal04', 'info_list': ['R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', 'R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'New Jersey, USA', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AdeloreSimiloluwa', 'info_list': ['R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', 'R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Heart-Disease-Logistic-Regression\nThis model uses Logistic regression to predict if a specific person, given 12 features has or does not have heart disease.\nThe csv files has 12 different features and respective target variable.\nThe 12 features helps us predict the heart disease in a person.\nWe have models:\n\nWith Feature Scaling\nWithout Feature Scaling\n\nThis repository helps us understand the use of Feature Scaling.\n'], 'url_profile': 'https://github.com/ArunJoseph19', 'info_list': ['R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', 'R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '251 contributions\n        in the last year', 'description': ['Logistic-Regression-Spam-Detection\nI fitted a Logistic Regression model to the spam dataset found in R. This initial fit used only the variables related to character frequency and occurence of special characters. The initial model showed a clear seperation between the two classes as seen below and all predictor variables were significant.\n\nI then examined the model accuracy using a ROC Curve. ROC is a probability curve and AUC (Area under the curve) represents degree or measure of separability. A higher AUC, close to 1, indicates the model is better at distinguishing between spam or non-spam emails.\n\nWe can see from the above plot the curve rises sharply to the top left before smoothing out. To achieve an\napproximately 80% true positive rate the model has a corresponding 10% false positive rate. The area under\nthe curve for our ï¬nal model is equal to 0.90204.\nThe threshold value, in this context, determines the probability with which the model will classify an email\nas spam. For example, if this threshold is set to 0.5, all emails with a predicted probability of spam higher\nthan 0.5 will be classiï¬ed as spam. The statistically optimal threshold is the value that maximises the sum of Sensitivity and Speciï¬city.\n\nHowever, I calculated at this threshold the Sensitivity (true positive rate) is equal to 0.87 and the false positive rate for this threshold value is 0.17. This means that 17% of non-spam emails are being incorrectly classiï¬ed as spam. With a new threshold value of 0.52 we see a reduction to the False Positive Rate of approximately 41% to 10% while maintaining a True Positive Rate (Sensitivity) of 81%.\nFor spam email prediction it is essential that very few non-spam emails are incorrectly classiï¬ed as spam because vital corporate or personal information could be lost. It is less important for the user to see a few extra actual spam emails in their inbox.\n'], 'url_profile': 'https://github.com/jackapbutler', 'info_list': ['R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', 'R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/singha96panda', 'info_list': ['R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', 'R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': [""Regression-Is-King\nProject Description\nIn this project, we will be using the income dataset to answer these questions:\n\nBuild visualizations and plots to see how the dependent variable varies with some of the other variables, such as 'education', 'marital-status', 'occupation', etc and study other interrelationships of the variables. Use them to tell a story/stories.\nHow would you transform your categorical variables and why? Which of them would you transform? If you do not want to transform any, justify why.\nUse a technique of your choice to eliminate some of the features. Explain your method.\nFit a regression model to predict whether income of an individual is more than 50k. What regression model do you use? Why? How well is your model performing?\nUse the income dataset to regress work hours on the rest of the data. Compare the R^2 with R^2 of another model that you fit with only the significant variables. Explore some interaction terms and explain why they were of interest.\nRegress work hours of only people who are working in Sales. Once you fit a reasonable (as per your judgement) model for this data, compare the R^2 with the ones you calculated for Question 5. Explain your findings.\nYou can use scikit-learn, statsmodels, or any other package of your choice. Explore the packages and play around with them to get a hang of how to use them. Always refer to the documentation and look up examples of usage.\n\nFindings\nQuestion 1\nImportance of job titles and education on income:\nWe notice that for most occupations, the number of low-income employees greatly outweigh the number of high-income employees, except for â€œExec-managerialâ€ and â€œProf-specialtyâ€ roles.\nMoreover, the highest number listings are for those of â€œHS-gradâ€ and â€œSome-collegeâ€ of which there is a disproportionately large number of high-income wage workers compared to low income wage workers. As the education level increases, the proportion of high to low-income wage workers improves to a point where there are more high-income wage workers than low income wage workers (â€œDoctorateâ€).\nMaybe I need to get married and old to get rich?\nInteresting to see that the chances of having a high income are much higher if youâ€™re married and neither â€œDivorcedâ€ nor â€œNever-marriedâ€. This could be due to additional allowances or incentives to work to support a family. Nonetheless, it looks like we need to find a partner!\nMost of the information recorded is for early to mid-level professionals. The proportion of high-income to low-income wage workers is relatively high up to the age of roughly 60 before it goes down again (probably due to retirement). Then these retirees and other older workers may turn to lower-paid part-time roles, which would explain the higher proportion of lower-income workers over age 60.\nA closeup into the gender pay gap and racial economic inequality:\nWhile there is a significantly higher number of records for male workers, we notice that the proportion of high-income wages is also much higher for males, which is concerning. This could be due to multiple factors (such as gender discrimination or perhaps males assuming higher paying job titles or working longer hours).\nWhile we observed that the records collected predominantly represented the white race, it is evident that the proportion of high-income wages are significantly higher for whites.\nWork hard, play hard?\nAs we know most conventional jobs are 40 hours per week, which explains the high frequency in that segment. We observe that anything below that will be low paying and the odds of having higher incomes increases as â€œhours-per-weekâ€ increases.\nInterestingly, we notice that there is a significantly higher number of males who work past 50 hours; whereas, there is a higher number of women working fewer hours. This could be one explanation behind the gender pay gap.\nQuestion 2\nCertain categorical variables that might be worth transforming include the following:\n\n\nâ€œworkingclassâ€ â€“ Combine the â€œSelf-emp-not-incâ€ and â€œSelf-emp-incâ€, because the key is to know whether a person is self-employed, not whether they are incorporated or not incorporated. Combining the â€œlocal-govâ€ and â€œstate-govâ€ levels into one group might be appropriate, as their incomes are likely similar.  The â€œFederal-govâ€ level may need to remain separate as these jobs are oft considered more elite than state and local government jobs.\n\n\nâ€œmarital-statusâ€ â€“ Combine â€œMarried-AF-spouseâ€ and â€œMarried-civ-spouseâ€, because the key is to know whether a person is married, not whether their spouse is in the Armed Forces or is a civilian. Keeping â€œMarried-spouse-absentâ€ by itself might be appropriate as that may have an affect on whether the person makes more or less than $50k. Also, combining â€œDivorcedâ€, â€œNever-marriedâ€, â€œSeparatedâ€, and â€œWidowedâ€ into â€œNot marriedâ€ also seems appropriate. The key here is that the person does not have partner with whom they have combined income, so this may affect whether they make more or less than $50k.\n\n\nâ€œnative-countryâ€ â€“ Combine the 42 countries (approximately) into larger regions such as â€œNorth Americaâ€, â€œEuropeâ€, â€œAsiaâ€. This likely will not lose any information, but will simplify the number of levels involved.\n\n\nQuestion 3\nWith the Y-variable as â€œincome_ >50kâ€, we eliminated certain variables by finding correlations between all the variables with all the other variables. Then we focused on just the correlations between all the variables and the variable â€œincome_ >50kâ€. The following variables had high correlations with the variable â€œincome_ >50kâ€, so were kept:\nAge, education-num, marital-status_Married, marital-status_Never-married, marital-status_Widowed, capital-gain, relationship_ Husband\nrelationship_ Not-in-family, relationship_ Other-relative,\nrelationship_ Own-child, relationship_ Unmarried, hours-per-week.\nAll the other variables had low correlations with the variable â€œincome_ >50kâ€, so they were eliminated. See the output of   in the code to see the correlations listed from largest to smallest. (The output is too large to fit in this document.)\nQuestion 4\nThe goal here is to predict the whether the income of an individual is more than 50k. The regression model that we have used in this case is logistic regression. Logistic regression is generally used when the dependent variable is dichotomous. Since we are trying to predict binary answer for the question here, logic regression is used.\nWe picked significant predictors by evaluating their correlation with the dependent variable.\nTo evaluate the model, we have used train and test dataset (out-of-sample testing) with a 70-30 split. We fit the model using the train dataset and got the metrics such as precision, F1 score using predicted values for the test dataset. We have a model with an accuracy of 84%. The precision and recall are 0.83 and 0.84 (weighted average). Refer to Part 1 of Appendix for the model scores and ROC graph.\nQuestion 5\nThe R-squared for the full model is very low at 0.200. It consisted of all the given data as independent variable with hours-per-week as the dependent variable.\nWe then used a combination of backward p-elimination method and evaluation of adjusted R-squared, AIC and BIC values to arrive at a harmonious reduced model which has an R-squared value of 0.199. We have removed insignificant variables while keeping an Adjusted R-squared value as high as 0.198 and AIC and BIC values lesser than the full model. Refer to the Appendix part 2 for details on the R-squared, AIC and BIC values.\nTo further improve the model, we explored few interaction term effects on the independent variable. We assessed the interaction between the education and sex, workclass and education. We wanted to see if the sex impacts the effect of education on workhours. We plotted the values to see if there is any interaction and since there was, we included them to the model (Appendix Part 2). It resulted in a slight increase in the R-squared value and further decrease in the AIC and BIC values.\nQuestion 6\nWe regressed the work hours with data for people who work in Sales. With just comparing the R-square values, there was more improvement when we ran the model for people who work in Sales. The R-square value went up to almost 0.3. The AIC and BIC values reduced by a large value. One of the reasons could be because the model fit the specific data on Sales people better than all the data points. The degree of freedom is also less for this model which explains the difference in the metrics.\nThe model showed improvements when we removed least significant variables like native country. It explains that the works hours is not impacted by the native country.\n\u2003\n""], 'url_profile': 'https://github.com/khaledimad', 'info_list': ['R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', 'R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ChrisWestlake', 'info_list': ['R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', 'R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'New Jersey, USA', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['Random-Forest-Regression\nPersonal Mini-Projects (R and Python)\nTruth Detector\nIs you new Employee lying about how much he got paid on his previous job?\nA potential employee told his new company that his current salary is $250k, the human resource retrieved the salary amount\nassociated position levels in his previous company. The potential new employee has a position level 7.5, we would like to\nbuild a salary bluffing detector to see if he told the truth.\n'], 'url_profile': 'https://github.com/AdeloreSimiloluwa', 'info_list': ['R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', 'R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['LinearRegression\nLogisticRegression\n'], 'url_profile': 'https://github.com/sunnysavita10', 'info_list': ['R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '1', 'HTML', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 25, 2020', 'R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['MultiLinearRegression\n'], 'url_profile': 'https://github.com/amzkamble', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Julia', 'MIT license', 'Updated Apr 6, 2020', '1', 'HTML', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'Paris, France', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/emileten', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Julia', 'MIT license', 'Updated Apr 6, 2020', '1', 'HTML', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Multivariate-Linear-Regression\n'], 'url_profile': 'https://github.com/santiagocantu98', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Julia', 'MIT license', 'Updated Apr 6, 2020', '1', 'HTML', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'Navi Mumbai', 'stats_list': [], 'contributions': '255 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mandal-21', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Julia', 'MIT license', 'Updated Apr 6, 2020', '1', 'HTML', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['LogisticRegression\nSimple logistic regression package.\n\n\n\n\n\n'], 'url_profile': 'https://github.com/LAMPSPUC', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Julia', 'MIT license', 'Updated Apr 6, 2020', '1', 'HTML', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'New Jersey, USA', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AdeloreSimiloluwa', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Julia', 'MIT license', 'Updated Apr 6, 2020', '1', 'HTML', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '236 contributions\n        in the last year', 'description': ['Predicting Diamond Prices with Multiple Linear Regression\nIn this project we predict the price of diamonds using a dataset containing 54,000 data points. Data includes diamond price and the following 9 features:\n\nCarat\nClarity\nCut\nColor\nDepth\nTable\nX (length)\nY (width)\nZ (depth)\n\nWe also perform EDA and test two hypotheses based on our findings:\nHypothesis Test 1 - Do people pay extra for a little more Carat? Carat Weight vs. Price\nOur Curiosity: We have heard that there is a significant increase in price when the carat hits 2. Are people spending much more to hit the 2 carat breakpoint? We split up the diamonds by carat right before 2 carat (1.9-1.999) and compared it to the average prices of 2-2.1 carat diamonds.\nNull Hypothesis : There is no significant difference between the price of diamonds of weights 1.9-1.999 and weights 2-2.1.\nAlternate Hypothesis : There is a significant difference between the price of diamonds of weights 1.9-1.999 and weights 2-2.1.\nAlpha value = 0.05\nP-value is less than our alpha value of 0.05, therefore:\nReject the null hypothesis meaning there is a statistically significant difference between the prices of diamonds weighted 1.9-2 and weights 2-2.1.\nInference:\nCarat weight is the most critical predictor of price, it appears that the weight of 2 is a significant point where the value rises.\nHypothesis Test 2 Cut vs. Clarity\nScenario: If someone went to buy a wedding ring for their partner, would the person look for the best cut? The best clarity? We wanted to find out if a diamond with the best cut is more expensive than one with the best clarity.\nNull Hypothesis : There is no significant difference between the price of diamonds with the best cut and diamonds with the best clarity.\nAlternate Hypothesis : There is a significant difference between the price of diamonds with the best cut and diamonds with the best clarity.\n\nAlpha value = 0.05\np_value = 2.744688067675461e-10, reject null hypothesis\nP-value is less than our alpha value of 0.05, therefore:\nReject the null hypothesis meaning there is a statistically significant difference between the prices of diamonds with the best cut and diamonds with the best clarity.\nInference:\nThe best cut diamonds have a higher price on average and are more valuable than diamonds with the best clarity.\nWe run a multiple linear regression\nOur first model performed very well on the training and testing dataset. We decided not to create another model. Our results:\nTrain R2: 0.9806005580054734\n\nTest R2: 0.9804433773941287\n\n\nTrain Root Mean Squared Error: 0.018207304797526316\n\nTest Root Mean Squared Error: 0.01841116334943573\n\n\nBest Features Interpretation\nOur strongest features are Carat and IF Clarity.\nCarat Interpretation: For every 1 increase in Carat, the price increases by 18%. \nIF Clarity Interpretation: If the diamond has IF clarity, the price increases by 15%.\nBusiness Inferences for Seller:\n\nA high grade color may not increase the diamond value, but having a lower grade color will actually decrease it.\nDiamond cut doesnâ€™t have a significant effect on the diamond value either positively or negatively.\nCarat has the highest influence on the diamondâ€™s value.\nAll clarity grades have the second highest influence on the diamondâ€™s value.\n\nBusiness Inferences for a Budget Buyer\n\nBuy under 2 carats as the price of a diamond just under 2 is significantly less.\nIf you want the largest diamond for your money, and thatâ€™s the only parameter of interest, then look for a high carat diamond with a low grade color and clarity.\nCut doesnâ€™t influence the price as much as the other qualities.\n\nBEST DEAL:\n\nLow grade clarity, low grade color, any cut, under 2 carats.\n\n'], 'url_profile': 'https://github.com/chrispfchung', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Julia', 'MIT license', 'Updated Apr 6, 2020', '1', 'HTML', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'Melbourne, Australia', 'stats_list': [], 'contributions': '276 contributions\n        in the last year', 'description': ['Linear & Polynomial Regression\n'], 'url_profile': 'https://github.com/mohitkhosla', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Julia', 'MIT license', 'Updated Apr 6, 2020', '1', 'HTML', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '123 contributions\n        in the last year', 'description': [""Bike-Sharing-Python-Regression\nPerforming data cleaning, preprocessing for 2 years of bike rental service with 17,000 datapoints and implemented a Machine Learning models with Python including Regression, Ensemble Learning, Dimension Reduction and Deep Learning.\n\nProject Highlight\n\nPerformed data cleaning, preprocessing for 2 years of bike rental service with 17,000 datapoints using Python Jupyter Notebook\nForecasted 80% accurate count of rentals as per season, working days and weather details for better anticipation of bookings\nImplemented Machine Learning models for regression like KNN, Linear, Polynomial, Ridge, Lasso, SVM simple and kernel based\n\n\nProject Introduction & Scope\nLink: https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\nDescription: Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues.\nApart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.\nInitial Analysis: Dataset is supposed to have few missing values and some pre-processed columns. These columns will first be converted to view their original data and processed as per teachings of the course.\nProject Objective Phase 1: Data has attributes like season, year, month, hour, holiday, weekday, working day, weather situation, temperature, humidity, windspeed. These features might correspond to total rental bikes. Our objective is to:\n\nInitializations: Process data correctly to reflect right data impute missing values if required\nPreprocessing: Preprocess data for our machine learning models\nData Visualization\nApply ML Regression Models:\n\nKNN regressor\nLinear regression\nRidge\nLasso\nPolynomial regression\nSVM (simple, rbf, poly, linear)\n\n\nGrid Search for best parameters\nCross-Validaiton to find average training and test scores\nBest Regressor with best parameters\n\nProject Objective Phase 2: We'll continue implementing new algorithms and adding to main comparison table for accuracy. We'll apply PCA and reiterate previously run models for checking improvement in accuracy. Focussed implementation includes:\n\nApply Bagging on:\n\nSimple SVR\nLinear SVR\n\n\nApply Pasting:\n\nSimple SVR\nLinear SVR\n\n\nApply Adaboost:\n\nSimple SVR\nLinear SVR\n\n\nVisualize accuracy comparison between Grid Search, Bagging, Pasting and Adaboost.\nApply Gradient Boosting\nPCA Data initialisations\nPost-PCA Model Re-runs:\n\nKNN regressor\nLinear regression\nRidge\nLasso\nPolynomial regression\nSVM (simple, rbf, poly, linear)\n\n\nPCA Results Comparison:\n\nVisualization - Bar Graph Comparison\nTable Comparison\nResult Automation\n\n\nApply Deep Learning model.\n\n""], 'url_profile': 'https://github.com/hchirag7', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Julia', 'MIT license', 'Updated Apr 6, 2020', '1', 'HTML', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Heart-Disease-Logistic-Regression\nThis model uses Logistic regression to predict if a specific person, given 12 features has or does not have heart disease.\nThe csv files has 12 different features and respective target variable.\nThe 12 features helps us predict the heart disease in a person.\nWe have models:\n\nWith Feature Scaling\nWithout Feature Scaling\n\nThis repository helps us understand the use of Feature Scaling.\n'], 'url_profile': 'https://github.com/ArunJoseph19', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2021', 'Python', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Julia', 'MIT license', 'Updated Apr 6, 2020', '1', 'HTML', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 3, 2020', 'Python', 'Updated Mar 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '251 contributions\n        in the last year', 'description': ['Logistic-Regression-Spam-Detection\nI fitted a Logistic Regression model to the spam dataset found in R. This initial fit used only the variables related to character frequency and occurence of special characters. The initial model showed a clear seperation between the two classes as seen below and all predictor variables were significant.\n\nI then examined the model accuracy using a ROC Curve. ROC is a probability curve and AUC (Area under the curve) represents degree or measure of separability. A higher AUC, close to 1, indicates the model is better at distinguishing between spam or non-spam emails.\n\nWe can see from the above plot the curve rises sharply to the top left before smoothing out. To achieve an\napproximately 80% true positive rate the model has a corresponding 10% false positive rate. The area under\nthe curve for our ï¬nal model is equal to 0.90204.\nThe threshold value, in this context, determines the probability with which the model will classify an email\nas spam. For example, if this threshold is set to 0.5, all emails with a predicted probability of spam higher\nthan 0.5 will be classiï¬ed as spam. The statistically optimal threshold is the value that maximises the sum of Sensitivity and Speciï¬city.\n\nHowever, I calculated at this threshold the Sensitivity (true positive rate) is equal to 0.87 and the false positive rate for this threshold value is 0.17. This means that 17% of non-spam emails are being incorrectly classiï¬ed as spam. With a new threshold value of 0.52 we see a reduction to the False Positive Rate of approximately 41% to 10% while maintaining a True Positive Rate (Sensitivity) of 81%.\nFor spam email prediction it is essential that very few non-spam emails are incorrectly classiï¬ed as spam because vital corporate or personal information could be lost. It is less important for the user to see a few extra actual spam emails in their inbox.\n'], 'url_profile': 'https://github.com/jackapbutler', 'info_list': ['R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/singha96panda', 'info_list': ['R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': [""Regression-Is-King\nProject Description\nIn this project, we will be using the income dataset to answer these questions:\n\nBuild visualizations and plots to see how the dependent variable varies with some of the other variables, such as 'education', 'marital-status', 'occupation', etc and study other interrelationships of the variables. Use them to tell a story/stories.\nHow would you transform your categorical variables and why? Which of them would you transform? If you do not want to transform any, justify why.\nUse a technique of your choice to eliminate some of the features. Explain your method.\nFit a regression model to predict whether income of an individual is more than 50k. What regression model do you use? Why? How well is your model performing?\nUse the income dataset to regress work hours on the rest of the data. Compare the R^2 with R^2 of another model that you fit with only the significant variables. Explore some interaction terms and explain why they were of interest.\nRegress work hours of only people who are working in Sales. Once you fit a reasonable (as per your judgement) model for this data, compare the R^2 with the ones you calculated for Question 5. Explain your findings.\nYou can use scikit-learn, statsmodels, or any other package of your choice. Explore the packages and play around with them to get a hang of how to use them. Always refer to the documentation and look up examples of usage.\n\nFindings\nQuestion 1\nImportance of job titles and education on income:\nWe notice that for most occupations, the number of low-income employees greatly outweigh the number of high-income employees, except for â€œExec-managerialâ€ and â€œProf-specialtyâ€ roles.\nMoreover, the highest number listings are for those of â€œHS-gradâ€ and â€œSome-collegeâ€ of which there is a disproportionately large number of high-income wage workers compared to low income wage workers. As the education level increases, the proportion of high to low-income wage workers improves to a point where there are more high-income wage workers than low income wage workers (â€œDoctorateâ€).\nMaybe I need to get married and old to get rich?\nInteresting to see that the chances of having a high income are much higher if youâ€™re married and neither â€œDivorcedâ€ nor â€œNever-marriedâ€. This could be due to additional allowances or incentives to work to support a family. Nonetheless, it looks like we need to find a partner!\nMost of the information recorded is for early to mid-level professionals. The proportion of high-income to low-income wage workers is relatively high up to the age of roughly 60 before it goes down again (probably due to retirement). Then these retirees and other older workers may turn to lower-paid part-time roles, which would explain the higher proportion of lower-income workers over age 60.\nA closeup into the gender pay gap and racial economic inequality:\nWhile there is a significantly higher number of records for male workers, we notice that the proportion of high-income wages is also much higher for males, which is concerning. This could be due to multiple factors (such as gender discrimination or perhaps males assuming higher paying job titles or working longer hours).\nWhile we observed that the records collected predominantly represented the white race, it is evident that the proportion of high-income wages are significantly higher for whites.\nWork hard, play hard?\nAs we know most conventional jobs are 40 hours per week, which explains the high frequency in that segment. We observe that anything below that will be low paying and the odds of having higher incomes increases as â€œhours-per-weekâ€ increases.\nInterestingly, we notice that there is a significantly higher number of males who work past 50 hours; whereas, there is a higher number of women working fewer hours. This could be one explanation behind the gender pay gap.\nQuestion 2\nCertain categorical variables that might be worth transforming include the following:\n\n\nâ€œworkingclassâ€ â€“ Combine the â€œSelf-emp-not-incâ€ and â€œSelf-emp-incâ€, because the key is to know whether a person is self-employed, not whether they are incorporated or not incorporated. Combining the â€œlocal-govâ€ and â€œstate-govâ€ levels into one group might be appropriate, as their incomes are likely similar.  The â€œFederal-govâ€ level may need to remain separate as these jobs are oft considered more elite than state and local government jobs.\n\n\nâ€œmarital-statusâ€ â€“ Combine â€œMarried-AF-spouseâ€ and â€œMarried-civ-spouseâ€, because the key is to know whether a person is married, not whether their spouse is in the Armed Forces or is a civilian. Keeping â€œMarried-spouse-absentâ€ by itself might be appropriate as that may have an affect on whether the person makes more or less than $50k. Also, combining â€œDivorcedâ€, â€œNever-marriedâ€, â€œSeparatedâ€, and â€œWidowedâ€ into â€œNot marriedâ€ also seems appropriate. The key here is that the person does not have partner with whom they have combined income, so this may affect whether they make more or less than $50k.\n\n\nâ€œnative-countryâ€ â€“ Combine the 42 countries (approximately) into larger regions such as â€œNorth Americaâ€, â€œEuropeâ€, â€œAsiaâ€. This likely will not lose any information, but will simplify the number of levels involved.\n\n\nQuestion 3\nWith the Y-variable as â€œincome_ >50kâ€, we eliminated certain variables by finding correlations between all the variables with all the other variables. Then we focused on just the correlations between all the variables and the variable â€œincome_ >50kâ€. The following variables had high correlations with the variable â€œincome_ >50kâ€, so were kept:\nAge, education-num, marital-status_Married, marital-status_Never-married, marital-status_Widowed, capital-gain, relationship_ Husband\nrelationship_ Not-in-family, relationship_ Other-relative,\nrelationship_ Own-child, relationship_ Unmarried, hours-per-week.\nAll the other variables had low correlations with the variable â€œincome_ >50kâ€, so they were eliminated. See the output of   in the code to see the correlations listed from largest to smallest. (The output is too large to fit in this document.)\nQuestion 4\nThe goal here is to predict the whether the income of an individual is more than 50k. The regression model that we have used in this case is logistic regression. Logistic regression is generally used when the dependent variable is dichotomous. Since we are trying to predict binary answer for the question here, logic regression is used.\nWe picked significant predictors by evaluating their correlation with the dependent variable.\nTo evaluate the model, we have used train and test dataset (out-of-sample testing) with a 70-30 split. We fit the model using the train dataset and got the metrics such as precision, F1 score using predicted values for the test dataset. We have a model with an accuracy of 84%. The precision and recall are 0.83 and 0.84 (weighted average). Refer to Part 1 of Appendix for the model scores and ROC graph.\nQuestion 5\nThe R-squared for the full model is very low at 0.200. It consisted of all the given data as independent variable with hours-per-week as the dependent variable.\nWe then used a combination of backward p-elimination method and evaluation of adjusted R-squared, AIC and BIC values to arrive at a harmonious reduced model which has an R-squared value of 0.199. We have removed insignificant variables while keeping an Adjusted R-squared value as high as 0.198 and AIC and BIC values lesser than the full model. Refer to the Appendix part 2 for details on the R-squared, AIC and BIC values.\nTo further improve the model, we explored few interaction term effects on the independent variable. We assessed the interaction between the education and sex, workclass and education. We wanted to see if the sex impacts the effect of education on workhours. We plotted the values to see if there is any interaction and since there was, we included them to the model (Appendix Part 2). It resulted in a slight increase in the R-squared value and further decrease in the AIC and BIC values.\nQuestion 6\nWe regressed the work hours with data for people who work in Sales. With just comparing the R-square values, there was more improvement when we ran the model for people who work in Sales. The R-square value went up to almost 0.3. The AIC and BIC values reduced by a large value. One of the reasons could be because the model fit the specific data on Sales people better than all the data points. The degree of freedom is also less for this model which explains the difference in the metrics.\nThe model showed improvements when we removed least significant variables like native country. It explains that the works hours is not impacted by the native country.\n\u2003\n""], 'url_profile': 'https://github.com/khaledimad', 'info_list': ['R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ChrisWestlake', 'info_list': ['R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'New Jersey, USA', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['Random-Forest-Regression\nPersonal Mini-Projects (R and Python)\nTruth Detector\nIs you new Employee lying about how much he got paid on his previous job?\nA potential employee told his new company that his current salary is $250k, the human resource retrieved the salary amount\nassociated position levels in his previous company. The potential new employee has a position level 7.5, we would like to\nbuild a salary bluffing detector to see if he told the truth.\n'], 'url_profile': 'https://github.com/AdeloreSimiloluwa', 'info_list': ['R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['LinearRegression\nLogisticRegression\n'], 'url_profile': 'https://github.com/sunnysavita10', 'info_list': ['R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['A project on builting Bayes Classiffier and Regression algorithm from scratch.\nIt consist of four subparts-\n(a) - Binary Bayes Classifier\n(b) - Multiclass Bayes Classifier\n(c) - Bias-Variance trade-off for Regression\n(d) - Underfitting-Overfitting analysis for Regression\n'], 'url_profile': 'https://github.com/Arnav-Mishra', 'info_list': ['R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '473 contributions\n        in the last year', 'description': [""Logistic_Regression_Project\nIn this 2-hour long project-based course, I learnt how to implement Logistic Regression using Python and Numpy.\nLogistic Regression is an important fundamental concept if you want break into Machine Learning and Deep Learning. Even though popular machine learning frameworks have implementations of logistic regression available, it's still a great idea to learn to implement it on your own to understand the mechanics of optimization algorithm, and the training and validation process.\nI created and trained a logistic model that was able to predict if a given image is of hand-written digit zero or of hand-written digit one. The model was able to distinguish between images, or 0s and 1s, and it does that with a very high accuracy.\nContributing\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\nPlease make sure to update tests as appropriate.\n""], 'url_profile': 'https://github.com/RaghavK16', 'info_list': ['R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': [""Salary-Predictor-Linear-Regression\nSalary Predictor based on years of experience of employee using Linear Regression model\nExecute 'salary.py' python file\nModules Used -\n\nmatplotlib\nsklearn\npandas\nnumpy\n\nScreenshot -\n\n""], 'url_profile': 'https://github.com/adityanair239', 'info_list': ['R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['SGD-Linear-Regression\nImplementing mini-batch Stochastic Gradient Descent (SGD) algorithm from scratch in python.\nHere we are  minimizing Squared Loss in Linear Regression and applying it on Boston Housing Price Dataset which is inbuilt in Sklearn.\n'], 'url_profile': 'https://github.com/sanjeevpalla', 'info_list': ['R', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'R', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '144 contributions\n        in the last year', 'description': ['Heart-Disease-Linear-Regression\n'], 'url_profile': 'https://github.com/RidaMalik', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 8, 2020', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Diabetes-logistic-regression\nhttps://www.kaggle.com/uciml/pima-indians-diabetes-database\nPredict a person is having diabetes or not using logistic regression.\n'], 'url_profile': 'https://github.com/ranjaniramjee', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 8, 2020', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'Navi Mumbai', 'stats_list': [], 'contributions': '255 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mandal-21', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 8, 2020', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '169 contributions\n        in the last year', 'description': [""Logistic-Regression-with-MNIST\nBuild and train a Multiclass Logistic Regression model using the popular MNIST dataset, this experiment demonstrate that even though\ntraditional statistic methods might work just fine for computer vision tasks, they are not well fitted.\nThat's why deep learning has been so popular for the recent years. For computer vision, new methods like ConvNet, YOLO, Unet...are being\napplied throughout the indsutry, and opensource package like OpenCV, Tensorflow, Keras, Pytorch all can deal with computer vision\ntasks really well.\n""], 'url_profile': 'https://github.com/changseverus', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 8, 2020', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aemanju', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 8, 2020', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['House_Linear_Regression\nSaveHTML2file.py asks the user for a website to webscrape and puts it in a text file.\nI put used to websites and I put them into houses.txt and houses1.txt.\nI use TxtParse.py to parse through the html and find relevant house details like the price,\nnumber of bedrooms/ bathrooms, square footage, etc.\nThen I put that infomation into two textfiles. all_houses.txt and all_houses_num.txt.\nall_houses.txt has all the information without any changes to the data.\nall_houses_num.txt has the categorical information changed into numeric.\nHouse_Analysis.R is where I created the model\nI was trying to predict the price of a house and my model did a really good job\n'], 'url_profile': 'https://github.com/EricChagoya', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 8, 2020', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Multiple-Logsitic-Regression\n'], 'url_profile': 'https://github.com/abhishekjaglan', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 8, 2020', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Linear_Regression_Project\n'], 'url_profile': 'https://github.com/geralddejean', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 8, 2020', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'Munich', 'stats_list': [], 'contributions': '562 contributions\n        in the last year', 'description': ['Installation:\nrun ./setup.sh\nIt will:\n\nmake sure the pip is installed\ninstall venv\ncreate a virtual environment and activate it\ninstall the necessary python packages matplotlib and numpy\nstart the program\n\nStart the program:\n\nsource ./bin/activate\npython main.py\n\nDescription:\nEach click on the left window will create a new point.\nIf there are at least two points, a regression curve will appear and\nwill take the existing points into consideration.\nEach new point will update the curve.\nBoth graphs on the right coordiante system display the squared and absolute\nregression error.\nBy clicking on the CLEAR-Button, the window will be resetted.\n'], 'url_profile': 'https://github.com/MartinKlapacz', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 8, 2020', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anitakowalczyk', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 8, 2020', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 29, 2020', 'R', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Housing-Market-Multivariable-Regression\nData Cleaning and Exploration on  real life dataset - Melbourne Housing Market\nPredicting house price using Random Forests and Ridge regression\n'], 'url_profile': 'https://github.com/MagdaW19', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'MATLAB', 'Updated Jun 29, 2020', 'Python', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '144 contributions\n        in the last year', 'description': ['Breast-Cancer-Logistic-Regression\n'], 'url_profile': 'https://github.com/RidaMalik', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'MATLAB', 'Updated Jun 29, 2020', 'Python', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '304 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/duminicaoctavian', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'MATLAB', 'Updated Jun 29, 2020', 'Python', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Regression\n\n\n\n\n'], 'url_profile': 'https://github.com/Develop-Packt', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'MATLAB', 'Updated Jun 29, 2020', 'Python', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'Boston ', 'stats_list': [], 'contributions': '132 contributions\n        in the last year', 'description': ['Modified Spline Regression\nThis is the repository for our paper titled ""Modified Spline Regression for Neual Spiking Data"" available on bioRxiv.\nHere we provide our MATLAB code for a simulated neural spiking data.\nCode\nThe code is divided to four categoreis:\n\nThe main.m MATLAB file contains the main framework to run the experiment.\nThe SimulateSpike.m file generates simulated neural spiking data\nThe Hist.m file is to build design matrix for multiplicative hisory model\nThe Indicator.m, RaisedCos.m, CardinalSpline.m and ModifiedCardinalSpline.m are four basis functions that used for point process generalizes linear model (GLM).\n\nTo run the model, you should clone all the files and run main.m file.\nResults\nThe following figure shows point process GLM fit using four different basis function on the simulated data.\n\nIn order to quantify the effect of the choice of basis function on the size of the confidence bounds, we computed a confidence interval width ratio (CIWR) of the confidence interval width at the end points over the average of confidence interval width in the interior regions for four different bases.\n\n\n\nFirst Header\nIndicator\nRaised Cosine\nCardinsl Spline\nModified Cardinal Spline\n\n\n\n\nCIWR_start\n3.97E-5\n1.836\n5.589\n1.477\n\n\nCIWR_end\n1.96E-4\n0.691\n2.553\n0.941\n\n\n\n'], 'url_profile': 'https://github.com/MehradSm', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'MATLAB', 'Updated Jun 29, 2020', 'Python', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '183 contributions\n        in the last year', 'description': [""Sklearn_Linear_Regression\nUsing sklearn's linear regression model on data, bias variance tradeoff was analyzed\nView Report.pdf for the detailed analysis\n""], 'url_profile': 'https://github.com/kushagragarwal2443', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'MATLAB', 'Updated Jun 29, 2020', 'Python', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '260 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JafarSadikGhub', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'MATLAB', 'Updated Jun 29, 2020', 'Python', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'New Jersey, USA', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['Decision-Tree-Regression\nPersonal Mini-Projects (Python)\nTruth Detector\nIs you new Employee lying about how much he got paid on his previous job?\nA potential employee told his new company that his current salary is $250k, the human resource retrieved the salary amount\nassociated position levels in his previous company. The potential new employee has a position level 7.5, we would like to\nbuild a salary bluffing detector to see if he told the truth.\n'], 'url_profile': 'https://github.com/AdeloreSimiloluwa', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'MATLAB', 'Updated Jun 29, 2020', 'Python', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'New Jersey, USA', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['Support-Vector-Regression\nPersonal Mini-Projects (R and Python)\nTruth Detector\nIs you new Employee lying about how much he got paid on his previous job?\nA potential employee told his new company that his current salary is $250k, the human resource retrieved the salary amount\nassociated position levels in his previous company. The potential new employee has a position level 7.5, we would like to\nbuild a salary bluffing detector to see if he told the truth.\n'], 'url_profile': 'https://github.com/AdeloreSimiloluwa', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'MATLAB', 'Updated Jun 29, 2020', 'Python', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Regularized-Logistic-Regression\nThis is regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\nTwo test were conducted for some microchips. From these two tests, we would like to determine whether the microchips should be accepted or rejected. To help make the decision, the dataset of test results on past microchips are provided, from which we can build a logistic regression model.\nAccept: Y = 1\nRejected: Y = 0\n'], 'url_profile': 'https://github.com/waltwissle', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'MATLAB', 'Updated Jun 29, 2020', 'Python', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'HTML', 'Updated Mar 28, 2020', '1', 'HTML', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '144 contributions\n        in the last year', 'description': ['Fraud-Detection-Logistic-Regression\n'], 'url_profile': 'https://github.com/RidaMalik', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Regression-Practice-Examples\nContains Examples of regression model computtation using Tensorflow 2.0\n'], 'url_profile': 'https://github.com/ArchanGhosh', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '303 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aliensmart', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['examples\nPie Chart: https://pythontic.com/visualization/charts/piechart\nColum Chart: https://projectcodeed.blogspot.com/2019/09/simple-data-visualisation-with-pandas.html\n'], 'url_profile': 'https://github.com/Miguel-SG6', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Gradient Descent and Logistic Model\nGetting Started\nInstalling Python\nTo use the script, you will need to install Python 3.6.x and add to path:\n\nPython 3.6.x\n\nInstalling Dependencies\nAfter cloning the project, go to the root directory:\nInstall the dependent libraries by typing the following in cmd/terminal:\n$ pip install -r requirements.txt\n\nStarting the Script\nTo run the script, go to the root directory and run python in cmd/terminal and type the following in the python console:\n>>> from lg_main import *\n\nNote: ensure that python refers to Python 3.6.x\nRunning Manual GD & SGD on Logistic Regression Model (Q1)\nType the following in the python console:\n>>> run_Q1()\n\nTraining Neural Network Model on MNIST_small (Q2)\nType the following in the python console:\n>>> run_Q2()\n\nBuilt With\n\nnumpy - all variables are numpy arrays\n\n'], 'url_profile': 'https://github.com/thomas-enxuli', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cugzj', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harshitroy2605', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '225 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bharath02', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'Seattle, WA', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': [""Spring into Data Science... with the Iris dataset!\nThis README.md file will serve as a roadmap to this repository. The repository is open and available to the public.\nDirectories and files to be aware of:\n\n\nA notebooks/ directory that contains Jupyter notebooks:\n\nA notebook that explores predicting petal length with linear regression (predicting_petal_length.ipynb)\nA notebook exploring simple linear regression basics (simple_linear_regression_basics.ipynb)\n\n\n\nA data/ directory containing one data file:\n\nDue to GitHub upload restrictions, this is included as a .gitignore file. It is, in brief:\n\nIRIS.csv\nThe data file described above can be found on Kaggle or from UCI's Machine Learning Laboratory (details on obtaining this data can be found in data/README.md).\n\n\n\n\n\nMethodology: We create simple and multiple linear regression models with the Iris data.\nResult: The functionality is used in the following blogs on Medium:\n\nSpring into Linear Regression (link here)\nSpring into Linear Regression--Part 2 (link here)\n\n""], 'url_profile': 'https://github.com/karenkathryn', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '167 contributions\n        in the last year', 'description': ['SalaryPrediction-Linear_Regression\n'], 'url_profile': 'https://github.com/tushargupta9412', 'info_list': ['Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020', '1', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}"
"{'location': 'Localhost', 'stats_list': [], 'contributions': '634 contributions\n        in the last year', 'description': ['Titanic-Dataset-Analysis\nA Logistic  Regression approach to predicting who survived the titanic Disaster\n'], 'url_profile': 'https://github.com/Dhaxor', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Mar 25, 2020', 'Updated Dec 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['MeasurementErrorAnalysis\nThe function calculates the parameters estimates of the measurement error regression model. The model is equipped for data of any size, one or multiple predictors, and varied days of data per individual.\nDeveloped as part of an undergraduate research project at Moravian College.\nEnvironment Requirements\n\nR (version 3.4.3)\nR libraries\n\nknitr\ntidyverse\nmosaic\nMASS\n\n\n\nFunctions\nThe MEM_functionMult function requires three inputs passed as .csv files: a nmÃ—1 matrix for the response variable, a nmÃ—p matrix for the predictor variables, and a nmÃ—1 matrix with unique indentification variables for the participants. Additionally, the function requires a confidence level as a numeric value. Here, n is the number of participants,   is the number of replications for each individual i, and p is the number of predictors for the model. The function returns a a px1 for the estimator of , a single value for , and a pxp  matrix. In addition to the parameter estimates, the funtion returns the standard error of each  value, the corresponding test statistic, and the confidence interval. The function also returns the unstandardized residual values for each predictor.\nRunning the Model\nR --slave --no-restore --file=MEM_function.R --args PredictorMatrix.csv ResponseMatrix.csv IDMatrix.csv CIlevel(numeric)\n'], 'url_profile': 'https://github.com/egbolger', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Mar 25, 2020', 'Updated Dec 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nilaj-c', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Mar 25, 2020', 'Updated Dec 17, 2020']}","{'location': 'Dublin , Ireland', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashishpatil2017', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Mar 25, 2020', 'Updated Dec 17, 2020']}","{'location': 'Pune,India', 'stats_list': [], 'contributions': '691 contributions\n        in the last year', 'description': ['iris_dataset\nUsed KNN and Logistic Regression for the UCI classifier dataset\n'], 'url_profile': 'https://github.com/rylp', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Mar 25, 2020', 'Updated Dec 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/swapster22', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Mar 25, 2020', 'Updated Dec 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/3awny', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Mar 25, 2020', 'Updated Dec 17, 2020']}","{'location': 'BANGALORE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Predict_Book_Prices\nObjective is to create a Multiple Regression model to predict book prices\n'], 'url_profile': 'https://github.com/princetiwari93', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Mar 25, 2020', 'Updated Dec 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '169 contributions\n        in the last year', 'description': [""R-logistic-regression\nUse logistic regression model to predict credit card applicants that are more likely to default, 'stats' package required.\nSince the portion of default credit card holders are very small (usually <5%), so recall is chosen as the performance metrics used to evaluate model performance rather than accuracy, since we want the percentage of true positive among real positive to be higher.\nSince the dataset contain some sensitive privacy information like gender, income... and it's important not to discriminate,\nso the model actually put into use has been modified and removed all the sensitive features.\n""], 'url_profile': 'https://github.com/changseverus', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Mar 25, 2020', 'Updated Dec 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Luca-Menghini', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'R', 'Updated Mar 25, 2020', 'Updated Dec 17, 2020']}"
"{'location': 'Charlotte, NC', 'stats_list': [], 'contributions': '263 contributions\n        in the last year', 'description': ['FlatIron-Mod1\nFor the Flatiron Mod1 final, I was tasked with applying linear regression models to the King County, WA, real estate dataset.  I took the approach of a startup real-estate company developing a model to predict home prices.\n\nOur company, Move on Up, is set to disrupt the real estate industry in Seattle by giving homebuyers and sellers an alternative to the 4-6% fees typically charged by traditional real estate agents, using a predictive model for home pricing, and connecting buyers and sellers through an on-line portal.\nAt the early age of our startup, we\'re beginning to develop a linear-regression model using Seattle residential sales data from the King County, WA real estate data set. Middle-class homes tend to be more standardized, while luxury homes contain non-standard features not reflected in the dataset which can affect price (i.e., built-in electronics, artisan or high-end woodwork, tilework, and appliances/fixtures).  According, the model focuses on middle-class homes.\nIn this repository, the Jupyter Notebook (""index.ipynb"") contains the development of our early linear-regression model, including an explanation of which variables from the King County, WA dataset were included, dropped, and transformed (and why), and how the model was developed.\nThe non-technical presentation is included both in video and PDF formats (""presentation.pdf""), explaining our approach and our early results. The video presentation may be viewed here: https://drive.google.com/open?id=1fh8rhT9pYPnBuxGetf4pyuTUNKLRabY7\nFinally, my blog for the Mod-1 final can be viewed at https://law2ds.blogspot.com/2020/04/flatiron-mod1-final-lessons-learned.html.\n'], 'url_profile': 'https://github.com/jnels13', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Logistic-Regression-using-Batch_Gradient-Newton\'s_Method-and-Gradient_descendent\nTo find the accuracy of ""Breast Cancer"" dataset using 3 different methods in Logistic Regression.\nThe aim here is to find the patient with breast cancer or not using the dataset which has various medical factors.\n'], 'url_profile': 'https://github.com/Nisa123', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'Philadelphia', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': [""Harvard-Business-Case-Study-on-Ship-Valuation\nExecutive Summary:\nCompass Maritime Services offers consulting services for clients who are interested in purchasing and selling maritime ships and offshore vessels. Basil Karatzas, director for projects and finance will help the client who is interested in purchasing a ship named BetPerformer, to determine the valuation and negotiation strategy of the ship. This report will show methodology, valuation analysis, concerns, best value and also suggest the bid price for the ship. The value accounts for all variables provided and gives the client the best chance to secure Bet Performer for its current market value.\nBet Performer:\nBet Performer is a bulk carrier with 172000 DWT and 12479 Capesize index. The ship is 11 years old and was built in Japan in 1997. This ship has a MAN-B&W engine with an engine power of 14710 KW and has nine holds and hatches. The same ship was sold two years back under a different name Mineral Poterne for $70 million.\nValuation Approach\nShips are valued using three different approaches. The first one is the market approach which is the most popular one. As we are valuing our ships using the market approach, we compare the price and other variables with those of other ships that were sold recently in the market. This approach is also called the â€˜mark-to-marketâ€™ approach. The second approach called income approach which uses the forecast of future cash flows such as daily charter rates to estimate the net present value of the ship. This is also called a â€˜mark-to-modelâ€™ approach. This approach would most likely produce a rationally deducted price that reflects the value of the vessel. The final approach is known as cost approach where vessels are valued on the assumption that how much it will cost to build the ship from base today in the original conditions and retrofit to our present conditions. This is a less popular approach.\nVariables\nData consists of recent Sale Date, Vessel Name, Price, Sale Year, Year Built, DWT and Capesize. The weight of these ships is measured in deadweight tons and is the sum of cargo, fuel, fresh water, passengers and crew weights. DWT for Capesize is greater than 100000. The baltic dry index calculates multiple shipping costs for different raw materials among various routes.Year built and Age at Sale represents the same factor. Age at Sale is the best predictor of the price when compared to other variables owing to the highest correlation. DWT and Capesize also has a significant correlation with the price of the ship and can affect the price. Sale Date has the least correlation and lowest influence on the price. In addition to the data collected about similar ships factors such as main engine type, repairs, building company reputation, charter contracts with counter-parties, loading equipment, shipyard and location of the ship at the time sale also influence the price of the ship. The following correlation table shows the relationship between price and the other variables.\nVariable Correlation:\nSale Date\t0.04\nYear Built\t0.81\nAge at Sale\t-0.79\nDWT\t0.51\nCapesize\t0.35\nMETHODS:\nIn order to come up with the best reference transaction and find the closest ship with respect to features compared with those of Bet Performer, the following approaches were used.\n\n\nRank Ships\n\n\nKNN Regression\n\n\nRANKING SHIPS:\nThe whole dataset of 48 other ships with all the features was considered. The objective was to find the transaction of a ship which was the closest to Bet Performer based on features like Age at Sale, DWT, Capesize, Sale Date, Year Built etc. Initial data analysis indicated that the Sale Date had a minimal correlation with the price of the ship. Year Built was directly correlated to Age at Sale, thus can be omitted for the analysis as it conveys the same meaning. Thus as part of data preprocessing the Year Built and Sale date were omitted from being considered for analysis. It led to DWT, Age at Sale and Capsize to be the major independent variables to predict the closest ship. The following steps were performed for each of the features i.e DWT, Capesize and Age at Sale.\nâ€¢\tEach feature was compared to the respective feature of Bet Performer and the difference was calculated.\nâ€¢\tThe difference calculated for each feature was normalized to a range of 0 to 1.\nâ€¢\tThe normalized difference was multiplied with the correlation between each feature and the price of the ship in order to give more weight to features that were strongly correlated.\nâ€¢\tThe weighted difference for all the features was added together to derive the net weighted difference.\nâ€¢\tAll the ships were sorted based on the net difference and the ships with the lowest net difference were labeled as the closest with respect to the feature of Bet Performer.\nKNN Regression:\nIn pattern recognition, the k-nearest neighbor's algorithm (K-NN) is a non-parametric method. The input consists of the k closest training examples in the feature space and the output is the property value for the object. This value is the average of the values of k nearest neighbors.\nIn order to test the KNN Regression model for the dataset, 70% of the data was set aside to train the model. The left over 30 % of the data was used to test the model. DWT, Age at Sale and Capesize are used as the predictors and price as the response.\nThe scatter plot was plotted for the predicted price against the actual price of the ships in the test data set. For a perfectly predicted value, we would expect to see points along the y=x line.\nAs most of the points followed the y=x line, we can conclude that KNN can be used as one of the approaches to predict the price for Bet Performer and went ahead to test the model with Bet Performers features i.e Age at Sale, Capesize and DWT.\nK Value Selection:\nRoot mean square error is a quadratic scoring rule that measures the average magnitude of the error. Itâ€™s the square root of the average of squared differences between prediction and actual observations. Since the errors are squared before they are averaged, it gives a relatively high weight to large errors. The method is more useful when a large error is particularly undesirable. The model with the lowest values is a better fitting model i.e K=5\nInference:\nAs the purpose was to get the closest ship, Sumihou emerged out to be the closest neighbor [K=1] with predicted price as $105M. As per the KNN Regression model, the price predicted for Bet Performer for the optimum value of K ie K=5 is $81.94M.\nLimitations:\nThe data set was quite scattered where there were lots of data points for ships for which the price was less than $100 M. The practical limitations of the algorithm is it just considers the small number of the sample from a dataset for feature matching and prediction. In the above scenario, the algorithm considers just the 5 closest ships to predict the price of BetPerformer. There are lot of observations and outliers which are not considered, thus it's not the best approach to predict the price. It can be used as one of the methods to find the best reference transaction or the closest ship with respect to features compared with those of Bet Performer\nIn order to predict the price of Bet Performer ,couple of approaches were considered.\n\n\nSimple Linear Regression\n\n\nMultiple Linear Regression\n\n\nSimple Linear Regression:\nSimple linear regression lives up to its name, it is a very straightforward approach for predicting a quantitative response Y on the basis of a single predictor variable X. It assumes that there is approximately a linear relationship between X and Y. Mathematically, we can write this linear relationship as Y â‰ˆ Î²0 + Î²1X. We computed a simple linear regression model on Age at Sale, Capesize and DWT (independent variables which are highly correlated to price) individually to predict the price of the Bet Performer as shown in the table below.\nPredicted Price:\nAge at Sale\t$86.74 M\nCape Size\t$96.06 M\nDWT\t$85.87 M\nBy analyzing the price of the ships against independent variable ie Age at Sale, DWT and Capesize we found out that:\nâ€¢\tAge at Sale : Ships value depreciates at a rate of around $4.2 million with every year as it ages.\nâ€¢\tDWT : For every one unit increase in DWT, the price of the ship increases by $0.98 million.\nâ€¢\tCapeSize : For every one unit increase in CapeSize, the price of the ship increases by $0.004 million.\nMultiple Linear Regression\nSimple linear regression is a good approach for predicting the dependent variable on the basis of a single predictor variable. In our case, we have examined the relationship between Price (dependent variable) and Age at Sale, Deadweight tonnage (DWT) and Capesize individually (independent variables). We ran three separate simple linear regressions, each of which uses a different independent variable as a predictor as shown in the results above.\nHowever, this approach of fitting a separate simple linear regression model for each predictor is not entirely satisfactory. First of all, it is unclear how to make a single prediction of Price given levels of the three variables Age at Sale, Deadweight tonnage (DWT) and Capesize Index, since each of the variables is associated with a separate regression equation. Second, each of the three regression equations ignores the other two variables in forming estimates for the regression coefficients. If theseindependent variables are correlated with each other in the ship data, then this can lead to very misleading estimates of the individual effects on Price. Instead of fitting a separate simple linear regression model for each predictor, a better approach is to directly accommodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model. We can go for 4 different predictor combinations for our Multiple Regression model.\nThe combinations are listed below.\nCombination\t              Adj R2\t  RSE\nAge At Sale+DWT\t            0.64\t  19.68\nAge At Sale+Capesize\t      0.90\t  10.52\nDWT+Capesize\t              0.34\t  27.2\nAge at Sale+DWT+Capesize\t  0.91\t  9.88\nThe RSE estimate gives a measure of the error of prediction. The lower the RSE, the more accurate the model (on the data in hand). Adjusted R^2 estimates the % of the variation in the measure of the sale price of the ships that can be predicted by independent variables. Based on the Adjusted R Square and Residual Standard Error the combination of age at sale, DWT and Capesize combination emerge out as the best model for multiple linear regression to predict the price of Bet Performer. This resulted in a model where 91.5% of the variation in the sale price of ships could be explained by Age, DWT, and Capesize.\nWe went ahead and used this model to predict the price of the Bet Performer. The predicted price as per multiple linear regression came out to be $125.83 M.\nConfidence Interval\t$ 118.80M\t$132.85 M\nPrediction Interval\t$104.48 M\t$147.17 M\nPredicted Price of Bet Performer under different scenarios\n5 years younger\t        $148.54 M\n20K DWT lighter\t        $120.98 M\n30% lower charter rate\t$98.84 M\nThe above inference was considered to adjust the underpriced prediction. The final price predicted for Bet Performer as per our case study will be $ 132.19 M.\nConclusion:\nThe purpose of analysis was to come up with the predicted price of Bet Performer which would fall under both the client and the seller's satisfaction. We did a detailed study using the Market Approach as the base and came to a final price of $ 132.19M for the Bet Performer which we think will be the amount to bid and successfully buy the bulk carrier ship which the client had interest in.\n""], 'url_profile': 'https://github.com/Sasidhar-Sirivella', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '648 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/iamdeepak42', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'Shanghai, China', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['RR-MCA\nRidge regression (RR) combined with model complexity analysis (MCA) for updating of near infrared (NIR) quantitative models.\nReference:\nZhang Feiyu, Zhang Ruoqiu, Wang Wenming, Yang Wuye, Li Long, Xiong Yinran, Kang Qidi, Du Yiping.\nRidge regression combined with model complexity analysis for near infrared (NIR) spectroscopic model updating\n[J]. Chemometrics and Intelligent Laboratory Systems, 2019, 195: 103896.\nMATLAB and Python codes of RR-MCA are given, as well as their corresponding examples.\nThe tablet dataset is a publicly available dataset, which can be downloaded from http://www.eigenvector.com/data/tablets/index.html.\nHere, the tablet dataset in the examples has been pretreated, including outlier elimination, wavelength region selection and\ndata split. The details of the pretreatment can be found in the reference above.\nThe principles of RR-MCA can be found in the reference above.\n'], 'url_profile': 'https://github.com/zhangfy1993', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'Singaore', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': [""Project 2 - Ames Housing Data and Kaggle Challenge\nProblem Statement\nTo predict as accurately as possible the sale price of a house based on the Ames Housing Dataset using regression techniques enhanced by feature engineering and regularization.\nAdditionally, the model should also help to perform inferential learning to provide homeowners with recommendation on the features that would fetch good sale price and those that would hurt the sale price.\nExecutive Summary\nBackground\nThis project examines housing data from Ames, IA with the goal of building a model to predict housing prices. The dataset is an exceptionally detailed and robust dataset with over 70 columns of different features relating to houses.\nThe objective is to :\n\nConstruct a regression model that accurately predict the sale prices of houses within test dataset.\nUsing Kaggle to practice the modeling process.\nProvide business insights through reporting and presentation.\n\nModeling Process and Evaluation\nFor modeling and performance measurement, the data are separated into a training and a test set.\n\n\nData cleaning was performed on both training and test set. Major cleaning performned that affected the data are as follows:\na) Dropped off columns with huge number of null values i.e. more than 250 cell with null values after checking their   correlation to sale price. Columns falling into this category are : Lot Frontage, Alley, FireplaceQu, Pool Qc, Misc Features, Fence\nb) Replaced cell values with less that 250 cells with null values : 0 for numeric and NA for categorical columns\nc) Related/ dependent columns were dropped off : Basement finished sqft1, Basement finished sqft2 and Basement unfinished sqft are redundant as these are summed up and stored in column : Total Basement Sqft.\n\n\nEDA involved the following studies:\na) Correlation of features to sale price\nb) Correlation of fatures among themselves - looking out of multicolinearity\nc) Distribution of numeric variables\nd) Spotting outliers and removing them to avoid noise learning by models\ne) Normalized sale price to follow a normal distribution for the regressors to work better\n\n\nPreprocessing and Feature Engineering involved :\na) Replacing oridnal columns with integer ranking.\nb) Creating dummy variables for remaining categorical variables.\nc) Train-test-split with 90% training set.\nd) Standardizing training sets.\n\n\nModeling, Evaluation and Final Prediction\na) Created 4 models - Linear Regression (LR), LR with regularization methods - Ridge, Lasso and Elastic Net.\nb) The models were measures with R-squared (relative measure of model fit) and RMSE (absolute measure of model fit).\nc) The higher the R-squared and lower the RMSE, the more accurate the model is. The best model turned out to be Lasso. (not surprising)\nc) The best model was selected to predict sale price in test dataset.\nd) The predicted saleprice was uploaded to Kaggle and scored RMSE Kaggle score of 21247.42348 which is the lowest thus far.\ne) The coefficients' information obtained from Lasso modeling can also be used for inferential learning resulting as recommendations to home owners on the features affecting the sale price of their property postively and negatively.\n\n\nConclusions and Recommendations\n\n\nThe importance of data cleaning and removing outliers cannot be stressed enough. It helped improve the R2 score by over 10%. Even though it is time consuming and rather dirty, the performance enhancement it brings to the accuracy/ scoring of the model is worth every bit of cleaning.\n\n\nThe model shows strong R Squared score and is certainly a strong prediction model. Linear regression may be a basic prediction model, but prediction is not the only output of it. It helps much with inferential learning showing what factors influence the selling price of a house. This may well be more valuable than the predictions themselves.\n\n\nAddressed problem statement by developing a relatively accurate model to predict housing prices based on various features\n\nTop 3 features that can fetch higher sale price : Bigger houses, Well renovated (or good quality) houses and Newer houses\nFeatures that ma hurt the sale price : Unfinished basement, Houses located in commercial zone and Townhouses\n\n\n\nA lot more can be done to analyse the relationship between features like age of the property, year sold, style of the house etc. that may lead to next level of feature engineering.\n\n\n""], 'url_profile': 'https://github.com/akhilarj', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Srinivas-Rao-Uchicago', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '199 contributions\n        in the last year', 'description': ['predicting-student-grades\nVisualizing and using multiple regression / classification algorithms as well as deep learning to predict student grades.\nDataset from UC Irvine\nhttps://archive.ics.uci.edu/ml/datasets/student+performance\nhttp://www3.dsi.uminho.pt/pcortez/student.pdf\nPlease include this citation if you plan to use this database:\nP. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.\n\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/sam-brady', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '283 contributions\n        in the last year', 'description': ['Analysis-of-HIV-Drug\nAnalysis of HIV Drug Resistance Data (Linear Regression Models: Final Project)\nThe scientific goal is to determine which mutations of the Human Immunodeficiency Virus Type 1 (HIV-1) are associated with drug resistance. The data set, publicly available from the Stanford HIV Drug Resistance Database http://hivdb.stanford.edu/pages/published_analysis/genophenoPNAS2006/, was originally analyzed in (Rhee et al. 2006).\n'], 'url_profile': 'https://github.com/yipeichan', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/EbsHirani', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 23, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020']}"
"{'location': 'Stuttgart, Germany', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Predict-a-customer-subscription-to-a-telemarketing-campaign-using-Python\nApplied Machine Learning to a classification task using Random-Forests, Gradient Boosting, Logistic Regression, etc\nThe main purpose of this report is to setup the benchmark experiment to compare 5 selected\nmachine learning algorithms. For each algorithm the setup and the result will be explained in\nterms of cross-validation method (holdout, k-fold CV, etc.), evaluation metric (AUC, Accuracy,\netc.), hyperparameter tuning, variable selection, resampling method (over-sampling,\nundersampling, etc.), etc.).\nThe dataset used is the same for the Kaggle competition with the aim to predict whether a\ncustomer will subscribe to a telemarketing campaign or not.\nModels Selected:\nThis is a classification problem. But what is a classification problem ?\nA classification problem is when independent variables are continuous in nature and dependent\nvariable is in categorical form. Here is to categorize customers as they are going to subscribe or\nnot. All these problemâ€™s answers are in categorical form i.e. Yes or No. and that is why they are\ntwo class classification problems.\nTherefore the following ML models were used:\n\nLogistic Regression\nRandom Forrest\nXGBoost\nSupport Vector Machine\nLinear Discriminant Analysis,\n\n'], 'url_profile': 'https://github.com/omarabdelgelil', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2021', 'Python', 'Updated Aug 14, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Jul 7, 2020', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['Visual Speaker Recognition biometric\nThis work presents a biometric system for speaker recognition using visual-only speech features.\nThe dataset used can be found at AV DATASET, which is an open dataset.\nThe libraries used in all programs are listed in requirements.txt and can be installed with Pip using the following command:\nRun:\npip install -r requirements.txt\nFiles and directory structure\nâ”œâ”€â”€ auxiliars\nâ”‚   â”œâ”€â”€ faceDetection.py    \t  \t#Detect faces in a frame\nâ”‚   â””â”€â”€ lipsExtraction.py\t\t#Extract coordinates of lips and save lips points (txt file) and images with their points and curves around.\nâ”œâ”€â”€ AVOriginalDataset\t\t\t#Folder that contains the AV dataset mentioned before.\n|   â”œâ”€â”€Phrases\n|   |\tâ”œâ”€â”€ **/*\t\t\t#Folders containing .mp4 and csv timestamps     \n|   â””â”€â”€Digits\n|\tâ”œâ”€â”€ **/*\t\t\t#Folders containing .mp4 and csv timestamps\nâ”œâ”€â”€ AVSegmentedDataset\t\t\t#Folder that contains the AV dataset, segmented by utterances.\n|\tâ”œâ”€â”€ Digits\n|\t|\tâ”œâ”€â”€ Normal\t\t# Folders containing .mp4 segment by utterance and speech mode\n|\t|\tâ”œâ”€â”€ Whispered\n|\t|\tâ”œâ”€â”€ Silent\n|\tâ”œâ”€â”€ Phrases\n|\t|\tâ”œâ”€â”€ Normal\n|\t|\tâ”œâ”€â”€ Whispered\n|\t|\tâ”œâ”€â”€ Silent\nâ”œâ”€â”€ LipsFrames\t\t\t\t#Folder containing all lips images generated.\n|   â”œâ”€â”€\t**/*.jpg\t\t\t\nâ”œâ”€â”€ modelsFaceRecognition\t\t#Contains neccesary files for computer vision functions. (if not included, could be found on internet)\n|   â”œâ”€â”€ haarcascade_frontalface_alt.xml\n|   â”œâ”€â”€ opencv_face_detector_uint8.pb\n|   â”œâ”€â”€ opencv_face_detector.pbtxt\n|   â””â”€â”€ shape_predictor_68_face_landmarks.dat\nâ”œâ”€â”€ AV_lips_coordinates_v0.txt\t\t#File containing dictiory with all lip coordinates of uterances (is generated by lipsCoordExtraction.py)\nâ”œâ”€â”€ featuresProcessing.py\t\t#Functions that process the coordinates.\nâ”œâ”€â”€ hmm.py\t\t\t\t#Program that uses features to generate HMM\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ lipsCoordExtraction.py\t\t#Program that generate lips coordinates for all utterances and also the lips images.\nâ”œâ”€â”€ requirements.txt\t\t\t#Libraries needed run the programs\nâ””â”€â”€ segmentVideos.py\t\t\t#Script to segment original videos into utterances ussing CSV files timestamps in each video.\n\nVideos segmentation\nFirst we run the segmentVideos.py, which will generate the videos separated by each speech mode and specific utterance. This script uses the timestamp provided by the dataset to segment each utterance.\n\nNote: The script will generate only the segmentation for phrases, to apply segmentation for digits user should change the paths used in the script.\n\nLips coordinates extraction\nNow we execute:\npython3 lipsCoordExtraction.py\n\nWith this, we generate files containing the coordinates of the lips in each frame for all videos. To specify number of coordinates, type of face location algorithm and dataset, we need to change commented parts in code.\nFeatures processing (lips coordinates)\nFunction used for feature processing in this case, normalization of lip coordinates can be found in featuresProcessing.py, these function are used directly when required in the step of generating HMMs.\nGenerate HMMs from features\nIdentify speakers using HMM for each utterance\n'], 'url_profile': 'https://github.com/joseamoroso', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2021', 'Python', 'Updated Aug 14, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Jul 7, 2020', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '283 contributions\n        in the last year', 'description': ['Analysis-of-HIV-Drug\nAnalysis of HIV Drug Resistance Data (Linear Regression Models: Final Project)\nThe scientific goal is to determine which mutations of the Human Immunodeficiency Virus Type 1 (HIV-1) are associated with drug resistance. The data set, publicly available from the Stanford HIV Drug Resistance Database http://hivdb.stanford.edu/pages/published_analysis/genophenoPNAS2006/, was originally analyzed in (Rhee et al. 2006).\n'], 'url_profile': 'https://github.com/yipeichan', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2021', 'Python', 'Updated Aug 14, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Jul 7, 2020', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/EbsHirani', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2021', 'Python', 'Updated Aug 14, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Jul 7, 2020', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['KaggleHousePrediction\nfirst assignment from school for train a model with regression for house prediction\n'], 'url_profile': 'https://github.com/slps20425', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2021', 'Python', 'Updated Aug 14, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Jul 7, 2020', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['Semi-Supervised Support Vector Regression (SVR)\nLaplacian Embedded Support Vector Regression (Chen et al., 2012)\nThe picture below shows the decision surface on Two-Moons data set created by LapESVR when the unlabelled is minimally utilized, $\\mu = 1$. Note that only the data points near the center are labelled.\n\nWhen $\\mu = 1000$, the model learns the structure of the two moons through the unlabelled data.\n\nWindows Binary\nCurrently only Windows binary is supported.\n\nSet up VC++ environment variables by running vcvars64.bat.\nTo clean the existing binary.\n\nnmake -f Makefile.win clean\n\n\nTo build Windows binary.\n\nnmake -f Makefile.win lib\n\nPython Interface\nCurrently only Python interface is supported.\nExample is given in python/LapESVR.ipynb.\nReferences\nChen, L., Tsang, I. W., & Xu, D. (2012). Laplacian embedded regression for scalable manifold regularization. IEEE Transactions on Neural Networks and Learning Systems, 23(6), 902â€“915. https://doi.org/10.1109/TNNLS.2012.2190420\n'], 'url_profile': 'https://github.com/ChinHuan', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2021', 'Python', 'Updated Aug 14, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Jul 7, 2020', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Multiple-Regression-Predicting-hospital-length-of-stay\nPredicting hospital length of stay using ICD-9 data in python with multiple regression analysis\n'], 'url_profile': 'https://github.com/aprihoda', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2021', 'Python', 'Updated Aug 14, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Jul 7, 2020', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'Lille, France', 'stats_list': [], 'contributions': '384 contributions\n        in the last year', 'description': ['Kaggle Competition - Regression Algorithms with Python to predict House price\nPython Project - Kaggle Competition - Top 11%: This repository contains the data, the code and a short presentation explaining how we processed the data and tunes our models based on regression in order to predict the house price regarding many factors.\nKaggle Competiton: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview\n\n'], 'url_profile': 'https://github.com/SebastienPavot', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2021', 'Python', 'Updated Aug 14, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Jul 7, 2020', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amansinghal123', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2021', 'Python', 'Updated Aug 14, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Jul 7, 2020', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '99 contributions\n        in the last year', 'description': ['Machine-Learning-SP500\nIntroduction\nObjective\nThe objective of this project is aimed at using python and different machine learning algorithms(LSTM, KNN, Linear Regression) to conduct predictions on the S&P500 index. This problem is interesting because it can compare the performance of different machine learning algorithms when it applies to the prediction of the S&P500 index. At the end of the project, it should bee able to conclude which algorithms are better for the prediction of the S&P500 index.\nData\nAll data are obtained from Yahoo Finance S&P500 index. There are 8850 instances in the data set from the period 2/1/1985 to 3/13/2020. The reason this period is chosen because it contains at least three economic recession in USA history. The data type of this data only contains Date, Numeric data type.\nOutcome\nThe project experienced three different machine learning algorithms which are LSTM, KNN, and linear regression. LSTM obviously has a better result than linear regression, and linear regression has a better result than KNN.\n'], 'url_profile': 'https://github.com/kwokmoonho', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2021', 'Python', 'Updated Aug 14, 2020', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Python', 'Updated Jul 7, 2020', 'Updated Apr 25, 2020', 'Python', 'Updated Mar 31, 2020']}"
"{'location': 'United Kingdom', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': [""CNN project for Ising model generated via MCMC\nThis project was part of my BSc dissertation at the University of Strathclyde and continued over summer 2019 as a research project. The BSc dissertation is the data/main_thesis.pdf. There are typos that require errata, however this is the thesis as submitted.\nThe project has two major components, the MCMC system generation and the actual CNN training, prediction and evaluation. This project was started with the goal of producing a paper on an overview of deep regression in late 2018/early 2019, however another paper came up A Comprehensive Analysis of Deep Regression which was not only far more expansive in its analysis but also had access to significantly more compute. It was however good to see that the same conclusions were drawn.\nThis README is split into an introduction, which you are reading now, a config/dependency section, a MCMC module section, the CNN section, then a theory section. The Theory section is not required to run this.\nPlease keep in mind this project was made in mind with utilizing AWS infrastructure, particularly S3 for file storage/backups and EC2 for compute. the EC2 p3 instances were used for training using the AWS DLAMI Version 24.\nHow to Run Basics\nNothing in code directory needs to be directly touched/changed other than the config.ini. Everything that you should need can be called from two scripts, generate_systems.py  and main_gpu.py. The former generates systems using MCMC, and can prepare synthetic Ising systems with the latter being used for training. Inference and evaluation of the CNN models does need access to code/run_predictions.py.\nExamples of how to call these would be:\npython3 generate_system.py mcmc 5000 \npython3 generate_systems.py combine\npython3 generate_systems.py compress\npython3 main_gpu.py\ncd code\npython3 run_predictions.py\nThis would generate 5000 systems (and 10,000 .npy files), combine them, then compress them into a nice npz format, then train your model (assuming models.csv is filled), cd into code folder, then predict using that model.\nConfiguration, Dependencies and Directory Structure\nThis section discusses the config.ini file, as well as the folder structure that the project comes with and its behaviours in terms of filling the folder structure. Dependencies are also discussed.\nConfig File\nAll module paths are configured using the config.ini file, as well as calling AWS S3. The config file needs to be updated, as currently the path points to C:/Users/alexi/Desktop/deep_regression_new with a commented out path for the Ubuntu DLAMI. You also need to change the s3_bucket:  from HIDDEN to your bucket if you wish to use the backup functionality in the utils.py module.\nDependencies\nThis project uses a few libraries, the obvious ones are NumPy and Numba. Pathlib is used for all file directory management and tqdm is used for progress bars.\nThe deep learning module was built through Keras API in TensorFlow 1.13, it will NOT work for TensorFlow 2.0, but can quite easily be upgraded to it. Sci-kit learn is also used purely for its accuracy metric. As noted previously, AWS DLAMI (Ubuntu Version 24) was used for running all training/inference code. MCMC modules were trained on AWS EC2 C4/C5 instances using Clear Linux due to its nice Intel Xeon optimizations that allowed for 20% better performance.\nDirectory Structure\nThe directory structure is not final. Currently the two main scripts are main_gpu.py for the CNN and generate_systems.py for MCMC. The directory structure can be changed by altering the relative paths within config.ini.\n> code\n\t> __init__.py\n\t> config.ini\n\t> logging_module.py\n\t> metropolis_ising.py\n\t> model.py\n\t> model_details.csv\n\t> model_evaluations.csv\n\t> predictions.py\n\t> run_predictions.py\n\t> utils.py\n> data\n\t> ising\n\t> logs\n\t\t> loss_histories\n\t\t> model_logs\n\t\t> training_logs\n\t> models\n\t> predictions\n\tmodels.csv\n\tmain_thesis.pdf\ngenerate_systems.py\nmain_gpu.py\nREADME.md\n\nMCMC Module\nThe MCMC module is really the Metropolis-Hastings module. The core implementation can be found in code/metropolis_ising.py where the function mcmc_ising is a Metropolis-Hastings implementation of the 2D Isotropic Ising model with periodic boundary conditions.\nYou should not really need to call this function directly,\nCNN Module\nThe CNN script main_gpu.py calls model.py in order to train the CNN. main_gpu.py passes args to model.py that are defined within it's docstring. These args can technically be passed manually, however this doesn't make much sense. main_gpu.py can easily be used to train an arbitrary number of models on single GPUs easily.\nFor example during this project, a p3 instance with 4 V100s was used throughout the project, 4 tmux windows were opened and main_gpu.py was ran for each one with the argument of GPU ID passed (0 - 3 in this case).\nmain_gpu.py will look within data/models.csv to find the configuration of the type of CNN you wish to train.\nA recommended workflow is to write all the different networks you wish to train in data/models.csv that are then separated by GPU. So for example a dozen per GPU were used, and these were then all automatically run and saved with no manual intervention being necessary.\nThe models will be automatically saved to data/models in .h5 file formats, log files for loss history, and very importantly for the actual model (all arguments passed + date it was trained) are saved within data/logs in the aptly named folders.\nTheory, Questions that were asked/Answers\n""], 'url_profile': 'https://github.com/alexisdrakopoulos', 'info_list': ['1', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Alverca do Ribatejo', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Supervised_Learning_House_Prices\nSupervised Learning with Python - Predicting House Prices with Multivariate Linear Regression\n'], 'url_profile': 'https://github.com/RFJC21', 'info_list': ['1', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Chennai, India', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': [""Employability-Rating-System-using-Flask\nAn Employability Rating System using Flask integrated with a linear regression model.\nStep 1: install the following packages using pip: \n\xa0\xa0\t-flask \t\n\xa0\xa0\t-joblib  \n\xa0\xa0\t-pickle\t\n\xa0\xa0\t-werkzeug  \n\xa0\xa0\t-pdfminer   \n\xa0\xa0\t-nltk  \n\xa0\xa0\t-sklearn  \nStep 2: incase of error while downloading the 'punkt' library: \n\xa0\xa0\t-Open the python REPL in command prompt. \n\xa0\xa0\t-import nltk  \n\xa0\xa0\t-nltk.download() \n\xa0\xa0\t-a new window opens with all the nltk packages available for dowload, download punkt from models. \nStep 3: run app.py\n""], 'url_profile': 'https://github.com/yk-2310', 'info_list': ['1', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Chile', 'stats_list': [], 'contributions': '536 contributions\n        in the last year', 'description': ['Image Classification with CIFAR-10 dataset\nThis notebooks have the purpose to use Convolutional Neural Network (TensorFlow2) and Conventionals Models like Random Forest, Logistic Regression, KNN and SVM to classify images from the CIFAR-10 DATA SET.\nThis project was done together with my friend Francisca Hernandez PiÃ±a.\nAbstract\nCIFAR-10 is a computer vision data set used for object recognition.\nThis dataset contains 60,000 32x32 pixel color images distributed in 10 classes of objects, with 6,000 images per class, these are:\n\n1 - airplane\n2 - automobile\n3 - bird\n4 - cat\n5 - deer\n6 - dog\n7 - frog\n8 - horse\n9 - ship\n10 - truck\n\nOf this total of 60,000 images, 50,000 are for training and 10,000 for testing. For this specific project, 10% of training images were removed to validate, leaving the data distribution as follows:\n\nThe purpose of this project is to recognize an image and predict which of the 10 CIFAR-10 classes it belongs to.\nModels\nThe following models are used in this project:\n\nCNN\nPCA\nRANDOM FOREST\nLOGISTIC REGRESSION\nKNN\nSVM\n\nThe Data Set\nThe data to be analyzed is distributed in 6 files called batches, each one has a set of 10,000 images, one of these being used as model testing information. Below is an image with the distribution:\n\nThe dimension of a color image is 32x32 pixels and is made up of 3 channels (Red, Green, Blue = RGB) for each pixel present in them. The values for each channel range from 0 to 255, allowing each pixel in the image to be colored. Considering the above, you have 32 x 32 x 3 = 3072 data per image.\n\nDue to the composition of the data to be used, it is not necessary to carry out a comprehensive data cleaning as was done during the course. In this case, a normalization is only carried out by applying the formula:\n\nNormalization is done so that all the images have a common scale, in addition, it allows the models to work as a normal distribution.\nResults\nFor this specific project, the most relevant model metric is precision, since it only matters if the model is accurate when identifying an image.\nThe results obtained for each of the mentioned models were:\n\nCNN\t: 0.7088\nRANDOM FOREST : 0.3908\nLOGISTIC REGRESSION : 0.1453\nKNN\t: 0.4019\nSVM : 0.4833\n\nConclusion\nFinally, when we compare the results of the metrics of each model, it follows that the best applied model is convolutional neural networks. (Backpropagation algorithm for the win :D)\nExtras\n\n\nMIT License\nCopyright (c) 2020 FILLIKENESUCN\n'], 'url_profile': 'https://github.com/chelosky', 'info_list': ['1', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Maroc', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ismailktami', 'info_list': ['1', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Peshawar, Pakistan', 'stats_list': [], 'contributions': '218 contributions\n        in the last year', 'description': ['document-classification\nA project on classificaion of documents using doc2vec and logistic regression\n'], 'url_profile': 'https://github.com/hassanms', 'info_list': ['1', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Raleigh, NC', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Machine-Learning-using-Python\nData Science and Machine learning algorithms to solve classification and regression problems\n'], 'url_profile': 'https://github.com/sbalanoptima', 'info_list': ['1', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['CORONIVIUS-PREDICTION-USING-LINEAR-REGRESSION\nTHIS IS PREDICTION OF NUMBER OF CASES USING LINEAR REGRESSION MODEL\n'], 'url_profile': 'https://github.com/venkyvt7', 'info_list': ['1', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': [""Home Sale Price Prediction - Ames, IA  |  Jena Brentano\n\u200b\n\u200b\nProblem Statement\nThe company, Zillow, needs to improve their predictive home sale price models in general, but here specifically for the city of Ames, Iowa or similar cities with huge growth potential. This will in turn positively affect their bottom line, as they maintain their leading position in the industry by providing reliable information to customers. These customers include prospective home buyers, sellers, real estate agents, and those in other related sectors. Zillow provides important information for prospective home buyers and sellers, and is a powerful platform for advertising or referrals for real-estate agents and service providers. Zillow is also expanding into the physical real estate market. What makes them an excellent resource to it's current customers can also enable a lucrative business model of flipping homes for a profit.\n\nDescription of Data\nZillow has provided a dataset describing about 2050 homes with about 80 attributes which include qualitative and quantitative categories such as Overall Quality, Number of Rooms, Kitchen Quality, Garage Type, Misc. Feature Value, and of course, Sale Price.\nData Dictionary\nThe data is available in detail here: http://jse.amstat.org/v19n3/decock/DataDocumentation.txt\nMethods:\n\nData cleaning, converting categorical features to numerical, feature selection.\n\u200b\nUsed Linear Regression Model and was able to predict home sale prices within $14,000 of actual sale price.\n\nPrimary Findings/Conclusions:\n\nFeatures that are most highly correlated to Sale Price in the given data are: Overall Quality, Ground Floor Living Area, Exterior Quality, Kitchen Quality, Garage Cars (# of cars).\n\u200b\nBecause many of the provided home features are categorical/ordinal, it is important to convert these features using dummy variables or translating to numerical ratings in order to explore their potential uesfulness as predictors in a model.\n\u200b\nA powerful predictive model can be made with Linear Regression just by choosing features from the provided data with high correlations to Sale Price, but it's important to look at the statistical significance of these features on sale price to create a model that works well on new data.\n\u200b\nMany of the 80 features in the dataset had little to no impact on the accuracy of the model. If it helps the bottom line to collect less data, Zillow has an opportunity to explore and tune which features are unecessary to record.\n\u200b\nMany feature categories are inter-related, and the features for a final predictive model should be carefully considered to avoid multi-collinearity, or inter-relatedness. This is to preserve the integrity of the model and to enable us to understand clearly the relationship of a specific feature to Sale Price. Creating many new interaction features automatically with Polynomial Features functions and using LASSO to select the most impactful features is an excellent method for creating a great predictive model, but requires careful scrutiny to avoid many potential cases of collinearity of model features\n\n""], 'url_profile': 'https://github.com/jenajeanmartine', 'info_list': ['1', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Benniah', 'info_list': ['1', 'Python', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'HTML', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}"
"{'location': 'Berkeley, CA, USA', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jasperlin110', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['nba-player-modeling\nmodeling average points scored by nba players from 1996 using polynomial, ridge, and lasso regression\n'], 'url_profile': 'https://github.com/enoo24', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'Ann Arbor', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': [""Sleep quality defined\nThis repository includes the gourp project II for BIOSTAT620 @Umich SPH and the modified final project to define the sleep quality score.\nGroup member: Litian Zhou, Bangyao Zhaoï¼ŒNingyuan Wang\nHere are some introduction of some key functions\ndata2df.R: a function to process raw data\ndata_preprocess.R: use the result from data2df.R to create epoch data, create a dataframe for every person\nnormalization.R: use the result from data_preprocess.R, we can discuss multiple ways to normalize everyone's data and merge them into one big dataframe.\nglm_analysis.R based on the normalized data, run a glm logistic regression\nrandom_forest.R model random forest and tune hyperparameters\nlogistic_regression_box_up.R fit a diagnosed logistic model and draw box plot to check sleep qualiy in subgroups\n""], 'url_profile': 'https://github.com/LitianZhou', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Max-Temperature-Prediction\nThis is a linear regression model wherein maximum temperature is predicted .\nuseful resource for reference :\nhttps://www.kdnuggets.com/2019/03/beginners-guide-linear-regression-python-scikit-learn.html\n'], 'url_profile': 'https://github.com/JanhaviJain', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'Budapest, HU', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Rent price analysis in Budapest\nWelcome!\nIn last couple of years (2015-2020) rental listings in Budapest became YTD more and more expensive, and, due to my curious nature, I wanted to understand why.\nThis project aims to quantify the effects of housing variables (e.g. condition, is it furnished) to the price of the listing in Budapest.\nWhat to know\nThis project is completely written in python, but I also used powerBI for my data vizualizations and I created my own geojson for the data mapping using folium.\nProject parts\n\nWebscraping\nChange Data Capture\nLoading data to DB2 database\nData analysis\n\ndata cleaning\noutlier detection & handling\nnull handling\ndescriptive statistics\nmodel building\n\nlinear regression\nregularized regression (lasso & ridge)\n\n\n\n\n\n'], 'url_profile': 'https://github.com/tamasmrton', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '215 contributions\n        in the last year', 'description': ['Car-Purchase Amount-Prediction-using-ANN\nAIM: To develop a model to predict the total dollar amount that customers are willing to pay given the following attributes:\n\nCustomer Name\nCustomer e-mail\nCountry\nGender\nAge\nAnnual Salary \nCredit Card Debt \nNet Worth \n\nOutput:\nCar Purchase Amount\n'], 'url_profile': 'https://github.com/VarunV991', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'Dublin , Ireland', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashishpatil2017', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': [""logistic-regression-for-covid-19\nModÃ©lisation naÃ¯ve de l'Ã©pidÃ©mie Covid-19 par une fonction logistique\nImplÃ©mentation en python2 (a priori compatible python3)\nDÃ©pendances\n\nnumpy\npymc2\nmatplotlib.pyplot\n\nPrincipe\nOn cherche Ã  approcher la sÃ©rie temporelle des valeurs cumulÃ©es des {dÃ©cÃ¨s | hospitalisÃ©s | cas dÃ©tectÃ©s | â€¦ } par une fonction logistique gÃ©nÃ©ralisÃ©e Ã  5 paramÃ¨tres :\n y(t) = amp / ((1 + q * exp(pente * (x - bias))) ** (1 / nu))\n\n(voir https://en.wikipedia.org/wiki/Generalised_logistic_function)\nL'approche est bayesienne : on considÃ¨re les 5 paramÃ¨tres comme des variables stochastiques, au dÃ©part de distribution uniforme (priors). AprÃ¨s application d'un Monte-Calo Markov Chaining (MCMC), on obtient les distributions posterior des paramÃ¨tres Â«fittantÂ» les observations.\nLes paramÃ¨tres sont ensuite Ã©chantillonÃ©s depuis ces distributions posterior et sont utilisÃ©s pour appliquer la fonction logistique Ã  un domaine Ã©tendu, permettant d'obtenir une famille de courbes de prÃ©vision d'Ã©volution du phÃ©nomÃ¨ne.\n\nUsage\npython fit-function-logistic-5params.py <N>\nSi N=0 prise en compte de l'ensemble des observations, sinon prise en compte des N premiÃ¨res\nLe code fit-function-logistic-3params.py est une version antÃ©rieure, implÃ©mentant la fonction logistique non gÃ©nÃ©ralisÃ©e.\nRÃ©fÃ©rences\nProbabilistic Programming and Bayesian Methods for Hackers, Cameron Davidson-Pilon https://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Ch1_Introduction_PyMC2.ipynb\nBugs\nLes bornes actuelles utilisÃ©es pour les priors provoquent souvent des RuntimeWarning overflow. En gÃ©nÃ©ral, cela ne compromet pas la convergence du fitting.\nCode Ã  largement restructurer !\n""], 'url_profile': 'https://github.com/rriv', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['data-science-process-daily-channel-dsc\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '450 contributions\n        in the last year', 'description': [""linear-regression-without-library\nThis is a multivariate linear regression project without scikit-learn.\n-Regression is a method of modelling a target value based on independent predictors.\nThis method is mostly used for forecasting and finding out cause and effect\nrelationship between variables. Regression techniques mostly differ based on the\nnumber of independent variables and the type of relationship between the independent\nand dependent variables.\n-Simple linear regression is a type of regression analysis where the number of\nindependent variables is one and there is a linear relationship between the\nindependent(x) and dependent(y) variable.\nFORMULA: Y = a + bX1 + cX2 + dX3 + Ïµ\nWhere,\nY â€“ Dependent variable\nX1, X2, X3 â€“ Independent (explanatory) variables\na â€“ Intercept\nb, c, d â€“ Slopes\nÏµ â€“ Residual (error)\nSTEPS:\n-load the data, X, Y\n-turn X and Y into numpy arrays\nY â€“ the observed value\nplot the data\nÅ· â€“ the value estimated by the regression\nY hat (written Å· ) is the predicted value of y (the dependent variable)\nin a regression equation. It can also be considered to be the average\nvalue of the response variable.\nPredicted Value Y-hat. Y-hat ( ) is the symbol that represents the predicted\nequation for a line of best fit in linear regression. The equation takes the\nform where b is the slope and a is the y-intercept. It is used to differentiate\nbetween the predicted (or fitted) data and the observed data y.\nFORMULA:\nb = (X^T . X)^-1 . X^T . y\nyhat = X.dot(b)\nFORMULA: È³ â€“ the mean value of a sample\nMODEL\nthe sum of square(due to regression):\nFORMULA: SSr = np.sum((Yhat - Ymean)**2)\nRESIDUAL\nthe sum of square(due to error):\nFORMULA: SSe = np.sum((Y - Yhat)**2)\nTOTAL\nthe the sum of square(total):\nFORMULA: SSt = np.sum((Y - Ymean)**2)\nR2\n-determine how good the model is by computing the r-squared\n-perfect value of r2 = 1 means the perfect linear relationship.\n-when the value decreases it means weaker relationship of the observations\n(x is explaining less of y)\n-r2 = 0 means no linear relationship\nFORMULA: R2 =SSR/SST = 1 âˆ’ SSE/SST\nnumber of obs, n\nFORMULA: n = Y.size\n\neverytime we include a new variable the r2 is increased, we keep adding\nand the r2 keeps increasing, this is greatsince the model will get better.\nbut when we look at the adj r2 if wesee the value of adj r2 decreases when\nn variable added, say its now 5 variables, this indicates that we had the best\nsituation when we had 4 variables not when 5 variables\n\n-The model is multiple because we have k > 1 predictors.\n-If k = 1, we have a simple linear regression model\np-value, p = number of explanatory (X variables)\nAdj R2\n-tutorial: https://www.youtube.com/watch?v=4otEcA3gjLk\n\nas k increases adj r2 will tend to decrease(holding everything else constant),\nreflecting the reduced power in the model when you have low numbers of df.\nif we add useful variables to the model, adj will also increase, nut if we\nare adding use less variables then the adj r2 will decreases to reflect the\nfact that you have lost degrees of freedom.\nthe value is not bounded to 0 and 1, it can be negative\nthe higher the value of adj r2, better the model is in terms of explanatory\npower.\n0 â‰¤ R2 â‰¤ 1\n-Large R2 values do not necessarily imply a good model\n\nFORMULA: adjr2 = 1 - (1 - r2) * ((n - 1) / (n - p - 1))\nDEGREES OF FREEDOM (MODEL/ REGRESSION)\nRegression df is the number of independent variables in our regression model.\nFORMULA: DFr = p\nDEGREES OF FREEDOM (RESIDUAL/ ERROR)\nFORMULA: DFe = n - p - 1\nDEGREES OF FREEDOM (TOTAL)\ntutorial:\nhttps://www.youtube.com/watch?v=4otEcA3gjLk&list=PLTNMv857s9WUI1Nz4SssXDKAELESXz-bi&index=2\nhttps://www.youtube.com/watch?v=-4aiKmPC994\nhttps://www.youtube.com/watch?v=VIlVWeUQ0vs\n-when, number of obs, n = 10,\nnumber of variables, k = 7\nthen df = 10 - 7 - 1 = 2, that's not much at all.. it's not a very healthy regression\nwe need quite a few degrees of freedom to actually be able to explain anything to get\nto allow the model to have error to see whether the two or three or four variables are\nrelated to each other.\n-for plane 3 obs/var needed. so in this case for n obs, df = n - 3\nFORMULA: dfT = dfR + dfE\nor, DFt = n - 1\nMean Absolute Error, or L1 loss, MAE:\nFORMULA: mae = np.sum(np.absolute(Yhat - Y))\nRegression Mean Square, MSr\nFORMULA: Regression MS = âˆ‘ (Å· â€” Ó¯)Â²/Reg. df\nMSr = np.sum(((Yhat - Ymean)**2)/DFr)\nResidual Mean Square, MSe\nFORMULA: Residual MS = âˆ‘ (y â€” Å·)Â²/Res. df\nMSe = np.sum(((Y - Yhat)**2)/DFr)\nTotal Mean Square\nFORMULA: MSt = MSr + MSe\nMSE â€” Mean Squared Error for cost function\nMean Squared Errors (MS) â€” are the mean of the sum of squares or the sum of squares\ndivided by the degrees of freedom for both, regression and residuals.\nFORMULA: mse = np.sum((Yhat - Y)**2) / n\nF â€” is used to test the hypothesis that the slope of the independent variable is zero.\nMathematically, it can also be calculated as\nFORMULA: F = MSr / MSe\nSTANDERD ERROR â€” provides the estimated standard deviation of the distribution of\ncoefficients. It is the amount by which the coefficient varies across different cases.\nA coefficient much greater than its standard error implies a probability that the\ncoefficient is not 0.\nFORMULA: Std. Error = âˆš(Res.MS)\nSTDe = math.sqrt(MSe)\n\nt-Stat â€” is the t-statistic or t-value of the test and its value is equal to the\ncoefficient divided by the standard error.\nthe larger the coefficient with respect to the standard error, the larger the\nt-Stat is and higher the probability that the coefficient is away from 0.\n\nFORMULA: t-Stat = Coefficients/Standard Error\nt = p/STDe\n""], 'url_profile': 'https://github.com/nabilatajrin', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Apr 30, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Python', 'Updated Apr 17, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Boston-house_price-linear-regression\nThis is a linear regression model that trains the model to predict possible house prices based on the features of each dataset. The dataset used here is boston dataset which is loaded in scikit library.\nExplaination of the code:\nFirst we import the required libraries which will be used to train the dataset next we load our dataset from scikit library.\nNext we store all the feature in df_x variable and the price in the variable df_y.\nUsing the train_test_split we divide our training and testing data for the model I have used 0.33 of the data for testing rest is used to train the model.\nFit the training data and then store the predictions on test data in another variable.\nTo find the accuracy we can import mean squared error in scikit learn, I have obtained an error of 20.72 in this model.\n'], 'url_profile': 'https://github.com/phoenixcool12', 'info_list': ['Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['Stacked-auto-encoder-fnn-regression\nA deep regression model and hyperspectral imaging for rapid detection of nitrogen concentration in potato leaf, using Stacked auto-encoders (SAE) and fully-connected networks (FNN).\n'], 'url_profile': 'https://github.com/sepidehhosseinzadeh', 'info_list': ['Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Simple-Linear-Regression-Dummy-Data\nThis project created to gain experience using Linear Regression for Machine Learning. We used dummy data training and test.\n'], 'url_profile': 'https://github.com/yperdana', 'info_list': ['Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['linear_regression_predict_house_pricing\nWe have expand our training from dummy data into real data. we use price house in California which based on 1990 census data from California. on this training we generate hypotesis based on raw value correlate with the label.  The hypotesis Are: 1. Median house value will increase the median income.\n'], 'url_profile': 'https://github.com/yperdana', 'info_list': ['Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Regression-Analysis-on-Houseprices-Dataset\nThe original dataset was obtained from Kaggle\n'], 'url_profile': 'https://github.com/ssembahen', 'info_list': ['Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Vancouver, BC', 'stats_list': [], 'contributions': '163 contributions\n        in the last year', 'description': [""Leads Conversion - Logistic Regression Model Case\n\nIn order to help eCommerce organizations respond and identify the most potential leads, known as the 'hot leads', which convert to profitable customers, this regression model analyzes customer actions and uses data to determine which class each unit will fall into - Converted or Not Converted. This data is based from X Education's marketing dataset, which stores information on over 9000 data points and track their actions throughout the sales cycle. The metrics tracked which determine lead potential include: Total Time Spent on Website, Total Visits, Source of Lead, Page Views, and Email & Call Preference.\nBy utilizing this logictics regression model, the company and sales team will be able to focus more marketing spend on those 'hot leads' and increase their return on investment ratio.\nFor more information on the data sources used, see the Leads Dataset. More information on the data dictionary can be found in the data dictionary section below and under the Leads Data Dictionary folder.\nCompany Background\nThis Leads Dataset is taken from the marketing data of X Education - an online education company that sells courses to industry professionals. X Education provides marketing for their online courses across several websites and search engines. Users who land on the website can browse through the courses, watch videos, or fill out a form with their information, therefore becoming leads. Once a lead is identified, they are then contacted by the sales team via email or phone. However, the company has recognized that only 1/3 of leads are converted into customers, and marketing is distributed evenly across all leads. In order to make the process more efficient, X Education wants a way to identify leads which have the most potential, utilizing the given data. This will be done by creating a logistics regression model to analyze customer actions and determine if they will convert or not convert.\nDescription\n\nThe Marketing Leads case data includes the following content.\n\n\n\n\nDescription\n\n\n\n\nSources\nThe data source is taken from X Education leads with over 9000 data points. For a full description of the dataset sources see Leads Dataset.\n\n\nMeasures\nConverted or Not Converted.\n\n\nCoverage and Granularity\nThis dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not.\n\n\nReleased\n2020-04-10\n\n\n\nFurther Information\n\nSupport\nFor additional support please send as a direct message.\nFeedback\nPlease send feedback as a direct message.\nOther Resources\nFor other resources within the dataset please see Leads dataset.\nData Dictionary\n\n\n\n\nVariables\nDescription\n\n\n\n\nProspect ID\nA unique ID with which the customer is identified.\n\n\nLead Number\nA lead number assigned to each lead procured.\n\n\nLead Origin\nThe origin identifier with which the customer was identified to be a lead. Includes API, Landing Page Submission, etc.\n\n\nLead Source\nThe source of the lead. Includes Google, Organic Search, Olark Chat, etc.\n\n\nDo Not Email\nAn indicator variable selected by the customer wherein they select whether of not they want to be emailed about the course or not.\n\n\nDo Not Call\nAn indicator variable selected by the customer wherein they select whether of not they want to be called about the course or not.\n\n\nConverted\nThe target variable. Indicates whether a lead has been successfully converted or not.\n\n\nTotalVisits\nThe total number of visits made by the customer on the website.\n\n\nTotal Time Spent on Website\nThe total time spent by the customer on the website.\n\n\nPage Views Per Visit\nAverage number of pages on the website viewed during the visits.\n\n\nLast Activity\nLast activity performed by the customer. Includes Email Opened, Olark Chat Conversation, etc.\n\n\nCountry\nThe country of the customer.\n\n\nSpecialization\nThe industry domain in which the customer worked before. Includes the level 'Select Specialization' which means the customer had not selected this option while filling the form.\n\n\nHow did you hear about X Education\nThe source from which the customer heard about X Education.\n\n\nWhat is your current occupation\nIndicates whether the customer is a student, umemployed or employed.\n\n\nWhat matters most to you in choosing this course\nAn option selected by the customer indicating what is their main motto behind doing this course.\n\n\nSearch\nIndicating whether the customer had seen the ad in any of the listed items.\n\n\nMagazine\nIndicating whether the customer had seen the ad in any of the listed items.\n\n\nNewspaper Article\nIndicating whether the customer had seen the ad in any of the listed items.\n\n\nX Education Forums\nIndicating whether the customer had seen the ad in any of the listed items.\n\n\nNewspaper\nIndicating whether the customer had seen the ad in any of the listed items.\n\n\nDigital Advertisement\nIndicating whether the customer had seen the ad in any of the listed items.\n\n\nThrough Recommendations\nIndicates whether the customer came in through recommendations.\n\n\nReceive More Updates About Our Courses\nIndicates whether the customer chose to receive more updates about the courses.\n\n\nTags\nTags assigned to customers indicating the current status of the lead.\n\n\nLead Quality\nIndicates the quality of lead based on the data and intuition the the employee who has been assigned to the lead.\n\n\nUpdate me on Supply Chain Content\nIndicates whether the customer wants updates on the Supply Chain Content.\n\n\nGet updates on DM Content\nIndicates whether the customer wants updates on the DM Content.\n\n\nLead Profile\nA lead level assigned to each customer based on their profile.\n\n\nCity\nThe city of the customer.\n\n\nAsymmetrique Activity Index\nAn index and score assigned to each customer based on their activity and their profile.\n\n\nAsymmetrique Profile Index\nAn index and score assigned to each customer based on their activity and their profile.\n\n\nAsymmetrique Activity Score\nAn index and score assigned to each customer based on their activity and their profile.\n\n\nAsymmetrique Profile Score\nAn index and score assigned to each customer based on their activity and their profile.\n\n\nI agree to pay the amount through cheque\nIndicates whether the customer has agreed to pay the amount through cheque or not.\n\n\na free copy of Mastering The Interview\nIndicates whether the customer wants a free copy of 'Mastering the Interview' or not.\n\n\nLast Notable Activity\nThe last notable acitivity performed by the student.\n\n\n\n""], 'url_profile': 'https://github.com/EvelinPaulik', 'info_list': ['Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JasonHymer', 'info_list': ['Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Simple-linear-regression-without-sklearn\n'], 'url_profile': 'https://github.com/White-Devil26', 'info_list': ['Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '149 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kishan9192', 'info_list': ['Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'himmatnagar gujarat india', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Linear_Regression-Randomly-created-dataset-\nPeace â˜®\n'], 'url_profile': 'https://github.com/jshahj', 'info_list': ['Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Sep 20, 2020', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mcastrov78', 'info_list': ['R', 'Updated Mar 27, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2019']}","{'location': 'Ä°zmir', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': [""Turkey_COVID19_Polynomial_Regression\nIn this project i analyse the increasing on how many people is going to be affect.\nThis codes are perfectly matching for Polynomial Regression example\nDon't hesitate to ask anything.\n""], 'url_profile': 'https://github.com/ugeure', 'info_list': ['R', 'Updated Mar 27, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2019']}","{'location': 'Stony Brook, New York', 'stats_list': [], 'contributions': '311 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rohitrawat13396', 'info_list': ['R', 'Updated Mar 27, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2019']}","{'location': 'Charlotte', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Google-Play-Store-App-Analysis\nUsed XL Miner to perform analysis on the Google play store apps dataset from Kaggle. Models like Multiple Linear Regression, Regression Tree, Classification Tree etc. are used for the analysis.\n'], 'url_profile': 'https://github.com/dedeepya42', 'info_list': ['R', 'Updated Mar 27, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2019']}","{'location': 'Germany', 'stats_list': [], 'contributions': '400 contributions\n        in the last year', 'description': ['boed-pytorch\n'], 'url_profile': 'https://github.com/kosmitive', 'info_list': ['R', 'Updated Mar 27, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2019']}","{'location': 'USA', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Spending-Data---Prediction\nBuild numerous predictive models like Linear Regression, Lasso, Ridge, SVR, KNN, Random Forest, Regression Tree, Ada Boost, Gradient Boost, XGboost, Neural Network and compared their results to find the best model.\n'], 'url_profile': 'https://github.com/anunay1992', 'info_list': ['R', 'Updated Mar 27, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps â€œdaysâ€ uniquely to â€œnumber of salesâ€, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Mar 27, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps â€œdaysâ€ uniquely to â€œnumber of salesâ€, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Mar 27, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Mar 27, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Logistic-regression-to-predict-heart-disease\nUsing Logistic regression (sigmoid function) and feature scaling for data in predicting overall risks of heart disease\n  Author: Dung Tuan Le\n  University of Rochester\n  Created: June 22, 2019\n\nUsing Python in Jupyter Notebook, numpy and pandas libraries to implement a logistic regression machine learning model to pretect the overall risks of heart disease.\nDataset taken from Kaggle.\n'], 'url_profile': 'https://github.com/MariaChristinaKalogera', 'info_list': ['R', 'Updated Mar 27, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Updated Mar 23, 2020', '1', 'Python', 'MIT license', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jun 22, 2019']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'Ghaziabad, India', 'stats_list': [], 'contributions': '129 contributions\n        in the last year', 'description': ['House-Prices-Advanced-Regression-Techniques-Kaggle-\n'], 'url_profile': 'https://github.com/ayush52056', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mikewilliams3', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jashwanth63', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazonâ€™s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'Karachi, Pakistan', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['House-Price-Regression-With-EDA\nDataset: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/UmerTariq1', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '448 contributions\n        in the last year', 'description': ['Logistic-Regression-with-Python-and-Numpy\nDeep Learning Fundamentals: Logistic Regression\nIn this hands on project, we will perform the following tasks:\nIntroduction\nHyperparameters\nDataset\nMini Batch of Examples\nCreate Model\nForward Pass\nBackward Pass\nUpdate Parameters\nCheck Model Performance\nTraining Loop\nTraining the Model and Results\nAdditional Example\n'], 'url_profile': 'https://github.com/cipheraxat', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'Dallas, Texas', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashokpanicker', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Logistic-regression-to-predict-status-of-flight\nPredicting the status of flight depending on various featuers\n'], 'url_profile': 'https://github.com/kshitijbaloothiya', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 19, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 25, 2020']}"
"{'location': 'Dallas, Texas', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashokpanicker', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazonâ€™s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Gradient-descent-algo-for-linear-regression\nThis MAT-LAB code imports a data-set and computes the optimum value for the coefficients of linear regression.\n'], 'url_profile': 'https://github.com/fchristofrank', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Logistic-Regression-dengan-data-diabetes-Jantung\nPada suatu sore, saya dan Fadli meminum kopi di suatu coffe shop . Fadil adalah teman saya. Dia adalah seorang dokter.\nFadil bercerita bahwa mempunyai data tentang studi kasus diabetes jantung. Data tersebut bercerita tentang variabel yang mempengaruhi penyakit diabets. Fadil meminta bantuan saya untuk menganalisa data tersebut, dan dibuatkan suatu marchine leaning agar nantinya Fadil bisa memprediksi keadaan pasien tersebut apakah pasien tersebut terkena penyakit diabets atau tidak   .Setelah saya lihat datanya , saya teringat tentang metode logistic Regression.\n(Sumber data dari Kaggle)\n'], 'url_profile': 'https://github.com/hanimustikaadi', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '183 contributions\n        in the last year', 'description': ['Student-marks-prediction-using-Linear-Regression\nUsing Machine Learning algorithm Linear Regression I have trained a model which predicts the final marks of the student based on the two test grades.\nThe model name \'lm\' can predict the marks of the student based on the two grades of the students. For the visualization Seaborn module has been used. However, the same could have done with Pandas visualization or Matplotlib.\nRegression Evaluation Metrics :\nHere are three common evaluation metrics for regression problems:\n\n\nMean Absolute Error (MAE) is the mean of the absolute value of the errors:\nMAE is the easiest to understand because it\'s the average error.\n\n\nMean Squared Error (MSE) is the mean of the squared errors:\nMSE is more popular than MAE because MSE ""punishes"" larger errors, which tends to be useful in the real world.\n\n\nRoot Mean Squared Error (RMSE) is the square root of the mean of the squared errors:\nRMSE is even more popular than MSE because RMSE is interpretable in the ""y"" units.\n\n\nAll of these are loss functions because we want to minimize them.\nInspiration : Tim Ruscica\n'], 'url_profile': 'https://github.com/abhi7585', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression-in-R-with-transformation\nWe are predicting charges based on various parameters\nHere we also experienced overfitting problem which was tackled by transformation technique\n'], 'url_profile': 'https://github.com/anujjohri', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'Sri Lanka', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PasindaBimsara', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Coef Lab\nThis repo can be used to help clarify misconceptions that students have on details of running a linear regression model.\nRight now the repo has information on multicollinearity, standardising predictors, and comparing sets of beta coefs.\nFuture versions of this should include the entire process and why people choose each step.\n\nYouTube\n\nLearning Objectives\nBy the end of this lesson, students should be able to...\n\n Interpret a corr() plot for multi collinearity\n Transform continuous variables\n Talk about advantages/disadvantages of standardised/unstandardised betas\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'Melbourne', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['Depth-to-basement-Regression-Mining-Exploration\nThe purpose of this project is to build a machine learning regression model to predict the depth to the Iron Ore Copper Gold (IOCG) deposit basements in the South Australia. Geophysics is crucial in understanding what is underneath the ground as as different rocks have different gravity and magnetic sognatures. Hence, Geophysical predictors sourced from the SARIG, the SA government website for geology data (\u200bhttps://map.sarig.sa.gov.au/\u200b) are used along with the location and depth of historical drillholes to build this regression model.\nDataset\n\nAll the predictors are aggregated over different kilometer ranges and different statistical functions so it had an impact on the dimesionality of the data. The dataset used for this project consists of 559 variables and 1777 observations. Therefore, implementing proper feature reduction method is an important aspect of this project to avoid overfitting.\nApproach\nMy approach to this project is breifly explained in chronological order below:\n\nData cleaning and exploration: Removing the outliers and preparing the dataset for analysis by coverting categorical variables to continuous variables. Understanding the data and finding collinearities within features.\nFeature reduction: The features that ar highly correlated or add no value to predicting the target variable are removed. Recursive feature elimination method with cross-validation (RFECV) and Pearson correlation are used to perform feature reduction.\nModelling using Grid Search cross validation for parameter optimisation: Random forest regressor is used to fit a regression line to predict the target variable with the selected features. Grid Search cross validation is used to find the optimal set of parameters for the model.\nUnderstanding the model: SHAP values are used to observe the importance of each feature in the model.\nFinal Model: 5 most important features in the model are used to build a new optimized and efficient model.\n\nKindly view the jupyter notebook for in-depth explanation of each the above mentioned tasks.\n'], 'url_profile': 'https://github.com/ShashankSharma26', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', '1', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['GRIP-TSF-Task1-LinearRegression\nA data science project involving EDA and Linear Regression model for predicting the Percentage score of a student based on the hours studied.\n'], 'url_profile': 'https://github.com/mauwazahmed', 'info_list': ['Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2021', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 31, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '204 contributions\n        in the last year', 'description': ['Package-Pricing-At-Mission-Hospital\nBuilt Predictive Model to estimate the price of treatment given the clinical factors at the time of admission.\nTABLE OF CONTENTS\n\nObjective\nConceptual-Model\nData-Preparation\nStatistical-Tests\nModel\nReferences\n\nOBJECTIVE\nEstimate the price of treatment given the clinical factors at the time of admission\nCONCEPTUAL-MODEL\n\nData Preparation\nRemoved the following columns with insufficient observations as ""1""\n\nCAD SVD\nCAD VSD\nOther general\nConfused state\n\nNumerical values were bucketed and One hot encoded for the following variables,\n\nAge Categories - <10, 11 25, 26 50, 50 and above (as per domain expertise)\nBP Ranges - Low, Normal, High, Critical (as per the medical charts)\nBMI - Underweight, Normal, Overweight, Obese (as per the medical charts)\nHemoglobin ""normal"": Female 12 to 15.5, Men 13 to 17.5, any value outside these limits will be ""abnormal""\nUrea ""normal"": 7 to 20 mg/dl any value outside these limits will be ""abrnormal""\n\nRemoving Variables based on avalability of data\n\nRemoved the variable - ""Stay at hospital ICU and Ward"".It is a reflective construct variable. Its not possible to predict the length of stay, accurately at the time of admission to compute total treatment cost.\n\nThe below graphs illustrates on the correlation between target variable and Hospital Admission - both at Intensive Care Unit and Normal care unit\n\n\nConsidered ln(Total Cost) instead of Total Cost to Hospital(INR). The total cost value variable has a right skew. Taking the log would make the distribution of our transformed variable appear more symmetric.\n\nFrom the below graphs we see that the data is normalized by taking the logarthmic value of the variable,\n\nHandling NULL Values\n\nBP Ranges Imputed \'Normal\' BP range for null values which were Juvenile Patients\nUrea Imputed \'Normal\' Urea level for 11 null values. Assumption: Urea measurement is not critical for that patient.\n\nStatistical-Tests\nT-Test and Anova was performed on specific variable to understand their effect on target variable. Code is available here\nT test was performed on variables with 2 categories. The following variables were removed as they proved insignificant,\n\nother-heart\nother-nervous\nother-tetralogy\nDiabetes\nHypertension\nHemoglobin\n\nANOVA test was done to test for variables with more than two levels and the following variables proved significant,\n\nAge\nBMI\nBP Ranges\n\nNew categorical variables were derived flagging subjects with multiple health issues. The following are the conditions that were hypothesized based on domain expertise, and T-test was performed\n\nModel\nClick here for the code\n\nLogistic Regression Model to understand the relation between target variable and body weight as the predictor variable, (this use case was based on the domain expert\'s requirement)\n\n\n\nLogistic Regression Model to understand the relation between target variable and feature engineered variables,\n\n\nReferences\n\n\nhttp ://www.bloodpressureuk.org/BloodPressureandyou/ Bloodpressurechart\n\n\nhttps ://www.mayoclinic.org/tests procedures/blood urea nitrogen/about/ pac 20384821\n\n\nhttps ://www.quora.com/Why do we log variables in regression model\n\n\n'], 'url_profile': 'https://github.com/abhilashhn1993', 'info_list': ['Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2021', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 31, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Data-Science-Project-\nData science project  - preprocessing research data and using machine learning (linear regression) in Jupyter Notebook . Integrated in  MySQL and Tableau Public\n""Absenteeism_data"" is a research on work absenteeism. It has been preprocessed and modeled in respectively \'Preprocessing.ipynb\' and \'Modelling.ipynb\' (Jupyter Notebook). Both files are merged in Module.py.\nIn \'Integration.ipynb\' with the Module.py , the \'mode\'l and  the \'scaler\' made in \'Modelling.ipynb\' we connect  with MYSQL database from where we are getting our new .csv file and integrating it in tableau for visual representation.\nHow to use the model with your new data :\n1.Collect your data in .csv file with the same information as the data in ""Absenteeism_data.csv"". Note : DO NOT change the names and the number of columns.\n2.You dont need to run the \'Preprocessing.ipynb\' and \'Modelling.ipyn\'. Everything important is stored in \'Module.py\' , \'model\' and \'scaler\'.\n3.Run \'Integration.ipynb\' with \'Module.py\' , \'model\' and \'scaler\' in the same folder. NOTE : In the \'Integration.ipynb\' code there is a line where you write your MYSQL username and password. Change it with your information , which you can find in the main window of the MYSQL Workbench.\n4.From the MYSQL Workbench you can download the new .csv file and upload it in tableau for visual representation. I have uploaded an image of probability for absenteeism in regards of transportation expense and children.\n'], 'url_profile': 'https://github.com/PavelStanoev', 'info_list': ['Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2021', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 31, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'Waco, TX', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Iris\nIn this simple project, I have trained the model using Logistic Regression and Random Classifier to obtain the perfect accuracy.\n'], 'url_profile': 'https://github.com/Sudeep0', 'info_list': ['Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2021', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 31, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '261 contributions\n        in the last year', 'description': ['Titanic Passengers Survival Prediction\n\n\n\nFeature name\nFeature description\n\n\n\n\nSurvived\nSurvived (1) or died (0)\n\n\nPclass\nPassengerâ€™s class\n\n\nName\nPassengerâ€™s name\n\n\nSex\nPassengerâ€™s sex\n\n\nAge\nPassengerâ€™s age\n\n\nSibSp\nNumber of siblings/spouses aboard\n\n\nParch\nNumber of parents/children aboard\n\n\nTicket\nTicket number\n\n\nFare\nFare\n\n\nCabin\nCabin\n\n\nEmbarked\nPort of embarkation\n\n\n\n'], 'url_profile': 'https://github.com/davisraimon', 'info_list': ['Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2021', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 31, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Yeakub-Ali', 'info_list': ['Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2021', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 31, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'Denmark', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SaraNunes', 'info_list': ['Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2021', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 31, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Chances-of-Admit-through-GREscore-using-Linear-regression-model\nHELLO, This is a Linear regression model which predict the chances of admission by your GRE score\n'], 'url_profile': 'https://github.com/Sbbarse787', 'info_list': ['Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2021', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 31, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2021', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 31, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 29, 2020']}","{'location': 'Corvallis, OR', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bolym', 'info_list': ['Jupyter Notebook', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2021', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 31, 2020', 'R', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Carlos-Montenegro', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Diabetes_Prediction\nExploratory Data Analysis on a given data set on Diabetes, and predicting diabetes amongst people using self built\nLogistic Regression Model.\n'], 'url_profile': 'https://github.com/dhruvs2', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Neural-Network-Task\nThis repository is created to store and save my works on Neural Network ( both regression and classification)\n'], 'url_profile': 'https://github.com/Lokehigbe', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AdarshSai', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Stock-Market\n'], 'url_profile': 'https://github.com/samuelvarkey', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': [""Medical-Insurance-Machine-Learning\nUsing linear regression, classification, and decision tree to build models based on Kaggle's medical insurance dataset.\n""], 'url_profile': 'https://github.com/liyongh1', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': [""NFL-Team-Wins-Proj2\nLinear regression model designed to predict an NFL team's next year win total. Also includes EDA notebooks and web scrape scripts. This was my second project but first personal project during the Metis Data Science Bootcamp.\nThe main files here are:\n\n\nbest_scape_nfl.ipynb\nThis notebook contains all of the scripts and processes that I wrote in order to web-scrape several years of NFL team based statistics from various free online resources.\n\n\nNFL_EDA_1.ipynb\nBrief exploratory data analysis notebook used to analyze correlations, trends and the features I engineered like net-points.\n\n\nFINAL_MODEL.ipynb\nAs the file name suggests, this notebook contains the code for training and testing various linear regression models and includes the final model I used as well as the final metric (r squared and MAE) scores.\n\n\nNFL Presentaion.pdf\nThese are the project slides I presented to my peers at Metis.\n\n\nThe remaining files with 'prelim' in the filename are scratchpad notebooks I used along the way.\n""], 'url_profile': 'https://github.com/samad4ms', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['predict-chances-of-college-admission\n'], 'url_profile': 'https://github.com/palashmoon', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/karthickganesan', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}","{'location': 'France', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['New York Taxi Demand Forecast\nThe objective is to forecast the demand for taxis by predicting the number of passengers in the next hour.\nAcknowledgements\nThe data was originally published by the NYC Taxi and Limousine Commission (TLC) and made publicly available\nvia NYC Open Data. The weather data was published on kaggle datasets:\nHistorical Hourly Weather Data 2012-2017.\nData\nThe data is hourly NYC Yellow Cab trip record data from January 2016 to December 2017. The data was\noriginally sampled on a per-trip basis and aggregated to hourly intervals.\nThe dataset is not cleaned and can contain invalid data points or outliers.\nAttributes\n\ntimestamp: unix seconds\npassenger_count (label):\nThe number of passengers in the vehicle (driver entered value), summed up over 1 hour.\ntrip_distance:\nThe elapsed trip distance in miles reported by the taximeter, summed up over 1 hour.\ntotal_amount:\nThe total amount charged to passengers. Does not include cash tips, summed up over 1 hour.\nhumidity: Relative humidity.\npressure: Air pressure.\ntemperature: Temperature in Kelvin.\nweather_description: Description of the weather situation.\nwind_direction: Wind direction measured in degrees clockwise.\nwind_speed: Wind speed in m/s.\n\n'], 'url_profile': 'https://github.com/PrathamSolanki', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', '2', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '165 contributions\n        in the last year', 'description': ['William Austin\nPrakash Dhimal\nGeorge Mason University\nCS 584 Theory and Applications of Data Mining\nSemester project: Predicting the Impact of COVID-19\n\nPredicting the Impact of COVID-19\nWhy Investigate COVID-19?\n\nCOVID-19 is a now a major global pandemic and is having a major impact on peopleâ€™s lives around the globe.\nThere is significant uncertainty about what the effects the pandemic will be going forward.\nLarge amounts of data have been collected as the spread of Covid-19 has progressed. Using data mining techniques to attempt to predict likely outcomes will support the task of building an appropriate and effective response strategy.\n\nPrimary Project Goal:\nFor this project, we will construct a model showing how cases of COVID-19 will spread around the globe. Our main focus will be on accurately predicting the number of infections, fatalities, and recoveries we should expect, along with what the time frame is for these events to unfold.\nCOVID-19 Data Sources\nPrimary Data Source:\nOur primary data source for mapping the global spread of COVID-19 will be the data provided by the Johns Hopkins Center for Systems Science and Engineering (CSSE). This contains day-by-day time series data, broken down by country and region.\n\nSource: https://github.com/CSSEGISandData/COVID-19\nKaggle Page: https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset\nVisualization: https://coronavirus.jhu.edu/map.html\n\nProject source code:\nThe source code for this project is located in the src directory. Jupyter notebooks are in the src/jupyter-notebook directory.\nProject report\nPlease refer to: report/report.pdf for the project report\nDependency:\nThis python program depends on the following modules:\n\ndatetime\nmath\nmatplotlib\nnumpy\nos\npandas\nscipy\nsklearn\nscipy\n\n\nTo run the SIR model and generate COVID-19 forecast plots, run:\n\nsrc/AnalyzeTransmissionRates.py\nFor validation work, run:\nsrc/PredictFutureBetaValues.py\n\nContributors:\nPrakash Dhimal\nWilliam Austin\nGeorge Mason University\n'], 'url_profile': 'https://github.com/pdhimal1', 'info_list': ['Jupyter Notebook', 'Updated May 20, 2020', 'Updated Mar 24, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': [""Projet de SÃ©ries Temporelles, printemps 2020\nContexte\nProjet de SÃ©ries Temporelles au second semestre de l'annÃ©e de M1 Ã  l'ENSAE Paris. En collaboration avec FYacine.\n""], 'url_profile': 'https://github.com/AesonF', 'info_list': ['Jupyter Notebook', 'Updated May 20, 2020', 'Updated Mar 24, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 24, 2020']}","{'location': 'Silicon Valley, CA', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JoaquinRV', 'info_list': ['Jupyter Notebook', 'Updated May 20, 2020', 'Updated Mar 24, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) â€“ mean(x*y)) / ( mean (x)^2 â€“ mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 20, 2020', 'Updated Mar 24, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 20, 2020', 'Updated Mar 24, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Qâ€“Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 20, 2020', 'Updated Mar 24, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 20, 2020', 'Updated Mar 24, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 20, 2020', 'Updated Mar 24, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 20, 2020', 'Updated Mar 24, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 24, 2020']}","{'location': 'Silicon Valley, CA', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Qwiklabs-Building-a-Regression-Model-in-AI-Platform-Notebooks\nThis is first lab assignment in Course #1 - Introduction to Trading, Machine Learning & GCP\nunder Machine Learning for Trading Specialization by Google Cloud & New York Institute of Finance.\nThe purposes of this lab assignment are the following:\n\nLoad data from BigQuery into a Pandas DataFrame\nBuild a linear regression model in Scikit-Learn\nUse AI Platform Notebooks\nUse Python3 as the Kernel\n\n'], 'url_profile': 'https://github.com/JoaquinRV', 'info_list': ['Jupyter Notebook', 'Updated May 20, 2020', 'Updated Mar 24, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['SGEMM GPU Kernel Performance using Python\nWe will be using the SGEMM GPU kernel performance Data Set available for download at https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance\nTasks Performed-\nPart 1: Download the dataset and partition it randomly into train and test set using a good train/test split percentage.\nPart 2: Design a linear regression model to model the average GPU run time. Include your regression model equation in the report.\nPart 3: Implement the gradient descent algorithm with batch update rule. Use the same cost function as in the class (sum of squared error). Report your initial parameter values.\nPart 4: Convert this problem into a binary classification problem. The target variable should have two categories. Implement logistic regression to carry out classification on this data set. Report accuracy/error metrics for train and test sets.\nI have experimented with  different values and provided answers to the following:\nExperiment with various parameters for linear and logistic regression (e.g. learning rate âˆ) and report on your findings as how the error/accuracy varies for train and test sets with varying these parameters. Plot the results. Report the best values of the parameters.\nExperiment with various thresholds for convergence for linear and logistic regression. Plot error results for train and test sets as a function of threshold and describe how varying the threshold affects error. Pick your best threshold and plot train and test error (in one figure) as a function of number of gradient descent iterations.\nPick eight features randomly and retrain your models only on these ten features. Compare train and test error results for the case of using your original set of features (14) and eight random features. Report the ten randomly selected features.\nNow pick eight features that you think are best suited to predict the output, and retrain your models using these ten features. Compare to the case of using your original set of features and to the random features case. Did your choice of features provide better results than picking random features? Why? Did your choice of features provide better results than using all features? Why?\n'], 'url_profile': 'https://github.com/manikandan1408', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K â€“ 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and Whiteâ€™s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. Itâ€™s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200aâ€”\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200aâ€”\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. Itâ€™s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200aâ€”\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200aâ€”\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Data-Pre-Processing-Multivariate-Regression-Using-10-steps\nWe are using 50 Startup data set and handel data Pre-Processing using 10 Steps\n'], 'url_profile': 'https://github.com/Soumya2050', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 25, 2020']}","{'location': 'Dublin, Ireland', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['Lasso-Regression-For-Feature-Selection-Boston-Housing-Prices-\nLasso Regression:\nUsing the lasso regression, we can obtain the important features of the dataset, that minimizes prediction error for a quantitative response variable. The lasso does this by imposing a constraint on the model parameters that cause regression coefficients for some variables to shrink toward zero.\nBoston Dataset:\nThe sklearn Boston dataset is used wisely in regression and is famous dataset from the 1970â€™s. There are 506 instances and 14 attributes in the dataset.\nConclusion\nHouse prices also tend to be higher closer to the Charles River, and houses with more rooms are pricier.\n\n\nPriority\nAttributes\n\n\n1\nRM: average number of rooms per dwelling.\n\n\n2\n CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\n\n3\n ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n\n\n4\n NOX: nitric oxides concentration (parts per 10 million).\n\n\n5\n CRIM: per capita crime rate by town.\n\n\n'], 'url_profile': 'https://github.com/AntonyHelsonChandy', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '261 contributions\n        in the last year', 'description': ['Predict-Bike-Trip-Duration-with-a-Regression-Model-in-BQML\nhttps://googlecoursera.qwiklabs.com/focuses/46793\n'], 'url_profile': 'https://github.com/fergmack', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'BENGALURU', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gaurav1210', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Apr 8, 2020', 'HTML', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Apr 8, 2020', 'HTML', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Apr 8, 2020', 'HTML', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Apr 8, 2020', 'HTML', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Apr 8, 2020', 'HTML', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows Ã— 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows Ã— 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows Ã— 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Apr 8, 2020', 'HTML', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Apr 8, 2020', 'HTML', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Boston', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['FIFA Player Assessment Model and Analytics\nAuthors - Akshit Jain, Naga Santhosh Kartheek Karnati, Praharsha Singaraju and Thomas Lindstrom-Vautrin\nFIFA Player Assessment Model and Analytics, DS5220 Project.\nSports analytics is a field that is growing in popularity and application throughout the world. One\nof the open problems in this field is the assessment of football players based on their skills, physical\nattributes and market value. The primary aim of this project is to establish football player assessment\nmodels using machine learning techniques to support transfer decisions of football clubs. To do so, we\nwill be using publicly available datasets from Kaggle that contain playersâ€™ data for the Career Mode from\nFIFA 2015 to FIFA 2020, where each player record is characterised by 104 features. Some of the few\nimportant player features include, year, age, body type, work rate, value, skills (e.g. pace, shooting,\npassing), wage, traits, position, nationality, club, ratings, preferred foot and physical attributes. These\nfeatures will enable us to analyse the performance of players across seasons and build player assessment\nmodels\nBased on domain specific knowledge of the game, we now propose a few low and high risk\nhypotheses for exploratory analysis.\nLow risk hypotheses include:\n\nTall, short and strong players are statistically good at heading, dribbling and tackling respectively.\nPlayer wage and age are positively correlated upto the age of 31 and negatively correlated after that.\nThere exists a positive correlation between player rating and value.\n\nHigh risk hypotheses include:\n\nLeft footed players have a higher overall rating compared to right footed players.\nThe starting eleven (i.e. FIFA World 11) with the highest overall rating for a given year wins the champions league that year.\n\n Next, we propose a few specific goals to build player assessment models to tackle regression and\nclassification problems. \nThe following goals will be addressed using linear and nonlinear machine learning techniques:\n\nClassify nationality based on attributes like shooting, passing, dribbling, defending and pace\n(e.g. players from Spain are expected to be efficient passers).\nClassify player position using physical attributes like height, weight, age, strength, speed and jump\n(e.g. defenders are expected to be relatively taller and stronger than players in other positions).\nClassify work rate using defense and attack traits (e.g. strikers and wingers have high work rates for attack).\nBuild regression models to predict player rating and value based on player attributes\n(i.e. player value and rating assessment models).\n\n The goals above will be implemented using  advanced supervised learning techniques  like ridge\nand lasso regression, logistic regression, KNN, LDA, QDA, decision trees, random forests and additive\nmodels depending on the type of problem. The goals of the assessment models require us to make\nassumptions about the importance of features. Nevertheless,  important predictors will be carefully chosen\nusing feature selection techniques for each proposed question. Cross-validation will be used to determine\nthe model with the least error. Finally, based on the problem we will evaluate the performance of the\nrespective models using metrics like RMSE, R^2, confusion matrix and ROC curve.\nConcepts:\n\nSupervised Machine Learning\nRegression\nClassification\nCross-Validation\nFeature Selection Techniques\nModel Assessment\nLinear and Non Linear Models\nExploratory Data Analysis\n\nLangauge/Tools:\n\nR\nRStudio\n\n'], 'url_profile': 'https://github.com/kartheekkarnati30', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Apr 8, 2020', 'HTML', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Trondheim, Norway', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['StatisticalLearningSpring2019\nLearning material from my NTNU course on Statistical learning in the spring of 2019, TMA4268.\nThe full overview of all learning material in the course is found at https://www.math.ntnu.no/emner/TMA4268/2019v/TMA4268overview.html\nThe textbook for the course was James et al: Introduction to statistical learning, plus an extra module on artificial neural networks.\nThis repo contains my Rmd code for the modules that I have made (not chapters 6,7 and 10 of the ISLR).\nThere is a folder named ISLR where you find figures from the book used in the Rmd, and these are downloaded from http://faculty.marshall.usc.edu/gareth-james/ISL/data.html\nand are of cause not my work.\nThe lisence is CC-BY-SA-NC(except for figures not created by me)\nPlease create an issue if you find errors or inconsitencies.\n-Mette\n'], 'url_profile': 'https://github.com/mettelang', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Apr 8, 2020', 'HTML', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Pueblo, CO, USA', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['Machine-Learning-binary-classification\nComparing Logistic regression, Support vector machine, Decision tree, Random forest and Gradient Boosting models for binary classification ML problem.\nThis project is about modeling a binary classification ML algorithm.\nThis project uses 80/20 train test split with KFolds equals to 5 for cross validation.\nFirst step is the data processing.\nSecondly,\nFitting the data into Logistic regression, Support vector machine, Decision tree, Random forest and Gradient Boosting.\nStoring the F1-scores for train data and solver of each model.\nFinally, choosing the best model based on best test score.\n'], 'url_profile': 'https://github.com/sudhan-bhattarai', 'info_list': ['Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'R', 'Updated Apr 8, 2020', 'HTML', 'Updated Apr 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '251 contributions\n        in the last year', 'description': ['Soccer-Season-Prediction\nTo summarise, there are multiple Excel files where I have calculated the final season 1 league standings from the raw results, the stage at which the league winners secured the title and the ""biggest upset"" in the season 1 fixture results.\nI then used a Poisson Generalised Linear Model to predict goals scored in a certain fixture. This is because the goals scored in a 90 minute soccer match is approximately distrbuted by a Poisson distribution as we can see below. I fitted this model to season 1 results and then used it to predict season 2.\n\nStochastic League Table Simulation\nAfter performing these predictions on the second season fixtures I wanted to generate a stochastic simulation of potential league table outcomes. This was completed using a Poisson distribution where the rate parameter is the average home/away goals scored by each individual team based on season 1.\n\n'], 'url_profile': 'https://github.com/jackapbutler', 'info_list': ['R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Python', 'Updated Sep 4, 2020', 'R', 'Updated Apr 18, 2020', '1', 'R', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['predicting-used-car-price\nThis project trying to predict the price of the car based on everything on dataset. Data has been received from kaggle. we use simple linear regression for this problem.\nData Source:\nhttps://www.kaggle.com/orgesleka/used-cars-database/data\nWe had filtered the data such as:\n\nOnly SUV type\nOnly Mercedes-Bens\n\nThere are csv files above for all SUV cars. but in this case we only use mercedes-bens brand only. For Filtered data, we use this link:\nhttps://docs.google.com/spreadsheets/d/e/2PACX-1vTkbsvshuhRMeir109MvC_3QvbpJwBJkMN9R0KeCWehSzu74CoU29Nlf11klUkdHWfSoH81HHkbxtru/pub?output=csv\nWe also attach link to provide massive data collection after preprocessing which includes selecting features, encoding, and simple exploratory data analysis\nWe add jupyter notebook files so everyone could look the process in the future.\nData Preprocessing drive link below:\nhttps://drive.google.com/drive/folders/1JJ94VtMkEhF0cjiAPo6PKsG-dulX3zZ8?usp=sharing\n'], 'url_profile': 'https://github.com/yperdana', 'info_list': ['R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Python', 'Updated Sep 4, 2020', 'R', 'Updated Apr 18, 2020', '1', 'R', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020']}","{'location': 'Vandol, Panauti, Kavre, Nepal', 'stats_list': [], 'contributions': '299 contributions\n        in the last year', 'description': ['Coronavirus-Probability-Detector\nPython code for coronavirus probability detector using data analysis and training machine learning model of logistic regression.\nTools used:\n\nPycharm Community Edition 2019.3.1\n\nLanguages used:\n\nPython 3.8\n\nSteps to Install:\n\nOpen any file in IDE or Create Virtual Environment\nInstall libraries pickle and framework flask\nRun.\n\n'], 'url_profile': 'https://github.com/ArpanMahatra1999', 'info_list': ['R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Python', 'Updated Sep 4, 2020', 'R', 'Updated Apr 18, 2020', '1', 'R', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NBA-Machine-Learning\nUsing Machine Learning algorithms such as Tensor FLow, SVC, Logistic Regression, Random Forest, K-Folds, etc, to predict stats such as Plus/Minus, Player Efficieny Rating , etc\nPlus/Minus - The running effectiveness of a player when he is on the court (i.e how much better or worse the team is when the player is on the court)\n'], 'url_profile': 'https://github.com/mehtaparam2002', 'info_list': ['R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Python', 'Updated Sep 4, 2020', 'R', 'Updated Apr 18, 2020', '1', 'R', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020']}","{'location': 'Oslo', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Statistical Data Analysis (example) using R\nA short script in R to perform data mining in a dataset (Housing.csv), perform visualizations and build a linear regression model.\nSteps to follow before running the script:\n\nDownload R https://cran.r-project.org/bin/windows/\nDownload RStudio https://rstudio.com/products/rstudio/download/\nOpen RStudio\nFile/Open File choose the script file.\nNote: Here you may want to change the dataset path used in the beginning.\nInstall the following packages\n-dplyr\n-ggplot2\n-caTools\nRun the script\n\n\n\nggplot(compare_result, aes(x=actual, y=predicted))+geom_point(col=""purple"") + geom_smooth(method=""lm"", se = T)\n\n'], 'url_profile': 'https://github.com/indritkello', 'info_list': ['R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Python', 'Updated Sep 4, 2020', 'R', 'Updated Apr 18, 2020', '1', 'R', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '204 contributions\n        in the last year', 'description': ['Factors Causing Chronic Kidney Disease\nBuilt the relationship model between factors causing chronic kidney disease. Identified the individuals who could be affected.\nTABLE OF CONTENTS\n\nObjective\nConceptual Model\nVariable Selection\nData Exploration\nLogistic Regression Model\nResults\n\nOBJECTIVE\n\n\nTo  identify factors causing chronic kidney disease\n\n\nBuild a predictive model to identify subjects, who could potentially be affected by CKD.\n\n\nInterpret the statistical model to estimate the impact of each factor leading to CKD.\n\n\nNOTE: Data is restricted for public access\nCONCEPTUAL Model\n\nVARIABLE SELECTION\nBased on the conceptual model\n\n\nThe variables capturing details such as income, martial status, education, source of medical care and insurance details, were discarded.\n\n\nThe variables like BMI, waist size, weight and height were discarded, considering no direct impact with the target variable.\n\n\nTotal cholesterol, inheritance  of diabetes and hypertension from family were removed\n\n\nSTATISTICAL TESTS\nChi â€“ Square test to identify the relation between categorical variables and target (binary) variables.\nT-test to identify the relationship with target variables and the variable under consideration\n\nLOGISTIC REGRESSION\nBuilt the logistic regression model to understand the relation the between variables (predictors for the chronic kidney disorder) and the traget variable (weather an individual is affected or not)\n\n\nRESULTS\nFactors like AGE, RACE(hispanic), HDL levels, HYPERTENSION, DIABETES and CARDIO VASCULAR DISEASES proved to be the significant predictors for the target variable.\nCode file can be found here\n'], 'url_profile': 'https://github.com/abhilashhn1993', 'info_list': ['R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Python', 'Updated Sep 4, 2020', 'R', 'Updated Apr 18, 2020', '1', 'R', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020']}","{'location': 'Santa Clara, California', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harshtandon23', 'info_list': ['R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Python', 'Updated Sep 4, 2020', 'R', 'Updated Apr 18, 2020', '1', 'R', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Analyzing the Bank Marketing Dataset\n\n\n\n\n'], 'url_profile': 'https://github.com/Develop-Packt', 'info_list': ['R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Python', 'Updated Sep 4, 2020', 'R', 'Updated Apr 18, 2020', '1', 'R', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '346 contributions\n        in the last year', 'description': ['TargetedMarketingPredictionEngine\nThe classification goal is to predict whether the client will subscribe (1/0) to a term deposit (variable y) using Logistic regression.\n\n\n\nClassification Report:\nprecision\nrecall\nf1-score\nsupport\n\n\n0\n0.80\n0.89\n0.84\n7666\n\n\n1\n0.88\n0.78\n0.82\n7675\n\n\naccuracy\n\n\n0.84\n15341\n\n\nmacro avg\n0.84\n0.84\n0.83\n15341\n\n\nweighted avg\n0.84\n0.84\n0.83\n15341\n\n\nConfusion Matrix: \n[[6852,  814] \n[1715, 5960]]\nAccuracy Score: \n0.84\n\n\n\nROC Curve\n\n\n'], 'url_profile': 'https://github.com/ZeroNP', 'info_list': ['R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Python', 'Updated Sep 4, 2020', 'R', 'Updated Apr 18, 2020', '1', 'R', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020']}","{'location': 'Calgary, Alberta', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': [""Deerfoot Trail Analysis using Spark and Linear Regression (PySpark MLlib)\nProject Abstract\nThe Deerfoot Trail commute analysis project involves the calculation of commute time\nstatistics along with the prediction of commute times given a set number of inputs. This\nproject will also determine how accurate a machine learning model is in predicting commute\ntimes given these inputs. The analysis is interesting since it uses readily-available public data\nto provide predictions that could potentially benefit a large number of people. While the\nanalysis focuses on a fixed time period for one specific roadway, it could potentially be\nexpanded to predict commute times for major roadways across the country in real-time.\nBesides providing the project team with an opportunity to learn and apply Spark concepts,\nthe results of the project could have real-world applications in transportation forecasting,\nplanning, and safety.\nHow to run?\nThese are jupyter notebook files. You should have jupyter installed on your machine.\nENSF612_Spark_Project_1.ipynb requires\n'deerfoot.csv' file\nENSF612_Spark_Project_2.ipynb requires\n'deerfoot_part2-1.csv', 'eng-daily-01012013-12312013.csv' and 'eng-daily-01012014-12312014.csv' files.\n""], 'url_profile': 'https://github.com/vaibhav50596', 'info_list': ['R', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'Python', 'Updated Sep 4, 2020', 'R', 'Updated Apr 18, 2020', '1', 'R', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Python', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Discovering the Building Blocks of Neural Networks with PyTorch\n\n\n\n\n'], 'url_profile': 'https://github.com/Develop-Packt', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Jul 8, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 25, 2020', 'C++', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Mar 24, 2020', '1', 'Go', 'Updated May 15, 2020', '1', 'Python', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 11, 2020']}","{'location': 'Villeurbanne, France', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['ECML-PKDD-MLJ-2020\nCode for submission for the Journal Track MLJ 2020. Title: ""A ridge regression approach for fast bilinear similarity learning with theoretical guarantees""\n'], 'url_profile': 'https://github.com/sofiendhouib', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Jul 8, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 25, 2020', 'C++', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Mar 24, 2020', '1', 'Go', 'Updated May 15, 2020', '1', 'Python', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Python-visionlization-method\nVisionliazation method of normal distribution,probability density distribution,cumulative probability distribution,correlation and regression. It is suitable for 0-1 learner.\n'], 'url_profile': 'https://github.com/superjulia777', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Jul 8, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 25, 2020', 'C++', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Mar 24, 2020', '1', 'Go', 'Updated May 15, 2020', '1', 'Python', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 11, 2020']}","{'location': 'Vancouver, BC', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['AnnualFloodPrediction_MLR\nUsing interpolated weather data and simple multiple linear regression to predict annual floods in a snowmelt dominated area of British Columbia, Canada\n'], 'url_profile': 'https://github.com/nickyrong', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Jul 8, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 25, 2020', 'C++', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Mar 24, 2020', '1', 'Go', 'Updated May 15, 2020', '1', 'Python', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Jul 8, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 25, 2020', 'C++', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Mar 24, 2020', '1', 'Go', 'Updated May 15, 2020', '1', 'Python', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/arathyrose', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Jul 8, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 25, 2020', 'C++', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Mar 24, 2020', '1', 'Go', 'Updated May 15, 2020', '1', 'Python', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Soccer_Match_Outcomes_Project\nFinal project of Data Preparation and Analysis course.\nBuilt a prediction and a classification model in R to establish the outcome of a soccer match and compared both methodologies.\nApplied a Poisson Regression Model to obtain the number of scores and used a Random Forest Model to get the winning team.\n'], 'url_profile': 'https://github.com/sarainigo', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Jul 8, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 25, 2020', 'C++', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Mar 24, 2020', '1', 'Go', 'Updated May 15, 2020', '1', 'Python', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 11, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['\n\n\ngo-test-it\nAutomated regression testing package written in Go.This tool supports both REST API and GraphQL API. All you need to do is write json files with all the possible scenarios. examples are available in /tests directory.\nInstallation instructions are listed below.\nPre-Requsite\nRequired go(>1.11) installed on machine.\n\nif you are using MAC or Linux, make sure go/bin is added to PATH\n\nInstallation\ngo get github.com/kotanetes/go-test-it\nUsage\n\nCreate a JSON file similar to example file in /test/example.json in any directory\nCan\'t creating JSON files? Tool will create sample file and couple of test sceanrios\ngo-test-it init #generates json file with test scenarios\n\nNeed help?\n\ngo-test-it -help\n\nCreate a direcotry and add json file, which have all the test scenarios refer to file in /test/example.json\nBy Default, go-test-it will read all .json files in the current directory and run the tests\n go-test-it\n\nTool has the ability to run tests in a specific file.\ngo-test-it -file-path=./rest_services -file-name=service1.json\n\nIgnore Test Scenario\n\nAdd indicator ""ignore"":true to the test scenario and tool will skip the test scenario\n\n\n\nResults\n\n\nTool generates results as a HTMl Report,Pie Chart and also prints the results to console\n\n\n\nTool also generate the JSON Results of all scenarios per file, So that User can go through errors.\nExamples are available here\n\nIgnored Results\nPassed Results\nFailed Results\nErrors\n\n\n\n'], 'url_profile': 'https://github.com/kotanetes', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Jul 8, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 25, 2020', 'C++', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Mar 24, 2020', '1', 'Go', 'Updated May 15, 2020', '1', 'Python', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 11, 2020']}","{'location': 'Rishon LeZion', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ophirbh', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Jul 8, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 25, 2020', 'C++', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Mar 24, 2020', '1', 'Go', 'Updated May 15, 2020', '1', 'Python', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 11, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '211 contributions\n        in the last year', 'description': ['Statistics-Modules\nSeveral software modules that automatically performs different statistical tests. (One Sample T Test, Two independent samples t-test, One-way ANOVA, Regression Coefficient Testing, Descriptive Statistics, etc.)\nSet path in each script to location of corresponding module\n'], 'url_profile': 'https://github.com/ConradP3', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Jul 8, 2020', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 25, 2020', 'C++', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Mar 24, 2020', '1', 'Go', 'Updated May 15, 2020', '1', 'Python', 'Updated Mar 29, 2020', 'Python', 'Updated Apr 11, 2020']}"
"{'location': 'Ann Arbor', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shixu0830', 'info_list': ['R', 'Updated May 16, 2020', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'Baton Rouge, LA, USA', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prasoonsamir', 'info_list': ['R', 'Updated May 16, 2020', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SonneikooftheLaputa', 'info_list': ['R', 'Updated May 16, 2020', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'Kiel, Germany', 'stats_list': [], 'contributions': '332 contributions\n        in the last year', 'description': ['Implementation: Machine Learning (ScikitLearn)\nImplementaion of Classical Machine Learning algorithms in Python (Ipynb Notebooks)\nNotebooks :\n\n\nRegression - Linear, Multiple-Linear Regression\n\n\nSupport Vector Machine - Iris Dataset ( Linear & RBF kernel)\n\n\n- Decision Tree\n\n\nDecision Tree Classification - UCI Banknote Authentication Dataset\n\n\nDecision Tree Regression -  FSU Petron Consumption Dataset\n\n\n- Clustering\n\n\nK-Means Clustering\n\n\nHeirarchical Clustering - Customer Segmentation Using Clustering\n\n\n- KNN\n\nK Nearest Neighbors Classificaiton - Iris DataSet Classification\n\n- Exploration of high-dimensional data\n\n\nLDA, PCA - Mice Protein data exploration and clustering\n\n\nt-SNE, ISOMAP - MNIST Handwritten digits in High-dimensional manifold\n\n\n- Model Selection\n\n\nHoldout - Problem associated with model evaluation using Holdout\n\n\nAlgorithm Selection - Performance Comparison ( SVM, KNN, and Logistic Regression) Iris DataSet\n\n\nk-Fold and Nested k-Fold CV - Breast Cancer Dataset Algorithm Selection( KNN, Logistic Regression, SVM, Random Forest etc.) - Performance and Learning Curve.\n\n\nModel Selection Pipeline - Feature Selection, Tuning, Model Evaluation and Selection\n\n\nAIC, BIC, MDL - Model Selection Criteria - Petrol Consumption Dataset.\n\n\nDocumentation :\n\n\nIntroduction\n\n\nClassification\n\n\nRegression\n\n\nClusterring\n\n\nDecision Tree\n\n\nSVM\n\n\nPerceptron\n\n\nKNN\n\n\nDimensionality Reduction\n\n\nGeneralization\n\n\n\n'], 'url_profile': 'https://github.com/Mnpr', 'info_list': ['R', 'Updated May 16, 2020', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Wisconsin-Breast-Cancer-Classification\nUsed multiple predictive classification algorithms like Decision Tree, KNN, Logistic Regression and SVM and compared the results to identify the algorithm giving best result.\n'], 'url_profile': 'https://github.com/anunay1992', 'info_list': ['R', 'Updated May 16, 2020', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '341 contributions\n        in the last year', 'description': ['Python-OLS-gradient-descent-and-normal-equation-linear-regresion-from-scratch\nThe repository contains code to implement the gradient descent method of linear regression and normal equation method of linear regression. The code is in the form of a well commented jupyter notebook to help the audience understand the code and concept better.\nThe code is implemented without use of any preexisting library and only using the core concept behind the two algorithms.\nThe two algorithms are implemented using Ordinary Least Squares approach and compared on the basis of RMSE and R2.\n\n\n'], 'url_profile': 'https://github.com/ashide2729', 'info_list': ['R', 'Updated May 16, 2020', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': [""Household-Stock-Holding_Prediction-with-Logistic-and-Polynomial-Regressions-and-Deep-Learning\nI estimate household consumption using the Panel Study of Income Dynamics (PSID) Dataset. Household consumption data is very noisy. I combine the Ridge Regression with the Polynomial Regression. The Polynomial of degree 3 produces the best result.\nMoreover, I estimate household's stock market participation decision using the Panel Study of Income Dynamics (PSID) Dataset. I combine the Logistic Regression with the Polynomial Regression. The Polynomial of degree 3 produces the best result.\nFinally, I build the Deep Learning model with 3 layers and each layer is with 50 units. Relu activation function is used as the activation function. For the last output layer, the sigmoid functions is used as the activation function. We can see that the Dense Deep Learning model produces slightly better result than the combination of Logistic and Polynomial Regressions.\n""], 'url_profile': 'https://github.com/tsenguun0106', 'info_list': ['R', 'Updated May 16, 2020', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated May 16, 2020', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated May 16, 2020', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated May 16, 2020', 'Updated Mar 25, 2020', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'TeX', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 11, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jul 15, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ananyabardhan', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'TeX', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 11, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jul 15, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020']}","{'location': 'NYC', 'stats_list': [], 'contributions': '1,584 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/adamhowardprice', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'TeX', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 11, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jul 15, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '169 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AnnanyaV', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'TeX', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 11, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jul 15, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '618 contributions\n        in the last year', 'description': [""corona-scaling\n\nThis Jupyter notebook is a replication attempt of Stier et al.'s preprint on Arxiv [1]. This notebook brings together MSA definitions and census data to allow demographic calculations for MSAs in relation to the coronavirus outbreak. The MSAs are a county-level unit delineated by the Census Bureau (see https://www.census.gov/programs-surveys/metro-micro/about/delineation-files.html). The coronavirus outbreak data are provided by USAFacts (https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/, last downloaded March 25th, 2020 14:34 EDT).\nThis notebook can be executed in Binder online. Please click the badge above or click the link below/paste the following into your web browser: https://mybinder.org/v2/gh/acabaniss/corona-scaling/master.\nReferences\n.. [1] Andrew J Stier, Marc G. Berman, and Luis M. A. Bettencourt. March 23rd, 2020. COVID-19 attack rate increases with city size. arXiv:2003.10376v1 [q-bio.PE]\n""], 'url_profile': 'https://github.com/acabaniss', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'TeX', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 11, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jul 15, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '263 contributions\n        in the last year', 'description': ['Smart Pricing\nEver wonder how much your car is worth? Well, now you can find out! In this notebook is a smart pricing model for\na used car dataset scraped from Craigslist. Included in the notebook are all the links that inspired this notebook\nas well as the link to the data. A requirements.txt is included for reproducibility.\nRunning the Notebook\n\n\n(optional) Create a new virtual environment\n\n\nInstall the dependencies\npip install -r requirements.txt\n\n\nStart the jupyter notebook and run all the cells\n\n\n'], 'url_profile': 'https://github.com/Spartee', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'TeX', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 11, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jul 15, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Machine Learning and Developing a Text Classifier\n\n\n\n\n'], 'url_profile': 'https://github.com/Develop-Packt', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'TeX', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 11, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jul 15, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020']}","{'location': 'US', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['-CKD-Screening-Tool\nABSTARCT\nBased on the given patient data, the objective of this case study is to create an easy-to-use screening tool on a digital platform to identify patients at risk for Chronic Kidney Disease (CKD). Furthermore, we perform this by creating and implementing a multivariate logistic regression model to predict and see if high-risk patients can be identified using easily obtainable patient data.\nChronic Kidney Disease (CKD) is a progressive condition that explains the gradual loss of kidney function over time. The two main causes of CKD medically known to be are diabetes and high blood pressure or hypertension. Other factors that can be positively correlated with the onset of CKD are Cardiovascular disease (CVD), family history of kidney disease, age, and race (especially Pacific Islanders, African-American/Black, and Hispanics). Treatments can help slow down the progressive nature of CKD; however, this disease cannot be cured. This indicates the importance of recognizing the early onset of CKD and intervening as soon as possible to halt the progress and damage that CKD can pose.\nMETHODOLOGY\nThe dataset for the case study consists of responses for specifically designed questionnaire from 8819 individuals, aged 20 years or older taken between 1999-2000 and 2001-2002 during a survey across various states in USA. The dataset is divided into two sets 1) Training set with 6000 observations in which 33 variables along with the status of CKD is provided. 2) Validation set consisting of 2819 observations with same set of variables in which the CKD has to be predicted. The predictions were to be made based on â€“ 1) a statistical model and  2)a screening tool. The predictions for the statistical model were made using logistic regression. The data driven screening tool with calculated risk scores which was based on the logistic regression model used 6 of the 8 variables and had an AUC score of 0.82.\nThe in-sample data consisting of 6000 observations was divided into one training (3000) and two test sets (1500 each).\nMISSING DATA\nOur dataset consists of 8819 responses against 33 attributes (8819 x 33) 291027 individual responses are to be recorded. But only 283285 are recorded and 7742 records are missing (which is about 2.6 % of the data set). Four dummy variables have been created for Race group (Black, White, Hispanic and others).\nIMPUTATION\nTo deal with the missing data here, MICE package has been used with mean imputation so that the overall mean will not be affected. Each of the three data sets â€“ train, test1 and test2 were imputed separately.\nVARIABLE SELECTION\nAttribute selection methods are used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model.\nBased on correlation, LASSO regression and p-values of the logistics regression we created two models:\nModel 1: Age, Weight, Height, Hypertension, Diabetes, CVD, Female, Race(white)\nModel 2: Age, Waist, Hypertension, Diabetes, CVD, Female, Race(white).\nFinally, the AIC value and several accuracy metrics such as TPR, FPR, f-measure, accuracy hlped zero it down on model 1. Although model 2 had an accuracy value of 91%, the other metrics were way better and also a better performance indicator for a model, especially when the goal is to reduce the number of false positives.\nTHRESHOLD SELECTION\nIn order to convert probability values to actual predictions â€“ 0â€™s and 1â€™s; a threshold value is slected. For a given threshold value if the probability value of CKD is greater than the threshold value, we predict the individual has CKD.\nThe threshold value for the logistic regression output was 0.2. This value was chosen between the 3rd quantile and half of the maximum values in the probabilities predicted by the Logistic Regression. The model performance was found to be optimal at a probability cut-off value of 0.2. This value was also in line with the ultimate goal of reducing the false positive rates.\nLIMITATIONS\nThe white race group was overrepresented in this data set, while the Hispanic race group were underrepresented.\nThe model does its best to accurately predict individuals at higher for CKD; however, it does not account for outliers.\nThe sample presented in this data set was not a random sample, the findings cannot be representative for the whole population. A random sample is necessary for an unbiased representation of the population.\nSCREENING TOOL LINK: https://docs.google.com/forms/d/e/1FAIpQLScNwSOVMqPx8R4HNUP6gtIZCBsFy3AFYKf-LsXlhSSdNyLBow/viewform?vc=0&c=0&w=1\n'], 'url_profile': 'https://github.com/Drexno', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'TeX', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 11, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jul 15, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Multiclass-Classification-Numeric-Vs-Categorical-Car-Evaluation\nPerformed multiclass classification on UCI Car Evaluation Data. Compared analysis result for target variable taken as numeric and target variable taken as categorical. Used Decision Tree, Logistic Regression, KNN, SVM and Multinomial NB to get the best model.\n'], 'url_profile': 'https://github.com/anunay1992', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'TeX', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 11, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jul 15, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['#Binary_Classification\n'], 'url_profile': 'https://github.com/shyDaniel', 'info_list': ['Jupyter Notebook', 'Updated Aug 10, 2020', 'TeX', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 11, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jul 15, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Aug 24, 2020']}"
"{'location': 'Lund, Sweden', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/manhhungtran89', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'HTML', 'Updated Jan 27, 2021', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Rio de Janeiro', 'stats_list': [], 'contributions': '179 contributions\n        in the last year', 'description': ['E-Commerce-analise_dos_usuarios\n(Linear Regression- Mchn Lrn)\nCONTEXTO DO SOFTWARE\nUma anÃ¡lise detalhada sobre a relaÃ§Ã£o entre diversos dados dos usuÃ¡rios com sua conversÃ£o de cliques em publicidade de uma loja de roupas norte americana\nAbstract:\nEstes dados representam uma loja no ramo de E-Commerce e uma comparaÃ§Ã£o de suas vendas pelo aplicativo de celular e pelo site da loja, demonstrando as diferenÃ§as dos tipos de clientes que fazem as compras em cada tipo de plataforma e suas caracterÃ­sticas. AtravÃ©s destas informaÃ§Ãµes Ã© possÃ­vel um direcionamento dos esforÃ§os para a plataforma que apresenta maior resultado ou um empenho em melhorar a plataforma menos eficaz.\nSobre o desenvolvimento\nForam utilizados arquivos disponibilizados pelo Kaggle. O cÃ³digo foi desenvolvido em python e apresentado em jupyter notebook.\n'], 'url_profile': 'https://github.com/lucasvascrocha', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'HTML', 'Updated Jan 27, 2021', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Mumbai,Maharashtra', 'stats_list': [], 'contributions': '303 contributions\n        in the last year', 'description': ['titanic_survival_predictor\nA predictor for if a person will survive the sinking of titanic using logistic regression in machine learning and its dweb app and android app using flask and flutter\n'], 'url_profile': 'https://github.com/surya8barca', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'HTML', 'Updated Jan 27, 2021', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Auckland', 'stats_list': [], 'contributions': '845 contributions\n        in the last year', 'description': [""What-should-we-pay-her-\nThis repository contains the implementation of various regression algorithms to calculate that the upcoming employee should receive based on her experience and the role she is being offered. This project was for purpose of practice only, I would not recommend the use of any ML/AI algorithm to be used for determining the salary of an employee because these algorithms can have bias in the dataset itself. The intention behind the project is just to familiarise myself and the users of the repository, with the various regression algorthms.\nDataset\nThe dataset contains following features:\n\nPosition - This is the job title for which the employee is being hired (Text)\nLevel - This is some internal band kind of metric to a company defining various levels of job titles (Int)\nSalary - This represents the salary (Whole numbers)\n\nRequirements\nPython\nInstall the required libraries through command line\npip3 intsall -r requirements.txt\nInstallation\nClone this repository:\ngit clone https://github.com/manvimadan12/What-should-we-pay-her-.git\nor click Download ZIP in right panel of repository and extract the code\nSteps to run the notebook\nOpen Jupyter Notebook to view the following:\n\nPredict salary using SVM regression - Support Vector Machines for regression analysis\nPredict salary using Random Forest regression - Random Forest for regression analysis\nPredict salary using Decision tree regression - Decision Trees for regression analysis\n\nResults\n\n\nResults from polynomial regression\n\n\n\nResults from polynomial regression with high resolution and smoother curve\n\n\n\nResults from support vector regression\n\n\n\nResults with support vector regression with high resolution and smoother curve\n\n\n\nLimitations\nAn ML algorithm for such a small dataset can be an overkill. Basic statistical measures should suffice. However, for the educational purposes, it isn't a bad example.\n""], 'url_profile': 'https://github.com/manvimadandotai', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'HTML', 'Updated Jan 27, 2021', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Sydney, Australia', 'stats_list': [], 'contributions': '174 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/iknowcss', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'HTML', 'Updated Jan 27, 2021', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Robust Mixture Regression\n\n\n\n\n\n\n\nInstall from CRAN\ninstall.packages(""RobMixReg)\nlibrary(""RobMixReg"")\n\nInstall from github for most updated package.\nPlease report the bug as the description in the Question&Problem.\nlibrary(""devtools"")\ndevtools::install_github(""changwn/RobMixReg"")\n\nTutorial\nA comprehensive and complete tutorial is here.\nNews\nThe package version control is in News.md\nCitations\nIf you find the code helpful in your resarch or work, please cite us.\n@article{wennan2020cat,\n  title={A New Algorithm using Component-wise Adaptive Trimming For Robust Mixture Regression},\n  author={Chang, Wennan and Wan, Changlin and Zhou, Xinyu and Zhang, Chi and Cao, Sha},\n  journal={arXiv preprint arXiv:2005.11599},\n  year={2020}\n}\n\n@article{chang2020supervised,\n  title={Supervised clustering of high dimensional data using regularized mixture modeling},\n  author={Chang, Wennan and Wan, Changlin and Zang, Yong and Zhang, Chi and Cao, Sha},\n  journal={arXiv preprint arXiv:2007.09720},\n  year={2020}\n}\nQuestions & Problems\nIf you have any questions or problems, please feel free to open a new issue here. We will fix the new issue ASAP.  You can also email the maintainers and authors below.\n\nWennan Chang\n(wnchang@iu.edu)\n\nPhD candidate at Biomedical Data Research Lab (BDRL) , Indiana University School of Medicine\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'HTML', 'Updated Jan 27, 2021', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['UCI-Spambase-Spam-Non-Spam-Classification\nUsed numerous classification models like Decision Tree, KNN, Logistic Regression, SVC, Naive Bayes, Random Forest, Ada Boost, Gradient Boost, XGBoost, Neural Network and also created misclassification metics and compared the results of various models.\n'], 'url_profile': 'https://github.com/anunay1992', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'HTML', 'Updated Jan 27, 2021', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['DataModelling\nDAO2702/DBA3803 project template. Abstracts away from code and allows students to focus on business insights/recommendations. Functions available for data loading, printing summaries, data cleaning, descriptive analytics, and data modelling (only linear regression so far)\n'], 'url_profile': 'https://github.com/nelsonlim96', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'HTML', 'Updated Jan 27, 2021', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'West Lafayette, Indiana', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Estimate-Trending-Duration-of-YouTube-Videos\nBuilt a multivariate linear regression model to predict trending duration of a video on YouTube based on feature engineering, A/B testing and sentiment score generated from user comments (https://www.youtube.com/watch?v=oPr8Sx9L9Mg)\n                                                    Executive Summary\n\nThe content explosion in the media industry, and online space, in the past few years has attracted a huge number of investors. These days, investors and advertisers often sponsor channels and blogs. For instance, in social advertising, YouTube ads convert more customers than any other medium. There are two popular ways in which advertisements are shown:\n\nTargeting in-market audience based on browsing history\nProduct placement, where you place your productsâ€™ ads based on trending blogs and videos\nThe latter is widely being used on platforms like YouTube, with the rise of channel sponsoring, where ads are shown on particular videos or channels.\nTherefore, we have analyzed YouTube trending videos from 2017-2018 to derive insights and build a model which can predict how long a video will trend, considering variables such as likes, comments, dislikes, sentiment scores of comments, and video category. This can help us devise a marketing strategy to place our product ads on recommended trending videos.\n\nExploratory Data Analysis and Modelling\nWe built a regression model based on the YouTube data of day1 of trending videos which can predict how long a video can trend considering factors such as like to view ratio, dislike to view ratio, comment to view ratio, video category, sentiment score and number of views as our input variable. Before fitting the model, we did exploratory data analysis which showed that most of our input variables were right skewed. Hence, we preprocessed the data using logarithmic transformation, capping and flooring outlier treatment, and standardization to account for skewness in our data.  Initial analysis showed that if other parameters are constant, the median of trending duration is higher for Film & Animation, Music, People & Blogs, Comedy and Entertainment as compared to other video categories. We calculated sentiment score for each video - average of sentiment score of all comments for that video. Since the number of likes, and dislikes can be misleading as an input variable, we did feature engineering to convert these variables with respect to number of views as base. We generated dummy variables for categorical variable - video category, keeping Film & Animation as base.\nConclusions and Recommendations\nFilm and Animation category videos are likely to trend for the longest duration as compared to other video categories, provided other parameters are kept constant. Hence, we recommend potential advertisers/channel sponsors to place their product ads on Film and Animation category videos. Moreover, advertisers should invest in videos having comments with more negative sentiments to reach out to a larger audience. Based on the raw data, we can easily find the top ten trending channels from Film and Animation category videos where advertisers can invest. Our model can be improved further using more predictor variables and advanced machine learning techniques.\n'], 'url_profile': 'https://github.com/varunsingh2695', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'HTML', 'Updated Jan 27, 2021', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Pisa, Italy', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['\n\n\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/Bochicchio3', 'info_list': ['Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'HTML', 'Updated Jan 27, 2021', 'R', 'Updated Aug 12, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Python', 'Updated Mar 27, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020']}"
"{'location': 'Lacey, WA', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/finbarr91', 'info_list': ['1', 'Python', 'Updated Feb 3, 2021', 'Jupyter Notebook', 'Updated Mar 28, 2020', '3', 'MIT license', 'Updated Mar 26, 2020', '1', 'HTML', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'HTML', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 23, 2019', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 25, 2020']}","{'location': 'Coimbatore', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rajapriyan53', 'info_list': ['1', 'Python', 'Updated Feb 3, 2021', 'Jupyter Notebook', 'Updated Mar 28, 2020', '3', 'MIT license', 'Updated Mar 26, 2020', '1', 'HTML', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'HTML', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 23, 2019', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction\nAbout Course\nIn this course, you will learn the absolute basics of Data Science and Machine learning. We will go through commonly used terms and write some code in Python\nPrerequisites\nNone\nWhat is Data Science and Machine Learning\nMachine learning is a field of computer science that gives computer the ability to learn without being explicitly programmed\nIn traditional programing languages, we feed in data (also called features) with the predefined algorithm to get the output. However, in machine learning programming we feed in data with the ouput (also called labels) and let the model learn and find out the values that fits with the input and output data\n\nTypes of Machine Learning algorithms\nSupervised machine learning is the type of machine learning approach where we injest the labeled datasets. For example, detecting if the email is spam or ham. This type of algorithm is mostly used for classification problems.\nUnsupervised machine learning is the type of machine learning approach where we inject the unlabled datasets and let the model find the patterns in the data. For example,\nNote: Your inputs should be relevant to the machine learning algorithm you are choosing\nOther Resources\n\nPython Bootcamp\nThe Numpy Stack\n\nGetting Setup with Tools\nInstalling Anaconda\n\n\nWindows\n\n\nLinux\n\n\nMacOS\n\n\nScikit-Learn\nScikit-Learn is a statistical machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines.\nImporting scikit-learn\nimport sklearn as sk\nDocumentation: https://scikit-learn.org/stable/user_guide.html\nTypes of Algorithms\nSupervised\n\nLinear Regression\nLogistic Regression\nK Nearest Neighbours\nDecision Trees\nNaive Bayes\n\nUnsupervised\n\nCentroid-based algorithms\nConnectivity-based algorithms\nDensity-based algorithms\nProbabilistic\n\nPlaying with iris data\nAbout Iris: https://archive.ics.uci.edu/ml/datasets/iris\nImporting the modules\nfrom sklearn import datasets\nLoading dataset\niris_data = datasets.load_iris()\ntype(iris_data)\nsklearn.utils.Bunch\n\nThis is basically a dict, therefore using .keys() to find out the keys\niris_data.keys()\ndict_keys([\'data\', \'target\', \'target_names\', \'DESCR\', \'feature_names\', \'filename\'])\n\n\n\n\nKeyname\nDescription\n\n\n\n\ndata\nfeatures\n\n\ntarget\nnormalized label in number\n\n\ntarget_names\nname of labels in string\n\n\nDESCR\ndescription of the dataset (same as here)\n\n\nfeature_names\nnames of features\n\n\nfilename\nfilelocation of dataset\n\n\n\nGetting all the feature names\niris_data.feature_names\n[\'sepal length (cm)\',\n \'sepal width (cm)\',\n \'petal length (cm)\',\n \'petal width (cm)\']\n\nGetting all the target names\niris_data.target_names\narray([\'setosa\', \'versicolor\', \'virginica\'], dtype=\'<U10\')\n\nGetting Deeper into Machine Learning Frameworks and Algorithms\nK-Nearest Neighbours\nThe KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n\nBirds of a feather flock together.\n\nWorking of KNN\n\nLoad the data\nInitialize K to your chosen number of neighbors\nFor each example in the data\n\nCalculate the distance between the query example and the current example from the data.\nAdd the distance and the index of the example to an ordered collection\n\n\nSort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\nPick the first K entries from the sorted collection\nGet the labels of the selected K entries\nIf regression, return the mean of the K labels\nIf classification, return the mode of the K labels\n\nLet\'s use the KNN in the code\nfrom sklearn.neighbors import KNeighborsClassifier\nSince our problem is of classification, using KNeighborsClassifier\nInstancing KNN model with K=3\nknn = KNeighborsClassifier(n_neighbors=3)\nSo,\n\niris_data.data â†’ features (denoted by X)\niris_data.target â†’ labels (denoted by y)\n\nBefore actually fitting you should split the data into what we say training and testing. And while training, the testing data should not be touched, and vice-versa.\nThis is done to actually check whether the model is performing well or not\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, shuffle=True, train_size=0.6)\nThe training data will be 60% of the original data and testing data will be 40% of the original data. The shuffle=True is used to shuffle the data so that the model doesn\'t not memorize the pattern. This helps in increasing accuracy\nX_train.shape\n(90, 4)\n\nX_test.shape\n(60, 4)\n\nNow let\'s fit the model\nknn.fit(X_train, y_train)\nKNeighborsClassifier(algorithm=\'auto\', leaf_size=30, metric=\'minkowski\',\n                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n                     weights=\'uniform\')\n\nMaking predictions\npred = knn.predict([X_test[0]])\npred\narray([2])\n\nGetting the label name from the prediction\niris_data.target_names[pred[0]]\n\'virginica\'\n\nTesting if the predicted model is correct of not\niris_data.target_names[y_test[0]]\n\'virginica\'\n\nNow to find the score of the model, simply use .score() function\nknn.score(X_test, y_test)\n0.9166666666666666\n\nThe accuracy of our model is 91.66%\nEvaluating and Enhancing Models\nNow how would you find if this is the best model or not? Does it seems frustrating.\nLuckily the sklearn provides Cross validators to check and find the best model\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nCreating a fresh model\nknn = KNeighborsClassifier()\nDefining hyper parameters (n_neighbors is a type of hyper parameter)\nparams = {\n    ""n_neighbors"": [*range(1, 11)] # from 1 - 10\n}\nCreating a GridSearchCV object with 3 cross validations per dataset\ngcv = GridSearchCV(knn, params, cv=3, verbose=True)\nFitting the model\ngcv.fit(X_test, y_test)\nFitting 3 folds for each of 10 candidates, totalling 30 fits\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    0.1s finished\n\n\n\n\n\nGridSearchCV(cv=3, error_score=nan,\n             estimator=KNeighborsClassifier(algorithm=\'auto\', leaf_size=30,\n                                            metric=\'minkowski\',\n                                            metric_params=None, n_jobs=None,\n                                            n_neighbors=5, p=2,\n                                            weights=\'uniform\'),\n             iid=\'deprecated\', n_jobs=None,\n             param_grid={\'n_neighbors\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},\n             pre_dispatch=\'2*n_jobs\', refit=True, return_train_score=False,\n             scoring=None, verbose=True)\n\nGetting the best model\ngcv.best_estimator_\nKNeighborsClassifier(algorithm=\'auto\', leaf_size=30, metric=\'minkowski\',\n                     metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n                     weights=\'uniform\')\n\nThe parameters of the model\ngcv.best_params_\n{\'n_neighbors\': 6}\n\nAs you can see it is different from what we have used earlier (n_neighbors=3)\nGetting the best score of the model\ngcv.best_score_\n0.9666666666666667\n\nThe score is indeed increased from 91%\n'], 'url_profile': 'https://github.com/guides-to', 'info_list': ['1', 'Python', 'Updated Feb 3, 2021', 'Jupyter Notebook', 'Updated Mar 28, 2020', '3', 'MIT license', 'Updated Mar 26, 2020', '1', 'HTML', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'HTML', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 23, 2019', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': [""CKME136---Data-Analytics-Capstone\nMy capstone project for Ryerson's Data Analytics, Big Data, and Predictive Analytics. The project I worked on was based on an open competition project called DengAI: Predicting Disease Spread. The objective of the project was to predict the total number of cases in the testing set of Iquitos, Peru & San Juan, Puerto Rico.\nHere is the link to the competition: https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/\nA brief summary of the objective:\nUsing environmental data collected by various U.S. Federal Government agenciesâ€”from the Centers for Disease Control and Prevention to the National Oceanic and Atmospheric Administration in the U.S. Department of Commerceâ€”can you predict the number of dengue fever cases reported each week in San Juan, Puerto Rico and Iquitos, Peru?\nLink to my RPubs: https://rpubs.com/pycay\n\n""], 'url_profile': 'https://github.com/pycay', 'info_list': ['1', 'Python', 'Updated Feb 3, 2021', 'Jupyter Notebook', 'Updated Mar 28, 2020', '3', 'MIT license', 'Updated Mar 26, 2020', '1', 'HTML', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'HTML', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 23, 2019', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akshay0619', 'info_list': ['1', 'Python', 'Updated Feb 3, 2021', 'Jupyter Notebook', 'Updated Mar 28, 2020', '3', 'MIT license', 'Updated Mar 26, 2020', '1', 'HTML', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'HTML', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 23, 2019', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 25, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': ['COVID-19, Argentina\nThis script generates a web page with information of the evolution of COVID-19 in Argentina. The website provides information on the number of infections and deaths per day, based on data from the Argentine Ministry of Health. In addition, basic forecasts of the aforementioned variables are presented. These forecasts were calculated on the basis of exponential function fits, with data from the last 3, 7 and 14 days. The source code for the generation of this website is open source.\nEste script genera una pÃ¡gina web con informaciÃ³n de la evoluciÃ³n del COVID-19 en Argentina. La web proporciona informaciÃ³n sobre el nÃºmero de infecciones y muertes por dÃ­a, basada en datos del Ministerio de Salud de Argentina. AdemÃ¡s, se presentan pronÃ³sticos bÃ¡sicos de las variables mencionadas. Los mismos se calcularon en base a ajustes de funciones exponenciales a partir de datos de los Ãºltimos 3, 7 y 14 dÃ­as. El cÃ³digo fuente para la generaciÃ³n de este sitio web es de cÃ³digo abierto.\n'], 'url_profile': 'https://github.com/emmanuellujan', 'info_list': ['1', 'Python', 'Updated Feb 3, 2021', 'Jupyter Notebook', 'Updated Mar 28, 2020', '3', 'MIT license', 'Updated Mar 26, 2020', '1', 'HTML', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'HTML', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 23, 2019', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['basic-statistics-with-jupyter\nIn this repo some basic statistic calculations are made with pandas, numpy, scikit and matplotlib: histograms, linear regressions, normal and Poisson distributions... etc\n'], 'url_profile': 'https://github.com/guillermogarcialopez', 'info_list': ['1', 'Python', 'Updated Feb 3, 2021', 'Jupyter Notebook', 'Updated Mar 28, 2020', '3', 'MIT license', 'Updated Mar 26, 2020', '1', 'HTML', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'HTML', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 23, 2019', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Classification-of-Handwritten-digits\nThe MNIST dataset, contained in mnist-original.mat (matlab format), consists of 70,000 digitized handwritten digits and their labels. I classified them using 2 different classifiers,  a stochastic gradient descent classifier called SGDClassifier (we will talk about this in class) and a logistic regression classifier called LogisticRegression.\n'], 'url_profile': 'https://github.com/zhanggaofeng1120', 'info_list': ['1', 'Python', 'Updated Feb 3, 2021', 'Jupyter Notebook', 'Updated Mar 28, 2020', '3', 'MIT license', 'Updated Mar 26, 2020', '1', 'HTML', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'HTML', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 23, 2019', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 25, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Predicting-compressive-strength-of-samples\nThis project predicts compressive strength of cements based on the ingredients of the sample. All the procedure is explained in the code with graphs\nAuthor: Pantea Tavakolian, Ph.D.\nCADIPT Lab: https://cadipt.mie.utoronto.ca/\nWe used Keras regression Neural Network to build our model. The dataset is about the compressive strength of different samples of concrete based on the volumes of the different ingredients that were used to make them.\n'], 'url_profile': 'https://github.com/ptavakolian', 'info_list': ['1', 'Python', 'Updated Feb 3, 2021', 'Jupyter Notebook', 'Updated Mar 28, 2020', '3', 'MIT license', 'Updated Mar 26, 2020', '1', 'HTML', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'HTML', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 23, 2019', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 25, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': [""Timeseries Forecast Project\nThis is the solution to time series prediction problem. Traditional time series models like ARIMA, ARIMAX, VAR (mostly VAR because of multivariate inputs), could be used to incorporate trend and seasonality. However, these solutions do not really do any kind of continuous input-->output mapping. The given data does not show increasing or decreasing trend and the problem statement of the coding challenge specifically states:\n\nGiven ('low', 'high', 'weight_avg') prices for the past 'm' time steps, predict the next 'n' prices in the sequence.\n\nUltimately, the provided solution uses regression models mapping m inputs to n outputs. I have tried to make this code as configurable as possible by using .yaml configurations. Please read the config comments and set appropriate config variables to successfully run the application. Also, documentation is generated using Sphinx 1.8.0 and can be read at 'docs_build\\html\\index.html' in the parent directory\nTwo types of tasks can be performed by this solution :\n\nTraining :\nload the training data\nperform test_train (validation_train) split.\ntrain the specific type of model with specific hyperparameters (as selected in config)\nvalidate the model\nsave the model and metadata at specified output directory.\nDeployment :\nload the test data\nload the desired models\npredict results\nsave the test metadata at specified output directory.\n\nGetting Started\nPrerequisits\nRequires Python 3.x.x\nInstalling\nTo install all the dependencies, navigate to project folder\nRun 'pip install -r requirements.txt'\nRunning\nOnce all the dependencies are installed, set appropriate configuration at 'config.yaml'\nRun 'python main.py'\nFor extensive documentation please refer to 'docs_build\\html\\index.html'\n""], 'url_profile': 'https://github.com/SayaliPatkar', 'info_list': ['1', 'Python', 'Updated Feb 3, 2021', 'Jupyter Notebook', 'Updated Mar 28, 2020', '3', 'MIT license', 'Updated Mar 26, 2020', '1', 'HTML', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'HTML', 'Updated Apr 12, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 23, 2019', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Python', 'Updated Mar 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['COVID-19-Prediction-SL\nThis is an analysis done based on the confirmed cases of COVID-19 virus up to 24th of March. Using Linear Regression, the confirmed cases are predicted for coming 10 days (upto 05th of April). Lets hope this wont be accurate.\n'], 'url_profile': 'https://github.com/aieml', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'MIT license', 'Updated Mar 3, 2021', 'R', 'Updated Apr 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'Coimbatore', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rajapriyan53', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'MIT license', 'Updated Mar 3, 2021', 'R', 'Updated Apr 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Developing-Statistical-Model-Using-Consumer-Variables-of-Airbnb\nAnalyzed the Airbnb listings in Seattle, Washington, to gauge the impact of various parameters on the rating received from the customers.  Generated an improved rating score by transforming reviews to scores using Google NLP API and gauged impact of amenities provided by Airbnb(s) on their reviews using linear regression models.\n'], 'url_profile': 'https://github.com/goyal64', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'MIT license', 'Updated Mar 3, 2021', 'R', 'Updated Apr 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'Knoxville, TN', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['covid19-Michigan\nThis repo contains data and script for predicting COVID 19 cases for MI across all counties. The prediction model uses Exponential regression. The summary and metrics of the model are as follows\n\nCall:\nlm(formula = log(Cases) ~ Day + I(Day^2), data = data[samples, \n    ])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62949 -0.11913  0.07627  0.18488  0.43085 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.421068   0.340882   1.235   0.2480    \nDay          0.635627   0.091046   6.981 6.46e-05 ***\nI(Day^2)    -0.010972   0.005013  -2.189   0.0564 .  \n---\nSignif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1\n\nResidual standard error: 0.3344 on 9 degrees of freedom\nMultiple R-squared:  0.9815,\tAdjusted R-squared:  0.9774 \nF-statistic: 238.4 on 2 and 9 DF,  p-value: 1.603e-08\n\nPredicted data graph\nIn the below actual vs prediction graph, we can see a good exponential growth curve.\n\nData Source\n[1] https://www.clickondetroit.com/news/local/2020/03/20/michigan-covid-19-data-tracking-case-count-cases-by-county-deaths-cases-by-age-tests/\n[2] https://www.michigan.gov/coronavirus/\n'], 'url_profile': 'https://github.com/nagdevAmruthnath', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'MIT license', 'Updated Mar 3, 2021', 'R', 'Updated Apr 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': [""We're doing a bit of open ended exploration of the coronavirus data provided by the New York Times. (https://github.com/nytimes/covid-19-data)\n\n\nWe make forecasts on coronavirus cases on US states through reframing the problem in terms of simple OLS regression and solving using a python convex optimization package.\n\n\nUnder some assumptions we construct an ordinary differential equation (ODE) and run simulations on different parameters to see different scenarios of how the infection could spread and saturate a given population (and the effects of mitigation measures on infection).\n\n\nWe use basic parameter estimation techniques to narrow down the simulations that reflect our data.\n\n\nWith each day we create revisions on our data, and revise our models. We track model revisions to see how well stay-at-home measures are working on the states.\n\n\nTODO:\n\nautomated hyperparameter tuning of parameter penalty.\nrun different models such as sigmoid on data.\n\nContents are in the jupyter notebook COVID-19_explore_20200328.ipynb\nus-states.csv contains data up to and including 20200326.\nTo generate point-in-time snapshots run generate_PIT_data.sh\n""], 'url_profile': 'https://github.com/vjain05', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'MIT license', 'Updated Mar 3, 2021', 'R', 'Updated Apr 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '147 contributions\n        in the last year', 'description': ['Welcome to GitHub Pages\nYou can use the editor on GitHub to maintain and preview the content for your website in Markdown files.\nWhenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.\nMarkdown\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\nFor more details see GitHub Flavored Markdown.\nJekyll Themes\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings. The name of this theme is saved in the Jekyll _config.yml configuration file.\nSupport or Contact\nHaving trouble with Pages? Check out our documentation or contact support and weâ€™ll help you sort it out.\n'], 'url_profile': 'https://github.com/pratikbarjatya', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'MIT license', 'Updated Mar 3, 2021', 'R', 'Updated Apr 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'Nashik', 'stats_list': [], 'contributions': '1,334 contributions\n        in the last year', 'description': ['COVID-19-Tracker\nThis repository houses code for the visualization of data pertaining to the spread of COVID-19.\nIntroduction:\n\nThis code was developed as an activity to keep myself busy, on the first day of the Total Lockdown in India, following the rise in the cases of COVID-19 pandemic. This was also, co-incidentally, the first day of the Indian New Year (Gudhi Padwa / Yugadi). The subsequent additions were made later.\nThese programmes fetch the latest data pertaining to the spread of the COVID-19 pandemic from web-based data sources and plot/tabulate them.\nThe programmes do not manipulate or add any new data. The purpose of these programmes is to visualize the real-time data on a global as well as regional (within India) scale.\n\nDependencies:\n\nThis repository relies on the COVID-19-India_Data repository for visualizing the Indian regional data.\nThis repository relies on the covid-19 repository by the @datasets team for visualizing the global data. I am grateful to the team for regularly compiling, cleaning and updating this dataset from various reliable upstream data sources.\nExternal Python libraries: pandas, numpy and matplotlib.\n\nLicensing:\n\nThe upstream datasets are available under the Open Data Commons Public Domain and Dedication License (ODC PDDL) and the MIT License.\nHowever, this package does not modify data or provide any new data.\nThis package provides visualizing tools which can be freely used and modified by anyone. Therefore, this package is available under the MIT License.\n\nCommon Issues:\n\nThe data from the USA are represented using the country name US.\nThe data from the UK are represented using the country name United Kingdom.\n\n'], 'url_profile': 'https://github.com/coder-amey', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'MIT license', 'Updated Mar 3, 2021', 'R', 'Updated Apr 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['math391_independent_study\nHolds R code for my end of semester project for my independent study (math 391) at Colgate for applied mathematics major. Goal of the final project was to predict workplace culture values from employee surveys given on sites like Glassdoor. Data set was provided by Professor. Project explored multiple different predictive models including logistic regression, LDA, Decision Tree, Random Forest, and SVC. Adjacency accuracy (accurate ranking +/- 1)\n'], 'url_profile': 'https://github.com/mgentile5252', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'MIT license', 'Updated Mar 3, 2021', 'R', 'Updated Apr 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '122 contributions\n        in the last year', 'description': ['Credit_Card_Fraud_Kaggle\nThis repository contains the possible solutions to the Credit Card Fraud problem set in Kaggle. It contains EDA, including dimensionality reduction,visualisations, sampling techniques , synthetic oversampling. A contrastive k fold analysis of decision trees,neighbors classifier, regression and discriminant analysis along with their confusion matrices. Alternative solutions using boosting trees and forests(lightgbm classifiers) are also used. A neural network of 4 intermediate hidden odes with relu and sigmoid activations is also provided as an alternative approach to get a higher accuracy.\nThe dataset and benchmarks can be found here:https://www.kaggle.com/mlg-ulb/creditcardfraud/\nThis notebook is a primer on credit risk analysis and can be used as per MIT license\nKaggle version of the solution: https://www.kaggle.com/abhilash1910/credit-card-fraud?scriptVersionId=32223529\n'], 'url_profile': 'https://github.com/abhilash1910', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'MIT license', 'Updated Mar 3, 2021', 'R', 'Updated Apr 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '562 contributions\n        in the last year', 'description': [""Age_DOE_buildings\nFor the first part of this accessibility analysis, please see https://github.com/kpananjady/Accessibility-NYC-Schools.\nIt seems that more recent schools are more likely to be rated higher. I'm refining this analysis by looking at:\n\n\nThe exact relationship between building age and accessibility (correlation, regression)\n\n\nwhere schools built after the Americans with Disabilities Act was passed are; a lot more in Queens than other boroughs\n\n\nwhat the age distribution of NYC schools is and how they correspond with the tenures of school building superintendents in the last century\n\n\n\nDATA SOURCES FOR ANALYSIS 1:\n2017-2018 School_Locations.csv (https://data.cityofnewyork.us/Education/2017-2018-School-Locations/p6h4-mpyy), because I want to combine DOE Building Codes with Borough Block and Lot codes\nSchool-NYC-Accessibility-geolocated.csv (https://github.com/kpananjady/Accessibility-NYC-Schools/blob/master/School-NYC-Accessibility-geolocated.csv), because I'm only interested in the codes that have been rated already by the DOE\npluto_20v1.csv (https://www1.nyc.gov/site/planning/data-maps/open-data/dwn-pluto-mappluto.page), to get the year buildings were built and whether or not they're landmarks/located in historic districts\n\nANALYSIS 1: GETTING THE DATA READY\nIn the file Merges and Wrangling.ipynb (https://github.com/kpananjady/Age_DOE_buildings/blob/master/Merges%20and%20wrangling.ipynb), I get all my data ready so I can access the year built for 95 percent of the school buildings in my rated dataset.\nI start by combining school locations and accessibility ratings on DOE building codes, and then combine the resulting file on the Pluto database on borough, block and lot codes. This gives me the year built for most of the rated building in the DOE dataset.\nI then try to get year built for every single school location â€” whether or not they've been rated so far. I plot the distribution of construction years for all buildings, and then another histogram for all\nRESULT: years_built_school.csv (https://github.com/kpananjady/Age_DOE_buildings/blob/master/year_built_school.csv) for the years when rated schools were built; all_schools_years.csv (https://github.com/kpananjady/Age_DOE_buildings/blob/master/all_schools_years.csv) for the years when all schools were built\n\nDATA SOURCES FOR ANALYSIS 2:\nyears_built_school.csv (https://github.com/kpananjady/Age_DOE_buildings/blob/master/year_built_school.csv) for the years when rated schools were built, generated above\n\nANALYSIS 2: RELATIONSHIP BETWEEN BUILDING AGE AND ACCESSIBILITY\nIn Age Analysis.ipynb (https://github.com/kpananjady/Age_DOE_buildings/blob/master/Age%20Analysis.ipynb), I explore the relationship between building age and accessibility rating mathematically. In the QJIS files, I map out the patterns I see, the results of which you can see in the screenshots.\nScreenshot 1( https://github.com/kpananjady/Age_DOE_buildings/blob/master/Screen%20Shot%202020-03-29%20at%206.33.10%20PM.png) shows all the buildings rated 10; Screenshot 2 (https://github.com/kpananjady/Age_DOE_buildings/blob/master/Screen%20Shot%202020-03-29%20at%206.33.22%20PM.png) overlays schools built between 1984-2017.\n""], 'url_profile': 'https://github.com/kpananjady', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'R', 'Updated Mar 26, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 15, 2020', 'Python', 'MIT license', 'Updated Mar 3, 2021', 'R', 'Updated Apr 8, 2020', '1', 'Jupyter Notebook', 'Updated Apr 18, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}"
"{'location': 'Cape Town, South Africa', 'stats_list': [], 'contributions': '198 contributions\n        in the last year', 'description': ['Recommender Systems\nIntroduction\nRecommender systems are software tools and techniques that provide suggestions for items to be of use to a user. The collaborative filtering approach evaluates items using the opinions or ratings of other users. Alternatively, the content-based approach works by learning the itemsâ€™ features to match the userâ€™s preferences and interests. The code found in this repository implements several collaborative filtering and content-based methods: K-nearest neighbours, hierarchical clustering, association rule mining, ordinal logistic regression, classification trees, TF-IDF, and matrix factorisation.\n\n\n\nAlgorithm\nMAE\nRMSE\n\n\n\n\nNaive\n0.722\n0.927\n\n\nUser-based KNN\n0.659\n0.859\n\n\nItem-based KNN\n0.493\n0.663\n\n\nHierarchical\n0.824\n1.077\n\n\nMF-SGD\n0.737\n1.010\n\n\nBias MF-SGD\n0.767\n1.120\n\n\nK-medoid\n1.973\n2.018\n\n\nRegression\n0.766\n1.040\n\n\n\nPrerequisites\n\nA good understanding of popular recommender system techniques.\nCompetency in R.\nCompetency in C++.\n\nData\nThis dataset describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100836 ratings and 3683 tag applications across 9742 movies. These data were created by 610 users between March 29, 1996 and September 24, 2018. The data are contained in the files links.csv, movies.csv, ratings.csv and tags.csv. More details about the contents and use of all these files follows. This and other GroupLens data sets are publicly available for download at http://grouplens.org/datasets/.\nMovieLens users were selected at random for inclusion. User ids have been anonymized. User ids are consistent between ratings.csv and tags.csv (i.e., the same id refers to the same user across the two files).\nOnly movies with at least one rating or tag are included in the dataset. These movie ids are consistent with those used on the MovieLens web site (e.g., id 1 corresponds to the URL https://movielens.org/movies/1). Movie ids are consistent between ratings.csv, tags.csv, movies.csv, and links.csv (i.e., the same id refers to the same movie across these four data files).\n\n\nratings.csv:\nAll ratings are contained in the file ratings.csv. Each line of this file after the header row represents one rating of one movie by one user, and has the following format: userId,movieId,rating,timestamp. The lines within this file are ordered first by userId, then, within user, by movieId. Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars). Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n\n\ntags.csv:\nAll tags are contained in the file tags.csv. Each line of this file after the header row represents one tag applied to one movie by one user, and has the following format: userId,movieId,tag,timestamp. The lines within this file are ordered first by userId, then, within user, by movieId. Tags are user-generated metadata about movies. Each tag is typically a single word or short phrase. The meaning, value, and purpose of a particular tag is determined by each user. Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n\n\nmovies.csv:\nMovie information is contained in the file movies.csv. Each line of this file after the header row represents one movie, and has the following format: movieId,title,genres. Movie titles are entered manually or imported from https://www.themoviedb.org/, and include the year of release in parentheses. Errors and inconsistencies may exist in these titles.\n\n\nFunctions\nEach of the files in the Functions folder contain functions that are supplementary to implementing the recommender systems techniques used in the R files contained in the root of the repository. They are imported as follows:\nsourceCpp(paste0(getwd(), ""/Functions/Pearson Correlation.cpp""))\nsourceCpp(paste0(getwd(), ""/Functions/KNN.cpp""))\nsource(paste0(getwd(), ""/Functions/Supplementary.R""))\nFunctions/KNN.cpp contains the item-based and user-based K-nearest neighbour algorithms. Defining these functions in C++ allows for faster computation and are implemented in the R files contained in the root of the repository using the Rcpp package. Both KNN functions return a vector of predicted ratings in the range 0 to 5 for user = userid on all movies. The item-based KNN function takes in three arguments and is implemented as follows:\nItem_Based_Recommendation(userId = 10, W = W_items, R = R_userId)\nWhere userId specifies the user for which ratings are to be predicted; W_items is the item correlation matrix; R_userId is the vector ratings made by userId on all movies. This returns a vector of predicted ratings in the range 0 to 5 for user 10 on all movies. In this case k is specified implicitly by all those items which have been rated by userId. The user-based KNN function takes in four arguments and is implemented as follows:\nUser_Based_Recommendation(userId = 10, W = W_users, k = nrow(R) R = R)\nWhere userId specifies the user for which ratings are to be predicted; W_users is the user correlation matrix; k ; R is the user-item ratings matrix. In this case k is specified explicitly as all users.\nFunctions/Pearson Correlation.cpp contains the functions for computing both the item and user correlation matrices. Both take the user-item ratings matrix R as their argument (which is large - hence the need for computation in C++) and return a matrix.\nFunctions/Supplementary.cpp contains two functions for data preparation (substrLeft and substrRight) and two functions for evaluating the performance of each recommender system (MAE and RMSE).\nLastly, each of the R files in the root of the repository are subsectioned. In each file, before implementing the main technique, the first two sections should be run as is. The first section, ""Preliminaries"", installs and loads required packages and import required functions. The second section, ""Data Preparation"", is self-explanatory.\nNaive\nA naive recommender sytem is specified as setting predicted ratings by each user to be their mean rating across all movies rated by that user. This serves only as a benchmark on which to compare the MAE and RMSE performance of all other techniques.\nKNN\nRating predictions on all movies by all users are conducted in a parallelized farmework and then compared to the actual user-item ratings matrix using the mean absolute error and root mean squared error performance metrics. The example below shows how to produce a sorted list of recommendations on movies unwatched by user 10.\n## Get movie recommendations for user userId = 10\n# Item-Based KNN\nitem_based_recommendations = as.numeric(names(sort(Get_Recommendations(userId = 10, W = W_items)[is.na(R[10,])], decreasing = TRUE)))\n# User-Based KNN\nuser_based_recommendations = as.numeric(names(sort(User_Based_Recommendation(userId = 10, W = W_users, k = nrow(R), R = R)[is.na(R[10,])], decreasing = TRUE)))\n'], 'url_profile': 'https://github.com/IvanJericevich', 'info_list': ['1', 'R', 'Updated Apr 20, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['SLOPEsolversTESTS GSoC\nTests description\n\nEasy:\ndownload the development version of the R package SLOPE (devtools::install_github(""jolars/SLOPE"")). Fit SLOPE and lasso (hint: see the lambda argument in SLOPE()) models using the SLOPE package to the abalone data set that comes with SLOPE. Plot the results. What are the similarities and differences?\n\nCode to this test is in easy_test_code.R, results and description are in plots_description.pdf.\n\n\nMedium:\nwrite a function using RcppArmadillo that computes the proximal operator for SLOPE using Algorithm 3 (FastProxSL1) from Bogdan et al 2015 (SLOPE: adaptive variable selection via convex optimization). Compare the result with SLOPE:::prox_sorted_L1() (observe that this function uses a different algorithm than the one you are supposed to implement)\n\n\nHard:\nwrite an R package using RcppArmadillo (as a backend) that uses FISTA or ADMM to solve ordinary least squares regression using SLOPE. Make use of the function to compute the proximal operator that you implemented in the previous test.\n\n\n'], 'url_profile': 'https://github.com/AleksandraSteiner', 'info_list': ['1', 'R', 'Updated Apr 20, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 26, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/deepanshu-b94', 'info_list': ['1', 'R', 'Updated Apr 20, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '494 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashindecodes', 'info_list': ['1', 'R', 'Updated Apr 20, 2020', 'R', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '3', 'Jupyter Notebook', 'Updated Mar 26, 2020']}",,,,,,
