"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\n\n\n Visual Regression Tracker \n Open source, self hosted solution for visual testing and managing results of visual testing. \nHello\n\nTake 1-minute survey to help us priorities tasks better ‚ù§Ô∏è\nTwitter\nTelegram group\n\nHow it works\nService receives images, performs pixel by pixel comparison with it‚Äôs previously accepted baseline and provides immediate results in order to catch unexpected changes.\n\nFeatures\n\nAutomation framework independent - no need to stick with specific automation tool, integrate with existing one\nPlatform independent - web, mobile, desktop etc. as long as you could make a screenshot\nBaseline history - track how baseline image changed since the beginning\nIgnore regions - improve stability by ignoring not important or not controllable parts of image\nLanguage support - JS, Java, Python, .Net or any other via REST API (need more?)\nEasy setup - everything is inside Docker images\nSelf-hosted - keep your data save inside your intranet\n\nGlossary\n\nTestVariation - historical record of Baselines by Name + Branch + OS + Browser + Viewport + Device,\nBaseline - validated and accepted image, latest will be used as expected result in TestRun\nTestRun - result of comparing image against Baseline\nBuild - list of TestRuns\nProject - list of Builds and TestVariations\n\nSet up\nLinux, macOS, WSL\n\nInstall Docker\nDownload the installation script\n\ncurl https://raw.githubusercontent.com/Visual-Regression-Tracker/Visual-Regression-Tracker/master/vrt-install.sh -o vrt-install.sh\nchmod a+x vrt-install.sh\n\n\nRun the installation script\n\n./vrt-install.sh\nCommand line arguments\nInstalls the Visual Regression Tracker\n\nUsage: ./vrt-install.sh\n\nArguments:\n    -h | --help\n    -a | --frontend-url <url>   Set the Front-end url. Default: http://localhost:8080\n    -r | --backend-url <url>    Set the API url. Default: http://localhost:4200\n    --jwt-secret <secret>       Set the JWT secret. Default: randomly generated\n\nBy Hand\n\nInstall Docker\nCopy docker-compose.yml\n\n$ curl https://raw.githubusercontent.com/Visual-Regression-Tracker/Visual-Regression-Tracker/master/docker-compose.yml -o docker-compose.yml\n\nCopy .env\n\n$ curl https://raw.githubusercontent.com/Visual-Regression-Tracker/Visual-Regression-Tracker/master/.env -o .env\n\nStart service\n\n$ docker-compose up\nWait untill you see your creds printed.\nNew users and projects could be created via frontend app by default on http://localhost:8080\n\nIntegration\nUse implemented libraries to integrate with existing automated suites by adding assertions based on image comparison.\nWe provide native integration with automation libraries, core SDK and Rest API interfaces that allow the system to be used with any existing programming language.\nAgents\n\nPlaywright\nCypress\nCodeceptJS\n\nCore SDK\nBasic wrapper over API to be used for integration with existing tools\n\nJavaScript\nJava\nPython\nDotnet\n\nGetting started guide\nVideos\n\nHow to set up on Remote machine via ssh\n\nWiki\n\nCypress\nPlaywright\nStorybook\nSelenide (Java)\n\nIntegration examples\nHere you could find examples\n\nJavaScript\nJava\nPython\nDotnet\n\nContribution\n\nTry it, raise tickets with ideas, questions, bugs and share feedback :)\nMore language support for SDK\nMore integration with specific testing frameworks (agents)\n\nContributors ‚ú®\nThanks goes to these wonderful people (emoji key):\n\n\nPavel Strunkinüíª üíº ü§î üîå\nDaniel Croweüîå\nSurat Dasüíª üîå\nOleksandr Romanovüîå\nTerentev Denisüîå\nJustSittinHereüîå\n\n\nThis project follows the all-contributors specification. Contributions of any kind welcome!\n'], 'url_profile': 'https://github.com/Visual-Regression-Tracker', 'info_list': ['198', 'Shell', 'Apache-2.0 license', 'Updated Mar 2, 2021', '22', 'Python', 'BSD-2-Clause license', 'Updated Jan 24, 2021', '4', 'HTML', 'MIT license', 'Updated May 16, 2020', '17', 'Python', 'MIT license', 'Updated May 6, 2020', '7', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['fracridge\n\nIs an implementation of fractional ridge regression (FRR).\nInstallation:\nMATLAB\nDownload and copy the files from the\n[https://github.com/nrdg/fracridge/tree/master/matlab](MATLAB directory) into\nyour MATLAB path.\nPython\nTo install the release version:\npip install fracridge\n\nOr to install the development version:\npip install -r requirements.txt\npip install .\n\nUsage\nMATLAB\n[coef,alphas] = fracridge(X,fracs,y,tol,mode)\n\nPython\nThere\'s a functional API:\nfrom fracridge import fracridge\ncoefs, alphas = fracridge(X, y, fracs)\n\nOr a sklearn-compatible OO API:\nfrom fracridge import FracRidge\nfr = FracRridge(fracs=fracs)\nfr.fit(X, y)\ncoefs = fr.coef_\nalphas = fr.alpha_\n\nOnline documentation\nhttps://nrdg.github.io/fracridge/\nHow to cite\nIf you use fracridge, please cite our paper: ""Fractional ridge regression: a fast, interpretable\nreparameterization of ridge regression"" (2020)  GigaScience, Volume 9, Issue 12, December 2020, https://doi.org/10.1093/gigascience/giaa133 link.\nFor your convenience, here is the bibtex entry\n\n@ARTICLE{fracridge2020,\n  title    = ""Fractional ridge regression: a fast, interpretable\n              reparameterization of ridge regression"",\n  author   = ""Rokem, Ariel and Kay, Kendrick"",\n  journal  = ""Gigascience"",\n  volume   =  9,\n  number   =  12,\n  month    =  nov,\n  year     =  2020\n  }\n\n\n\n'], 'url_profile': 'https://github.com/nrdg', 'info_list': ['198', 'Shell', 'Apache-2.0 license', 'Updated Mar 2, 2021', '22', 'Python', 'BSD-2-Clause license', 'Updated Jan 24, 2021', '4', 'HTML', 'MIT license', 'Updated May 16, 2020', '17', 'Python', 'MIT license', 'Updated May 6, 2020', '7', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\nIn these labs, you will write functions to implement regression\ncalculations. You will be adding functionality to a package called\nregress431.\nLab 5 Instructions\nLab 6 Instructions\n'], 'url_profile': 'https://github.com/Cal-Poly-Advanced-R', 'info_list': ['198', 'Shell', 'Apache-2.0 license', 'Updated Mar 2, 2021', '22', 'Python', 'BSD-2-Clause license', 'Updated Jan 24, 2021', '4', 'HTML', 'MIT license', 'Updated May 16, 2020', '17', 'Python', 'MIT license', 'Updated May 6, 2020', '7', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['This code performs preference-based GP regression, inference and active query generation.\nCompanion code to RSS 2020 paper:\nE Bƒ±yƒ±k*, N Huynh*, MJ Kochenderfer, D Sadigh, ""Active Preference-Based Gaussian Process Regression for Reward Learning"", Proceedings of Robotics: Science and Systems (RSS), Corvallis, Oregon, USA, Jul. 2020.\nDependencies\nYou need to have the following libraries with Python3:\n\nNumPy\nSciPy\n\nRunning\nYou simply read test.py to understand how to use the package. For testing, just run\n\tpython test.py\nPaper citation\nIf you used this code or found it helpful, consider citing the following paper:\n@inproceedings{biyik2020active,\n  title={Active Preference-Based Gaussian Process Regression for Reward Learning},\n  author={Biyik, Erdem and Huynh, Nicolas and Kochenderfer, Mykel J. and Sadigh, Dorsa},\n  booktitle={Proceedings of Robotics: Science and Systems (RSS)},\n  year={2020},\n  month={July}\n}\n\n'], 'url_profile': 'https://github.com/Stanford-ILIAD', 'info_list': ['198', 'Shell', 'Apache-2.0 license', 'Updated Mar 2, 2021', '22', 'Python', 'BSD-2-Clause license', 'Updated Jan 24, 2021', '4', 'HTML', 'MIT license', 'Updated May 16, 2020', '17', 'Python', 'MIT license', 'Updated May 6, 2020', '7', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '192 contributions\n        in the last year', 'description': [""[Machine Learning Algorithms]\nMy journey into Machine Learning started with the essentials of Python. I gradually moved towards to concepts of advanced algorithms and, finally moved into the cores of Machine Learning. With my key focus being the live projects, I dive deeper into the fundamentals of Regression Techniques and Neural Networks enabling the essential skills required in optimizing solutions to the real-world problems. It was just a matter of some time before I actually begin building intelligent systems, working on AI algorithms and data crunching.\n** Contents**\nPart 1. Introduction to Machine Learning\n\nPython Recap\nIntermediate Python\nMachine Learning Introduction\nData Generation & Visualisation\nLinear Algebra in Python\n\nPart 2. Supervised Learning Algorithms\n\nLinear Regression\nLocally Weighted Regression\nMultivariate Regression\nLogistic Regression\nK-Nearest Neighbours\nNaive Bayes\nSupport Vector Machines\nDecision Trees & Random Forests\n\nPart 3. Unsupervised Learning\n\nK-Means\nPrincipal Component Analysis\nAutoencoders(Deep Learning)\nGenerative Adversial Networks(Deep Learning)\n\nPart 4. Deep Learning\n\nDeep Learning Fundamentals\nKeras Framework, Tensorflow Basics\nNeural Networks Basics\nBuilding Text & Image Pipelines\nMultilayer Perceptrons\nOptimizers, Loss Functions\n\nPart 5. Deep Learning in Computer Vision\n\nConvolution Neural Networks\nImage Classification Pipeline\nAlexnet, VGG, Resnet, Inception\nTransfer Learning & Fine Tuning\n\nPart 6. Deep Learning Natural Language Processing\n\nSequence Models\nRecurrent Neural Networks\nLSTM Based Models\nTransfer Learning\nNatural Lang Processing\nWord Embeddings\nLangauge Models\n\nPart 7. Reinforcement Learning\n\nBasics of Reinforcement Learning\nQ Learning\nBuilding AI for Games\n\nLibraries, Frameworks used\n\nMost of the codes are build from scratch using-\nthe following libraries.\n\n\nPandas (Data Handling)\nMatplotlib (Data Visualisation)\nNumpy (Maths)\nKeras (Deep learning)\nTensorflow(Introduction)\nSci-kit Learn(ML Algorithms)\nOpenAI Gym (Reinforcement Learning)\n\n20 Mini Projects completed!\n\n\nHardwork Pays Off (Regression Prediction)\nAir Quality Prediction (Multivariate Regression)\nSeparating Chemicals (Logistic Regression)\nFace Recognition (OpenCV, K-Nearest Neighbours)\nHandwritten Digits Classifier\nNaive Bayes Mushroom Classification\nMovie Review Prediction (Naive Bayes, LSTM etc)\nImage Dominant Color Extraction (K-Means)\nImage Classification using SVM\nTitanic Survivor Prediction using Decision Trees\nDiabetic Patients Classification\nNon-Linear Data Separation using MLP\nPokemon Classification using CNN, Transfer Learning\nSentiment Analysis using MLP, LSTM\nText/Lyrics Generation using Markov Chains\nEmoji Prediction using Transfer Learning & LSTM\nOdd One Out (Word2Vec)\nBollywood Word Analgoies (Word Embeddings)\nGenerating Cartoon Avatars using GAN's (Generative Adversial Networks)\nReinforcement Learning based Cartpole Game Player\n\nFinal Project\nImage Captioning\nGenerating Captions for images using CNN & LSTM on Flickr8K dataset.\n""], 'url_profile': 'https://github.com/ShubhikaBhardwaj', 'info_list': ['198', 'Shell', 'Apache-2.0 license', 'Updated Mar 2, 2021', '22', 'Python', 'BSD-2-Clause license', 'Updated Jan 24, 2021', '4', 'HTML', 'MIT license', 'Updated May 16, 2020', '17', 'Python', 'MIT license', 'Updated May 6, 2020', '7', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['198', 'Shell', 'Apache-2.0 license', 'Updated Mar 2, 2021', '22', 'Python', 'BSD-2-Clause license', 'Updated Jan 24, 2021', '4', 'HTML', 'MIT license', 'Updated May 16, 2020', '17', 'Python', 'MIT license', 'Updated May 6, 2020', '7', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['198', 'Shell', 'Apache-2.0 license', 'Updated Mar 2, 2021', '22', 'Python', 'BSD-2-Clause license', 'Updated Jan 24, 2021', '4', 'HTML', 'MIT license', 'Updated May 16, 2020', '17', 'Python', 'MIT license', 'Updated May 6, 2020', '7', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '294 contributions\n        in the last year', 'description': ['Regression\nThis is a repositary consisting of the Python codes to build different types of Regression models to evaluate and predict various results.\n'], 'url_profile': 'https://github.com/mk-gurucharan', 'info_list': ['198', 'Shell', 'Apache-2.0 license', 'Updated Mar 2, 2021', '22', 'Python', 'BSD-2-Clause license', 'Updated Jan 24, 2021', '4', 'HTML', 'MIT license', 'Updated May 16, 2020', '17', 'Python', 'MIT license', 'Updated May 6, 2020', '7', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['198', 'Shell', 'Apache-2.0 license', 'Updated Mar 2, 2021', '22', 'Python', 'BSD-2-Clause license', 'Updated Jan 24, 2021', '4', 'HTML', 'MIT license', 'Updated May 16, 2020', '17', 'Python', 'MIT license', 'Updated May 6, 2020', '7', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q‚ÄìQ (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['198', 'Shell', 'Apache-2.0 license', 'Updated Mar 2, 2021', '22', 'Python', 'BSD-2-Clause license', 'Updated Jan 24, 2021', '4', 'HTML', 'MIT license', 'Updated May 16, 2020', '17', 'Python', 'MIT license', 'Updated May 6, 2020', '7', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '3', 'Jupyter Notebook', 'Updated Jul 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': [""py4linear-regression\nLinear Regression Python Library\nGetting Started\nThis project is simply implementation of linear regression algorithm in python programming language.\nPrerequisites\nNumpy\nInstalling\nThe easiest way to install py4linear-regression is using pip\npip install py4linear-regression\n\nUsage\nThere is 2 public method of Linear Regression class. It is learn and predict method, learn method takes 5 argument namely x_train, t_train, alpha, and epoch. It is the training data, it's label, learning rate, and number of iteration respectively. predict method takes 1 argument namely x_test. It is the data to be predicted\nfrom py4linear_regression.regression import linear_regression\nx_train = [[0,0],[0,1],[1,0],[1,1]]\nt_train = [0,1,2,3]\nclassifier = linear_regression()\nclassifier.learn(x_train,t_train,0.01,250)\nx_test = [[0.01,0.99],[0.99,0.01]]\ny = classifier.predict(x_test)\n\n""], 'url_profile': 'https://github.com/luthfi118', 'info_list': ['5', 'Python', 'Updated May 14, 2020', '5', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Python', 'MIT license', 'Updated May 14, 2020', '4', 'Jupyter Notebook', 'Updated May 6, 2020', '4', 'Python', 'Updated May 14, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Updated May 4, 2020', 'Updated May 5, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 7, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Trading_Strategy_S&P500\nTrading Strategy on S&P500 with different method (Linear Regression, XGBOOST, LSTM, HMM\n'], 'url_profile': 'https://github.com/nikkizhao1202', 'info_list': ['5', 'Python', 'Updated May 14, 2020', '5', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Python', 'MIT license', 'Updated May 14, 2020', '4', 'Jupyter Notebook', 'Updated May 6, 2020', '4', 'Python', 'Updated May 14, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Updated May 4, 2020', 'Updated May 5, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '189 contributions\n        in the last year', 'description': ['PREDICT_CORPUS\n***** New May 4th, 2020: Linear/polynominal regression, random forest, feature selection *****\n***** New May 4th, 2020: Deep learning, LSTM *****\n***** New May 15th, 2020: SVR regression *****\nOur goal is to gather useful methods to make better prediction.\nNote that you can download all source code, but any effect and results have not been guaranteed.\nDisclaimer\nThis is not an official product.\nContact information\nFor help or issues using predict_corpus, please submit a GitHub issue.\nFor personal communication related to PREDICT_CORPUS, please contact Jack yin\n(yinzhiwu@126.com).\n'], 'url_profile': 'https://github.com/jackyin68', 'info_list': ['5', 'Python', 'Updated May 14, 2020', '5', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Python', 'MIT license', 'Updated May 14, 2020', '4', 'Jupyter Notebook', 'Updated May 6, 2020', '4', 'Python', 'Updated May 14, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Updated May 4, 2020', 'Updated May 5, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['Cement-Strength-Polynomial-Regression\nHere, we use Polynomial Regression in order to predict the concrete strength based on parameters like cement,ash,slag etc..,\nInstallation\nRequiremenmts\n\nPython 3.3+ or Python 2.7\nscikit learn\nnumpy\npandas\nmatplotlib\n\nInstall the libraries from the pypi.org with the following commands.\npip install scikit-learn\npip install numpy\npip install pandas\npip install matplotlib\n\nPlease do refer stackoverflow for any errors during installation.\n'], 'url_profile': 'https://github.com/dharineeshramtp2000', 'info_list': ['5', 'Python', 'Updated May 14, 2020', '5', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Python', 'MIT license', 'Updated May 14, 2020', '4', 'Jupyter Notebook', 'Updated May 6, 2020', '4', 'Python', 'Updated May 14, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Updated May 4, 2020', 'Updated May 5, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': [""py4logistic-regression\nLogistic Regression Python Library\nGetting Started\nThis project is simply implementation of logistic regression algorithm in python programming language.\nPrerequisites\nNumpy\nInstalling\nThe easiest way to install py4logistic-regression is using pip\npip install py4logistic-regression\n\nUsage\nThere is 2 public method of Logistic Regression class. It is learn and predict method, learn method takes 5 argument namely x_train, t_train, alpha, and epoch. It is the training data, it's label, learning rate, and number of iteration respectively. predict method takes 1 argument namely x_test. It is the data to be predicted\nfrom py4logistic_regression.regression import logistic_regression\nx_train = [[0,0],[0,1],[1,0],[1,1]]\nt_train = [0,0,0,1]\nclassifier = logistic_regression()\nclassifier.learn(x_train,t_train,0.1,50)\nx_test = [[0.02,0.25],[0.97,0.89]]\ny = classifier.predict(x_test)\n\n""], 'url_profile': 'https://github.com/luthfi118', 'info_list': ['5', 'Python', 'Updated May 14, 2020', '5', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Python', 'MIT license', 'Updated May 14, 2020', '4', 'Jupyter Notebook', 'Updated May 6, 2020', '4', 'Python', 'Updated May 14, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Updated May 4, 2020', 'Updated May 5, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '158 contributions\n        in the last year', 'description': ['polynomial-regression\nIn this repository we will see how polynomial regression works with the same insurance example.In this we use sgd\n(Stochastic Gradient Descent) method. we can also use the ols(Ordinary least squares) method, but both will give same results!!!\nTo run the code follow the below steps:\n1.Install python(3.6+) and need packages.\npip install numpy\n\npip instll pandas\n\npip install matplotlib\n\npip install -U scikit-learn\n\npip install seaborn\n\n2.Clone this repository .\nhttps://github.com/karthikeyanthanigai/polynomial-regression.git\n\n3.Open command line and set the directory to the cloned repository.\ncd polynomial-regression\n\n4.Enter the command.\npython poly.py\n\nif you got any error in install the packages then refer Stackoverflow.\n'], 'url_profile': 'https://github.com/karthikeyanthanigai', 'info_list': ['5', 'Python', 'Updated May 14, 2020', '5', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Python', 'MIT license', 'Updated May 14, 2020', '4', 'Jupyter Notebook', 'Updated May 6, 2020', '4', 'Python', 'Updated May 14, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Updated May 4, 2020', 'Updated May 5, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Regression\nmy first assignment on regression analysis\n'], 'url_profile': 'https://github.com/mrunalini26', 'info_list': ['5', 'Python', 'Updated May 14, 2020', '5', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Python', 'MIT license', 'Updated May 14, 2020', '4', 'Jupyter Notebook', 'Updated May 6, 2020', '4', 'Python', 'Updated May 14, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Updated May 4, 2020', 'Updated May 5, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ranjithpr1984', 'info_list': ['5', 'Python', 'Updated May 14, 2020', '5', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Python', 'MIT license', 'Updated May 14, 2020', '4', 'Jupyter Notebook', 'Updated May 6, 2020', '4', 'Python', 'Updated May 14, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Updated May 4, 2020', 'Updated May 5, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['Polynomial-Regression-Boston-Housing-Prices\nIn order to improve the the predictions of the Boston Housing Dataset, we use the Poly Regression.\nInstallation\nRequiremenmts\n\nPython 3.3+ or Python 2.7\nscikit learn\nnumpy\npandas\nmatplotlib\n\nInstall the libraries from the pypi.org with the following commands.\npip install scikit-learn\npip install numpy\npip install pandas\npip install matplotlib\n\nPlease do refer stackoverflow for any errors during installation.\n'], 'url_profile': 'https://github.com/dharineeshramtp2000', 'info_list': ['5', 'Python', 'Updated May 14, 2020', '5', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Python', 'MIT license', 'Updated May 14, 2020', '4', 'Jupyter Notebook', 'Updated May 6, 2020', '4', 'Python', 'Updated May 14, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Updated May 4, 2020', 'Updated May 5, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/joey-handsome', 'info_list': ['5', 'Python', 'Updated May 14, 2020', '5', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Python', 'MIT license', 'Updated May 14, 2020', '4', 'Jupyter Notebook', 'Updated May 6, 2020', '4', 'Python', 'Updated May 14, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Updated May 4, 2020', 'Updated May 5, 2020', '4', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/maverick199', 'info_list': ['Jupyter Notebook', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Dec 3, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['Logistic-Regression-Fish-Species-Predictor\nLogistic Regression, though  by name is not a regression model. It performs classification. It must be known that this Algorithm is very important and has a separate felxibility though its an old one. Today everyone has moved towards Deep Learning, but it must be known that each neural unit performs the job of a Logistic Regression. Hence today we have taken a Fish species dataset from kaggle to perform a simple Logistic Regression.\nInstallation\nRequiremenmts\n\nPython 3.3+ or Python 2.7\nscikit learn\nnumpy\npandas\nmatplotlib\n\nInstall the libraries from the pypi.org with the following commands.\npip install scikit-learn\npip install numpy\npip install pandas\npip install matplotlib\n\nPlease do refer stackoverflow for any errors during installation.\n'], 'url_profile': 'https://github.com/dharineeshramtp2000', 'info_list': ['Jupyter Notebook', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Dec 3, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'bangalore', 'stats_list': [], 'contributions': '177 contributions\n        in the last year', 'description': ['Regression\nRegression Models (Machine Learning)\n'], 'url_profile': 'https://github.com/Abhi-T', 'info_list': ['Jupyter Notebook', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Dec 3, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/knockwcs', 'info_list': ['Jupyter Notebook', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Dec 3, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'Yangon', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': [""Mean Squared Error and R2 Score\nR2 score, and mean square error using scikit-learn.\nMean Squared Error, MAE takes the average of this error from every sample in a dataset and gives the output.\nR squared value lies between 0 and 1 where 0 indicates that this model doesn't fit the given data and 1 indicates that the model fits perfectly to the dataset provided.\n""], 'url_profile': 'https://github.com/theingithetthetzaw', 'info_list': ['Jupyter Notebook', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Dec 3, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'Denver', 'stats_list': [], 'contributions': '146 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/vipin1211', 'info_list': ['Jupyter Notebook', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Dec 3, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/damini31', 'info_list': ['Jupyter Notebook', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Dec 3, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SuumCuique', 'info_list': ['Jupyter Notebook', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Dec 3, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yashdadeech1703', 'info_list': ['Jupyter Notebook', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Dec 3, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'Ghaziabad', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vik-sin', 'info_list': ['Jupyter Notebook', 'Updated May 24, 2020', '4', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Dec 3, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Python', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 7, 2020']}"
"{'location': 'Indore, Madhya Pradesh', 'stats_list': [], 'contributions': '126 contributions\n        in the last year', 'description': ['ChatBot\nThis is a Linear Regression pattern based chatbot and it is my initial practice towards machine learning....... GG\nIf you wanted to get more responses from Chatbot...\nyou can write your own pattern, tags, responses in intents.json\nand retrain model\nwhenever you wanted to retrain model be sure to delete ""model.pickle"" and ""data.pickle"" everytime you make changes in intents.json\nRead howto.md file\n'], 'url_profile': 'https://github.com/Ayushbendwal3', 'info_list': ['1', 'Python', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Updated May 10, 2020', '2', 'R', 'Updated Aug 13, 2020', '2', 'Jupyter Notebook', 'Updated May 9, 2020', '2', 'Python', 'Updated May 10, 2020', '4', 'Jupyter Notebook', 'Updated May 16, 2020', '3', 'Jupyter Notebook', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['Module 2 Final Project\nOutline\n\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ README.md                  <- The project layout (this file)\n‚îú‚îÄ‚îÄ zippedData                 <- This is where the project data is located\n‚îÇ\n‚îú‚îÄ‚îÄ student.ipynb              <- This is the main notebook (contains the python code)\n‚îÇ\n‚îú‚îÄ‚îÄ reports                    <- Reports and presentations\n‚îÇ   ‚îî‚îÄ‚îÄ presentation.pdf       <- Non-technical presentation\n‚îÇ\n‚îú‚îÄ‚îÄ images                     <- Where the graphs are saved\n‚îú‚îÄ‚îÄ requirements.txt           <- The requirements file for reproducing the analysis environment\n\nConclusion\nDoes renovation raise the value of a house?\n\n\nAccording to the Price vs. Was Renovated bar graph, renovation alone does slightly increase the average price of a house. However, the increase is not substantial enough to definitively support renovation before selling a house, unless that renovation increases the condition rating of the house.\nDoes the number of bathrooms and/or bedrooms affect the value of a house?\n\n\nAccording to the Price vs. No. of Bathrooms bar graph, the greater the number of bathrooms in a house, the higher the price. Therefore, if the house is being sold, it may be a good idea to add bathrooms to increase the value of the house. According to the Price vs. No. of Bedrooms bar graph, the greater the number of bedrooms (up until 8), the greater the price. So adding bedrooms could also increase the value of the house if it is being sold.\nIs the living sqft of a house correlated with its price?\n\nAccording to the Price vs. Sqft Living scatter plot, there is a positive correlation between the living sqft of a house and its price.\nDoes being on the water increase the value of a house?\n\nAccording to the Price vs. Waterfront bar graph, a house on the water has a higher average price than a house that is not on the water. However, there is not a significant difference between them.\nLinear Regression Model:\nModel 2 was the best model with the highest R^2 value, the coefficient of determination, with a value of 0.819681. That means that there is 81.9681% less variation around the best fit line than the mean. In other words, the model explains 81.9681% of the variation in the data.\n\nIn addition, model 2 had the coefficients in the above dataframe, with sqft_above having the highest coefficient of 0.553068 after scaling.\nModel 2 contained the following features: bedrooms, bathrooms, sqft_lot, floors, waterfront, view, condition, grade, sqft_above, yr_built, sqft_living15, sqft_lot15, sqft_basement, was_renovated, and zipcode.\nOneHotEncoding was applied to zipcodes since it is a nominal variable and thus has no order. Ordinal features were left as integers since they have a clear order.\nRecommendations\nBased on the above information, the number of bathrooms, bedrooms, amount of living space/sqft, the condition rating, and having a waterfront all affect the value of a house. However, only the first five can be utilized to increase the value of a current house before a sale. Therefore, to maximize the value of a house, renovation may increase the value of the house, especially if it increases the condition rating. Furthermore, if the house has less than 8 bedrooms, consider adding more. Also, adding bathrooms may increase the value of the house. These would also increase the sqft living of the house as well, which has a positive correlation with the price of the house.\n'], 'url_profile': 'https://github.com/luaymatalka', 'info_list': ['1', 'Python', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Updated May 10, 2020', '2', 'R', 'Updated Aug 13, 2020', '2', 'Jupyter Notebook', 'Updated May 9, 2020', '2', 'Python', 'Updated May 10, 2020', '4', 'Jupyter Notebook', 'Updated May 16, 2020', '3', 'Jupyter Notebook', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mitulkumarahirwal', 'info_list': ['1', 'Python', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Updated May 10, 2020', '2', 'R', 'Updated Aug 13, 2020', '2', 'Jupyter Notebook', 'Updated May 9, 2020', '2', 'Python', 'Updated May 10, 2020', '4', 'Jupyter Notebook', 'Updated May 16, 2020', '3', 'Jupyter Notebook', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'South Carolina, USA', 'stats_list': [], 'contributions': '234 contributions\n        in the last year', 'description': ['COVID-19 Case and Death in USA Projection\nAuthor: Tahmidul Islam\nApplication of Gaussian Process Regression with Richard‚Äôs Growth Curve Prior\nThis project aims at building a forecast model for cumulative COVID-19 cases and deaths in USA. To view the model and the projections visit:\nhttps://tahmid-usc.github.io/covidGP/\nThe folder prediction contains the CSV files of the projections.\n'], 'url_profile': 'https://github.com/tahmid-usc', 'info_list': ['1', 'Python', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Updated May 10, 2020', '2', 'R', 'Updated Aug 13, 2020', '2', 'Jupyter Notebook', 'Updated May 9, 2020', '2', 'Python', 'Updated May 10, 2020', '4', 'Jupyter Notebook', 'Updated May 16, 2020', '3', 'Jupyter Notebook', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'San Francisco,California', 'stats_list': [], 'contributions': '172 contributions\n        in the last year', 'description': ['Logistic-Regression-Cat-Classifier\nLogistic Regression with a Neural Network mindset : Cat Classifier\nPlease refer the Logistic_Regression_with_a_Neural_Network_mindset_v6a.ipynb for details.\n'], 'url_profile': 'https://github.com/SonaliSuri', 'info_list': ['1', 'Python', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Updated May 10, 2020', '2', 'R', 'Updated Aug 13, 2020', '2', 'Jupyter Notebook', 'Updated May 9, 2020', '2', 'Python', 'Updated May 10, 2020', '4', 'Jupyter Notebook', 'Updated May 16, 2020', '3', 'Jupyter Notebook', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'Banglore, India', 'stats_list': [], 'contributions': '318 contributions\n        in the last year', 'description': ['laptop-price-predictor\nHelps to predict laptop price using logistic regression.\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist.\nThe price is predicted by inputing features like graphics size,hard disk size, core generation etc.\nThe dataset of laptop pricing data is available.\nNB: We have taken the price of laptops having intel cores only!\nThanks to https://www.amazon.in/ for pricing data.\nInstagram: https://www.instagram.com/_andrewgeeks/\nTwitter: https://twitter.com/andrewissac20\n'], 'url_profile': 'https://github.com/andrew-geeks', 'info_list': ['1', 'Python', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Updated May 10, 2020', '2', 'R', 'Updated Aug 13, 2020', '2', 'Jupyter Notebook', 'Updated May 9, 2020', '2', 'Python', 'Updated May 10, 2020', '4', 'Jupyter Notebook', 'Updated May 16, 2020', '3', 'Jupyter Notebook', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '158 contributions\n        in the last year', 'description': ['Multiple linear regression\nIn this repository we will see what is this car price prediction problem parameters/variable values and try to fit this values in a multiple linear regression ..In this we will see both ols (Ordinary least squares) method and sgd\n(Stochastic Gradient Descent) method.\n\nTo run the code follow the below steps:\n1.Install python(3.6+) and need packages.\npip install numpy\n\npip instll pandas\n\npip install matplotlib\n\npip install -U scikit-learn\n\npip install seaborn\n\n2.Clone this repository .\nhttps://github.com/karthikeyanthanigai/Multiple-linear-regression-for-car-price-prediction.git\n\n3.Open command line and set the directory to the cloned repository.\ncd Multiple-linear-regression-for-car-price-prediction\n\n4.Enter the command.\npython multi.py\n\nif you got any error in install the packages then refer Stackoverflow.\nThe Prediction rate can be increased in other Regression models with Boosting and other methods/Algorithms.\n'], 'url_profile': 'https://github.com/karthikeyanthanigai', 'info_list': ['1', 'Python', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Updated May 10, 2020', '2', 'R', 'Updated Aug 13, 2020', '2', 'Jupyter Notebook', 'Updated May 9, 2020', '2', 'Python', 'Updated May 10, 2020', '4', 'Jupyter Notebook', 'Updated May 16, 2020', '3', 'Jupyter Notebook', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Chaitanyakaul97', 'info_list': ['1', 'Python', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Updated May 10, 2020', '2', 'R', 'Updated Aug 13, 2020', '2', 'Jupyter Notebook', 'Updated May 9, 2020', '2', 'Python', 'Updated May 10, 2020', '4', 'Jupyter Notebook', 'Updated May 16, 2020', '3', 'Jupyter Notebook', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['Iris-Flower-Type-Prediction-Logistic-Regression-\nPredicting the  flower type of the Iris plant species using Logistic Regression\n\nInstallation\nRequiremenmts\n\nPython 3.3+ or Python 2.7\nscikit learn\nnumpy\npandas\nmatplotlib\n\nInstall the libraries from the pypi.org with the following commands.\npip install scikit-learn\npip install numpy\npip install pandas\npip install matplotlib\n\nPlease do refer stackoverflow for any errors during installation.\n'], 'url_profile': 'https://github.com/dharineeshramtp2000', 'info_list': ['1', 'Python', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Updated May 10, 2020', '2', 'R', 'Updated Aug 13, 2020', '2', 'Jupyter Notebook', 'Updated May 9, 2020', '2', 'Python', 'Updated May 10, 2020', '4', 'Jupyter Notebook', 'Updated May 16, 2020', '3', 'Jupyter Notebook', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '158 contributions\n        in the last year', 'description': ['Breast-cancer-Prediction-Logistic-Regression\nIn this repository we will see what is this Breast cancer (benign or malignant) prediction problem parameters/variable values and try to fit this values in a Logistic regression.\nTo run the code follow the below steps:\n1.Install python(3.6+) and need packages.\npip install numpy\n\npip instll pandas\n\npip install matplotlib\n\npip install -U scikit-learn\n\n2.Clone this repository .\nhttps://github.com/karthikeyanthanigai/Breast-cancer-Prediction-Logistic-Regression.git\n\n3.Open command line and set the directory to the cloned repository.\ncd Breast-cancer-Prediction-Logistic-Regression\n\n4.Enter the command.\npython log.py\n\nif you got any error in install the packages then refer Stackoverflow.\n'], 'url_profile': 'https://github.com/karthikeyanthanigai', 'info_list': ['1', 'Python', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Updated May 10, 2020', '2', 'R', 'Updated Aug 13, 2020', '2', 'Jupyter Notebook', 'Updated May 9, 2020', '2', 'Python', 'Updated May 10, 2020', '4', 'Jupyter Notebook', 'Updated May 16, 2020', '3', 'Jupyter Notebook', 'Updated May 9, 2020', '3', 'Jupyter Notebook', 'Updated May 8, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020']}"
"{'location': 'Boston, MA', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Titanic-Predict-Survival\nLogistic Regression\nThis project uses Logistic Regression model to\n\npredict survival\nwhich variables were significant to predicting survival\nwhich variables had positive and negative impacts on survival (of the significant ones).\n\n'], 'url_profile': 'https://github.com/sirishakompella', 'info_list': ['R', 'Updated May 6, 2020', 'Rich Text Format', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Python', 'Updated Nov 30, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Python', 'MIT license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['deeplearning\nlinear regression\n'], 'url_profile': 'https://github.com/saicharan1312', 'info_list': ['R', 'Updated May 6, 2020', 'Rich Text Format', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Python', 'Updated Nov 30, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Python', 'MIT license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['LR_ML\nLinear regression\n'], 'url_profile': 'https://github.com/Benny1117Yen', 'info_list': ['R', 'Updated May 6, 2020', 'Rich Text Format', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Python', 'Updated Nov 30, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Python', 'MIT license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Bangalore, Karnataka', 'stats_list': [], 'contributions': '187 contributions\n        in the last year', 'description': ['COVID-19\nRegression Analysis of data\nCOVID-19\nEver since its outbreak in the city of Wuhan in China, COVID-19 has spread around the world at a rapid rate and has now been identified in over 187 countries. To combat the spread of the virus and to isolate the affected, one of the most significant measures taken by various countries around the world is to enforce a complete lock-down across the nation. This project aims to examine the effectiveness of the one such lock-down enforced in India by the Government on the 24th of March 2020 in order to curb the spread of COVID-19. In order to be able to estimate the effectiveness, an approach of Regression Analysis, which is a sub-branch of Predictive Analytics has been applied. Upon following this approach, it is observed that the 41-day long lock-down has been highly effective in limiting the total number of COVID-19 positive cases in India.\n'], 'url_profile': 'https://github.com/ananyakr99', 'info_list': ['R', 'Updated May 6, 2020', 'Rich Text Format', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Python', 'Updated Nov 30, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Python', 'MIT license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Logistic-Regression-Project\nLogistic Regression\n'], 'url_profile': 'https://github.com/ire-mide1', 'info_list': ['R', 'Updated May 6, 2020', 'Rich Text Format', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Python', 'Updated Nov 30, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Python', 'MIT license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '220 contributions\n        in the last year', 'description': ['Logistic\nLogistic regression\n'], 'url_profile': 'https://github.com/iqbalamo93', 'info_list': ['R', 'Updated May 6, 2020', 'Rich Text Format', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Python', 'Updated Nov 30, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Python', 'MIT license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'JAIPUR', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Simple_Linear_Regression\nThis Machine Learning Model to describe you to how a simple linear regression work and how to find accuracy and many plots within.\n'], 'url_profile': 'https://github.com/kushwahvikram15', 'info_list': ['R', 'Updated May 6, 2020', 'Rich Text Format', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Python', 'Updated Nov 30, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Python', 'MIT license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Link√∂ping, Sweden', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Probabilistic Regression using CGANs\nThis repository contains code and datasets from the master thesis: Probabilistic Regression using Conditional Generative Adversarial Networks.\nThe project includes Conditional GANs (CGANs) used for low-dimensional probabilistic regression problems.\nAuthor: Joel Oskarsson, Link√∂ping University\n\n\n\n\n\nContent\nThe repository includes pytorch implementations of the following models:\n\nStandard Conditional GANs (Mirza et al.)\nConditional f-GANs (based on Nowozin et al.)\nGenerative Moment Matching Networks (Li et al.) extended with conditioning\nMixture Density Networks (Christopher Bishop)\nHomoskedastic and heteroskedastic regression using neural networks\nDeep Conditional Target Densities (Gustafsson et al.)\n\nThe repository also contains a number of synthetic datasets used for experiments in the thesis.\nMultiple evaluation methods are implemented, including KDE-based log-likelihood and evaluation of CGANs using divergences proposed for training (Im et al.).\nRunning the Code\nThe project uses Python 3.7.\nAll required python packages are listed with version numbers in requirements.txt.\nThe easiest way to install these is:\npip install -r requirements.txt\n\nYou probably want to use a virtual python environment, for example through virtualenv or conda.\nWeights and Biases Integration\nThe project is fully integrated with Weights & Biases (W&B) for logging and visualization, but can just as easily be used without it.\nWhen W&B is used, training configuration, training/test statistics and plots are sent to the W&B servers and made available in an interactive web interface.\nIf W&B is turned off, logging instead saves everything locally to a directory like wandb/dryrun-2020....\nThe W&B python package is installed together with the rest in requirements.txt.\nThe W&B project name is set to cgan_regression, but this can be changed in the first line of constants.py.\nSee the W&B documentation for details.\nIf you would like to login and use W&B, run:\nwandb login\n\nIf you would like to turn off W&B and just log things locally, run:\nwandb off\n\nGenerating or Pre-processing Datasets\nSynthetic datasets used in the thesis are directly available in the datasets directory.\nIf you would like to regenerate one of these, run:\npython preprocess_dataset.py --dataset dataset_name\n\nReal world datasets are not directly available in this repository.\nInstructions for where to download each real dataset can be found as a comment in corresponding specification file, dataset_specifications/dataset_name.py.\nTo pre-process and save one of these in datasets, run:\npython preprocess_dataset.py --dataset dataset_name --file path/to/dataset/file\n\nNote that the housing dataset does not require a file path since the data is directly downloaded through sci-kit learn.\nIncluding new datasets should be straightforward, starting from one of the existing specifications (for example dataset_specifications/power.py for a real world dataset).\nEach dataset must also be listed in dataset_list.py.\nConfiguring a Training or Test Run\nTraining and testing is done by running the main.py script with different arguments.\nWhile it is possible to specify everything directly on the command line, it is far more convenient to create a JSON configuration file config.json and tell the script to read arguments from that:\npython main.py --config config.json\n\nTo get a list of all available arguments run:\npython main.py --help\n\nThey can also be found in main.py grouped by model.\nBinary arguments that only turn features on or off are set using the integers 0/1.\nSome of the most important arguments are:\n\ntrain Set to 1 or 0 for whether the model should be trained or not.\ntest Set to 1 or 0 for whether the model should be evaluated on test data.\ndataset Name of the dataset to train/evaluate on. Needs to be generated/pre-processed and available in the datasets directory. See dataset_list.py for a list of all datasets.\nmodel Model to train/evaluate. See start of main.py for a list of all models.\nnetwork Neural network architecture to use, for all models except CGANs. Should correspond to one of the files in nn_specs directory, without .json.\ncgan_nets Neural network architectures to use when model is CGAN. Should correspond to one of the files in cgan_specs directory, without .json.\ncgan_type Type of CGAN to train (training objective). See list at bottom of models/cgan_versions.py for available CGAN types.\nnoise_dim Noise dimensionality for CGANs and GMMNs.\nrestore W&B id of a previous run to restore model parameters (network weights etc.) from (when W&B is used).\nrestore_file Path to a file to restore model parameters from. Can be used when W&B is turned off. The best model parameters from training are saved in the local W&B run directory.\nepochs The amount of epochs to train for. Note that only the model parameters from the best (in validation) epoch are saved. So in practice this specifies the maximum amount of epochs.\n\nExample Configs\nHere follows some examples of configuration files.\nTrain a Mixture Density Network on the bimodal dataset for 200 epochs:\n{\n""train"": 1,\n""test"": 0,\n""model"": ""mdn"",\n""dataset"": ""bimodal"",\n""network"": ""medium_nn"",\n""lr"": 1e-3,\n""optimizer"": ""adam"",\n""epochs"": 200,\n""mixture_comp"": 10,\n""l2_reg"": 1e-4\n}\nTrain a Pearson œá2 CGAN for 1000 epochs on the trajectories10 dataset:\n{\n""train"": 1,\n""test"": 0,\n""model"": ""cgan"",\n""dataset"": ""trajectories10"",\n""cgan_type"": ""pearson"",\n""cgan_nets"": ""large_ni"",\n""noise_dim"": 5,\n""batch_size"": 100,\n""lr"": 1e-4,\n""optimizer"": ""rmsprop"",\n""epochs"": 1000,\n""val_interval"": 5\n}\nEvaluate a CGAN on the exponential dataset, loading parameters from an earlier W&B run, also estimating ""divergence"" based on a Least Squares CGAN discriminator:\n{\n""train"": 0,\n""test"": 1,\n""model"": ""cgan"",\n""dataset"": ""exponential"",\n""cgan_nets"": ""medium_ni"",\n""noise_dim"": 5,\n""restore"": ""fiopdk6g"",\n""test_runs"": 10,\n""eval_div"": ""ls"",\n""eval_cgan"": ""medium_ni"",\n""scatter"": 1,\n""lr"": 1e-3,\n""optimizer"": ""adam"",\n""epochs"": 200\n}\nEvaluate a joint GMMN on the const_noise dataset, loading model parameters from a local file:\n{\n""train"": 0,\n""test"": 1,\n""model"": ""gmmn"",\n""dataset"": ""const_noise"",\n""network"": ""medium_ni"",\n""noise_dim"": 10,\n""mmd_scales"": ""1,5,10"",\n""restore_file"": ""wandb/dryrun-20200514_101433-5mqvyivb/epoch_best.pt"",\n""test_runs"": 10,\n""scatter"": 1\n}\nDetailed Repository Structure\nThis is an explanation of each file and directory in the repository:\n‚îú‚îÄ‚îÄ cgan_specs                  - Specifications of CGAN neural network architectures\n‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ datasets                    - Available datasets, the actual data\n‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ dataset_specifications      - Specifications describing different datasets\n‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ images                      - A few example images from the thesis\n‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ models\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ cgan.py                 - General CGAN model\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ cgan_versions.py        - Alternative CGAN versions (f-GANs, Least Squares, etc.)\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ cgmmn.py                - CGMMN model\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ dctd.py                 - Deep Conditional Target Densities model\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ gmmn.py                 - GMMN model\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ gp.py                   - Gaussian Process model\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ mdn.py                  - Mixture Density Network model\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ networks.py             - Neural network building blocks for other models\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ nn_heteroskedastic.py   - Heteroskedastic neural network regression model\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ nn.py                   - Base neural network model\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ nn_regressor.py         - Homoskedastic neural network regression model\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ noise_dists.py          - Noise distributions for generative models\n‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ spec_reader.py          - Network specification reader utility\n‚îú‚îÄ‚îÄ nn_specs                    - Specifications of (non CGAN) neural network architecturs\n‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ util_scripts                - Utility scripts for setting up experiments\n‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ constants.py                - Constants\n‚îú‚îÄ‚îÄ dataset_list.py             - List of avaliable dataset specifications\n‚îú‚îÄ‚îÄ evaluation.py               - Code for evaluation steps in validation/testing\n‚îú‚îÄ‚îÄ main.py                     - Main script used for training and testing\n‚îú‚îÄ‚îÄ preprocess_dataset.py       - Script for generating/pre-processing datasets from specification\n‚îú‚îÄ‚îÄ README.md                   - This readme file\n‚îú‚îÄ‚îÄ requirements.txt            - List of required python packages\n‚îú‚îÄ‚îÄ tabular_dataset.py          - Pytorch dataset class for tabular regression data\n‚îú‚îÄ‚îÄ utils.py                    - Various utility functions\n‚îî‚îÄ‚îÄ visualization.py            - Plotting functions\n\n'], 'url_profile': 'https://github.com/joeloskarsson', 'info_list': ['R', 'Updated May 6, 2020', 'Rich Text Format', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Python', 'Updated Nov 30, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Python', 'MIT license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated May 6, 2020', 'Rich Text Format', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Python', 'Updated Nov 30, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Python', 'MIT license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': [""linear-regression-model-from-scratch\nImplemented linear regression from scratch with 2 features' linear regression\n""], 'url_profile': 'https://github.com/Pooja269', 'info_list': ['R', 'Updated May 6, 2020', 'Rich Text Format', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'Python', 'Updated Nov 30, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Python', 'MIT license', 'Updated Aug 15, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['JIFF\n\nJIFF is a JavaScript library for building applications that rely on secure multi-party computation. JIFF is built to be highly flexible with a focus on usability, with the ability to be run in the browser, on mobile phones, or via Node.js. JIFF is designed so that developers need not be familiar with MPC techniques or know the details of cryptographic protocols in order to build secure applications.\nRequirements\nServer\nRunning the server requires Node and npm.\nClient\nFor browsers, we provide a bundle including the base client side library and its dependencies (libsodium-wrappers and socket.io). Extensions have to be imported separately.\nFor node.js clients, npm install should install all the required dependencies.\nInstallation\nServer\nRun npm from inside the project directory to automatically install the dependencies listed in package.json:\nnpm install\nClient - Browser\nMake sure to include the library bundle:\n<!-- exposes JIFFClient to the global scope -->\n<script src=""/dist/jiff-client.js""></script>\nThen inside a script tag (and after the page loads), initialize a JIFF object and set up a computation:\nvar instance = new JIFFClient(""http://localhost:8080"", ""<computation_id>"", parties);\nThe instance object provides methods for sharing, opening, and performing operations on shares.\nClient - node.js\nIn node.js you must include the library (either the bundle or the source) and then use it:\nvar JIFFClient = require(\'./dist/jiff-client.js\');\nvar instance = new JIFFClient(""http://localhost:8000"", ""<computation_id>"", parties);\nProject Layout\n‚îú‚îÄ demos/               Example of common jiff use-cases and functionality\n‚îú‚îÄ docs/                JSDoc config and generated docs\n‚îú‚îÄ lib/                 Libraries for both client and server-side jiff instances\n‚îÇ  ‚îú‚îÄ client/           Implementation of the client side library\n‚îÇ  ‚îú‚îÄ server/           Implementation of the server side library\n‚îÇ  ‚îú‚îÄ ext/              Extended functionality for use cases (e.g. negative numbers): Includes server and client extensions\n‚îÇ  ‚îú‚îÄ common/           Some common helpers between both client and server code\n‚îÇ  ‚îú‚îÄ jiff-client.js    Main module for the client side library, include this (or the bundle under dist/) in your projects\n‚îÇ  ‚îî‚îÄ jiff-server.js    Main module for the server side library, include this in your server code\n‚îú‚îÄ test/                Unit testing for base Jiff, demos, and extensions\n‚îÇ  ‚îú‚îÄ dev/              Limited tests for testing some features under development\n‚îÇ  ‚îú‚îÄ live/             Template and setup for live coding with JIFF with nodejs\'s command line shell (REPL)\n‚îÇ  ‚îî‚îÄ suite/            Base Jiff and extension tests (See test/suite/README.md)\n‚îú‚îÄ tutorial/            Contains interactive tutorial files that can be run locally to learn JIFF!\n\nRunning Tutorials\nClone the github repo, and run npm run tutorial inside its root directory.\nOn your terminal, you will see a list of ""Routes/Documents"". Open either document in your browser to go through the tutorial.\nEach document is an independent tutorial. However, beginners are encouraged to view them in order.\nRunning Demos and Examples\nRun a sample server from one of the demos under demos in the following way:\nnode index.js demos/<demo-name>/server  # alternative way 1\nnode demos/<demo-name>/server.js  # alternative way 2\nThe output from the example server will direct you to open localhost:8080/demos/<demo-name>/client.html in a browser (you must open\nan instance in a separate window/tab for every distinct party participating in the protocol).\nYou can then proceed with the protocol using the client interfaces.\nNote that you can run Node.js parties that can also participate in the protocol by executing (e.g., a separate terminal for each party):\nnode demos/<demo-name>/party.js <input-value>\nDocumentation\nThe latest documentation can be viewed on the project page. The documentation can be generated using JSDoc; you will find these docs in docs/jsdocs/:\n./node_modules/.bin/jsdoc -r -c docs/jsdoc.conf.json\nnpm run-script gen-docs # shortcut\nWhere to Look in the Docs\nThe documentation for the client side library is separated into the distinct modules, namespaces, and classes:\n‚îú‚îÄ modules\n‚îÇ  ‚îî‚îÄ jiff-client            Parent module: represents the exposed JIFFClient global variable\n‚îú‚îÄ classes\n‚îÇ  ‚îú‚îÄ JIFFClient             Represents a client side jiff instance including the main API of JIFF\n‚îÇ  ‚îú‚îÄ SecretShare            Contains the API for SecretShare objects\n‚îÇ  ‚îú‚îÄ GuardedSocket          Internal wrapper around socket.io for added reliability\n‚îÇ  ‚îî‚îÄ Deferred               Polyfill to construct deferred from native Promises\n‚îú‚îÄ namespaces\n‚îÇ  ‚îú‚îÄ protocols              Common protocols exposed by jiff client instances, suitable for preprocessing\n‚îÇ  ‚îú‚îÄ bits                   Primitives for operating on bit-wise shared secrets (hybrid protocols)\n‚îÇ  ‚îî‚îÄ hooks                  Available hooks that can be used by users to customize behavior\n\nRunning Tests\nAll of the JIFF library test cases can be run in the following way:\nnpm test\nDemos are accompanied by test cases. The following command can be used to run the demo servers and test cases:\nnpm run-script test-demo -- demos/<demo-name>\nThe command assumes that the server is located at demos//server.js and the test cases are located at demos//test.js\nSee demos/run-test.sh for instructions on running test cases located in different directories or with different names.\nSee the testing suite framework documentation for more details on running and creating tests for the JIFF library.\nBundling\nIf you made changes to the library and would like to bundle it again into a single browser-friendly file, you can run this command:\nnpm run-script build # will override dist/jiff-client.js\nDevelopment\nThe JIFF libraries allow developers to customize or extend their functionality by introducing new hooks. Multiple hooks can be combined to form a library extension.\nHooks\nThe JIFF client and server libraries support hooks. Hooks can be provided in the options parameter during instantiation or afterwards. Hooks allow the introduction of custom functionality to be executed at critical times during the computation, or the introduction of different implementations of specified primitives and operations (e.g. using a different sharing scheme).\nThe client-side hooks documentation provides more details. If hooks are used to provide important reusable functionality, then it is recommended to bundle these hooks within a JIFF extension.\nExtensions\nJIFF supports implementing extensions on top of the base implementations that can provide additional extended functionality. Some extensions can be found under lib/ext. Two important modules are implemented and provided in this repository: bignumbers and fixed point arithmetic.\nSee the extensions documentation and the documentation inside src/ext/jiff-client-bignumber.js for instructions on how to create additional extensions.\nBoth client and server libraries support extensions. Some extensions require customizing both the server and client libraries to behave properly (such as the bignumbers extension). Other extensions may require only server or client-side modifications (e.g., the fixed point arithmetic module is only client-side). A server that wants to participate in the computation would require only the client-side extension to use the additional functionality (unless, of course, that extension depends on additional server-side modifications as in bignumbers).\nFor examples on how to use an extension, see the following files:\n\ndemos/sum-fixed/server.js: using the server with the Node bignumber.js module.\ndemos/sum-fixed/client.html: using fixed point arithmetic extension in the browser.\n\nRun the bignumber test suite in the following way:\nnpm run-script test-bignumber\nHow to Contribute\nCheck out our contribution guidelines and resources @ contributing.\nFor Cryptographers\nSecurity Model and Assumptions\nJIFF is secure against semi-honest adversaries.\nJIFF\'s default preprocessing protocol for beaver triples generation is based on bgw. All protocols that depend on triplets/multiplication are\nsecure with an honest majority in the preprocessing phase, and against a dishonest majority in the online stage. This is important, since the parties\nperforming the preprocessing may be different than the ones carrying out the online computation.\nIf preprocessing is not used, and crypto_provider option is set to true during instance creation, JIFF will acquire all required\ncorelated randomness and preprocessing material from the server. This yields an asymetric trust model, where the computation is secure\nagainst a dishonest majority of non-server parties, but insecure against coalitions of one or more party plus the server. Conretely, this\nreduces to more traditional models in certain cases. For example, if the computation is made out of two parties and a server, this becomes\nequivalent to 3-party computation with honest majority.\nCosts of Operations: [OUTDATED]\nBelow is a table of the current costs of operations in the base JIFF without extensions:\n\n\n\nOperation\nRounds\nTotal Messages\nPreprocessing Rounds\nPreprocessing Total Messages\nDependenices\n\n\n\n\nShare\n1\nsenders * receivers\n0\n0\nN/A\n\n\nOpen\n2\nsender + sender * receivers\n1\nsenders * senders\nN/A\n\n\n+, -, c+, c-, c*\n0\n0\n0\n0\nN/A\n\n\n*\n2\n2*parties + parties*(parties-1)\n2\n2 * (parties * parties - 1)\ntriplet,open\n\n\n<, <=, >, >=\n2*(bits+3)\nO( bits * parties^2 )\n3\nbits * (2*parties + parties^2)\n*, open\n\n\nc<, c<=, c>, c>=\n2*(bits+3)\nO( bits * parties^2 )\n3\nbits * (2*parties + parties^2)\n*, open\n\n\n=, c=, !=, c!=\n2*(bits+4)\nO( bits * parties^2 )\n3\n2*bits * (2*parties + parties^2)\nc<, c>, *\n\n\n/\nbits^2 + 5*bits\nO( bits^2 * parties^2 )\n3\nbits*(2*bits * (2*parties + parties^2))\n<, c<, *\n\n\nc/\n2*(bits+3) + 5\nO( bits * parties^2 )\n3\n4 * bits * (2*parties + parties^2)\nopen, *, c<\n\n\nbits+\n8*bits\nO( parties^2 * bits )\n2\n8 * bits * (parties * parties - 1)\ntriplet,open\n\n\nbits-\n8*bits\nO( parties^2 * bits )\n2\n8 * bits * (parties * parties - 1)\ntriplet,open\n\n\nbits*\n12*bits\nO( parties^4 * bits^2 )\n2\n12 * bits^2 * (parties * parties - 1)^2\ntriplet,open\n\n\nbits/\n25*bits^2\nO( parties^2 * bits^2 )\n2\n25 * bits^2 * (parties * parties - 1)\ntriplet,open\n\n\n\nSome exact costs not shown in the table:\n\nExact total number of messages for secret inequalities is: 3*(parties + parties^2 + (bits+1) * (2*parties + parties*(parties-1))) + 2*parties + parties*(parties-1)\nExact total number of messages for constant inequalities is: 2*(parties + parties^2 + (bits+1) * (2*parties + parties*(parties-1))) + 2*parties + parties*(parties-1)\nExact total number of messages for equality checks: 2*(*(parties + parties^2 + (bits+1) * (2*parties + parties*(parties-1))) + 2*parties + parties*(parties-1)) + 2*parties + parties*(parties-1)\nExact total number of messages for division is: bits * ( 5*(parties + parties^2 + (bits+1) * (2*parties + parties*(parties-1))) + 2*parties + parties*(parties-1) + 2*parties + parties*(parties-1) )\nExact total number of messages for constant division is: 1 + 7*parties + 4*parties^2 + 8*(parties + parties^2 + (bits+1) * (2*parties + parties*(parties-1)))\n\nDependenices:\n\nMultiplication has one message to synchronize beaver triplets and one open in sequence.\ninequality tests has 3 less than half primes in parallel, each has an open and as many multiplication in sequence as bits.\nconstant inequality test has 2 less than half primes in parallel.\nequality and constant equality tests have 2 inequalities in parallel, sequenced with a multiplication.\ndivision has as many sequential iterations as bits, each iteration contains a constant inequality, secret inequality, and multiplication.\nconstant division has one open sequenced with 4 parallel constant inequality checks and two multiplications.\nSecret XORs and ORs are equivalent to a single multiplication, constant XORs and ORs are free.\n\nInformation and Collaborators\nMore information about this project, including collaborators and publications, can be found at multiparty.org.\n'], 'url_profile': 'https://github.com/caciac', 'info_list': ['JavaScript', 'MIT license', 'Updated May 9, 2020', 'HTML', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 4, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': [""Edit a file, create a new file, and clone from Bitbucket in under 2 minutes\nWhen you're done, you can delete the content in this README and update the file with details for others getting started with your repository.\nWe recommend that you open this README in another tab as you perform the tasks below. You can watch our video for a full demo of all the steps in this tutorial. Open the video in a new tab to avoid leaving Bitbucket.\n\nEdit a file\nYou‚Äôll start by editing this README file to learn how to edit a file in Bitbucket.\n\nClick Source on the left side.\nClick the README.md link from the list of files.\nClick the Edit button.\nDelete the following text: Delete this line to make a change to the README from Bitbucket.\nAfter making your change, click Commit and then Commit again in the dialog. The commit page will open and you‚Äôll see the change you just made.\nGo back to the Source page.\n\n\nCreate a file\nNext, you‚Äôll add a new file to this repository.\n\nClick the New file button at the top of the Source page.\nGive the file a filename of contributors.txt.\nEnter your name in the empty file space.\nClick Commit and then Commit again in the dialog.\nGo back to the Source page.\n\nBefore you move on, go ahead and explore the repository. You've already seen the Source page, but check out the Commits, Branches, and Settings pages.\n\nClone a repository\nUse these steps to clone from SourceTree, our client for using the repository command-line free. Cloning allows you to work on your files locally. If you don't yet have SourceTree, download and install first. If you prefer to clone from the command line, see Clone a repository.\n\nYou‚Äôll see the clone button under the Source heading. Click that button.\nNow click Check out in SourceTree. You may need to create a SourceTree account or log in.\nWhen you see the Clone New dialog in SourceTree, update the destination path and name if you‚Äôd like to and then click Clone.\nOpen the directory you just created to see your repository‚Äôs files.\n\nNow that you're more familiar with your Bitbucket repository, go ahead and add a new file locally. You can push your change back to Bitbucket with SourceTree, or you can add, commit, and push from the command line.\n""], 'url_profile': 'https://github.com/zepengxiao', 'info_list': ['JavaScript', 'MIT license', 'Updated May 9, 2020', 'HTML', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 4, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dimasuwandi', 'info_list': ['JavaScript', 'MIT license', 'Updated May 9, 2020', 'HTML', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 4, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Tunisia', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Simple-linear-regression\n'], 'url_profile': 'https://github.com/oussamadhouib', 'info_list': ['JavaScript', 'MIT license', 'Updated May 9, 2020', 'HTML', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 4, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/moooor', 'info_list': ['JavaScript', 'MIT license', 'Updated May 9, 2020', 'HTML', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 4, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Guess the date of reddits (large edition)\nGuess a reddit date based on its text.\nThis is larger version with more reddits and subrredits (topics) than in https://gonito.net/challenge/guess-reddit-date.\nOutput label is FLOAT-YEAR, a human friendly timestamp.\nFLOAT-YEAR=1970 + posix_time/(60*60*24*365.25)\nSources\nData taken from https://archive.org/details/2015_reddit_comments_corpus.\n'], 'url_profile': 'https://github.com/damlit', 'info_list': ['JavaScript', 'MIT license', 'Updated May 9, 2020', 'HTML', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 4, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': [""Reggie's Linear Regression\nCodeacademy Linear Regression Project\n""], 'url_profile': 'https://github.com/ashwinjohn3', 'info_list': ['JavaScript', 'MIT license', 'Updated May 9, 2020', 'HTML', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 4, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['-Multiple-Linear-Regression-\nMultiple Linear Regression\n'], 'url_profile': 'https://github.com/Abhilashavadhanula', 'info_list': ['JavaScript', 'MIT license', 'Updated May 9, 2020', 'HTML', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 4, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/11sleahy', 'info_list': ['JavaScript', 'MIT license', 'Updated May 9, 2020', 'HTML', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 4, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['-Multiple-Linear-Regression-Visualization\nMultiple Linear Regression  Visualisation\n'], 'url_profile': 'https://github.com/Abhilashavadhanula', 'info_list': ['JavaScript', 'MIT license', 'Updated May 9, 2020', 'HTML', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 4, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Machine-Learning\nLinear Regression model\n'], 'url_profile': 'https://github.com/sanshekh', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': [""House Prices Advanced Regression Techniques\nIn this project we will be using a Kaggle Dataset containing 79 variables holding information about residential properties sold in Iowa. We will explore the data and identify key variables, process and clean the data and engineer new features. Once ready, we will attempt to predict the sale prices held within our test set and test the performance of our algorithm using root mean squared error (RMSE) & adjusted R squared.\nAs this problem requires a regression model, RMSE is a good performance measure. RMSE indicates how closely (in absolute terms) our predicted values are to the actual sale prices and is easy to interpret as it will be in the same units as the original Sale Price. R squared represents a value, from 0 to 1, showing how much of an improvement the model is over simply taking the mean, showing us the 'goodness of fit'. We will use a variant of this, 'adjusted R squared' which will adjust for having multiple variables, especially if they contribute nothing to the model. It should show us the proportion of total variance explained by the model.\n""], 'url_profile': 'https://github.com/dave-head-13', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '193 contributions\n        in the last year', 'description': ['Linear Regresssion with PyTorch\nDataset\nThe simple cengage systolic blood pressure dataset:\nhttps://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/frame.html\nPurpose\nSince I implemented linear regression from scratch,\nthis is to see if PyTorch can make it easier\nRun verified with python 3.7 and numpy 1.18.1\npython Runner.py\nLoss Per PyTorch Step (Learning Curve)\n\nPredicted vs Actual\n\nPrediction Results over the Training set\n\n\n\nAge\nWeight\nActual Blood Pressure\nPredicted Blood Pressure\nLoss\n\n\n\n\n52.0\n173.0\n132.0\n128.0\n4.0\n\n\n59.0\n184.0\n143.0\n140.0\n3.0\n\n\n67.0\n194.0\n153.0\n153.0\n0.0\n\n\n73.0\n211.0\n162.0\n166.0\n4.0\n\n\n64.0\n196.0\n154.0\n150.0\n4.0\n\n\n74.0\n220.0\n168.0\n171.0\n3.0\n\n\n54.0\n188.0\n137.0\n137.0\n0.0\n\n\n61.0\n188.0\n149.0\n144.0\n5.0\n\n\n65.0\n207.0\n159.0\n156.0\n3.0\n\n\n46.0\n167.0\n128.0\n119.0\n9.0\n\n\n72.0\n217.0\n166.0\n168.0\n2.0\n\n\n\n'], 'url_profile': 'https://github.com/boyko11', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Gurgaon', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Multiple-linear-regression-python\nMultiple linear regression with model evaluation expalined in python\n'], 'url_profile': 'https://github.com/KumarRohan1996', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['VO2max_activity\nPractice building linear regression\nOverview\nThis activity will provide you with the opportunity to practice building multiple linear regression models using R. This activity has been adapted from an excellent resource provided on www.statistics.laerd.com. This site provides tutorials that are specific to the statistical software SPSS, however the concepts behind each of the statistical models are still relevant. I recommend checking out this site for more information on a range of different statistical concepts.\n'], 'url_profile': 'https://github.com/JCDrummond', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Portland, Oregon', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RezaAIDL77', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'mumbai', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': [""Fundamental-analysis-using-Regression\nFundamental analysis using Regression\nThis module would introduce us to the Regression related inferences to be drawn from the data.\nRegression is basically a statistical approach to find the relationship between variables. In machine learning, this is used to predict the outcome of an event based on the relationship between variables obtained from the data-set. More often than not, we utilize linear regression to come up with an ideal inference. We'd be using the regression model to solve the following problems:\nProblem Statements:\n3.1 Import the file 'gold.csv' (you will find this in the intro section to download or in '/Data/gold.csv' if you are using the jupyter notebook), which contains the data of the last 2 years price action of Indian (MCX) gold standard. Explore the dataframe. You'd see 2 unique columns - 'Pred' and 'new'. One of the 2 columns is a linear combination of the OHLC prices with varying coefficients while the other is a polynomial function of the same inputs. Also, one of the 2 columns is partially filled.\nUsing linear regression, find the coefficients of the inputs and using the same trained model, complete the entire column.\nAlso, try to fit the other column as well using a new linear regression model. Check if the predictions are accurate. Mention which column is a linear function and which is polynomial. (Hint: Plotting a histogram & distplot helps in recognizing the discrepencies in prediction, if any.)\n3.2 Import the stock of your choosing AND the Nifty index.\nUsing linear regression (OLS), calculate\nThe daily Beta value for the past 3 months. (Daily = Daily returns)\nThe monthly Beta value. (Monthly= Monthly returns)\nThen answer:\nRefrain from using the (covariance(x,y)/variance(x)) formula.\nAttempt the question using regression. (Regression Reference)\nWere the Beta values more or less than 1 ?\nWhat if it was negative ? Discuss.\nInclude a brief writeup in the bottom of your jupyter notebook with your inferences from the Beta values and regression results\nCAPM Analysis and Beta\nCalculation using regression - CAPM(Capital Asset Pricing Model) attempts to price securities by examining the relationship that exists between expected returns and risk.\nCAPM on Investopedia\nThe Beta of an asset is a measure of the sensitivity of its returns relative to a market benchmark (usually a market index). How sensitive/insensitive is the returns of an asset to the overall market returns (usually a market index like S&P 500 index). What happens when the market jumps, does the returns of the asset jump accordingly or jump somehow?\nBeta on Investopedia\n""], 'url_profile': 'https://github.com/ashu1447', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MarquesThiago', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Hyderabad,India', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harika140901', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['linear_regression\nlinear regression code\n'], 'url_profile': 'https://github.com/naveenpanickar', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 28, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}"
"{'location': 'Hyderabad,Telangana,India', 'stats_list': [], 'contributions': '219 contributions\n        in the last year', 'description': ['In this Jupyter Notebook, The Ecommerce data is used to work out an example for Linear Regression.\nIt is a simple project with a lot of Data Visualization and explantions\n'], 'url_profile': 'https://github.com/narsym', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 4, 2020', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '3', 'Jupyter Notebook', 'Updated May 11, 2020', '3', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Toronto, Ontario', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['\nmicore\nMicrobiome Covariance Regression (micore) performs covariance regression in a multinomial logistic-normal model to estimate how microbiome co-occurrence networks vary with respect to covariates.  This work was developed in the Greenwood Lab at McGill University.\nInstallation\nYou can install micore directly from Github:\nif (!require(devtools)) {\n  install.packages(""devtools"")\n  library(devtools)\n}\ninstall_github(""kevinmcgregor/micore"", dependencies=TRUE, build_vignettes = TRUE)\nVignette\nOnce you\'ve successfully installed micore, you can access the vignette by running:\nvignette(""micore-vignette"")\n'], 'url_profile': 'https://github.com/kevinmcgregor', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 4, 2020', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '3', 'Jupyter Notebook', 'Updated May 11, 2020', '3', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['linear_regression_with_reg\nLinear regression with regularization\n'], 'url_profile': 'https://github.com/wraimi', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 4, 2020', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '3', 'Jupyter Notebook', 'Updated May 11, 2020', '3', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '193 contributions\n        in the last year', 'description': ['Linear Regresssion with Tensorflow\nDataset\nThe simple cengage systolic blood pressure dataset:\nhttps://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/frame.html\nPurpose\nSince I implemented linear regression from scratch,\nthis is to see if Tensorflow can make it easier\nRun verified with python 3.7 and numpy 1.18.1\npython lin_reg.py\nLoss Per Tensorflow Step (Learning Curve)\n\nPredicted vs Actual\n\nPrediction Results over the Training set\n\n\n\nAge\nWeight\nActual Blood Pressure\nPredicted Blood Pressure\nLoss\n\n\n\n\n52.0\n173.0\n132.0\n128.0\n4.0\n\n\n59.0\n184.0\n143.0\n142.0\n1.0\n\n\n67.0\n194.0\n153.0\n159.0\n6.0\n\n\n73.0\n211.0\n162.0\n171.0\n9.0\n\n\n64.0\n196.0\n154.0\n153.0\n1.0\n\n\n74.0\n220.0\n168.0\n173.0\n5.0\n\n\n54.0\n188.0\n137.0\n132.0\n5.0\n\n\n61.0\n188.0\n149.0\n146.0\n3.0\n\n\n65.0\n207.0\n159.0\n155.0\n4.0\n\n\n46.0\n167.0\n128.0\n116.0\n12.0\n\n\n72.0\n217.0\n166.0\n169.0\n3.0\n\n\n\n'], 'url_profile': 'https://github.com/boyko11', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 4, 2020', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '3', 'Jupyter Notebook', 'Updated May 11, 2020', '3', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Buenos Aires, Argentina', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/exequielmoneva', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 4, 2020', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '3', 'Jupyter Notebook', 'Updated May 11, 2020', '3', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Bayesian Inference on Adult Income\nBayesian Regression Analysis\n'], 'url_profile': 'https://github.com/amer6693', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 4, 2020', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '3', 'Jupyter Notebook', 'Updated May 11, 2020', '3', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abhishekrao500', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 4, 2020', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '3', 'Jupyter Notebook', 'Updated May 11, 2020', '3', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '158 contributions\n        in the last year', 'description': [""Simple-linear-regression-ols-vs-sgd\nIn this repository we will see the basic different in ols(Ordinary Least Squares) method sdg(Stochastic Gradient Descent) method for a simple linear regression.\nGenerally in simple linear regression only one indepent variable(x) will Determine the dependent variable(y) in which we have 2 methods to predict/make the best fit line to the regression that is Ordinary Least Squares and the other one is Stochastic Gradient Descent.\nwith this method's we will draw a good/best fit line to the dataset and find out which method is the best.\nTo run the code follow the below steps:\n1.Install python(3.6+) and need packages.\npip install numpy\n\npip instll pandas\n\npip install matplotlib\n\npip install -U scikit-learn\n\n2.Clone this repository .\nhttps://github.com/karthikeyanthanigai/Simple-linear-regression-ols-vs-sgd-\n\n3.Open command line and set the directory to the cloned repository.\ncd Simple-linear-regression-ols-vs-sgd-\n\n4.Enter the command.\npython simple_linear_reg.py\n\nif you got any error in install the packages then refer Stackoverflow.\n""], 'url_profile': 'https://github.com/karthikeyanthanigai', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 4, 2020', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '3', 'Jupyter Notebook', 'Updated May 11, 2020', '3', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['House Price Prediction(Regression) with Tensorflow - Keras\n\n\nTensorflow and Keras\nTensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.\nKeras is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, R, Theano, or PlaidML. Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible.\nDataset\nThe data contains information from the 1990 California census. The columns are as follows:\nlongitude: A measure of how far west a house is; a higher value is farther west\nlatitude: A measure of how far north a house is; a higher value is farther north\nhousingMedianAge: Median age of a house within a block; a lower number is a newer building\ntotalRooms: Total number of rooms within a block\ntotalBedrooms: Total number of bedrooms within a block\npopulation: Total number of people residing within a block\nhouseholds: Total number of households, a group of people residing within a home unit, for a block\nmedianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\nmedianHouseValue: Median house value for households within a block (measured in US Dollars)\noceanProximity: Location of the house w.r.t ocean/sea\nData Source: California Housing Prices Dataset\n\n'], 'url_profile': 'https://github.com/Harshita9511', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 4, 2020', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '3', 'Jupyter Notebook', 'Updated May 11, 2020', '3', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saloni123-S', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 4, 2020', 'Updated Jul 20, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '3', 'Jupyter Notebook', 'Updated May 11, 2020', '3', 'Jupyter Notebook', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}"
"{'location': 'Hyderabad', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nSimple Linear Regression\n'], 'url_profile': 'https://github.com/Abhilashavadhanula', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'JavaScript', 'Updated Jul 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '235 contributions\n        in the last year', 'description': ['A telecommunications company is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is leaving.\nWe need to find out who is leaving and why.\n'], 'url_profile': 'https://github.com/golpiraelmi', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'JavaScript', 'Updated Jul 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['SAT-VS-GAT\nPERFORMIMG SIMPLE LINEAR REGRESSION\n'], 'url_profile': 'https://github.com/mp20mp', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'JavaScript', 'Updated Jul 20, 2020']}","{'location': 'Vellore. Tamil Nadu', 'stats_list': [], 'contributions': '486 contributions\n        in the last year', 'description': ['Chest-X-ray-Classification-ImageProcessing\nChest X-ray Classification using Image Processing in Keras\n'], 'url_profile': 'https://github.com/Tekraj15', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'JavaScript', 'Updated Jul 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '158 contributions\n        in the last year', 'description': ['Polynomial-regression-for-house-price-dataset\nIn this repository we will see what is this housing price prediction problem parameters/variable values and try to fit this values in a multiple linear regression.In this we will see both ols (Ordinary least squares) method and sgd\n(Stochastic Gradient Descent) method.\nTo run the code follow the below steps:\n\n1.Install python(3.6+) and need packages.\npip install numpy\n\npip instll pandas\n\npip install matplotlib\n\npip install -U scikit-learn\n\npip install seaborn\n\n2.Clone this repository .\nhttps://github.com/karthikeyanthanigai/Polynomial-regression-for-house-price-dataset.git\n\n3.Open command line and set the directory to the cloned repository.\ncd Polynomial-regression-for-house-price-dataset\n\n4.Enter the command.\npython poly.py\n\nif you got any error in install the packages then refer Stackoverflow.\nThe Prediction rate can be increased in other Regression models with Boosting and other methods/Algorithms.\n'], 'url_profile': 'https://github.com/karthikeyanthanigai', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'JavaScript', 'Updated Jul 20, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Saravana08', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'JavaScript', 'Updated Jul 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Linear_vs_KNN_Regression\nComparing the predictive power of 2 most popular algorithms for supervised learning (continuous target) using the cruise ship\xa0dataset\nAuthor: Benjamin O. Tayo\nDate: 5/7/2020\n'], 'url_profile': 'https://github.com/bot13956', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'JavaScript', 'Updated Jul 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '158 contributions\n        in the last year', 'description': [""Multiple-Linear-Regression\nIn this repository we will see multi independent variables (x's) to determine the dependent variable (y).In this we will see both ols (Ordinary least squares) method and sgd\n(Stochastic Gradient Descent) method for insurance dataset.\nTo run the code follow the below steps:\n1.Install python(3.6+) and need packages.\npip install numpy\n\npip instll pandas\n\npip install matplotlib\n\npip install -U scikit-learn\n\npip install seaborn\n\n2.Clone this repository .\nhttps://github.com/karthikeyanthanigai/Multiple-Linear-Regression.git\n\n3.Open command line and set the directory to the cloned repository.\ncd Multiple-Linear-Regression\n\n4.Enter the command.\npython multi.py\n\nif you got any error in install the packages then refer Stackoverflow.\n""], 'url_profile': 'https://github.com/karthikeyanthanigai', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'JavaScript', 'Updated Jul 20, 2020']}","{'location': 'Karachi,Pakistan', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['PREDICT-PORFIT\nPredict profit using Linear Regression\nFirst of fall\nlabel the column\n""Population "" and  ""Profit""\nRead data in csv.\nthen apply different basic function of Pandas library\nProfit is not negative then make it positive.\nGraphical repsentation of give data\nIn the end\nlinear regression is applied\naccuracy is 0.77\n'], 'url_profile': 'https://github.com/kumarrahul02', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'JavaScript', 'Updated Jul 20, 2020']}","{'location': 'S√£o Paulo - Brasil', 'stats_list': [], 'contributions': '253 contributions\n        in the last year', 'description': ['backstopJS-example\nExample of Visual Regression Tests\nhttps://github.com/marciovrl/backstopJS-example.git\n\nRun\nnpm run reference\nTake a screenshot to use as a reference.\nnpm run test\nCreate a screenshot sequence and compare with the references.\nnpm run approve\nApproves the new screenshots to be used as references.\n'], 'url_profile': 'https://github.com/marciovrl', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', '3', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 20, 2020', '2', 'Jupyter Notebook', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'JavaScript', 'Updated Jul 20, 2020']}"
"{'location': 'Hyderabad', 'stats_list': [], 'contributions': '298 contributions\n        in the last year', 'description': ['Linear-Regression\nPyTorch implementation of linear regression\n'], 'url_profile': 'https://github.com/srimanthtenneti', 'info_list': ['Python', 'Updated May 5, 2020', 'R', 'Updated May 28, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'TypeScript', 'Apache-2.0 license', 'Updated May 11, 2020', 'HTML', 'Updated May 10, 2020']}","{'location': 'Athens, Greece', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['TimeSeriesAnalysis\nSimple forecasting with Regression Model\n'], 'url_profile': 'https://github.com/panagiotis131', 'info_list': ['Python', 'Updated May 5, 2020', 'R', 'Updated May 28, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'TypeScript', 'Apache-2.0 license', 'Updated May 11, 2020', 'HTML', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kje980714', 'info_list': ['Python', 'Updated May 5, 2020', 'R', 'Updated May 28, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'TypeScript', 'Apache-2.0 license', 'Updated May 11, 2020', 'HTML', 'Updated May 10, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '188 contributions\n        in the last year', 'description': ['ml_regression_ar\n'], 'url_profile': 'https://github.com/anupomr', 'info_list': ['Python', 'Updated May 5, 2020', 'R', 'Updated May 28, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'TypeScript', 'Apache-2.0 license', 'Updated May 11, 2020', 'HTML', 'Updated May 10, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['Mod2_Linear_regression_project\nMod 2 linear regression project\nDataset\nA Fifa 19 data set from kaggle\nDataset contains 18207 rows and 88 columns\nhttps://www.kaggle.com/karangadiya/fifa19\nRows consist of players\nColumns consist of features related to the players.\nThe Objectives\n\nBuild a Linear regression model to predict the overall ratings for players.\nDetermine the most important factors that predict an overall rating.\nAnalyze how the factors change depending on the position of the player\n\nThe Process\nFirst linear regression model predicted very well with a RMSE of 1.8.\nSplit the data set into positions to see how it would affect the model.\nDetermined what factors led to the overall ratings of each position based on the models.\nThe Outcome\nEach model predicted differently for each position.\nThe goalies model had the best RMSE of 0.3.\nEach of the factors had different weights depending on the position.\nThe Goalies model had all of the goalie factors highly weighted and the other factors very low.\nThe Offense model showed a higher weight in Skill moves and Special rating.\nDefense showed a higher weight in Short Passing and Stamina.\nMidfielders showed a higher weight in Short passing, Stamina, and Skill moves.\nEach of the models showed high weights in Potential, Age, and Ball Control.\nThe Conclusion\nFor different positions, players h=should focus on their own specific skills.\nGoalies should focus on Positioning and Reflexes\nOffense should focus on skill moves and special rating\nDefense should focus on short passing and stamina.\nMidfielders should focus on short passing stamina and skill moves.\nAll players should focus on potential and ball control.\nThere was a significant difference in Overall for goalies and other positions. Goalies will have a lower rating than the other positions.\n'], 'url_profile': 'https://github.com/ChrisFiorentine', 'info_list': ['Python', 'Updated May 5, 2020', 'R', 'Updated May 28, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'TypeScript', 'Apache-2.0 license', 'Updated May 11, 2020', 'HTML', 'Updated May 10, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '204 contributions\n        in the last year', 'description': ['2019 Stack Overflow Survey Data Linear Regression Model Project\nThis project aims to answer the question, ""do different backgrounds of developers make an impact on their salary?""\nContents of this project repository:\n\nEDA-linreg.ipynb\nDatackeabubg.ipynb\ndev_df.csv\nsurvey_results_pulibc.csv.zip\nsurvey_results_schema.csv\nREADME_SURVEY_2019.txt\n2019 Developer Survey A Linear (Regression) Story.pdf\n\n'], 'url_profile': 'https://github.com/jbuenaseda', 'info_list': ['Python', 'Updated May 5, 2020', 'R', 'Updated May 28, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'TypeScript', 'Apache-2.0 license', 'Updated May 11, 2020', 'HTML', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': [""Kaggle House-Prices-Advanced-Regression-Techniques\nHouse Prices: Advanced Regression Techniques\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n""], 'url_profile': 'https://github.com/Koustubh18', 'info_list': ['Python', 'Updated May 5, 2020', 'R', 'Updated May 28, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'TypeScript', 'Apache-2.0 license', 'Updated May 11, 2020', 'HTML', 'Updated May 10, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': [""Keras-library\nRegression Model using Keras library\nRegression Models with Keras\nIntroduction\nAs we discussed in the videos, despite the popularity of more powerful libraries such as PyToch and TensorFlow, they are not easy to use and have a steep learning curve. So, for people who are just starting to learn deep learning, there is no better library to use other than the Keras library.\nKeras is a high-level API for building deep learning models. It has gained favor for its ease of use and syntactic simplicity facilitating fast development. As you will see in this lab and the other labs in this course, building a very complex deep learning network can be achieved with Keras with only few lines of code. You will appreciate Keras even more, once you learn how to build deep models using PyTorch and TensorFlow in the other courses.\nSo, in this lab, you will learn how to use the Keras library to build a regression model.\nTable of Contents\nDownload and Clean Dataset\nImport Keras\nBuild a Neural Network\nTrain and Test the Network\nDownload and Clean Dataset\nLet's start by importing the pandas and the Numpy libraries.\nimport pandas as pd\nimport numpy as np\nWe will be playing around with the same dataset that we used in the videos.\nThe dataset is about the compressive strength of different samples of concrete based on the volumes of the different ingredients that were used to make them. Ingredients include:\n\n\nCement\n\n\nBlast Furnace Slag\n\n\nFly Ash\n\n\nWater\n\n\nSuperplasticizer\n\n\nCoarse Aggregate\n\n\nFine Aggregate\n\n\nLet's download the data and read it into a pandas dataframe.\nconcrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\nconcrete_data.head()\nSo the first concrete sample has 540 cubic meter of cement, 0 cubic meter of blast furnace slag, 0 cubic meter of fly ash, 162 cubic meter of water, 2.5 cubic meter of superplaticizer, 1040 cubic meter of coarse aggregate, 676 cubic meter of fine aggregate. Such a concrete mix which is 28 days old, has a compressive strength of 79.99 MPa.\nLet's check how many data points we have.\nconcrete_data.shape\nSo, there are approximately 1000 samples to train our model on. Because of the few samples, we have to be careful not to overfit the training data.\nLet's check the dataset for any missing values.\nconcrete_data.describe()\nconcrete_data.isnull().sum()\nThe data looks very clean and is ready to be used to build our model.\nSplit data into predictors and target\nThe target variable in this problem is the concrete sample strength. Therefore, our predictors will be all the other columns.\nconcrete_data_columns = concrete_data.columns\n\u200b\npredictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\ntarget = concrete_data['Strength'] # Strength column\nLet's do a quick sanity check of the predictors and the target dataframes.\npredictors.head()\ntarget.head()\nFinally, the last step is to normalize the data by substracting the mean and dividing by the standard deviation.\npredictors_norm = (predictors - predictors.mean()) / predictors.std()\npredictors_norm.head()\nLet's save the number of predictors to n_cols since we will need this number when building our network.\nn_cols = predictors_norm.shape[1] # number of predictors\nImport Keras\nRecall from the videos that Keras normally runs on top of a low-level library such as TensorFlow. This means that to be able to use the Keras library, you will have to install TensorFlow first and when you import the Keras library, it will be explicitly displayed what backend was used to install the Keras library. In CC Labs, we used TensorFlow as the backend to install Keras, so it should clearly print that when we import Keras.\nLet's go ahead and import the Keras library\nimport keras\nAs you can see, the TensorFlow backend was used to install the Keras library.\nLet's import the rest of the packages from the Keras library that we will need to build our regressoin model.\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nBuild a Neural Network\nLet's define a function that defines our regression model for us so that we can conveniently call it to create our model.\ndefine regression model\ndef regression_model():\n# create model\nmodel = Sequential()\nmodel.add(Dense(50, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(1))\n# compile model\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nreturn model\n\nThe above function create a model that has two hidden layers, each of 50 hidden units.\nTrain and Test the Network\nLet's call the function now to create our model.\nbuild the model\nmodel = regression_model()\nNext, we will train and test the model at the same time using the fit method. We will leave out 30% of the data for validation and we will train the model for 100 epochs.\nfit the model\nmodel.fit(predictors_norm, target, validation_split=0.3, epochs=100, verbose=2)\nYou can refer to this link to learn about other functions that you can use for prediction or evaluation.\nFeel free to vary the following and note what impact each change has on the model's performance:\nIncrease or decreate number of neurons in hidden layers\nAdd more hidden layers\nIncrease number of epochs\n""], 'url_profile': 'https://github.com/yashwalke999', 'info_list': ['Python', 'Updated May 5, 2020', 'R', 'Updated May 28, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'TypeScript', 'Apache-2.0 license', 'Updated May 11, 2020', 'HTML', 'Updated May 10, 2020']}","{'location': 'Seoul', 'stats_list': [], 'contributions': '524 contributions\n        in the last year', 'description': ['Grafana plugin ÎßåÎì§Í∏∞\nÌòÑÏû¨ @next   Î≤ÑÏ†ÑÎßå Í∞ÄÎä• 7.0.0-beta.1\ngrafana docker image Î≤ÑÏ†ÑÎèÑ 7.0.0-beta1   Î°ú ÎßûÏ∂§\n\ndocker pull grafana/grafana:7.0.0-beta1\n\n\ndocker run -d -p 3000:3000 -v ""$(pwd)"":/var/lib/grafana/plugins --name=grafana grafana/grafana:7.0.0-beta1\n\n\nnpx ""@grafana/toolkit""@next plugin:create my-plugin\n\n[Ï∂úÏ≤ò] https://blog.naver.com/ganadara1379/221950835966\nGrafana Panel Plugin Template\nThis template is a starting point for building Grafana Panel Plugins in Grafana 7.0+\nWhat is Grafana Panel Plugin?\nPanels are the building blocks of Grafana. They allow you to visualize data in different ways. While Grafana has several types of panels already built-in, you can also build your own panel, to add support for other visualizations.\nFor more information about panels, refer to the documentation on Panels\nGetting started\n\nInstall dependencies\n\nyarn install\n\nBuild plugin in development mode or run in watch mode\n\nyarn dev\nor\nyarn watch\n\nBuild plugin in production mode\n\nyarn build\nLearn more\n\nBuild a panel plugin tutorial\nGrafana documentation\nGrafana Tutorials - Grafana Tutorials are step-by-step guides that help you make the most of Grafana\nGrafana UI Library - UI components to help you build interfaces using Grafana Design System\n\n'], 'url_profile': 'https://github.com/ganadara135', 'info_list': ['Python', 'Updated May 5, 2020', 'R', 'Updated May 28, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'TypeScript', 'Apache-2.0 license', 'Updated May 11, 2020', 'HTML', 'Updated May 10, 2020']}","{'location': 'ElGharbia, Egypt', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': [""[DLND] Project: Predicting Bike-Sharing Patterns\nNeural Network developed during my Deep Learning Fundamentals Nanodegree at Udacity 2018.\nIn this project, I created a simple neural network to use it to predict daily bike rental ridership.\nGetting Started\nThis Bike-Sharing-Dataset has the number of riders for each hour of each day from January 1 2011 to December 31 2012. The number of riders is split between casual and registered, summed up in the cnt column. You can see the first few rows of the data above.\nBelow is a plot showing the number of bike riders over the first 10 days or so in the data set. (Some days don't have exactly 24 entries in the data set, so it's not exactly 10 days.) You can see the hourly rentals here. This data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders. You'll be trying to capture all this with your model.\n\nPrerequisites\nThinks you have to install or installed on your working machine:\n\nPython 3.7\nNumpy (win-64 v1.15.4)\nPandas (win-64 v0.23.4)\nMatplotlib (win-64 v3.0.2)\nJupyter Notebook\nTorchvision (win-64 v0.2.1)\nPyTorch (win-64 v0.4.1)\n\nEnvironment\n\nMiniconda or Anaconda\n\nInstalling\nUse the package manager pip or\nminiconda or Anaconda to install your packages.\nA step by step guide to install the all necessary components in Anaconda for a Windows-64 System:\nconda install -c conda-forge numpy\nconda install -c conda-forge pandas\nconda install -c conda-forge matplotlib\npip install torchvision\nconda install -c pytorch pytorch\nJupyter Notebook\n\nYour_first_neural_network.ipynb\n\nThis jupyter notebook describe the whole project from udacity, from the beginning to the end.\nRunning the project\nThe whole project is located in the jupyter notebook file Your_first_neural_network.ipynb and it's include the training an the prediction part. The neural network is implemented in the file my_answer.py and used from the jupyter notebook.\nParameters of training\nTo change to interations, the amount of hidden notes and some other parameters for the neural network, you can adapt these global constants inside the python file my_answer.py and watch the results.\n#########################################################\n## I got my best results with these values of parameters.\n##########################################################\niterations = 12000\nlearning_rate = 0.5\nhidden_nodes = 20\noutput_nodes = 1\nCheckout the training results\nProgress: 100.0% ... Training loss: 0.055 ... Validation loss: 0.139\nSee the visual results over iteration:\n\nCheckout the prediction results\nI use the test data to show how well the neural network is modeling the data.\n\nImprovements\nThis is actually my best version of bike-sharing-patterns.\nThe next steps will be:\n\ndo some experiments with the neural network to change the hyperparameters / hidden units / optimizer / structure of fully connected layers, ...\nconvert the image classifier algorithm to keras\nconvert the image classifier algorithm to fast.ai\nI will see what's comming next ...\n\nAuthors\n\nKareem Allam\n\n""], 'url_profile': 'https://github.com/KareemAllam', 'info_list': ['Python', 'Updated May 5, 2020', 'R', 'Updated May 28, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'TypeScript', 'Apache-2.0 license', 'Updated May 11, 2020', 'HTML', 'Updated May 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '193 contributions\n        in the last year', 'description': ['Linear Regresssion with scikit-learn\nDataset\nThe simple cengage systolic blood pressure dataset:\nhttps://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/frame.html\nPurpose\nSince I implemented linear regression from scratch,\nthis is to see if scikit-learn can make it simple\nRun verified with python 3.7 and numpy 1.18.1\npython Runner.py\nPredicted vs Actual\n\nPrediction Results over the Training set\nMean Absolute Loss:  1.64\n\n\n\nAge\nWeight\nActual Blood Pressure\nPredicted Blood Pressure\nLoss\n\n\n\n\n52.0\n173.0\n132.0\n134.0\n2.0\n\n\n59.0\n184.0\n143.0\n143.0\n0.0\n\n\n67.0\n194.0\n153.0\n154.0\n1.0\n\n\n73.0\n211.0\n162.0\n165.0\n3.0\n\n\n64.0\n196.0\n154.0\n152.0\n2.0\n\n\n74.0\n220.0\n168.0\n168.0\n0.0\n\n\n54.0\n188.0\n137.0\n140.0\n3.0\n\n\n61.0\n188.0\n149.0\n146.0\n3.0\n\n\n65.0\n207.0\n159.0\n156.0\n3.0\n\n\n46.0\n167.0\n128.0\n127.0\n1.0\n\n\n72.0\n217.0\n166.0\n166.0\n0.0\n\n\n\n'], 'url_profile': 'https://github.com/boyko11', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'GPL-3.0 license', 'Updated May 9, 2020', 'R', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '194 contributions\n        in the last year', 'description': ['cat-detector-logistic-regression\nSingle layer logistic regression project\nLearnt from Andrew Ng on coursera\nCost vs iteration:\n\nHow to Run :\ngit clone, install dependencies (numpy, matplotlib ,etc.) , run models.py\nAdd your own images in images and change directory in the last fuction of models.py\n'], 'url_profile': 'https://github.com/AdityaKarn', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'GPL-3.0 license', 'Updated May 9, 2020', 'R', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['Regression-Models-Course-Project\nCourse project for Regression Models\n'], 'url_profile': 'https://github.com/JeevithieshDuggani', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'GPL-3.0 license', 'Updated May 9, 2020', 'R', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '188 contributions\n        in the last year', 'description': ['MultipleLinearRegression\nMultiple Linear Regression with R\n'], 'url_profile': 'https://github.com/anupomr', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'GPL-3.0 license', 'Updated May 9, 2020', 'R', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['DiabetesPrediction\nDiabetes prediction using Logistic Regression\nName: Ninad Nitin Khire\nRoll no.: 61\nBatch: T3\nT. Y. C. S. E.\n'], 'url_profile': 'https://github.com/ninadkhire', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'GPL-3.0 license', 'Updated May 9, 2020', 'R', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '549 contributions\n        in the last year', 'description': ['Cement-Strength-\nRegression through layers using keras\n'], 'url_profile': 'https://github.com/sidhu1012', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'GPL-3.0 license', 'Updated May 9, 2020', 'R', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogistic regression implementation in python.\n\nIn this project, a logistic regression model has been developed for the Bank Note Authentication Dataset using gradient descent method. Accuracy, Precision, Recall and F-score values have been calculated. L1 and L2 regularisation has been implemented to explore the effect of regularisation on accuracy and magnitude of weights. Feature importance was determined using the final weights obtained.\nDataset details\n\n\nNumber of rows: 1372\n\n\nAttributes:\n\nVariance\nSkewness\nCurtosis\nEntropy\nClass (Dependent Attribute)\n\n\n\nSample data points\n\n\n\n3.6216, 8.6661, -2.8073, -0.44699, 0\n4.5459, 8.1674, -2.4586, -1.4621, 0\n0.4283, -0.94981 ,-1.0731 ,0.3211, 1\n1.5904, 2.2121, -3.1183, -0.11725, 1\n\nMethodology\nThe dataset was normalised (standardised) using Z-score normalisation. The dataset was split into training and testing data using an 80-20 split. Logistic Regression without regularisation, and with L1 and L2 regularisation was carried out and the accuracy, precision, recall and f-score values were calculated for each model. Different weight initialisations (weights initialised to zeroes, weights taken from a normal distrubution, weights taken from a uniform distribution), learning rates, number of iterations and lambda (regularisation parameter) values were tried.\nResults\n\nFor logistic regression without regularisation, with weights initalised to all zeroes, learning rate = 0.8 , number of iterations = 1500, an accuracy of 99.27 and f-score of 99.25 was achieved on the test set. The final weights for this model were (including bias term):  \n[[-2.277894 ], [-6.41902018], [-6.67073906], [-6.19920365], [ 0.27300584]]\n \nFor logistic regression with L1 regularisation, with learning rate = 0.04 , regularisation parameter = 0.02, number of iterations = 1500, an accuracy of 96.00 and f-score of 95.78 was achieved on the test set. The final weights for this model were (including bias term):  \n[[-1.65585554e-01], [-2.27335876e+00], [-1.70111197e+00], [-1.34845301e+00], [1.33475071e-03]]\n \nFor logistic regression with L2 regularisation, with learning rate = 0.04 , regularisation parameter = 0.04, number of iterations = 1500, an accuracy of 97.09 and f-score of 96.87 was achieved on the test set. The final weights for this model were (including bias term):  \n[[-0.28227054], [-1.70082037], [-1.21894639], [-0.91768412], [ 0.05752311]]\n \n\nConclusion\n\nRegularisation effectively decreases the magnitude of weights.\nL1 regularisation shrinks the weight corresponding to the least important feature (Entropy) to zero.\nIn all the models, the magnitude of weights for the first three attributes (Variance, Skewness, Curtosis) are higher than that for the fourth attribute, Entropy. Thus, for unit increase in the value of the first three attributes, the log odds changes by a larger amount. Therefore, Variance, Skewness and Curtosis are more important in classification of bank note images than Entropy.\n\n'], 'url_profile': 'https://github.com/prathmachowksey', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'GPL-3.0 license', 'Updated May 9, 2020', 'R', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/myusernamesxy', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'GPL-3.0 license', 'Updated May 9, 2020', 'R', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression Machine learning model\nRidge regression\nLasso Regression\n'], 'url_profile': 'https://github.com/Izooo', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'GPL-3.0 license', 'Updated May 9, 2020', 'R', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Credit-Card-Approvals\nUsing Logistic Regression (Basic Model)\n'], 'url_profile': 'https://github.com/amsahu', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'GPL-3.0 license', 'Updated May 9, 2020', 'R', 'Updated May 14, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}"
"{'location': 'Kimberley', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['Bayesian-Probabilistic-Programming\nProbabilistic polynomial regression and posterior\n'], 'url_profile': 'https://github.com/Somdani', 'info_list': ['Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2021', '2', 'Jupyter Notebook', 'Updated May 29, 2020', '2', 'Jupyter Notebook', 'Updated May 10, 2020', '2', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Python', 'Updated Dec 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Aug 19, 2020']}","{'location': 'Mandi, Himachal Pradesh, India', 'stats_list': [], 'contributions': '231 contributions\n        in the last year', 'description': ['Logistic Regression-\nThis is a from scratch implementation of Logistic Regression Algorithm.\nAdaptive Boosting-\nThis is an implementation of the Adaptive.M1 algorithm as presented here http://web.eecs.utk.edu/~leparker/Courses/CS425-528-fall12/Handouts/AdaBoost.M1.pdf\n'], 'url_profile': 'https://github.com/NippunSharma', 'info_list': ['Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2021', '2', 'Jupyter Notebook', 'Updated May 29, 2020', '2', 'Jupyter Notebook', 'Updated May 10, 2020', '2', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Python', 'Updated Dec 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Aug 19, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '150 contributions\n        in the last year', 'description': ['BullDozer\nProblem Source: https://www.kaggle.com/c/bluebook-for-bulldozers/overview\nData Source: https://www.kaggle.com/c/bluebook-for-bulldozers/data\n'], 'url_profile': 'https://github.com/luigivendetta', 'info_list': ['Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2021', '2', 'Jupyter Notebook', 'Updated May 29, 2020', '2', 'Jupyter Notebook', 'Updated May 10, 2020', '2', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Python', 'Updated Dec 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Aug 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rohitkolankar', 'info_list': ['Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2021', '2', 'Jupyter Notebook', 'Updated May 29, 2020', '2', 'Jupyter Notebook', 'Updated May 10, 2020', '2', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Python', 'Updated Dec 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Aug 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['Heart-Disease-Prediction-Logistic-Regression\nUsing the famous Logistic Regression to predict whether a person has Heart Disease or not.\nInstallation\nRequiremenmts\n\nPython 3.3+ or Python 2.7\nscikit learn\nnumpy\npandas\nmatplotlib\n\nInstall the libraries from the pypi.org with the following commands.\npip install scikit-learn\npip install numpy\npip install pandas\npip install matplotlib\n\nPlease do refer stackoverflow for any errors during installation.\n'], 'url_profile': 'https://github.com/dharineeshramtp2000', 'info_list': ['Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2021', '2', 'Jupyter Notebook', 'Updated May 29, 2020', '2', 'Jupyter Notebook', 'Updated May 10, 2020', '2', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Python', 'Updated Dec 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Aug 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '251 contributions\n        in the last year', 'description': ['Regression Testing\n\n\n\nThis project leverages machine learning techniques to clean, analyze, and make predictions on any inputted csv dataset.\n  \n\nContents\nRegressions Backend - This file holds the backend that cleans, analyzes, and runs regressions on the input dataset.\nRegressions Frontend - This file lays out the results generated by the backend as graphs and visuals in a clean way.\nWebsite\nhttps://regression-testing.herokuapp.com - This website contains the final result of the project in an interactive and easy-to-use manner.\nLibaries Used\nBackend\n\nFlask\n\nFrontend\n\nAjax\nJinja\nJQuery\nPlotly\n\nData Cleaning\n\nNumpy\nPandas\n\nML Modeling\n\nSklearn\nXgboost\n\nVisualization\n\nPlotly\n\n'], 'url_profile': 'https://github.com/SidTheKid007', 'info_list': ['Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2021', '2', 'Jupyter Notebook', 'Updated May 29, 2020', '2', 'Jupyter Notebook', 'Updated May 10, 2020', '2', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Python', 'Updated Dec 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Aug 19, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '184 contributions\n        in the last year', 'description': ['Probability-Statistics\nIt Includes Linear Regression Model, Multiple Regression Model and Naive Bayes Classifier\n'], 'url_profile': 'https://github.com/Naman18055', 'info_list': ['Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2021', '2', 'Jupyter Notebook', 'Updated May 29, 2020', '2', 'Jupyter Notebook', 'Updated May 10, 2020', '2', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Python', 'Updated Dec 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Aug 19, 2020']}","{'location': 'Bengaluu', 'stats_list': [], 'contributions': '195 contributions\n        in the last year', 'description': ['Linear-Regression\nThis repo contain both simple linear regression and multiple linear regression with explaintaion.\nVarious technique like feature selection, data cleaning, optimization are implemented in both the model.\nFor explaintaion please chek out my articel here\n'], 'url_profile': 'https://github.com/benai9916', 'info_list': ['Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2021', '2', 'Jupyter Notebook', 'Updated May 29, 2020', '2', 'Jupyter Notebook', 'Updated May 10, 2020', '2', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Python', 'Updated Dec 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Aug 19, 2020']}","{'location': 'Gurgaon', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': [' Machine Learning with Pyspark\nApache Spark, once a component of the Hadoop ecosystem, is now becoming the big-data platform of choice for enterprises. It is a powerful open source engine that provides real-time stream processing, interactive processing, graph processing, in-memory processing as well as batch processing with very fast speed, ease of use and standard interface.\n In this project a simple Regression with Machine Learning is performed using various regression algorithms.\nThe dataset which I have used is of Housing prices.\nDataset\nThe data is related with housing prices. The regression goal is to predict median housing price\nApproach\nThe operations done on dataset are:\n\nData Visualization\nData Cleaning\nFeature Engineering\nMachine learning algorithms to train and predict\n\n'], 'url_profile': 'https://github.com/AshishBhatnagar022', 'info_list': ['Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2021', '2', 'Jupyter Notebook', 'Updated May 29, 2020', '2', 'Jupyter Notebook', 'Updated May 10, 2020', '2', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Python', 'Updated Dec 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Aug 19, 2020']}","{'location': 'waterloo', 'stats_list': [], 'contributions': '833 contributions\n        in the last year', 'description': ['Minor_dataset_explorations\nExploring Titanic and Boston_Housing datasets using simplistic Linear regression and logistic regression models.\n'], 'url_profile': 'https://github.com/kannavdhawan', 'info_list': ['Jupyter Notebook', 'Updated Aug 13, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2021', '2', 'Jupyter Notebook', 'Updated May 29, 2020', '2', 'Jupyter Notebook', 'Updated May 10, 2020', '2', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Python', 'Updated Dec 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Aug 19, 2020']}"
"{'location': 'waterloo', 'stats_list': [], 'contributions': '833 contributions\n        in the last year', 'description': ['Minor_dataset_explorations\nExploring Titanic and Boston_Housing datasets using simplistic Linear regression and logistic regression models.\n'], 'url_profile': 'https://github.com/kannavdhawan', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 19, 2020', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 22, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Syracuse, NY', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['RegressionAnalysis_linear_nonlinear\nRegression analysis assignment for Quantitative Analytics class; explores linear and non-linear regression models.\n'], 'url_profile': 'https://github.com/SamanthaJ011', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 19, 2020', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 22, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['LinearRegressionBostonHousePrice\nMachine learning project\n'], 'url_profile': 'https://github.com/hibatallahk', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 19, 2020', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 22, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['HomeWork12RProject\nPackage for Running Linear Regression Analysis\n'], 'url_profile': 'https://github.com/harrisonjoseph7', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 19, 2020', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 22, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '168 contributions\n        in the last year', 'description': ['HeartDiseasePrediction-LR-\nHeart Disease Prediction using Logistic Regression\n'], 'url_profile': 'https://github.com/ChandrashekharM3018', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 19, 2020', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 22, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dwang9846', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 19, 2020', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 22, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Istanbul', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['churn-telco\nSimple Churn Modelling with Logistic Regression\nIt will be used for Airflow Workshop\nDataset: https://www.kaggle.com/blastchar/telco-customer-churn\n'], 'url_profile': 'https://github.com/sercandogan', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 19, 2020', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 22, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Ahmedabad, India', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Magazine Purchase Predictor\n\nApproach: Logistic Regression\nUse: It helps the resellers in deciding what magazines to market to customers\n\n'], 'url_profile': 'https://github.com/agcy1210', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 19, 2020', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 22, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Ontario, Canada', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['Regularised Regression model to predict the value of properties.\nAdvanced Regression Techniques - Ridge and Lasso\nProblem\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.\nThe company is looking at prospective properties to buy to enter the market.\nBuild a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\nThe company wants to know:\n\nWhich variables are significant in predicting the price of a house, and\nHow well those variables describe the price of a house.\n\n'], 'url_profile': 'https://github.com/kanika1101', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 19, 2020', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 22, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/swasthikshetty', 'info_list': ['1', 'Jupyter Notebook', 'Updated Aug 19, 2020', 'Updated May 7, 2020', '2', 'Jupyter Notebook', 'Updated May 22, 2020', 'R', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 19, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020']}"
"{'location': 'Seoul', 'stats_list': [], 'contributions': '524 contributions\n        in the last year', 'description': ['InfluxDB ping test\n$> curl -sL -I http://localhost:8086/ping\ninfluxDB connection test\n$> curl -G http://yourhost:8086/query -u user:password --data-urlencode ""q=SHOW DATABASES""\nedit of your \'influxdb.conf\'\n[http]\nDetermines whether HTTP endpoint is enabled.\nDetermines whether user authentication is enabled over HTTP/HTTPS.\nauth-enabled = true\ninfluxDB query\n$> curl -G \'http://localhost:8086/query?pretty=true\' --data-urlencode ""db=mydb"" --data-urlencode ""q=SELECT ""value"" FROM ""cpu_load_short"" WHERE ""region""=\'us-west\'""\nex) curl -G http://remotehost:8086/query?pretty=true -u user:password --data-urlencode db=emsdb --data-urlencode ""q=SELECT * from data where time >= now()-1m""\nex) curl -G http://remotehost:8086/query?pretty=true -u user:password --data-urlencode db=emsdb --data-urlencode ""q=SELECT ""meter0/ActivePower"" from data where time >= now()-1m""\nex) curl -G http://remotehost:8086/query?pretty=true -u user:password --data-urlencode db=emsdb --data-urlencode ""q=SELECT mean(""meter0/ActivePower"") from data where time >= now()-1m GROUP BY time(10s)""\nÏ∞∏Í≥†ÏûêÎ£å\nhttps://docs.influxdata.com/influxdb/v1.4/guides/querying_data/\nyarn start\nRuns the app in the development mode.\nOpen http://localhost:3000 to view it in the browser.\nThe page will reload if you make edits.\nYou will also see any lint errors in the console.\nyarn test\nLaunches the test runner in the interactive watch mode.\nSee the section about running tests for more information.\nyarn build\nBuilds the app for production to the build folder.\nIt correctly bundles React in production mode and optimizes the build for the best performance.\nThe build is minified and the filenames include the hashes.\nYour app is ready to be deployed!\nSee the section about deployment for more information.\nyarn eject\nNote: this is a one-way operation. Once you eject, you can‚Äôt go back!\nIf you aren‚Äôt satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project.\nInstead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you‚Äôre on your own.\nYou don‚Äôt have to ever use eject. The curated feature set is suitable for small and middle deployments, and you shouldn‚Äôt feel obligated to use this feature. However we understand that this tool wouldn‚Äôt be useful if you couldn‚Äôt customize it when you are ready for it.\nLearn More\nYou can learn more in the Create React App documentation.\nTo learn React, check out the React documentation.\nCode Splitting\nThis section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting\nAnalyzing the Bundle Size\nThis section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size\nMaking a Progressive Web App\nThis section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app\nAdvanced Configuration\nThis section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration\nDeployment\nThis section has moved here: https://facebook.github.io/create-react-app/docs/deployment\nyarn build fails to minify\nThis section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify\n'], 'url_profile': 'https://github.com/ganadara135', 'info_list': ['JavaScript', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pranavseth', 'info_list': ['JavaScript', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Flipr_Assignment_3\nPerforming logistic regression on breast cancer dataset\n'], 'url_profile': 'https://github.com/Yasashvini', 'info_list': ['JavaScript', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Airbnb-Data-Analysis-and-Price-Prediction-Project\n\n'], 'url_profile': 'https://github.com/Rahul-Jyoti', 'info_list': ['JavaScript', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mazenr91', 'info_list': ['JavaScript', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Project-Linear-Regression-with-NumPy-and-Python-From-Scratch\nLinear Regression Implementation on food truck data\n'], 'url_profile': 'https://github.com/philsaurabh', 'info_list': ['JavaScript', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'delhi', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['boston-house-prediction-using-regression-\nprediction of house price using regression techniques\n'], 'url_profile': 'https://github.com/akstomar', 'info_list': ['JavaScript', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'Moscow', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['42Ecole.dslr\nLogistic regression project from 42 Ecole\n'], 'url_profile': 'https://github.com/GrigorySamokhin', 'info_list': ['JavaScript', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'Qu√©bec city, Qc', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/philmorin5', 'info_list': ['JavaScript', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '168 contributions\n        in the last year', 'description': ['Height-and-Weight-using-LR\n'], 'url_profile': 'https://github.com/ChandrashekharM3018', 'info_list': ['JavaScript', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '219 contributions\n        in the last year', 'description': ['Predicting-Housing-Prices-\nPrediction of housing prices using regression\n'], 'url_profile': 'https://github.com/AbhAgg', 'info_list': ['Python', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'JavaScript', 'Updated May 11, 2020', 'Julia', 'Updated May 6, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'C++', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saloni123-S', 'info_list': ['Python', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'JavaScript', 'Updated May 11, 2020', 'Julia', 'Updated May 6, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'C++', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/santoshkr3999', 'info_list': ['Python', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'JavaScript', 'Updated May 11, 2020', 'Julia', 'Updated May 6, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'C++', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '186 contributions\n        in the last year', 'description': ['House-Price-Prediction-using-Regression\nLinear, Polynomial, Ridge and Lasso Regresion Implemented from Scratch using Julia.\n'], 'url_profile': 'https://github.com/aditis1204', 'info_list': ['Python', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'JavaScript', 'Updated May 11, 2020', 'Julia', 'Updated May 6, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'C++', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AlexandrPerun', 'info_list': ['Python', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'JavaScript', 'Updated May 11, 2020', 'Julia', 'Updated May 6, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'C++', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['julia_blog\nLinear Regression Implementation in Julia & Python\nProblem Statement:\nPredict the price of Second-hand cars taking cars\' attributes into consideration.\nData Summary:\nThe Target is ""Price"" column, which is dependant on regressors such as ""Year""(Year of 1st purchase), ""Transmission Type"",""Kilometres-Driven"",""Mileage"", etc.,\nAs a part of Comparing Julia with Python, I have implemented Linear Regression in Python & replicated the same steps in Julia.I have considered Cars Dataset(split into Train & Test Data files), which I will include in this repo.\nThe comparison is done in each of the following stages:\n\nDescriptive Stats\nVisualisation\nData Pre-processing\nModelling\n\n'], 'url_profile': 'https://github.com/nagapavannukala', 'info_list': ['Python', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'JavaScript', 'Updated May 11, 2020', 'Julia', 'Updated May 6, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'C++', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': [""mypkg\nThis R pageage is coding part of my stat 440 (Computational Inference)'s final project, and modified from the generalized boosted models. It specialized in an bernoulli distribution, and the database coming from Rain in Australia\nThere are two addition function contain in this package, Quick Nodes Morph Mend and Weight boost. For algorithm detial, can read find our final report.\nQuick Nodes Morph Mend uses a gradually shrink learning rate to fit the model, and so increasing the accuracy and reduce the operation times.\nWeight boost gives a weight of each observation based on differenc method.\n""], 'url_profile': 'https://github.com/hehedaozuiteng', 'info_list': ['Python', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'JavaScript', 'Updated May 11, 2020', 'Julia', 'Updated May 6, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'C++', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Logistic-Regression\nTraining,Validation and Testing using Logistic Regression,\nApplying logistic regression on MNIST Handwritten Digits Database\n'], 'url_profile': 'https://github.com/mukesh18c', 'info_list': ['Python', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'JavaScript', 'Updated May 11, 2020', 'Julia', 'Updated May 6, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'C++', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020']}","{'location': 'Moscow', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['ft_linear_regression @ 42 Ecole\nUsage\n1. Train model:\n  python train.py\n\n2. Evaluate model:\n  python evaluate.py\n\n3. Visualization\nVisualizate regression line while training\n  python train.py --visu on\n\nVisualizate regression line after training\n  python train.py --visu last\n\nVisualizate data\n  python train.py --visu last\n\nVisualizate MSE curve\n  python train.py --visu mse\n\n4. Metrics\nPrint MSE every 10000 cicles\n  python train.py --metric mse\n\n'], 'url_profile': 'https://github.com/GrigorySamokhin', 'info_list': ['Python', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'JavaScript', 'Updated May 11, 2020', 'Julia', 'Updated May 6, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'C++', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['poker_liner_regression\nliner regression to caculate poker EHS\nrun\nanalyse.py to collect data;\nprosession.py to deal with the origin data;\nregression.py to train and test;\n'], 'url_profile': 'https://github.com/enjoygayhub', 'info_list': ['Python', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'JavaScript', 'Updated May 11, 2020', 'Julia', 'Updated May 6, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 23, 2020', 'C++', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akhilhumane99', 'info_list': ['Python', 'Updated May 8, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 15, 2020', 'R', 'Updated Aug 17, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RobertNardello', 'info_list': ['Python', 'Updated May 8, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 15, 2020', 'R', 'Updated Aug 17, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '331 contributions\n        in the last year', 'description': [""Modeling 2016 Election Results\nIntroduction\nI set out to build a Linear Regression model to identify the relative importance of county level demographic features in determining the proportion of that county that voted for Donald Trump in the 2016 presidential election.  I obtained my feature data from the US Census website in Excel format and merged this with 2016 county level election results from Harvard Dataverse.  My best performing model used Lasso Feature Selection and had a Testing RMSE of .092.  The two features that most positively predicted support for Donald Trump were Caucasian Population proportion and Veteran proportion, while the two features that most negatively influenced support for Donald Trump were Bachelors Degree Population proportion and Hispanic Population proportion.\nObtain Data\nI obtained county level data in excel format for 33 demographic features from the US Census website.  Data was abailable for the 785 largest counties in the US accounting for approximately 85% of the national population.  I also downloaded an Excel file with 2016 county level presidential election results from Harvard Dataverse.  I merged the county level demographics Excel files with the election results Excel file, yielding 785 datapoints.\nScrub Data\nThe first issues I faced while scrubbing my data was the presense of two county level equivalents in Virginia, counties and independent cities, which sometimes had duplicate names.  I identified duplicate county names and manually updated those names to specify whether they were counties or independent cities.  I replaced missing feature values with the mean for those features.  I also applied a standard scaler to my features, in order to make the relative importance of coefficients in my linear regression model comparable.  Finally, I created interactions to have more features to include in my model.\nExplore Data\n\nDescriptive statistics for the Donald Trump Vote Proportion dependent variable\nOn average, 52.7% of a counties' voters voted for Donald Trump with a standard deviation of .149.  In the process of plotting my feature relationships with the dependent variable, I discovered that many features had non-linear relationships with proportion of support for Donald Trump.  I used log transformations to update features with non-linear relationships to have linear relationships with the dependent variable.  I also created a correlation matrix of my independent variables and removed heavily correlated ones.\nModel Data\n\nBar graph of Lasso feature coefficients\nI used RMSE as my principle evaluation metric in order to measure the difference between my model predictions and the actual values, while disproportionately punishing large prediction misses. I used an 80/20 Train Test Split to validate my model.  Using Lasso feature selection, my testing RMSE was .092.  The most important features in increasing support for Donald Trump were Caucasian Population proportion, Veteran Population proportion and Mining Jobs per Capita with coefficients of .05, .03 and .02 respectively.  The most influential features in decreasing support for Donald Trump were population proportion with a Bachelors Degree, Hispanic Population proportion and Population Density with coefficients of -.04, -.02 and -.01 respectively.\nNext steps\nWith more time, I would like to train additional types of regression models to see if they could outperform linear regression in RMSE.  I would also like to use more recent census demographic data to predict how demographic changes from 2016 to 2020 might impact the 2020 presidential election.\nGithub Files\nMod_2_Final.ipynb :  2016 election results regression modeling\nSources\nUS Census Data: https://www.census.gov/data.html\nElection Results: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VOQCHQ\n""], 'url_profile': 'https://github.com/blantj', 'info_list': ['Python', 'Updated May 8, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 15, 2020', 'R', 'Updated Aug 17, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mrnegi', 'info_list': ['Python', 'Updated May 8, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 15, 2020', 'R', 'Updated Aug 17, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression for a small dataset\n'], 'url_profile': 'https://github.com/DOgaro', 'info_list': ['Python', 'Updated May 8, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 15, 2020', 'R', 'Updated Aug 17, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': [""robustsubsets\nOverview\nAn R implementation of robust subset selection as described\nhere.\nRobust subset selection is a robust adaption of the classic best subset\nselection estimator, and is defined by the constrained least squares\nproblem:\n\nRobust subsets seeks out the best subset of predictors and observations\nand performs a least squares fit on this subset. The number of\npredictors used in the fit is controlled by the parameter k and the\nobservations by the parameter h.\nInstallation\nYou should install Gurobi and the associated R package gurobi before\ninstalling robustsubsets. Gurobi is available for free under academic\nlicense at https://www.gurobi.com/.\nTo install robustsubsets from GitHub, run the following code:\ndevtools::install_github('ryan-thompson/robustsubsets')\nUsage\nThe rss function is the primary function for robust subset selection,\nit calls rss.fit and rss.cv to fit and cross-validate the model over\nvarious values of k and h.\nlibrary(robustsubsets)\n\n# Generate training data with contaminated predictor matrix\nset.seed(1)\nn <- 100 # Number of observations\np <- 10 # Number of predictors\np0 <- 5 # Number of relevant predictors\nn.c <- 5 # Number of contaminated observations\nbeta <- c(rep(1, p0), rep(0, p - p0))\nX <- matrix(rnorm(n * p), n, p)\ny <- X %*% beta + rnorm(n)\nX[1:n.c, ] <- matrix(rnorm(n.c * p, mean = 10), n.c, p)\n\n# Fit with k=0,...,10 and h=90,100\nfit <- rss(X, y, k = 0:10, h = function(n) round(c(0.90, 1.00) * n))\n\n# Plot coefficient profiles\nplot(fit, type = 'profile')\n\n# Plot cross-validation results\nplot(fit, type = 'cv')\n\n# Extract coefficients (corresponding to best parameters from cross-validation)\ncoef(fit)\n\n# Make predictions (using best parameters from cross-validation)\npredict(fit, X)\nDocumentation\nSee robustsubsets_1.0.2.pdf for\ndocumentation.\n""], 'url_profile': 'https://github.com/ryan-thompson', 'info_list': ['Python', 'Updated May 8, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 15, 2020', 'R', 'Updated Aug 17, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Ann Arbor, MI', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['R_prediction_models\nThis repository includes scripts for running 4 algorithms to build prediction models in R: logistic regression, regularized regression, and both classification (binary) and regression random forest.\n\nThe scripts assume the class and feature data have been pre-processed and are saved as RDS files.\nThe model-building is done with a valdiation set that is held out for each model. The validation set is used to report model performance.\nThe scripts make use of a bagging approach where multiple models are developed while selecting a random subset of training instances. The final prediction score for a validation instance is reported as the mean of model repetitions.\nEach script performs a parameter sweep to identify optimal parameters using cross-validation within the training set. Regularized regression tunes lamba (beta penalization) and alpha (LASSO vs. ridge regression vs. elastic nets) parameters. Logistic and both random forest algorithms utilize LASSO as a feature selection tool prior to model-building. The lambda beta penalization for feature selection is tuned.\n\nEach script takes the same 8 inputs:\n\nLocation of RDS file with class data matrix (scripts were developed for data with multiple response values per instance)\nColunm index of class data matrix to use: Integer\nLocation of RDS file with feature data matrix\nLocation of RDS file with vector of feature IDs to include in prediction model\nLocation of RDS file with vector of instance IDs to hold out as validation set\nProportion of training instances (i.e. non-validation) to include in prediction model: [0, 1]\n# of repetitions of prediction models to build, each with a random subset of the training data: Integer\nPrefix character string for output files\n\nExample usage:\nR --vanilla --slave --args CLASS_MATRIX COL_IND FEAT_MATRIX FEATURE_IDS VALIDATION_IDS TRAINING_PROP MODEL_REPETITIONS OUTPUT_PREFIX < 2_regularized_predictions.R\nThe scripts produce four output files:\n\n[prefix].prediction.RDS: Object with prediction on the validation set\n[prefix].featSelWeightsRDS: Object with feature selection weights (penalized betas)\n[prefix].parameters.RDS: Object with the parameters utilized to train the model, for reporting purposes.\n[prefix].trainTest.RDS: Object with training and testing instances for each repetition of model-building, for reporting purposes.\n\n'], 'url_profile': 'https://github.com/johnplloyd', 'info_list': ['Python', 'Updated May 8, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 15, 2020', 'R', 'Updated Aug 17, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Dublin', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShaikhSufiyanAhmed', 'info_list': ['Python', 'Updated May 8, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 15, 2020', 'R', 'Updated Aug 17, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YashGupta15', 'info_list': ['Python', 'Updated May 8, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 15, 2020', 'R', 'Updated Aug 17, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saloni123-S', 'info_list': ['Python', 'Updated May 8, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Oct 24, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Nov 15, 2020', 'R', 'Updated Aug 17, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Regression-and-Classification\nNotebook containing all regression and classification techniques\n'], 'url_profile': 'https://github.com/Muskandawar', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'MIT license', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 7, 2020']}","{'location': 'St Louis', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['#My first Mode, I took the INRvsUSD values from the website as CSV\n#and cleaned the data and tested the predictor values against the response variable\n#tested the mode using below techniques\n#R2(square) of the model\n#Cooks distance of the model\n#Confidence using Confint\n#AVPlots\n#Adjusted R2 value\n#AIC Value - Akaikie Information Criterion\n#Stepwise Analysis\n#Null hypothesis test method = Chi\n#Finally Chose the the best fit model as Price (response) vs (High & Open) (predictor)\n#Set up WOrking Directory\n\nsetwd(""C:/Users/asket/Documents/Exploratory Data Analysis/DataSet/INRvsUSD"")\n#install library\nlibrary(MASS)\nlibrary(Hmisc)\nlibrary(car)\n#upload the file\ninrvsusdprediction<-read.csv(""USD_INR_Historical_Data.csv"")\n#Data Cleaning\nnames(inrvsusdprediction) <- gsub("".."", """", names(inrvsusdprediction),fixed = TRUE)\nnames(inrvsusdprediction) <- gsub(""√Ø"", """", names(inrvsusdprediction),fixed = TRUE)\nas.character(inrvsusdprediction$Change)\ninrvsusdprediction$Change<-sub(""%"","""",inrvsusdprediction$Change)\ninrvsusdprediction$Change<-sub(""-"","""",inrvsusdprediction$Change)\nas.numeric(inrvsusdprediction$Change)*10\nas.numeric(inrvsusdprediction$Change)\nclass(inrvsusdprediction$Change)\ninrvsusdprediction<-inrvsusdprediction[c(-1)]\n#summary of the data\nsummary(inrvsusdprediction)\n#response variable is INR PRICE\n#learn about variables\n#to subplot\npar(mfrow=c(1,2))\n#INR Price\nboxplot(inrvsusdprediction$Price,main = \'INR-Price\')\nhist(inrvsusdprediction$Price,main = \'Hist-INR-Price\')\n#Open Price\nboxplot(inrvsusdprediction$Open,main = \'Open-Price\')\nhist(inrvsusdprediction$Open,main = \'Open-Price\')\n#High Price\nboxplot(inrvsusdprediction$High,main = \'High-Price\')\nhist(inrvsusdprediction$High,main = \'High-Price\')\n#High Price\nboxplot(inrvsusdprediction$Low,main = \'Low-Price\')\nhist(inrvsusdprediction$Low,main = \'Low-Price\')\n#%change in Price\nboxplot(inrvsusdprediction$Change,main = \'Change-in-Price\')\nhist(inrvsusdprediction$Change,main = \'Change-in-Price\')\n#to check the response variable noramlity, if the p value is > 0.05 then okay else take sqr\n#root of the reponse variable and bind them in the data frame\n#and check the box plot for normality\n#then run the shaprio test\nshapiro.test(inrvsusdprediction$Price)\n#check multi collienarity\npairs(inrvsusdprediction,gap = 0.5)\nattach(inrvsusdprediction)\n#linear relationship between the predictors\n#VIF - Variance Inflation factor\ninrvsus.lm<-lm(PriceOpen+High+Low,data = inrvsusdprediction)\nsummary(inrvsus.lm)\nvif(inrvsus.lm)\n#if any values are > 10 then there is collienarity between variables\n#we do not have any so we can move ahead\npar(mfrow=c(1,2))\nplot(PriceOpen)\nsmooth.line = smooth.spline(PriceOpen,spar = 0.99)\nlines(smooth.line,col= \'red\')\nplot(PriceHigh)\nsmooth.line = smooth.spline(PriceOpen,spar = 0.99)\nlines(smooth.line,col= \'red\')\n#check the linear model summary for the relationship between the variables.\ninrvsusd.model <- lm(PriceOpen+High, data = inrvsusdprediction)\nsummary(inrvsusd.model)\n#residual model\nResidualsInrUSD<-resid(inrvsusd.model)\nprint(ResidualsInrUSD)\n#fitted model\nPredictedINRUSD <-predict(inrvsusd.model)\nprint(PredictedINRUSD)\npar(mfrow=c(1,1))\nplot (ResidualsInrUSDPredictedINRUSD)\n#Cooks distance\npar(mfrow=c(2,2))\nplot(inrvsusd.model)\n#Confidence interval 2.5% & 97.5% should not be 0, if 0 then its\n#less influential\nconfint(inrvsusd.model)\n#Partial Regression Plots\navPlots(inrvsusd.model)\n#hypotheis test of each model\nOpenvsHigh.model   <- lm(PriceOpen+High, data = inrvsusdprediction)\ninrvsusdHigh.model <- lm(PriceHigh, data = inrvsusdprediction)\ninrvsusdOpen.model <- lm(PriceOpen, data = inrvsusdprediction)\ninrvsusdLow.model  <- lm(Price~Low, data = inrvsusdprediction)\n#Null Hypothesis Test\nanova(inrvsusdHigh.model,inrvsusdOpen.model,test =\'Chi\')\n#summary of each model the highest r2 value model is a good mo\nsummary(OpenvsHigh.model)\nsummary(inrvsusdHigh.model)\nsummary(inrvsusdLow.model)\n#Akaike Information Criterion - Lowest AIC value for the best model\nAIC(OpenvsHigh.model)\nAIC(inrvsusdHigh.model)\nAIC(inrvsusdLow.model)\n#Best model after all testing\nsummary(OpenvsHigh.model)\n#model final testing\npar(mfrow=c(2,2))\nplot(OpenvsHigh.model)\n#tested the price of the INR is highly depended on the Open & the high value against USD\nResidualsInrUSD<-resid(OpenvsHigh.model)\nshapiro.test(OpenvsHigh.model$residuals)\navPlot(OpenvsHigh.model)\nvif(OpenvsHigh.model)\n'], 'url_profile': 'https://github.com/AsikDataPshyco', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'MIT license', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 7, 2020']}","{'location': 'Gurgaon', 'stats_list': [], 'contributions': '755 contributions\n        in the last year', 'description': ['Statistical-Linear-Regression-\nLinear Regression Model along with transformations\n'], 'url_profile': 'https://github.com/piyushpathak03', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'MIT license', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Data_Science_Project\nHome Price prediction Using Linear Regression Model\n'], 'url_profile': 'https://github.com/Piyushg5', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'MIT license', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['RegressPy\n\n\n\n\n\nThis package provides functions for regression testing on github, by comparing the current commit to a given one.\nGetting Started\nInstallation is easily done via pip. The package is then used as regresspy.\npip install regresspy\n\nExample:\nimport regresspy\n\n# TODO: Include example once functionality is implemented\nDescription\nAuthors\n\npyLHC/OMC-Team - Working Group - pyLHC\n\nLicense\nThis project is licensed under the MIT License - see the LICENSE file for details.\n'], 'url_profile': 'https://github.com/pylhc', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'MIT license', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['sales_prediction_grocerystore\npredicting with XGboost and Linear regression\n'], 'url_profile': 'https://github.com/nithinkrishna2019', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'MIT license', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 7, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['Predicting-House-Prices-with-Regression-using-TensorFlow\nPredicting House Prices with Regression using TensorFlow\n'], 'url_profile': 'https://github.com/AmitaKashid', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'MIT license', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saloni123-S', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'MIT license', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Housing-\nmultiple linear regression using encoding method\n'], 'url_profile': 'https://github.com/mp20mp', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'MIT license', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '200 contributions\n        in the last year', 'description': ['predictive_logistic_regression\nIntroduction to Logistic Regression for Prediction\n'], 'url_profile': 'https://github.com/Bmcgarry194', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'MIT license', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Problem Statement\nA automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\n\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\nBusiness Goal\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/shabbirc', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'C', 'BSD-2-Clause license', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Capetown, South Africa', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['LogisticRegression\nA few more things to add:\n\nBuilt in training/validation split\nAdvanced Accuracy metrics - Precision and Recall\nSupport for multi-class classification\nPossibly implementing boosting/bagging\nBuilt in feature normalization\nAdditional optimization methods besides gradient descent\n\n'], 'url_profile': 'https://github.com/DBhugwandas', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'C', 'BSD-2-Clause license', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saloni123-S', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'C', 'BSD-2-Clause license', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['yrrc\nYRRC (YARA Regression Checker, or YARA Regular Regression Checker, pronounced\n""yar-kuh) is a tool to automate the testing of YARA signatures using a\ncontinuously built YARA repository.\nWhy?\nOver years of development, YARA has gained a good number of tests which attempt\nto make sure there are no regressions, but they are never complete enough. This\nproject is an attempt to collect signatures from the community and automate the\ntesting of these signatures using the most bleeding edge YARA code. The goal is\nto leverage the extensive signature base of the community in order to catch\nregressions in YARA before they make it into a release.\nA good example of a regression which should have been caught is\nhttps://github.com/VirusTotal/yara/pull/1269, but there were no unit tests which\ncovered this case at the time. Had YRRC existed back then and we had signatures\nwhich exercised this code on packed samples we could have caught this\nregression, fixed it before the release and even added a unit test to ensure\nit stays fixed in the future.\nDesign Goals\n\nAvoid yara-python. This one may seem not-obvious but using yara-python comes\nwith some problems which are just easier to avoid in this environment. One of\nthe major problems from using yara-python here is building it. yara-python\nships with it\'s own bundled version of YARA, which may or may not be behind\nthe official repository. If I were to attempt to always build the python\nbindings with the official repository master branch there may be API changes\nmade in master that have not been reflected in the python-bindings yet. It is\neasier to avoid yara-python entirely and just write my own C program to work\nwith libyara directly.\nHave a small series of tools which are loosely coupled together. I\'m not\ninterested in a monolithic testing application. I\'m willing to glue together\nsmall pieces.\nMake it as easy as possible to run. You should be able to edit config.json\nto suit your needs, kick off a single script and have the system run completely.\nThis single script should just be calling a bunch of other scripts, so you can\nstill run the individual pieces if you want.\nAutomate as much as possible, including producing reports visible on a\nwebpage somewhere.\n\nHow Does It Work (High Level)?\nYRRC works in a multi-step process.\n\nFetch and build YARA (by default it builds whatever is in master).\nBuild yrrc binary, linking against the libyara built in step 1.\nRun yrrc in collect mode (more on this later).\nFetch as many samples as possible from VirusTotal.\nRun yrrc in scan mode (more on this later).\nProcess the output into a presentable report.\n\nDetails\nyrrc_build.py\nThe first step is to fetch and build YARA. This is done with the yrrc_build.py\nscript. This script assumes you have everything you need to build YARA and just\ndoes a bunch of subprocess calls to pull down the repository (into the current\nworking directory) and build YARA for you.\nmake\nOnce you have YARA built you can build yrrc by running make. This will build\nthe yrrc binary using the version of YARA you built in the previous step.\nyrrc collect\nWith yrrc built you can now run it in ""collect"" mode.\nwxs@wxs-mbp yrrc % DYLD_LIBRARY_PATH=/Users/wxs/src/yara/libyara/.libs ./yrrc -c config.json -m collect | jq .\n{\n  ""c5c1ca4382f397481174914b1931e851a9c61f029e6b3eb8a65c9e92ddf7aa4c"": {\n    ""expected"": [\n      ""APT_MAL_DTRACK_Oct19_1""\n    ]\n  }\n}\nwxs@wxs-mbp yrrc %\n\nThis mode parses the YARA rules, looking for any metadata key named ""sample""\nand where the value looks like a valid hash, then generates some JSON on\nstdout, which is useful to redirect to a file - I suggest ""hashes.json"" for\nnow. ;)\nNOTE: I\'m building this on OS X and am not static linking yrrc yet, so I have to\nset DYLD_LIBRARY_PATH when running yrrc. This will change in the future.\nyrrc_fetch.py\nNow that we know which hashes we need to fetch we can use yrrc_fetch.py to\nretrieve them from VT. It will store them in a directory called ""_cache"" and\nwill only retrieve hashes it does not already have.\nFor this step you will need to provide your own VT API key that can download\nsamples. Just set the path to it in config.json.\n@wxs-mbp yrrc % mkdir _cache\nwxs@wxs-mbp yrrc % python3 ./yrrc_fetch.py\nc5c1ca4382f397481174914b1931e851a9c61f029e6b3eb8a65c9e92ddf7aa4c 200\nwxs@wxs-mbp yrrc %\n\nyrrc scan\nWith the files retrieved from VT you can now run yrrc in scan mode.\nwxs@wxs-mbp yrrc % DYLD_LIBRARY_PATH=/Users/wxs/src/yara/libyara/.libs ./yrrc -c config.json -m scan | jq .\n{\n  ""c5c1ca4382f397481174914b1931e851a9c61f029e6b3eb8a65c9e92ddf7aa4c"": {\n    ""expected"": [\n      ""APT_MAL_DTRACK_Oct19_1""\n    ],\n    ""matches"": [\n      ""APT_MAL_DTRACK_Oct19_1""\n    ],\n    ""yara_error"": 0\n  }\n}\nwxs@wxs-mbp yrrc %\n\nISSUES\nThere are so many issues with this code right now I almost don\'t want to publish\nit.\n\nThe YARA build process is so janky, with no sane error handling.\nThe Makefile assumes you don\'t change the YARA build location and doesn\'t\nsupport static linking of yrrc.\nThe yrrc code itself is very ugly. I need to make a pass through all of it\nand clean it up, making error handling better and just make it more well\ndesigned.\nThe fetch script is OK but could probably be a little better when it comes\nto error handling.\nThere is nothing which processes the output from yrrc in scan mode to tell\nyou which rules caused regressions. I need to add that. ;)\nI want to build a nice web front end for the reports and run it for the\ncommunity.\n\nContributions\nIt is my hope that the YARA community will find value in this project and make\nthe YARA developers more confident in their changes. Besides improvements to the\ncode the biggest area for contributions is in YARA rules. If you want to\ncontribute YARA rules to this project I am more than happy to take them,\nprovided they can be accepted. To be accepted, the rules must meet the following\ncriteria:\n\nThey MUST be publicly available, and the pull request MUST come with a link\nto a website where I can verify they are available.\nThey MUST include a hash in the metadata section of the rule and it MUST\nuse the key ""sample"" - this will likely change in the future to support things\nother than key, but I haven\'t got to that yet.\n\nI\'m not interested in making judgements on the quality of any submitted rules.\nI am purely interested in collecting as many rules as possible that exercise\ndifferent functionality of YARA and the modules, so that I can help out the\ndevelopers be more confident in changes they are making.\n'], 'url_profile': 'https://github.com/wxsBSD', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'C', 'BSD-2-Clause license', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Copenhagen, Denmark', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': [""AirBnb-PricePrediction-Project\nThis project was from business intelligence and big data(BIBA) course in Roskilde university in my master's program and it was done with\ncollabration of my group members.\nGroup members:\n\nBego√±a Sustatxa,\nFrederik Thulstrup,\nTomas Nemecek,\nYohannes Kidane kifle\n\n""], 'url_profile': 'https://github.com/Ykidane', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'C', 'BSD-2-Clause license', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': [""Flowability Regression and Classification Toolkit\nSummary\nThis Github hosts an API for:\n\nFeature engineering\nPCA (linear and nonlinear) dimensionality reduction\nTraining and Testing Regression and Classification models for predicting flowability (raw values or classes)\n\nThis work was created for WPI Army Research Laboratory in the Spring '20 semester. The goal of this project is to use Microtrac parameters (as well as potentially augmented features) to predict the Hall Flow values of new metal particle samples. For more details, please see the Research Summary Presentation.\nFiles\nAPI & helper files\nThere are 2 main classes: a DataPreprocessor (DataPreprocessor.py), and a TrainTestPipeline (TrainTestPipeline.py).\nDataPreprocessor\nThis class handles feature engineering on raw data, correlation computations, and PCA computations. It also allows for visualizing correlation matrices and PCA (linear and nonlinear) and saving it to a file.\nDependent files:\n\ndata_preprocessing.py : This contains helper functions for cleaning & preparing the dataset\n\nTo initialize, specify the:\n\nData Folder with raw, unprocessed CSV files of samples data (e.g., Ti-Nb-Zr (0-63) Particles.csv is in here)\nFlow Values Excel Sheet with the target flow values and target flow classes.\nThis also includes the Augmented Density (AugDensity) that the materials science team provided for each sample.\n\nIMPORTANT:\n\nIn the Flow Values Excel sheet, do not type outside of the colored rows, especially in rows below the data. If adding more data, make sure to follow the same format as in the sheet.\nMake sure the names of your samples, and size ranges of the samples, match exactly with how they are named in each raw Excel data sheet in your data folder.\n\nTrainTestPipeline\nThis class takes:\n\nPreprocessed X (predictors, after feature engineering or dimensionality reduction)\nY data (raw flow values for prediction or flow classes)\nA list of samples corresponding to each row in the X & y data\nA model type (a string, see below. Don't mistype this!)\nA list of heldout samples for testing. Each sample is a string, such as 'Ti-Nb-Zr (0-63)'. By default, 3 heldout samples will be chosen at random from the 8 samples in our dataset.\n\nThe models supported are:\nRegression:\n\nDecisionTreeRegressor\nSupportVectorRegressor\nRandomForestRegressor\nkNeighborsRegressor\n\nClassification:\n\nLogisticRegression\nDecisionTreeClassifier\nRandomForestClassifier\nKNeighborsClassifier\n\nThis class can run a full train/test pipeline using one command, do_train_test(...).\nYou can also use it to visualize test performance, visualize the internals of a tree model, and see the feature importance rankings produced by a tree model. All visualizations and feature rankings are saved to an output file.\nTo find a best training subset, use the exhaustive_train() command.\nJupyter Notebooks (.ipynb)\nStart with these! There are 3 notebooks:\n\nFlowability Regression: Pearson correlation matrix feature selection with a DecisionTreeRegressor\nFlowability Classification - Binary or Multiclass: Pearson correlation matrix feature selection with a DecisionTreeClassifier\nLinear and NonLinear PCA:\n\nEach of these gives a detailed walkthrough of how to utilize the two classes above, as well as how to save visualizations from data preprocessing and model testing. Please start here, and make sure you have RawData/ and TrueFlowValues_.xlsx downloaded.\n*If you have any questions, please email Alissa Ostapenko at aostapenko@wpi.edu.\n""], 'url_profile': 'https://github.com/ostapen', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'C', 'BSD-2-Clause license', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Rohtak, Haryana, India', 'stats_list': [], 'contributions': '619 contributions\n        in the last year', 'description': ['Linear Regression model\nCreating Linear Regression Model from Scratch\nLinear Regression is a method used to define a relationship between a dependent variable (Y) and independent variable (X).\nWhich is simply written as :-\n\ny = mx + b\n\n\nWhere y is the dependent variable, m is the scale factor or coefficient, b being the bias coefficient and X being the independent variable.\n\nThe bias coefficient gives an extra degree of freedom to this model. The goal is to draw the line of best fit between X and Y which estimates the relationship between X and Y.\nBut how do we find these coefficients, We can find these using different approaches. One is the Ordinary Least Mean Square Method approach and the Gradient Descent approach.\nWe will be implementing both the approaches here\n\n\nThis repository is to implement linear regression algorithm without the use of scikit-learn python library.\n'], 'url_profile': 'https://github.com/m0-k1', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'C', 'BSD-2-Clause license', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['Multi-colinearity-example\nMulti colinearity in linear regression eaxmple\n'], 'url_profile': 'https://github.com/priya-singh2411', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'C', 'BSD-2-Clause license', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Ahmedabad, India', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Used Car price predictor\n\nApproach: Linear Regression from scratch\nUse: Helps customers to estimate the price of their old cars\n\n'], 'url_profile': 'https://github.com/agcy1210', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'C', 'BSD-2-Clause license', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Business Problem Overview\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\nFor many incumbent operators, retaining high profitable customers is the number one business goal. To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\nHere, we will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\nUnderstanding and Defining Churn\nThere are two main models of payment in the telecom industry - postpaid (customers pay a monthly/annual bill after using the services) and prepaid (customers pay/recharge with a certain amount in advance and then use the services).\nIn the postpaid model, when customers want to switch to another operator, they usually inform the existing operator to terminate the services, and you directly know that this is an instance of churn.\nHowever, in the prepaid model, customers who want to switch to another network can simply stop using the services without any notice, and it is hard to know whether someone has actually churned or is simply not using the services temporarily (e.g. someone may be on a trip abroad for a month or two and then intend to resume using the services again).\nThus, churn prediction is usually more critical (and non-trivial) for prepaid customers, and the term ‚Äòchurn‚Äô should be defined carefully.  Also, prepaid is the most common model in India and southeast Asia, while postpaid is more common in Europe in North America.\nThis project is based on the Indian and Southeast Asian market.\nDefinitions of Churn\nThere are various ways to define churn, such as:\nRevenue-based churn: Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as ‚Äòcustomers who have generated less than INR 4 per month in total/average/median revenue‚Äô.\nThe main shortcoming of this definition is that there are customers who only receive calls/SMSes from their wage-earning counterparts, i.e. they don‚Äôt generate revenue but use the services. For example, many users in rural areas only receive calls from their wage-earning siblings in urban areas.\nUsage-based churn: Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time.\nA potential shortcoming of this definition is that when the customer has stopped using the services for a while, it may be too late to take any corrective actions to retain them. For e.g., if you define churn based on a ‚Äòtwo-months zero usage‚Äô period, predicting churn could be useless since by that time the customer would have already switched to another operator.\nIn this project, we will use the usage-based definition to define churn.\nHigh-value Churn\nIn the Indian and the southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.\nIn this project, we  will define high-value customers based on a certain metric (mentioned later below) and predict churn only on high-value customers.\nUnderstanding the Business Objective and the Data\nThe dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively.\nThe business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.\nUnderstanding Customer Behaviour During Churn\nCustomers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are three phases of customer lifecycle :\nThe ‚Äògood‚Äô phase: In this phase, the customer is happy with the service and behaves as usual.\n\nThe ‚Äòaction‚Äô phase: The customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‚Äògood‚Äô months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor‚Äôs offer/improving the service quality etc.)\n\nThe ‚Äòchurn‚Äô phase: In this phase, the customer is said to have churned. You define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1/0 based on this phase, you discard all data corresponding to this phase.\n\nIn this case, since you are working over a four-month window, the first two months are the ‚Äògood‚Äô phase, the third month is the ‚Äòaction‚Äô phase, while the fourth month is the ‚Äòchurn‚Äô phas\n'], 'url_profile': 'https://github.com/shabbirc', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'C', 'BSD-2-Clause license', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated Jun 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Piyushg5', 'info_list': ['Updated May 9, 2020', 'R', 'Updated May 5, 2020', '2', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'HTML', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['linear-Regression-model-for-ML\nML | Linear Regression. Linear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables.\n'], 'url_profile': 'https://github.com/sayantann11', 'info_list': ['Updated May 9, 2020', 'R', 'Updated May 5, 2020', '2', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'HTML', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Random-Forest-Regression-On-Temperature-Data\nPython Integrated tools, PyCharm is used to implement the project.\n'], 'url_profile': 'https://github.com/Arifur-ratul', 'info_list': ['Updated May 9, 2020', 'R', 'Updated May 5, 2020', '2', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'HTML', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Linear-Regression\nThis repository is all about linear regression. I am going to post materials related to linear regression time to time.\n'], 'url_profile': 'https://github.com/gari-sha', 'info_list': ['Updated May 9, 2020', 'R', 'Updated May 5, 2020', '2', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'HTML', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 7, 2020']}","{'location': 'Lubbock, TX ', 'stats_list': [], 'contributions': '305 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tung2921', 'info_list': ['Updated May 9, 2020', 'R', 'Updated May 5, 2020', '2', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'HTML', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 7, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '192 contributions\n        in the last year', 'description': ['Diamond-Prediction-Model\nApplying different ML algorithms to predict the price of diamond like -\n1.Linear Regression2.Polynomial Regression\n3.Random Forest\n4.Visualisation Methods\n'], 'url_profile': 'https://github.com/ShubhikaBhardwaj', 'info_list': ['Updated May 9, 2020', 'R', 'Updated May 5, 2020', '2', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'HTML', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dimasuwandi', 'info_list': ['Updated May 9, 2020', 'R', 'Updated May 5, 2020', '2', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'HTML', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': ['Regression Week 4: Ridge Regression (interpretation)\nIn this notebook, we will run ridge regression multiple times with different L2 penalties to see which one produces the best fit. We will revisit the example of polynomial regression as a means to see the effect of L2 regularization. In particular, we will:\n\nUse a pre-built implementation of regression (Turi Create) to run polynomial regression\nUse matplotlib to visualize polynomial regressions\nUse a pre-built implementation of regression (Turi Create) to run polynomial regression, this time with L2 penalty\nUse matplotlib to visualize polynomial regressions under L2 regularization\nChoose best L2 penalty using cross-validation.\nAssess the final fit using test data.\n\nWe will continue to use the House data from previous notebooks.  (In the next programming assignment for this module, you will implement your own ridge regression learning algorithm using gradient descent.)\nFire up Turi Create\nimport turicreate\nPolynomial regression, revisited\nWe build on the material from Week 3, where we wrote the function to produce an SFrame with columns containing the powers of a given input. Copy and paste the function polynomial_sframe from Week 3:\ndef polynomial_sframe(feature, degree):\n    # assume that degree >= 1\n    # initialize the SFrame:\n    poly_sframe = turicreate.SFrame()\n    # and set poly_sframe[\'power_1\'] equal to the passed feature\n    poly_sframe[\'power_1\'] = feature\n    # first check if degree > 1\n    if degree > 1:\n        # then loop over the remaining degrees:\n        # range usually starts at 0 and stops at the endpoint-1. We want it to start at 2 and stop at degree\n        for power in range(2, degree+1): \n            # first we\'ll give the column a name:\n            name = \'power_\' + str(power)\n            # then assign poly_sframe[name] to the appropriate power of feature\n            poly_sframe[name] = poly_sframe[\'power_1\'].apply(lambda x: x**power)\n    return poly_sframe\n    \nLet\'s use matplotlib to visualize what a polynomial regression looks like on the house data.\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsales = turicreate.SFrame(\'home_data.sframe/\')\nAs in Week 3, we will use the sqft_living variable. For plotting purposes (connecting the dots), you\'ll need to sort by the values of sqft_living. For houses with identical square footage, we break the tie by their prices.\nsales = sales.sort([\'sqft_living\',\'price\'])\nLet us revisit the 15th-order polynomial model using the \'sqft_living\' input. Generate polynomial features up to degree 15 using polynomial_sframe() and fit a model with these features. When fitting the model, use an L2 penalty of 1e-5:\nl2_small_penalty = 1e-5\nNote: When we have so many features and so few data points, the solution can become highly numerically unstable, which can sometimes lead to strange unpredictable results.  Thus, rather than using no regularization, we will introduce a tiny amount of regularization (l2_penalty=1e-5) to make the solution numerically stable.  (In lecture, we discussed the fact that regularization can also help with numerical stability, and here we are seeing a practical example.)\nWith the L2 penalty specified above, fit the model and print out the learned weights.\nHint: make sure to add \'price\' column to the new SFrame before calling turicreate.linear_regression.create(). Also, make sure Turi Create doesn\'t create its own validation set by using the option validation_set=None in this call.\npoly15_data = polynomial_sframe(sales[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names()\npoly15_data[\'price\'] = sales[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data,\n                                              target = \'price\',\n                                              features = my_features,\n                                              validation_set = None,\n                                              l2_penalty = l2_small_penalty)\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 21613\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 1.018939     | 2662555.735422     | 245656.462162                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\n[<matplotlib.lines.Line2D at 0x7f15d05634a8>,\n <matplotlib.lines.Line2D at 0x7f15c0440780>]\n\n\nmodel15.coefficients\n\n\nname\nindex\nvalue\nstderr\n\n\n(intercept)\nNone\n167924.8683106338\nnan\n\n\npower_1\nNone\n103.09091982258876\nnan\n\n\npower_2\nNone\n0.13460458520186014\nnan\n\n\npower_3\nNone\n-0.00012907138088840022\nnan\n\n\npower_4\nNone\n5.1892899347229256e-08\nnan\n\n\npower_5\nNone\n-7.771693113356376e-12\nnan\n\n\npower_6\nNone\n1.711447665310959e-16\nnan\n\n\npower_7\nNone\n4.5117800398292624e-20\nnan\n\n\npower_8\nNone\n-4.788383278136755e-25\nnan\n\n\npower_9\nNone\n-2.333436313252247e-28\nnan\n\n\n[16 rows x 4 columns]Note: Only the head of the SFrame is printed.You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n\nQUIZ QUESTION:  What\'s the learned value for the coefficient of feature power_1?\n103.09091982258876\nObserve overfitting\nRecall from Week 3 that the polynomial fit of degree 15 changed wildly whenever the data changed. In particular, when we split the sales data into four subsets and fit the model of degree 15, the result came out to be very different for each subset. The model had a high variance. We will see in a moment that ridge regression reduces such variance. But first, we must reproduce the experiment we did in Week 3.\nFirst, split the data into split the sales data into four subsets of roughly equal size and call them set_1, set_2, set_3, and set_4. Use .random_split function and make sure you set seed=0.\n(semi_split1, semi_split2) = sales.random_split(.5,seed=0)\n(set_1, set_2) = semi_split1.random_split(0.5, seed=0)\n(set_3, set_4) = semi_split2.random_split(0.5, seed=0)\nNext, fit a 15th degree polynomial on set_1, set_2, set_3, and set_4, using \'sqft_living\' to predict prices. Print the weights and make a plot of the resulting model.\nHint: When calling turicreate.linear_regression.create(), use the same L2 penalty as before (i.e. l2_small_penalty).  Also, make sure Turi Create doesn\'t create its own validation set by using the option validation_set = None in this call.\npoly15_data = polynomial_sframe(set_1[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names()\npoly15_data[\'price\'] = set_1[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data,\n                                              target = \'price\',\n                                              features = my_features,\n                                              validation_set = None,\n                                              l2_penalty = l2_small_penalty)\nmodel15.coefficients.print_rows(num_rows=16, num_columns=4)\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 5404\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.017008     | 2191984.900834     | 248699.117253                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n+-------------+-------+-------------------------+-----------------------+\n|     name    | index |          value          |         stderr        |\n+-------------+-------+-------------------------+-----------------------+\n| (intercept) |  None |    9306.460738574853    |          nan          |\n|   power_1   |  None |    585.8658227918767    |          nan          |\n|   power_2   |  None |   -0.39730589492643253  |          nan          |\n|   power_3   |  None |  0.00014147090040698705 |          nan          |\n|   power_4   |  None |  -1.529459910633883e-08 |          nan          |\n|   power_5   |  None |  -3.797562554472397e-13 |          nan          |\n|   power_6   |  None |  5.974816422113205e-17  |          nan          |\n|   power_7   |  None |  1.068885079424125e-20  | 9.827796414259983e-17 |\n|   power_8   |  None |  1.5934406685250742e-25 |          nan          |\n|   power_9   |  None |  -6.928348684589336e-29 |          nan          |\n|   power_10  |  None |  -6.83813476071358e-33  |          nan          |\n|   power_11  |  None |  -1.626860809887929e-37 | 7.525307345296993e-33 |\n|   power_12  |  None |  2.851186198304115e-41  |          nan          |\n|   power_13  |  None |  3.799982392452631e-45  |          nan          |\n|   power_14  |  None |  1.5265261843349395e-49 |          nan          |\n|   power_15  |  None | -2.3380732075473816e-53 |          nan          |\n+-------------+-------+-------------------------+-----------------------+\n[16 rows x 4 columns]\n\n\n\n\n\n\n[<matplotlib.lines.Line2D at 0x7f15c04006d8>,\n <matplotlib.lines.Line2D at 0x7f15c01edb00>]\n\n\npoly15_data = polynomial_sframe(set_2[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names()\npoly15_data[\'price\'] = set_2[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data,\n                                              target = \'price\',\n                                              features = my_features,\n                                              validation_set = None,\n                                              l2_penalty = l2_small_penalty)\nmodel15.coefficients.print_rows(num_rows=16, num_columns=4)\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 5398\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.022818     | 1975178.190550     | 234533.610645                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n+-------------+-------+-------------------------+----------------------+\n|     name    | index |          value          |        stderr        |\n+-------------+-------+-------------------------+----------------------+\n| (intercept) |  None |   -25115.912055661436   |  1066759.1384981384  |\n|   power_1   |  None |    783.4938272804999    |  5154.635347138643   |\n|   power_2   |  None |   -0.7677593406452785   |  9.753020273656633   |\n|   power_3   |  None |   0.000438766397164288  | 0.007976352435171218 |\n|   power_4   |  None | -1.1516917938560663e-07 |         nan          |\n|   power_5   |  None |   6.84281733897351e-12  |         nan          |\n|   power_6   |  None |  2.5119510955255233e-15 |         nan          |\n|   power_7   |  None | -2.0644049916397132e-19 |         nan          |\n|   power_8   |  None |  -4.596731075779906e-23 |         nan          |\n|   power_9   |  None |  -2.712776424995684e-29 |         nan          |\n|   power_10  |  None |   6.21818426987022e-31  |         nan          |\n|   power_11  |  None |  6.517414176316569e-35  |         nan          |\n|   power_12  |  None |  -9.41315223930008e-40  |         nan          |\n|   power_13  |  None | -1.0242139283793461e-42 |         nan          |\n|   power_14  |  None | -1.0039107807384084e-46 |         nan          |\n|   power_15  |  None |  1.3011336139980334e-50 |         nan          |\n+-------------+-------+-------------------------+----------------------+\n[16 rows x 4 columns]\n\n\n\n\n\n\n[<matplotlib.lines.Line2D at 0x7f15c013b198>,\n <matplotlib.lines.Line2D at 0x7f15c0168860>]\n\n\npoly15_data = polynomial_sframe(set_3[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names()\npoly15_data[\'price\'] = set_3[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data,\n                                              target = \'price\',\n                                              features = my_features,\n                                              validation_set = None,\n                                              l2_penalty = l2_small_penalty)\nmodel15.coefficients.print_rows(num_rows=16, num_columns=4)\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 5409\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.017799     | 2283722.685233     | 251097.728054                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n+-------------+-------+-------------------------+-----------------------+\n|     name    | index |          value          |         stderr        |\n+-------------+-------+-------------------------+-----------------------+\n| (intercept) |  None |    462426.5601171815    |   1310349.080391138   |\n|   power_1   |  None |    -759.251823483348    |   6357.396009618166   |\n|   power_2   |  None |    1.028670022953628    |   12.871561488676209  |\n|   power_3   |  None |  -0.0005282645136443045 |  0.014300230535834086 |\n|   power_4   |  None |  1.1542290544321842e-07 |  9.58250019181832e-06 |\n|   power_5   |  None |  -2.260959869224837e-12 | 3.920480702158251e-09 |\n|   power_6   |  None | -2.0821425559670597e-15 |  8.31747402726747e-13 |\n|   power_7   |  None |  4.087698878766004e-20  |          nan          |\n|   power_8   |  None |  2.5707916122050266e-23 |          nan          |\n|   power_9   |  None |  1.2431131050849085e-27 |          nan          |\n|   power_10  |  None | -1.7202591411366398e-31 |  6.21691609844703e-28 |\n|   power_11  |  None | -2.9676099903981554e-35 | 8.900669040558528e-32 |\n|   power_12  |  None | -1.0657497369273635e-39 | 9.023794547733603e-36 |\n|   power_13  |  None |  2.426357157130831e-43  | 7.848200421077859e-40 |\n|   power_14  |  None |  3.5559870251379337e-47 | 3.862234528305765e-44 |\n|   power_15  |  None |  -2.857774518124205e-51 | 7.744209502999505e-49 |\n+-------------+-------+-------------------------+-----------------------+\n[16 rows x 4 columns]\n\n\n\n\n\n\n[<matplotlib.lines.Line2D at 0x7f15c0119cc0>,\n <matplotlib.lines.Line2D at 0x7f15c00e90b8>]\n\n\npoly15_data = polynomial_sframe(set_4[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names()\npoly15_data[\'price\'] = set_4[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data,\n                                              target = \'price\',\n                                              features = my_features,\n                                              validation_set = None,\n                                              l2_penalty = l2_small_penalty)\nmodel15.coefficients.print_rows(num_rows=16, num_columns=4)\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 5402\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.013050     | 2378292.373331     | 244341.293208                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n+-------------+-------+-------------------------+------------------------+\n|     name    | index |          value          |         stderr         |\n+-------------+-------+-------------------------+------------------------+\n| (intercept) |  None |   -170240.04166485136   |   1554246.1261089714   |\n|   power_1   |  None |    1247.5903831275245   |   10197.07335608088    |\n|   power_2   |  None |    -1.224609183161148   |   27.919991642491652   |\n|   power_3   |  None |  0.0005552546758525209  |  0.04229553716799707   |\n|   power_4   |  None |  -6.38262585527143e-08  | 3.9580832892859916e-05 |\n|   power_5   |  None | -2.2021594562925072e-11 | 2.4003566731442154e-08 |\n|   power_6   |  None |  4.818346610997274e-15  | 9.436056395736535e-12  |\n|   power_7   |  None |  4.2146158179457224e-19 | 2.1404750695939616e-15 |\n|   power_8   |  None |  -7.998806727069852e-23 |          nan           |\n|   power_9   |  None | -1.3236591161465522e-26 |          nan           |\n|   power_10  |  None |  1.6019790435757102e-31 |          nan           |\n|   power_11  |  None |  2.399043553347965e-34  |          nan           |\n|   power_12  |  None |  2.3335443247666626e-38 | 9.874936494018852e-35  |\n|   power_13  |  None | -1.7987404588565005e-42 | 1.2390001750004404e-38 |\n|   power_14  |  None | -6.0286258702437944e-46 | 6.0927404707810355e-43 |\n|   power_15  |  None |  4.394726119172111e-50  | 1.2983652058413109e-47 |\n+-------------+-------+-------------------------+------------------------+\n[16 rows x 4 columns]\n\n\n\n\n\n\n[<matplotlib.lines.Line2D at 0x7f15c0080e10>,\n <matplotlib.lines.Line2D at 0x7f15c00329b0>]\n\n\nThe four curves should differ from one another a lot, as should the coefficients you learned.\nQUIZ QUESTION:  For the models learned in each of these training sets, what are the smallest and largest values you learned for the coefficient of feature power_1?  (For the purpose of answering this question, negative numbers are considered ""smaller"" than positive numbers. So -5 is smaller than -3, and -3 is smaller than 5 and so forth.)\nset_3: -759.251823483348\nset_4: 1247.5903831275245\nRidge regression comes to rescue\nGenerally, whenever we see weights change so much in response to change in data, we believe the variance of our estimate to be large. Ridge regression aims to address this issue by penalizing ""large"" weights. (Weights of model15 looked quite small, but they are not that small because \'sqft_living\' input is in the order of thousands.)\nWith the argument l2_penalty=1e5, fit a 15th-order polynomial model on set_1, set_2, set_3, and set_4. Other than the change in the l2_penalty parameter, the code should be the same as the experiment above. Also, make sure Turi Create doesn\'t create its own validation set by using the option validation_set = None in this call.\nl2_penalty=1e5\npoly15_data = polynomial_sframe(set_1[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names()\npoly15_data[\'price\'] = set_1[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data,\n                                              target = \'price\',\n                                              features = my_features,\n                                              validation_set = None,\n                                              l2_penalty = l2_penalty)\nmodel15.coefficients.print_rows(num_rows=16, num_columns=4)\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 5404\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.011180     | 5978778.434729     | 374261.720860                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n+-------------+-------+------------------------+------------------------+\n|     name    | index |         value          |         stderr         |\n+-------------+-------+------------------------+------------------------+\n| (intercept) |  None |   530317.0245158835    |          nan           |\n|   power_1   |  None |   2.5873887567286866   |          nan           |\n|   power_2   |  None | 0.0012741440059211371  |          nan           |\n|   power_3   |  None | 1.7493422693158899e-07 |          nan           |\n|   power_4   |  None | 1.0602211909664251e-11 |          nan           |\n|   power_5   |  None | 5.422476044821804e-16  |          nan           |\n|   power_6   |  None | 2.895638283427737e-20  |          nan           |\n|   power_7   |  None | 1.6500066635095529e-24 | 1.4789630292567112e-16 |\n|   power_8   |  None | 9.860815284092932e-29  |          nan           |\n|   power_9   |  None |  6.06589348254357e-33  |          nan           |\n|   power_10  |  None | 3.789178688696588e-37  |          nan           |\n|   power_11  |  None | 2.3822312131219896e-41 | 1.1324666159485423e-32 |\n|   power_12  |  None | 1.4984796921456947e-45 |          nan           |\n|   power_13  |  None | 9.391611902848278e-50  |          nan           |\n|   power_14  |  None |  5.84523161980618e-54  |          nan           |\n|   power_15  |  None | 3.601202072029721e-58  |          nan           |\n+-------------+-------+------------------------+------------------------+\n[16 rows x 4 columns]\n\n\n\n\n\n\n[<matplotlib.lines.Line2D at 0x7f15c00656d8>,\n <matplotlib.lines.Line2D at 0x7f159c5339e8>]\n\n\npoly15_data = polynomial_sframe(set_2[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names()\npoly15_data[\'price\'] = set_2[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data,\n                                              target = \'price\',\n                                              features = my_features,\n                                              validation_set = None,\n                                              l2_penalty = l2_penalty)\nmodel15.coefficients.print_rows(num_rows=16, num_columns=4)\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 5398\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.010992     | 2984894.541944     | 323238.809634                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n+-------------+-------+------------------------+----------------------+\n|     name    | index |         value          |        stderr        |\n+-------------+-------+------------------------+----------------------+\n| (intercept) |  None |   519216.89738342643   |  1470228.3103281604  |\n|   power_1   |  None |   2.044704741819378    |   7104.21925932691   |\n|   power_2   |  None | 0.0011314362683958127  |  13.441803308778976  |\n|   power_3   |  None | 2.930742775489716e-07  | 0.010993164942419803 |\n|   power_4   |  None | 4.4354059845325974e-11 |         nan          |\n|   power_5   |  None | 4.808491122043446e-15  |         nan          |\n|   power_6   |  None | 4.530917078263864e-19  |         nan          |\n|   power_7   |  None | 4.1604291057458376e-23 |         nan          |\n|   power_8   |  None | 3.900946351283382e-27  |         nan          |\n|   power_9   |  None | 3.7773187602026064e-31 |         nan          |\n|   power_10  |  None | 3.766503268417181e-35  |         nan          |\n|   power_11  |  None | 3.8422809475395966e-39 |         nan          |\n|   power_12  |  None | 3.985208284143722e-43  |         nan          |\n|   power_13  |  None | 4.1827276239367343e-47 |         nan          |\n|   power_14  |  None | 4.427383328777781e-51  |         nan          |\n|   power_15  |  None | 4.715182454121554e-55  |         nan          |\n+-------------+-------+------------------------+----------------------+\n[16 rows x 4 columns]\n\n\n\n\n\n\n[<matplotlib.lines.Line2D at 0x7f159c4ef518>,\n <matplotlib.lines.Line2D at 0x7f159c4a7438>]\n\n\npoly15_data = polynomial_sframe(set_3[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names()\npoly15_data[\'price\'] = set_3[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data,\n                                              target = \'price\',\n                                              features = my_features,\n                                              validation_set = None,\n                                              l2_penalty = l2_penalty)\nmodel15.coefficients.print_rows(num_rows=16, num_columns=4)\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 5409\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.016658     | 3695342.767093     | 350033.521294                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n+-------------+-------+------------------------+------------------------+\n|     name    | index |         value          |         stderr         |\n+-------------+-------+------------------------+------------------------+\n| (intercept) |  None |   522911.5180475718    |   1826643.7784532942   |\n|   power_1   |  None |   2.2689042187657877   |   8862.293294139941    |\n|   power_2   |  None | 0.0012590504184157225  |    17.9431252817388    |\n|   power_3   |  None | 2.7755291815451765e-07 |  0.019934708643388317  |\n|   power_4   |  None | 3.209330977903899e-11  | 1.3358130760230367e-05 |\n|   power_5   |  None |  2.87573572364483e-15  | 5.465201441592894e-09  |\n|   power_6   |  None | 2.5007611267119213e-19 | 1.1594667720009293e-12 |\n|   power_7   |  None | 2.2468526590627848e-23 |          nan           |\n|   power_8   |  None | 2.0934998313470215e-27 |          nan           |\n|   power_9   |  None | 2.0043538329631962e-31 |          nan           |\n|   power_10  |  None | 1.9541080024851158e-35 | 8.666462458236405e-28  |\n|   power_11  |  None | 1.9273411945583566e-39 | 1.2407649206084062e-31 |\n|   power_12  |  None |  1.91483699012907e-43  | 1.257928777554307e-35  |\n|   power_13  |  None |  1.91102277046499e-47  | 1.0940494167353595e-39 |\n|   power_14  |  None | 1.912462423017048e-51  | 5.384005512448125e-44  |\n|   power_15  |  None | 1.9169955803503674e-55 | 1.0795529465682826e-48 |\n+-------------+-------+------------------------+------------------------+\n[16 rows x 4 columns]\n\n\n\n\n\n\n[<matplotlib.lines.Line2D at 0x7f159c45aeb8>,\n <matplotlib.lines.Line2D at 0x7f159c402c88>]\n\n\npoly15_data = polynomial_sframe(set_4[\'sqft_living\'], 15)\nmy_features = poly15_data.column_names()\npoly15_data[\'price\'] = set_4[\'price\'] # add price to the data since it\'s the target\nmodel15 = turicreate.linear_regression.create(poly15_data,\n                                              target = \'price\',\n                                              features = my_features,\n                                              validation_set = None,\n                                              l2_penalty = l2_penalty)\nmodel15.coefficients.print_rows(num_rows=16, num_columns=4)\nplt.plot(poly15_data[\'power_1\'],poly15_data[\'price\'],\'.\',\n         poly15_data[\'power_1\'], model15.predict(poly15_data),\'-\')\nLinear regression:\n--------------------------------------------------------\nNumber of examples          : 5402\nNumber of features          : 15\nNumber of unpacked features : 15\nNumber of coefficients    : 16\nStarting Newton Method\n--------------------------------------------------------\n+-----------+----------+--------------+--------------------+---------------------------------+\n| Iteration | Passes   | Elapsed Time | Training Max Error | Training Root-Mean-Square Error |\n+-----------+----------+--------------+--------------------+---------------------------------+\n| 1         | 2        | 0.011929     | 3601895.280124     | 323111.582889                   |\n+-----------+----------+--------------+--------------------+---------------------------------+\nSUCCESS: Optimal solution found.\n\n+-------------+-------+------------------------+------------------------+\n|     name    | index |         value          |         stderr         |\n+-------------+-------+------------------------+------------------------+\n| (intercept) |  None |   513667.0870874073    |   2055301.088950528    |\n|   power_1   |  None |   1.9104093824432022   |   13484.38681673255    |\n|   power_2   |  None | 0.0011005802917477242  |   36.92078639434192    |\n|   power_3   |  None | 3.127539878788059e-07  |  0.05593069343319507   |\n|   power_4   |  None | 5.500678868246386e-11  | 5.2340827864839856e-05 |\n|   power_5   |  None |  7.20467557824708e-15  | 3.1741791736253475e-08 |\n|   power_6   |  None | 8.249772493837897e-19  | 1.2478034630273316e-11 |\n|   power_7   |  None | 9.065032234977414e-23  | 2.830517424174993e-15  |\n|   power_8   |  None | 9.956831604526312e-27  |          nan           |\n|   power_9   |  None | 1.1083812798160367e-30 |          nan           |\n|   power_10  |  None | 1.2531522414327033e-34 |          nan           |\n|   power_11  |  None | 1.4360078140197673e-38 |          nan           |\n|   power_12  |  None | 1.6626996780013466e-42 | 1.3058400074822684e-34 |\n|   power_13  |  None | 1.9398172452969622e-46 | 1.638426736995295e-38  |\n|   power_14  |  None |  2.27541485770272e-50  | 8.056906762662404e-43  |\n|   power_15  |  None | 2.679487848971385e-54  | 1.7169297555862313e-47 |\n+-------------+-------+------------------------+------------------------+\n[16 rows x 4 columns]\n\n\n\n\n\n\n[<matplotlib.lines.Line2D at 0x7f159c3c6048>,\n <matplotlib.lines.Line2D at 0x7f159c3f45c0>]\n\n\nThese curves should vary a lot less, now that you applied a high degree of regularization.\nQUIZ QUESTION:  For the models learned with the high level of regularization in each of these training sets, what are the smallest and largest values you learned for the coefficient of feature power_1? (For the purpose of answering this question, negative numbers are considered ""smaller"" than positive numbers. So -5 is smaller than -3, and -3 is smaller than 5 and so forth.)\nset_4: 1.9104093824432022\nset_1: 2.5873887567286866\nSelecting an L2 penalty via cross-validation\nJust like the polynomial degree, the L2 penalty is a ""magic"" parameter we need to select. We could use the validation set approach as we did in the last module, but that approach has a major disadvantage: it leaves fewer observations available for training. Cross-validation seeks to overcome this issue by using all of the training set in a smart way.\nWe will implement a kind of cross-validation called k-fold cross-validation. The method gets its name because it involves dividing the training set into k segments of roughtly equal size. Similar to the validation set method, we measure the validation error with one of the segments designated as the validation set. The major difference is that we repeat the process k times as follows:\nSet aside segment 0 as the validation set, and fit a model on rest of data, and evalutate it on this validation set\nSet aside segment 1 as the validation set, and fit a model on rest of data, and evalutate it on this validation set\n...\nSet aside segment k-1 as the validation set, and fit a model on rest of data, and evalutate it on this validation set\nAfter this process, we compute the average of the k validation errors, and use it as an estimate of the generalization error. Notice that  all observations are used for both training and validation, as we iterate over segments of data.\nTo estimate the generalization error well, it is crucial to shuffle the training data before dividing them into segments. The package turicreate_cross_validation (see below) has a utility function for shuffling a given SFrame. We reserve 10% of the data as the test set and shuffle the remainder. (Make sure to use seed=1 to get consistent answer.)\nNote: For applying cross-validation, we will import a package called turicreate_cross_validation. To install it, please run this command on your terminal:\npip install -e git+https://github.com/Kagandi/turicreate-cross-validation.git#egg=turicreate_cross_validation\nYou can find the documentation on this package here: https://github.com/Kagandi/turicreate-cross-validation\nimport turicreate_cross_validation.cross_validation as tcv\n\n(train_valid, test) = sales.random_split(.9, seed=1)\ntrain_valid_shuffled = tcv.shuffle_sframe(train_valid, random_seed=1)\nOnce the data is shuffled, we divide it into equal segments. Each segment should receive n/k elements, where n is the number of observations in the training set and k is the number of segments. Since the segment 0 starts at index 0 and contains n/k elements, it ends at index (n/k)-1. The segment 1 starts where the segment 0 left off, at index (n/k). With n/k elements, the segment 1 ends at index (n*2/k)-1. Continuing in this fashion, we deduce that the segment i starts at index (n*i/k) and ends at (n*(i+1)/k)-1.\nWith this pattern in mind, we write a short loop that prints the starting and ending indices of each segment, just to make sure you are getting the splits right.\nn = len(train_valid_shuffled)\nk = 10 # 10-fold cross-validation\n\nfor i in range(k):\n    start = (n*i)/k\n    end = (n*(i+1))/k-1\n    print(i, (start, end))\n0 (0.0, 1938.6)\n1 (1939.6, 3878.2)\n2 (3879.2, 5817.8)\n3 (5818.8, 7757.4)\n4 (7758.4, 9697.0)\n5 (9698.0, 11636.6)\n6 (11637.6, 13576.2)\n7 (13577.2, 15515.8)\n8 (15516.8, 17455.4)\n9 (17456.4, 19395.0)\n\nLet us familiarize ourselves with array slicing with SFrame. To extract a continuous slice from an SFrame, use colon in square brackets. For instance, the following cell extracts rows 0 to 9 of train_valid_shuffled. Notice that the first index (0) is included in the slice but the last index (10) is omitted.\ntrain_valid_shuffled[0:10] # rows 0 to 9\n\n\nid\ndate\nprice\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\n\n\n8645511350\n2014-12-01 00:00:00+00:00\n300000.0\n3.0\n1.75\n1810.0\n21138.0\n1.0\n0\n\n\n7237501370\n2014-07-17 00:00:00+00:00\n1079000.0\n4.0\n3.25\n4800.0\n12727.0\n2.0\n0\n\n\n7278700100\n2015-01-21 00:00:00+00:00\n625000.0\n4.0\n2.5\n2740.0\n9599.0\n1.0\n0\n\n\n1421079007\n2015-03-24 00:00:00+00:00\n408506.0\n3.0\n2.75\n2480.0\n209199.0\n1.5\n0\n\n\n4338800370\n2014-11-17 00:00:00+00:00\n220000.0\n3.0\n1.0\n1000.0\n6020.0\n1.0\n0\n\n\n7511200020\n2014-08-29 00:00:00+00:00\n509900.0\n3.0\n1.75\n1690.0\n53578.0\n1.0\n0\n\n\n3300701615\n2014-09-30 00:00:00+00:00\n655000.0\n4.0\n2.5\n2630.0\n4000.0\n3.0\n0\n\n\n7011200260\n2014-12-19 00:00:00+00:00\n485000.0\n4.0\n2.0\n1400.0\n3600.0\n1.0\n0\n\n\n3570000130\n2014-06-11 00:00:00+00:00\n580379.0\n4.0\n2.75\n2240.0\n27820.0\n1.5\n0\n\n\n2796100640\n2015-04-24 00:00:00+00:00\n264900.0\n4.0\n2.5\n2040.0\n7000.0\n1.0\n0\n\n\n\n\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\n\n\n0\n4\n7.0\n1240.0\n570.0\n1977.0\n0.0\n98058\n47.46736904\n\n\n0\n3\n10.0\n4800.0\n0.0\n2011.0\n0.0\n98059\n47.53108576\n\n\n2\n3\n8.0\n1820.0\n920.0\n1961.0\n0.0\n98177\n47.77279701\n\n\n0\n3\n8.0\n1870.0\n610.0\n2000.0\n0.0\n98010\n47.30847072\n\n\n0\n3\n6.0\n1000.0\n0.0\n1944.0\n0.0\n98166\n47.47933643\n\n\n0\n3\n8.0\n1690.0\n0.0\n1984.0\n0.0\n98053\n47.6545751\n\n\n0\n3\n8.0\n2630.0\n0.0\n2002.0\n0.0\n98117\n47.69151411\n\n\n0\n3\n7.0\n1100.0\n300.0\n1900.0\n0.0\n98119\n47.63846783\n\n\n0\n4\n8.0\n2240.0\n0.0\n1976.0\n0.0\n98075\n47.59357299\n\n\n0\n3\n7.0\n1250.0\n790.0\n1979.0\n0.0\n98031\n47.40555074\n\n\n\n\nlong\nsqft_living15\nsqft_lot15\n\n\n-122.17768631\n1850.0\n12200.0\n\n\n-122.13389261\n4750.0\n13602.0\n\n\n-122.38485302\n2660.0\n8280.0\n\n\n-121.88816296\n2040.0\n219229.0\n\n\n-122.34575463\n1300.0\n8640.0\n\n\n-122.04899568\n2290.0\n52707.0\n\n\n-122.38139901\n1640.0\n4000.0\n\n\n-122.36993806\n1630.0\n2048.0\n\n\n-122.05362447\n2330.0\n20000.0\n\n\n-122.17648783\n1900.0\n7378.0\n\n\n[10 rows x 21 columns]\n\nNow let us extract individual segments with array slicing. Consider the scenario where we group the houses in the train_valid_shuffled dataframe into k=10 segments of roughly equal size, with starting and ending indices computed as above.\nExtract the fourth segment (segment 3) and assign it to a variable called validation4.\nvalidation4 = train_valid_shuffled[5818.8:7757.4]\nTo verify that we have the right elements extracted, run the following cell, which computes the average price of the fourth segment. When rounded to nearest whole number, the average should be $559,642.\nprint(int(round(validation4[\'price\'].mean(), 0)))\n559642\n\nAfter designating one of the k segments as the validation set, we train a model using the rest of the data. To choose the remainder, we slice (0:start) and (end+1:n) of the data and paste them together. SFrame has append() method that pastes together two disjoint sets of rows originating from a common dataset. For instance, the following cell pastes together the first and last two rows of the train_valid_shuffled dataframe.\nn = len(train_valid_shuffled)\nfirst_two = train_valid_shuffled[0:2]\nlast_two = train_valid_shuffled[n-2:n]\nprint(first_two.append(last_two))\n+------------+---------------------------+-----------+----------+-----------+\n|     id     |            date           |   price   | bedrooms | bathrooms |\n+------------+---------------------------+-----------+----------+-----------+\n| 8645511350 | 2014-12-01 00:00:00+00:00 |  300000.0 |   3.0    |    1.75   |\n| 7237501370 | 2014-07-17 00:00:00+00:00 | 1079000.0 |   4.0    |    3.25   |\n| 4077800582 | 2014-09-12 00:00:00+00:00 |  522000.0 |   3.0    |    1.0    |\n| 7853370620 | 2015-02-06 00:00:00+00:00 |  605000.0 |   5.0    |    4.0    |\n+------------+---------------------------+-----------+----------+-----------+\n+-------------+----------+--------+------------+------+-----------+-------+\n| sqft_living | sqft_lot | floors | waterfront | view | condition | grade |\n+-------------+----------+--------+------------+------+-----------+-------+\n|    1810.0   | 21138.0  |  1.0   |     0      |  0   |     4     |  7.0  |\n|    4800.0   | 12727.0  |  2.0   |     0      |  0   |     3     |  10.0 |\n|    1150.0   |  7080.0  |  1.0   |     0      |  0   |     3     |  7.0  |\n|    3040.0   |  6000.0  |  2.0   |     0      |  0   |     3     |  8.0  |\n+-------------+----------+--------+------------+------+-----------+-------+\n+------------+---------------+----------+--------------+---------+-------------+\n| sqft_above | sqft_basement | yr_built | yr_renovated | zipcode |     lat     |\n+------------+---------------+----------+--------------+---------+-------------+\n|   1240.0   |     570.0     |  1977.0  |     0.0      |  98058  | 47.46736904 |\n|   4800.0   |      0.0      |  2011.0  |     0.0      |  98059  | 47.53108576 |\n|   1150.0   |      0.0      |  1952.0  |     0.0      |  98125  | 47.71063854 |\n|   2280.0   |     760.0     |  2011.0  |     0.0      |  98065  | 47.51887717 |\n+------------+---------------+----------+--------------+---------+-------------+\n+---------------+---------------+-----+\n|      long     | sqft_living15 | ... |\n+---------------+---------------+-----+\n| -122.17768631 |     1850.0    | ... |\n| -122.13389261 |     4750.0    | ... |\n| -122.28837299 |     1490.0    | ... |\n| -121.87558112 |     3070.0    | ... |\n+---------------+---------------+-----+\n[4 rows x 21 columns]\n\nExtract the remainder of the data after excluding fourth segment (segment 3) and assign the subset to train4.\ntrain4_1 = train_valid_shuffled[0:5818.8]\ntrain4_2 = train_valid_shuffled[7757.4+1:n]\ntrain4 = train4_1.append(train4_2)\nTo verify that we have the right elements extracted, run the following cell, which computes the average price of the data with fourth segment excluded. When rounded to nearest whole number, the average should be $536,865.\nprint(int(round(train4[\'price\'].mean(), 0)))\n536866\n\nNow we are ready to implement k-fold cross-validation. Write a function that computes k validation errors by designating each of the k segments as the validation set. It accepts as parameters (i) k, (ii) l2_penalty, (iii) dataframe, (iv) name of output column (e.g. price) and (v) list of feature names. The function returns the average validation error using k segments as validation sets.\n\nFor each i in [0, 1, ..., k-1]:\n\nCompute starting and ending indices of segment i and call \'start\' and \'end\'\nForm validation set by taking a slice (start:end+1) from the data.\nForm training set by appending slice (end+1:n) to the end of slice (0:start).\nTrain a linear model using training set just formed, with a given l2_penalty\nCompute validation error using validation set just formed\n\n\n\nimport numpy as np\ndef k_fold_cross_validation(k, l2_penalty, data, output_name, features_list):\n    val_errors = []\n    n = len(data)\n    for i in range(k):\n        start = (n*i)/k\n        end = (n*(i+1))/k-1\n        validation = data[start:end]\n        train_1 = data[0:start]\n        train_2 = data[end+1:n]\n        train = train_1.append(train_2)\n        model = turicreate.linear_regression.create(data,\n                                              target = output_name,\n                                              features = features_list,\n                                              validation_set = None,\n                                              l2_penalty = l2_penalty,\n                                              verbose = False)\n        # First get the predictions\n        predicted = model.predict(validation);\n        # Then compute the residuals/errors\n        errors = validation[output_name]-predicted;\n        # Then square and add them up    \n        val_errors.append((errors**2).sum());\n    return np.mean(val_errors)  \nOnce we have a function to compute the average validation error for a model, we can write a loop to find the model that minimizes the average validation error. Write a loop that does the following:\n\nWe will again be aiming to fit a 15th-order polynomial model using the sqft_living input\nFor l2_penalty in [10^1, 10^1.5, 10^2, 10^2.5, ..., 10^7] (to get this in Python, you can use this Numpy function: np.logspace(1, 7, num=13).)\n\nRun 10-fold cross-validation with l2_penalty\n\n\nReport which L2 penalty produced the lowest average validation error.\n\nNote: since the degree of the polynomial is now fixed to 15, to make things faster, you should generate polynomial features in advance and re-use them throughout the loop. Make sure to use train_valid_shuffled when generating polynomial features!\nk = 10\nval = []\nl2_penalties = np.logspace(1, 7, num=13)\noutput_name = \'price\'\nfeature = \'sqft_living\'\norder = 15\npoly_data = polynomial_sframe(train_valid_shuffled[feature], order)\npoly_data[output_name] = train_valid_shuffled[output_name]\nfor l2_penalty in l2_penalties:\n    val.append(k_fold_cross_validation(k = k,\n                                       l2_penalty = l2_penalty,\n                                       data = poly_data,\n                                       output_name = output_name,\n                                       features_list = [\'power_1\']))\nprint(val)\nmin_l2_penalty = l2_penalties[val.index(min(val))]\nprint(\'min: \' + str(min(val)) + \' corresponding to l2_penalty: \' + str(min_l2_penalty))\n[134815190674744.03, 134826762544992.8, 134936513681533.4, 135890041250087.4, 142333491208169.8, 167452241996168.44, 210349227220562.44, 243113604033400.66, 258014932599918.0, 263371413280573.8, 265138128087065.66, 265704396689880.6, 265884234809859.75]\nmin: 134815190674744.03 corresponding to l2_penalty: 10.0\n\nQUIZ QUESTIONS:  What is the best value for the L2 penalty according to 10-fold validation?\nYou may find it useful to plot the k-fold cross-validation errors you have obtained to better understand the behavior of the method.\n# Plot the l2_penalty values in the x axis and the cross-validation error in the y axis.\n# Using plt.xscale(\'log\') will make your plot more intuitive.\nplt.xscale(\'log\')\nplt.plot(l2_penalties,val,\'-\')\n[<matplotlib.lines.Line2D at 0x7f159c3ae710>]\n\n\nOnce you found the best value for the L2 penalty using cross-validation, it is important to retrain a final model on all of the training data using this value of l2_penalty. This way, your final model will be trained on the entire dataset.\nmodel = turicreate.linear_regression.create(train_valid,\n                                              target = \'price\',\n                                              features = [\'sqft_living\'],\n                                              validation_set = None,\n                                              l2_penalty = min_l2_penalty,\n                                              verbose = False)\n***QUIZ QUESTION: Using the best L2 penalty found above, train a model using all training data. What is the RSS on the TEST data of the model you learn with this L2 penalty? ***\ntest = test.sort([\'sqft_living\',\'price\'])\n# First get the predictions\npredicted = model.predict(test)\n# Then compute the residuals/errors\nerrors = test[\'price\']-predicted\n# Then square and add them up    \n(errors**2).sum()\n129028453845344.28\n\nplt.plot(test[\'sqft_living\'],test[\'price\'],\'.\',\n         test[\'sqft_living\'], predicted,\'-\')\n[<matplotlib.lines.Line2D at 0x7f159c1f4240>,\n <matplotlib.lines.Line2D at 0x7f159c1f4320>]\n\n\n'], 'url_profile': 'https://github.com/garabaya', 'info_list': ['Updated May 9, 2020', 'R', 'Updated May 5, 2020', '2', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'HTML', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': ['Regression Week 4: Ridge Regression (gradient descent)\nIn this notebook, you will implement ridge regression via gradient descent. You will:\n\nConvert an SFrame into a Numpy array\nWrite a Numpy function to compute the derivative of the regression weights with respect to a single feature\nWrite gradient descent function to compute the regression weights given an initial weight vector, step size, tolerance, and L2 penalty\n\nFire up Turi Create\nMake sure you have the latest version of Turi Create\nimport turicreate\nLoad in house sales data\nDataset is from house sales in King County, the region where the city of Seattle, WA is located.\nsales = turicreate.SFrame(\'home_data.sframe/\')\nIf we want to do any ""feature engineering"" like creating new features or adjusting existing ones we should do this directly using the SFrames as seen in the first notebook of Week 2. For this notebook, however, we will work with the existing features.\nImport useful functions from previous notebook\nAs in Week 2, we convert the SFrame into a 2D Numpy array. Copy and paste get_numpy_data() from the second notebook of Week 2.\nimport numpy as np # note this allows us to refer to numpy as np instead \ndef get_numpy_data(data_sframe, features, output):\n    data_sframe[\'constant\'] = 1 # this is how you add a constant column to an SFrame\n    # add the column \'constant\' to the front of the features list so that we can extract it along with the others:\n    features = [\'constant\'] + features # this is how you combine two lists\n    # select the columns of data_SFrame given by the features list into the SFrame features_sframe (now including constant):\n    features_sframe = data_sframe[features];\n    # the following line will convert the features_SFrame into a numpy matrix:\n    feature_matrix = features_sframe.to_numpy()\n    # assign the column of data_sframe associated with the output to the SArray output_sarray\n    output_sarray = data_sframe[output];\n    # the following will convert the SArray into a numpy array by first converting it to a list\n    output_array = output_sarray.to_numpy()\n    return(feature_matrix, output_array)\nAlso, copy and paste the predict_output() function to compute the predictions for an entire matrix of features given the matrix and the weights:\ndef predict_output(feature_matrix, weights):\n    # assume feature_matrix is a numpy matrix containing the features as columns and weights is a corresponding numpy array\n    # create the predictions vector by using np.dot()\n    predictions = np.dot(feature_matrix, weights)\n    return(predictions)\nComputing the Derivative\nWe are now going to move to computing the derivative of the regression cost function. Recall that the cost function is the sum over the data points of the squared difference between an observed output and a predicted output, plus the L2 penalty term.\nCost(w)\n= SUM[ (prediction - output)^2 ]\n+ l2_penalty*(w[0]^2 + w[1]^2 + ... + w[k]^2).\n\nSince the derivative of a sum is the sum of the derivatives, we can take the derivative of the first part (the RSS) as we did in the notebook for the unregularized case in Week 2 and add the derivative of the regularization part.  As we saw, the derivative of the RSS with respect to w[i] can be written as:\n2*SUM[ error*[feature_i] ].\n\nThe derivative of the regularization term with respect to w[i] is:\n2*l2_penalty*w[i].\n\nSumming both, we get\n2*SUM[ error*[feature_i] ] + 2*l2_penalty*w[i].\n\nThat is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself, plus 2*l2_penalty*w[i].\nWe will not regularize the constant.  Thus, in the case of the constant, the derivative is just twice the sum of the errors (without the 2*l2_penalty*w[0] term).\nRecall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors, plus 2*l2_penalty*w[i].\nWith this in mind complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points).  To decide when to we are dealing with the constant (so we don\'t regularize it) we added the extra parameter to the call feature_is_constant which you should set to True when computing the derivative of the constant and False otherwise.\ndef feature_derivative_ridge(errors, feature, weight, l2_penalty, feature_is_constant):\n    # If feature_is_constant is True, derivative is twice the dot product of errors and feature\n    if feature_is_constant:\n        derivative = 2*(np.dot(errors, feature))\n    # Otherwise, derivative is twice the dot product plus 2*l2_penalty*weight\n    else:\n        derivative = 2*(np.dot(errors, feature)) + 2*l2_penalty*weight\n    return derivative\nTo test your feature derivartive run the following:\n(example_features, example_output) = get_numpy_data(sales, [\'sqft_living\'], \'price\') \nmy_weights = np.array([1., 10.])\ntest_predictions = predict_output(example_features, my_weights) \nerrors = test_predictions - example_output # prediction errors\n\n# next two lines should print the same values\nprint(feature_derivative_ridge(errors, example_features[:,1], my_weights[1], 1, False))\nprint(np.sum(errors*example_features[:,1])*2+20.)\nprint(\'\')\n\n# next two lines should print the same values\nprint(feature_derivative_ridge(errors, example_features[:,0], my_weights[0], 1, True))\nprint(np.sum(errors)*2.)\n-56554166782350.0\n-56554166782350.0\n\n-22446749336.0\n-22446749336.0\n\nGradient Descent\nNow we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of increase and therefore the negative gradient is the direction of decrease and we\'re trying to minimize a cost function.\nThe amount by which we move in the negative gradient direction  is called the \'step size\'. We stop when we are \'sufficiently close\' to the optimum. Unlike in Week 2, this time we will set a maximum number of iterations and take gradient steps until we reach this maximum number. If no maximum number is supplied, the maximum should be set 100 by default. (Use default parameter values in Python.)\nWith this in mind, complete the following gradient descent function below using your derivative function above. For each step in the gradient descent, we update the weight for each feature before computing our stopping criteria.\ndef ridge_regression_gradient_descent(feature_matrix, output, initial_weights, step_size, l2_penalty, max_iterations=100):\n    print(\'Starting gradient descent with l2_penalty = \' + str(l2_penalty))\n    \n    weights = np.array(initial_weights) # make sure it\'s a numpy array\n    iteration = 0 # iteration counter\n    print_frequency = 1  # for adjusting frequency of debugging output\n    \n    #while not reached maximum number of iterations:\n    while (iteration < max_iterations):\n        iteration += 1  # increment iteration counter\n        ### === code section for adjusting frequency of debugging output. ===\n        if iteration == 10:\n            print_frequency = 10\n        if iteration == 100:\n            print_frequency = 100\n        if iteration%print_frequency==0:\n            print(\'Iteration = \' + str(iteration))\n        ### === end code section ===\n        \n        # compute the predictions based on feature_matrix and weights using your predict_output() function\n        predictions = predict_output(feature_matrix, weights) \n        # compute the errors as predictions - output\n        errors = predictions - output\n        # from time to time, print the value of the cost function\n        if iteration%print_frequency==0:\n            print(\'Cost function = \', str(np.dot(errors,errors) + l2_penalty*(np.dot(weights,weights) - weights[0]**2)))\n        \n        for i in range(len(weights)): # loop over each weight\n            # Recall that feature_matrix[:,i] is the feature column associated with weights[i]\n            # compute the derivative for weight[i].\n            #(Remember: when i=0, you are computing the derivative of the constant!)\n            if (i==0):\n                feature_is_constant=True\n            else:\n                feature_is_constant=False\n            derivative = feature_derivative_ridge(errors, feature_matrix[:,i], weights[i], l2_penalty, feature_is_constant)\n            # subtract the step size times the derivative from the current weight\n            weights[i] = weights[i] - step_size*derivative\n    print(\'Done with gradient descent at iteration \', iteration)\n    print(\'Learned weights = \', str(weights))\n    return weights\nVisualizing effect of L2 penalty\nThe L2 penalty gets its name because it causes weights to have small L2 norms than otherwise. Let\'s see how large weights get penalized. Let us consider a simple model with 1 feature:\nsimple_features = [\'sqft_living\']\nmy_output = \'price\'\nLet us split the dataset into training set and test set. Make sure to use seed=0:\ntrain_data,test_data = sales.random_split(.8,seed=0)\nIn this part, we will only use \'sqft_living\' to predict \'price\'. Use the get_numpy_data function to get a Numpy versions of your data with only this feature, for both the train_data and the test_data.\n(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, my_output)\n(simple_test_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, my_output)\nLet\'s set the parameters for our optimization:\ninitial_weights = np.array([0., 0.])\nstep_size = 1e-12\nmax_iterations=1000\nFirst, let\'s consider no regularization.  Set the l2_penalty to 0.0 and run your ridge regression algorithm to learn the weights of your model.  Call your weights:\nsimple_weights_0_penalty\nwe\'ll use them later.\nl2_penalty = 0.0\nsimple_weights_0_penalty = ridge_regression_gradient_descent(simple_feature_matrix,\n                                                             output,\n                                                             initial_weights,\n                                                             step_size,\n                                                             l2_penalty,\n                                                             max_iterations)\nStarting gradient descent with l2_penalty = 0.0\nIteration = 1\nCost function =  7433051851026171.0\nIteration = 2\nCost function =  5394267213135524.0\nIteration = 3\nCost function =  4023237736501158.0\nIteration = 4\nCost function =  3101256183922414.0\nIteration = 5\nCost function =  2481247644505114.0\nIteration = 6\nCost function =  2064308077891941.2\nIteration = 7\nCost function =  1783927097372279.8\nIteration = 8\nCost function =  1595378203154872.0\nIteration = 9\nCost function =  1468583991054997.0\nIteration = 10\nCost function =  1383318191484981.8\nIteration = 20\nCost function =  1211562140496239.0\nIteration = 30\nCost function =  1208313762678823.2\nIteration = 40\nCost function =  1208252326252870.0\nIteration = 50\nCost function =  1208251163612919.8\nIteration = 60\nCost function =  1208251140915263.0\nIteration = 70\nCost function =  1208251139777036.0\nIteration = 80\nCost function =  1208251139046557.0\nIteration = 90\nCost function =  1208251138323789.2\nIteration = 100\nCost function =  1208251137601167.8\nIteration = 200\nCost function =  1208251130374984.8\nIteration = 300\nCost function =  1208251123148810.0\nIteration = 400\nCost function =  1208251115922643.2\nIteration = 500\nCost function =  1208251108696485.2\nIteration = 600\nCost function =  1208251101470335.0\nIteration = 700\nCost function =  1208251094244193.2\nIteration = 800\nCost function =  1208251087018060.0\nIteration = 900\nCost function =  1208251079791934.5\nIteration = 1000\nCost function =  1208251072565817.5\nDone with gradient descent at iteration  1000\nLearned weights =  [-1.63113501e-01  2.63024369e+02]\n\nNext, let\'s consider high regularization.  Set the l2_penalty to 1e11 and run your ridge regression algorithm to learn the weights of your model.  Call your weights:\nsimple_weights_high_penalty\nwe\'ll use them later.\nl2_penalty = 1e11\nsimple_weights_high_penalty = ridge_regression_gradient_descent(simple_feature_matrix,\n                                                             output,\n                                                             initial_weights,\n                                                             step_size,\n                                                             l2_penalty,\n                                                             max_iterations)\nStarting gradient descent with l2_penalty = 100000000000.0\nIteration = 1\nCost function =  7433051851026171.0\nIteration = 2\nCost function =  5618303898412629.0\nIteration = 3\nCost function =  4920613278115385.0\nIteration = 4\nCost function =  4652381942612294.0\nIteration = 5\nCost function =  4549258764014157.0\nIteration = 6\nCost function =  4509612390882265.0\nIteration = 7\nCost function =  4494370050281118.5\nIteration = 8\nCost function =  4488509984030221.5\nIteration = 9\nCost function =  4486256988531770.0\nIteration = 10\nCost function =  4485390752674687.5\nIteration = 20\nCost function =  4484848868034300.0\nIteration = 30\nCost function =  4484847880479026.0\nIteration = 40\nCost function =  4484846931081658.0\nIteration = 50\nCost function =  4484845981687379.0\nIteration = 60\nCost function =  4484845032293500.0\nIteration = 70\nCost function =  4484844082900019.0\nIteration = 80\nCost function =  4484843133506938.0\nIteration = 90\nCost function =  4484842184114254.5\nIteration = 100\nCost function =  4484841234721971.0\nIteration = 200\nCost function =  4484831740821062.0\nIteration = 300\nCost function =  4484822246960036.0\nIteration = 400\nCost function =  4484812753138891.0\nIteration = 500\nCost function =  4484803259357624.0\nIteration = 600\nCost function =  4484793765616238.0\nIteration = 700\nCost function =  4484784271914732.0\nIteration = 800\nCost function =  4484774778253106.0\nIteration = 900\nCost function =  4484765284631358.5\nIteration = 1000\nCost function =  4484755791049491.5\nDone with gradient descent at iteration  1000\nLearned weights =  [  9.76730383 124.57217565]\n\nThis code will plot the two learned models.  (The blue line is for the model with no regularization and the red line is for the one with high regularization.)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(simple_feature_matrix,output,\'k.\',\n         simple_feature_matrix,predict_output(simple_feature_matrix, simple_weights_0_penalty),\'b-\',\n        simple_feature_matrix,predict_output(simple_feature_matrix, simple_weights_high_penalty),\'r-\')\n[<matplotlib.lines.Line2D at 0x7f1c18109400>,\n <matplotlib.lines.Line2D at 0x7f1c18109550>,\n <matplotlib.lines.Line2D at 0x7f1c18109668>,\n <matplotlib.lines.Line2D at 0x7f1c181097f0>,\n <matplotlib.lines.Line2D at 0x7f1c18109940>,\n <matplotlib.lines.Line2D at 0x7f1c18109a90>]\n\n\nCompute the RSS on the TEST data for the following three sets of weights:\n\nThe initial weights (all zeros)\nThe weights learned with no regularization\nThe weights learned with high regularization\n\nWhich weights perform best?\nprediction_0 = predict_output(simple_test_feature_matrix,[0,0])\nerrors_0 = simple_test_feature_matrix[:,1] - prediction_0\nprint(\'RSS on TEST data for the initial weights: \' + str((errors_0**2).sum()))\nRSS on TEST data for the initial weights: 21750465996.0\n\nprediction_no_reg = predict_output(simple_test_feature_matrix, simple_weights_0_penalty)\nerrors_no_reg = simple_test_feature_matrix[:,1] - prediction_no_reg\nprint(\'RSS on TEST data for the weights learned with no regularization: \' + str((errors_no_reg**2).sum()))\nRSS on TEST data for the weights learned with no regularization: 1493315987449443.2\n\nprediction_reg = predict_output(simple_test_feature_matrix, simple_weights_high_penalty)\nerrors_reg = simple_test_feature_matrix[:,1] - prediction_reg\nprint(\'RSS on TEST data for the weights learned with high regularization: \' + str((errors_reg**2).sum()))\nRSS on TEST data for the weights learned with high regularization: 332152639885756.1\n\nQUIZ QUESTIONS\n\nWhat is the value of the coefficient for sqft_living that you learned with no regularization, rounded to 1 decimal place? 263  What about the one with high regularization? 124.6\nComparing the lines you fit with the with no regularization versus high regularization, which one is steeper? no regularization\nWhat are the RSS on the test data for each of the set of weights above (initial, no regularization, high regularization)? 21750465996.0, 1493315987449443.2 332152639885756.1\n\nRunning a multiple regression with L2 penalty\nLet us now consider a model with 2 features: [\'sqft_living\', \'sqft_living15\'].\nFirst, create Numpy versions of your training and test data with these two features.\nmodel_features = [\'sqft_living\', \'sqft_living15\'] # sqft_living15 is the average squarefeet for the nearest 15 neighbors. \nmy_output = \'price\'\n(feature_matrix, output) = get_numpy_data(train_data, model_features, my_output)\n(test_feature_matrix, test_output) = get_numpy_data(test_data, model_features, my_output)\nWe need to re-inialize the weights, since we have one extra parameter. Let us also set the step size and maximum number of iterations.\ninitial_weights = np.array([0.0,0.0,0.0])\nstep_size = 1e-12\nmax_iterations = 1000\nFirst, let\'s consider no regularization.  Set the l2_penalty to 0.0 and run your ridge regression algorithm to learn the weights of your model.  Call your weights:\nmultiple_weights_0_penalty\nl2_penalty = 0.0\nmultiple_weights_0_penalty = ridge_regression_gradient_descent(feature_matrix,\n                                                             output,\n                                                             initial_weights,\n                                                             step_size,\n                                                             l2_penalty,\n                                                             max_iterations)\nStarting gradient descent with l2_penalty = 0.0\nIteration = 1\nCost function =  7433051851026171.0\nIteration = 2\nCost function =  4056752331500972.0\nIteration = 3\nCost function =  2529565114333592.5\nIteration = 4\nCost function =  1838556694275926.8\nIteration = 5\nCost function =  1525675575208603.5\nIteration = 6\nCost function =  1383789498674794.0\nIteration = 7\nCost function =  1319232606276634.5\nIteration = 8\nCost function =  1289648872028921.0\nIteration = 9\nCost function =  1275884724079266.8\nIteration = 10\nCost function =  1269278807577156.2\nIteration = 20\nCost function =  1257812386316614.8\nIteration = 30\nCost function =  1251954571266786.2\nIteration = 40\nCost function =  1246755423155437.5\nIteration = 50\nCost function =  1242139508748821.0\nIteration = 60\nCost function =  1238041401137187.8\nIteration = 70\nCost function =  1234403013463993.2\nIteration = 80\nCost function =  1231172774976820.5\nIteration = 90\nCost function =  1228304900059555.0\nIteration = 100\nCost function =  1225758739263726.0\nIteration = 200\nCost function =  1211738881421532.8\nIteration = 300\nCost function =  1207473080962631.8\nIteration = 400\nCost function =  1206175125770959.8\nIteration = 500\nCost function =  1205780190233995.8\nIteration = 600\nCost function =  1205660014471675.5\nIteration = 700\nCost function =  1205623439252682.0\nIteration = 800\nCost function =  1205612300984401.0\nIteration = 900\nCost function =  1205608902360341.5\nIteration = 1000\nCost function =  1205607858660559.5\nDone with gradient descent at iteration  1000\nLearned weights =  [ -0.35743482 243.0541689   22.41481594]\n\nNext, let\'s consider high regularization.  Set the l2_penalty to 1e11 and run your ridge regression algorithm to learn the weights of your model.  Call your weights:\nmultiple_weights_high_penalty\nl2_penalty = 1e11\nmultiple_weights_high_penalty = ridge_regression_gradient_descent(feature_matrix,\n                                                             output,\n                                                             initial_weights,\n                                                             step_size,\n                                                             l2_penalty,\n                                                             max_iterations)\nStarting gradient descent with l2_penalty = 100000000000.0\nIteration = 1\nCost function =  7433051851026171.0\nIteration = 2\nCost function =  4460489790285891.0\nIteration = 3\nCost function =  3796674468844608.0\nIteration = 4\nCost function =  3648319530437361.0\nIteration = 5\nCost function =  3615091103216102.0\nIteration = 6\nCost function =  3607602742514732.0\nIteration = 7\nCost function =  3605886322161655.5\nIteration = 8\nCost function =  3605474874533295.0\nIteration = 9\nCost function =  3605365167765576.5\nIteration = 10\nCost function =  3605329402184649.0\nIteration = 20\nCost function =  3605294281022695.0\nIteration = 30\nCost function =  3605293537267099.5\nIteration = 40\nCost function =  3605293082749905.0\nIteration = 50\nCost function =  3605292631106357.0\nIteration = 60\nCost function =  3605292179491501.0\nIteration = 70\nCost function =  3605291727877070.0\nIteration = 80\nCost function =  3605291276262784.5\nIteration = 90\nCost function =  3605290824648642.0\nIteration = 100\nCost function =  3605290373034643.0\nIteration = 200\nCost function =  3605285856902500.0\nIteration = 300\nCost function =  3605281340784635.0\nIteration = 400\nCost function =  3605276824681046.0\nIteration = 500\nCost function =  3605272308591735.0\nIteration = 600\nCost function =  3605267792516700.0\nIteration = 700\nCost function =  3605263276455942.0\nIteration = 800\nCost function =  3605258760409461.0\nIteration = 900\nCost function =  3605254244377257.0\nIteration = 1000\nCost function =  3605249728359329.0\nDone with gradient descent at iteration  1000\nLearned weights =  [ 6.7429658  91.48927361 78.43658768]\n\nCompute the RSS on the TEST data for the following three sets of weights:\n\nThe initial weights (all zeros)\nThe weights learned with no regularization\nThe weights learned with high regularization\n\nWhich weights perform best? 1\nprediction_0 = predict_output(test_feature_matrix,[0,0,0])\nerrors_0 = test_feature_matrix[:,1] - prediction_0\nprint(\'RSS on TEST data for the initial weights: \' + str((errors_0**2).sum()))\nRSS on TEST data for the initial weights: 21750465996.0\n\nprediction_no_reg = predict_output(test_feature_matrix, multiple_weights_0_penalty)\nerrors_no_reg = test_feature_matrix[:,1] - prediction_no_reg\nprint(\'RSS on TEST data for the weights learned with no regularization: \' + str((errors_no_reg**2).sum()))\nRSS on TEST data for the weights learned with no regularization: 1494939561128742.5\n\nprediction_reg = predict_output(test_feature_matrix, multiple_weights_high_penalty)\nerrors_reg = test_feature_matrix[:,1] - prediction_reg\nprint(\'RSS on TEST data for the weights learned with high regularization: \' + str((errors_reg**2).sum()))\nRSS on TEST data for the weights learned with high regularization: 569362813179714.5\n\nPredict the house price for the 1st house in the test set using the no regularization and high regularization models. (Remember that python starts indexing from 0.) How far is the prediction from the actual price?  Which weights perform best for the 1st house?\npredicted_price_1st_no_reg = multiple_weights_0_penalty[0] + multiple_weights_0_penalty[1] * test_feature_matrix[0][1] + multiple_weights_0_penalty[1] * (test_feature_matrix[0][2])**2\npredicted_price_1st_no_reg\n770440395.8464879\n\npredicted_price_1st_high_reg = multiple_weights_high_penalty[0] + multiple_weights_high_penalty[1] * test_feature_matrix[0][1] + multiple_weights_high_penalty[1] * (test_feature_matrix[0][2])**2\npredicted_price_1st_high_reg\n290005450.91373503\n\ntest_data[0][\'price\']\n310000.0\n\nQUIZ QUESTIONS\n\nWhat is the value of the coefficient for sqft_living that you learned with no regularization, rounded to 1 decimal place? 243.1  What about the one with high regularization? 91.5\nWhat are the RSS on the test data for each of the set of weights above (initial, no regularization, high regularization)?\n\n21750465996.0 1494939561128742.5 569362813179714.5\n3. We make prediction for the first house in the test set using two sets of weights (no regularization vs high regularization). Which weights make better prediction for that particular house? high regularization\n'], 'url_profile': 'https://github.com/garabaya', 'info_list': ['Updated May 9, 2020', 'R', 'Updated May 5, 2020', '2', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'HTML', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 7, 2020']}","{'location': 'Bandung, West Java, Indonesia', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['regressionanalysis\nMaster thesis in Institut Teknologi Bandung\n'], 'url_profile': 'https://github.com/aufarudiawan', 'info_list': ['Updated May 9, 2020', 'R', 'Updated May 5, 2020', '2', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'HTML', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 7, 2020']}"
"{'location': 'India', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Vinaykumargond', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 20, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', 'R', 'Updated May 5, 2020', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""cookiecutter-helloword\nA test fixture for yehua's regression test\n""], 'url_profile': 'https://github.com/moremoban', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 20, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', 'R', 'Updated May 5, 2020', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Final Year Project\nData Science Regression Project:Predicting Home Prices in Bangalore\n'], 'url_profile': 'https://github.com/kalyani1812', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 20, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', 'R', 'Updated May 5, 2020', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '235 contributions\n        in the last year', 'description': ['California-House-Price-Prediction\n\nThis is a regression problem to predict california housing prices.\nThe dataset contains 20640 entries and 10 variables.\nLongitude\nLatitude\nHousing Median Age\nTotal Rooms\nTotal Bedrooms\nPopulation\nHouseholds\nMedian Income\nMedian House Value\nOcean Proximity\nMedian House Value is to be predicted in this problem.\n1) EDA and Data Cleaning\nI have done the exploratory data analysis and done following manipulations on data.\nCreating new features\nRemoving outliers\nTransforming skewed features\nChecking for multicoliniearity\n2) Training machine learning algorithms:\nHere, I have trained various machine learning algorithms like\nLinear Regression\nRidge Regression\nSupport Vector Regression\nGradient Boosting Regression\nStacking of various models\n'], 'url_profile': 'https://github.com/Swarupa567', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 20, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', 'R', 'Updated May 5, 2020', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020']}","{'location': 'Seattle, WA', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nyama8', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 20, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', 'R', 'Updated May 5, 2020', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020']}","{'location': 'chennai', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['log-log-regression\nApplication of log log regression on Boston housing dataset\n'], 'url_profile': 'https://github.com/roopchandu', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 20, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', 'R', 'Updated May 5, 2020', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': [""NYC-Airbnb---High-Price-Contribution-Factors\nR for Linear Regression Decision Trees and XG Boost\nUsed NYC Airbnb dataset from Kaggle.\nUsed Linear Regression to predict the rental price of accomodations in NYC listed in Airbnb.\n-We have cleaned and visualized the data.\n-Understand which variables are important to contribute to price prediction (finding correlations between variables).\n-Created Linear Regression model to predict price, calculated RMSE and MAPE to understand accuracy of prediction.\n-Plotted the prediction values' accuracy comparing with original dataset values.\n-Implemented a stepwise regression on the model to check if it optimizes the prediction.\nBuilt a regression tree model to check if it gives a better prediction than linear model.\nBuilt XGBoost model to check which among the 3 models gives a better prediction.\nConclusion: XG Boost gave us highest accuracy with 94% and has been the fastest model among the three.\n""], 'url_profile': 'https://github.com/sirishakompella', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 20, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', 'R', 'Updated May 5, 2020', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '648 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/skamranahmed', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 20, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', 'R', 'Updated May 5, 2020', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020']}","{'location': 'Pune, India', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/souroy12', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 20, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', 'R', 'Updated May 5, 2020', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Amir-Manafpour', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jun 20, 2020', '1', 'Jupyter Notebook', 'Updated May 16, 2020', 'R', 'Updated May 5, 2020', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020']}"
"{'location': 'Pune, India', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/souroy12', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 14, 2020', 'Python', 'Updated May 4, 2020', '1', 'C', 'LGPL-3.0 license', 'Updated Feb 12, 2021', 'R', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Dec 17, 2020', 'Python', 'Updated May 4, 2020']}","{'location': 'South Korea', 'stats_list': [], 'contributions': '834 contributions\n        in the last year', 'description': [""Gas Consumption Prediction Project (20200409)\n\nPrediction of  Busan city's gas consumption from 2018-06 to 2019-05\nLinear Regression, LSTM models\n\nFirst project in data analysis club\n\nNIMS PLIM industrial problem competition\nThis project is maintained by Ïò§ÏÑúÏòÅ, ÌóàÏßÄÌòú, Ïù¥ÏàòÎπà, Í∞ïÏàòÏó∞ (Department of Mathematics)\n\n[Presentation]\nDataset\n[1] BUSAN_gas_consumption dataset, https://icim.nims.re.kr/platform/question/16#summary  \n[2] BUSAN_average_tempurature, https://data.kma.go.kr/stcs/grnd/grndTaList.do?pgmNo=70\n\nResults\n1. Linear Regression\nRMSE error : 28.55174\n2. Linear Regression with Gradient Descent\nRMSE error : 29.39890\n\ngradient descent TOL (tolerance)\n\n3. LSTM\nRMSE error : 23.38848\n""], 'url_profile': 'https://github.com/OH-Seoyoung', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 14, 2020', 'Python', 'Updated May 4, 2020', '1', 'C', 'LGPL-3.0 license', 'Updated Feb 12, 2021', 'R', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Dec 17, 2020', 'Python', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['Parallelization of Support Vector Regression\nPython implementation of serial and parallel support vector regression\nThe report on the project is contained in the root directory\nThe code, datasets used, and screenshots on time run results are contained within the Code and Datasets directory\n'], 'url_profile': 'https://github.com/atlas4292', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 14, 2020', 'Python', 'Updated May 4, 2020', '1', 'C', 'LGPL-3.0 license', 'Updated Feb 12, 2021', 'R', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Dec 17, 2020', 'Python', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['\nSA-GPR\nThis repository contains a Python code for carrying out Symmetry-Adapted Gaussian Process Regression (SA-GPR) for the machine-learning of tensors. For more information, see:\n\nAndrea Grisafi, David M. Wilkins, Gabor Cs√°nyi, Michele Ceriotti, ""Symmetry-Adapted Machine Learning for Tensorial Properties of Atomistic Systems"", Phys. Rev. Lett. 120, 036002 (2018)\nDavid M. Wilkins, Andrea Grisafi, Yang Yang, Ka Un Lao, Robert A. DiStasio, Michele Ceriotti, ""Accurate Molecular Polarizabilities with Coupled-Cluster Theory and Machine Learning"", Proc. Natl. Acad. Sci. 116, 3401 (2019)\nAndrea Grisafi, David M. Wilkins, Benjamin A. R. Meyer, Alberto Fabrizio, Clemence Corminboeuf, Michele Ceriotti, ""A Transferable Machine-Learning Model of the Electron Density"", ACS Cent. Sci. 5, 57 (2019)\nAndrea Grisafi, Michele Ceriotti, ""Incorporating long-range physics in atomic-scale machine learning"", J. Chem. Phys. 151, 204105 (2019)\nF√©lix Musil, Michael J. Willatt, Mikhail A. Langovoy, Michele Ceriotti, ""Fast and Accurate Uncertainty Prediction in Chemical Machine Learning"", J. Chem. Theory Comput. 15, 906 (2019)\nAndrea Grisafi, Jigyasa Nigam, Michele Ceriotti, ""Multi-scale approach for the prediction of atomic scale properties."", arXiv:2008.12122 (2020)\n\n\nVersions\nThe current version of SOAPFAST (v3.0.1) is written in python 3. It has the same functionality as the previous release (v2.3), which is written in python 2.\n\nRequirements\nThe python packages ase, scipy, sympy and cython are required to run this code.\n\nInstallation\nThis program is installed using a makefile, found in the soapfast subdirectory. After editing the variables in this makefile, to install the python packages needed, the command make python is used. To compile the cython parts of this code, make cython is used, and to compile the fortran code needed for long-range descriptors, make LODE. The commands make or make all will compile both the cython and fortran codes. To remove object files run make clean.\nNote that the makefile is set up to install python packages only for the current user. If you have the requisite permissions and want instead to install them for all users, the PIPOPTS variable in the makefile should be set to a blank string.\n\nWorkflow\nThere are two steps to applying SA-GPR to a physical problem:\n\nCalculation of the similarities (kernels) between systems.\nMinimization of the prediction error and generation of the weights for prediction.\n\nThe first step is applied by running sagpr_get_PS and sagpr_get_kernel, and the second by running sagpr_train. The following examples aim to make this workflow clearer.\n\nExamples\nThe example/ directory contains four sub-directories, with data for the response properties of water monomers, water dimers, Zundel cations and boxes of 32 water molecules. We illustrate three examples of how to use SA-GPR for these directories.\nBefore starting, source the environment settings file env.sh using $ source env.sh.\n\n1. Water Monomer - Cartesian Tensor Learning\nWe start by learning the energy of a water monomer. The energy only has a scalar (L=0) component, so that the kernel used is the standard SOAP kernel. We begin by computing the L=0 kernels between the 1000 molecules. The first step is to find the power spectra of these configurations:\n$ cd example/water_monomer\n$ sagpr_get_PS -n 8 -l 6 -rc 4.0 -sg 0.3 -c \'O\' -lm 0 -f coords_1000.xyz -o PS0\n\nSince most of these options are the default, we could equally well use the command:\n$ sagpr_get_PS -c \'O\' -lm 0 -f coords_1000.xyz -o PS0\n\nNext, we combine the power spectrum with itself to obtain the kernel matrix:\n$ sagpr_get_kernel -z 2 -ps PS0.npy -s PS0_natoms.npy -o kernel0\n\nWe create a nonlinear (zeta=2) L=0 kernel matrix, kernel0.npy, using the coordinates in coords_1000.xyz, with Gaussian width 0.3 Angstrom, an angular cutoff of l=6, 8 radial functions, a radial cutoff of 4 Angstrom, central atom weighting of 1.0 and centering of the environment on oxygen atoms. This kernel can now be used to perform the regression:\n$ sagpr_train -r 0 -reg 1e-8 -f coords_1000.xyz -k kernel0.npy -p potential -rdm 200 -pr\n\nThe regression is performed for a rank-0 tensor, using the kernel file we produced, with a training set containing 200 randomly selected configurations. The file coords_1000.xyz contains the energies of the 1000 molecules under the heading ""potential"", and we use a regularization parameter of 1e-8. By varying the value of the ftr variable from 0 to 1, it is possible to create a learning curve which spans a range of training examples from 0 to the full data set.\nWe will next learn the polarizability of the monomers: since the symmetric polarizability tensor has an L=0 and an L=2 component, we must also compute the L=2 kernel:\n$ sagpr_get_PS -c \'O\' -lm 2 -f coords_1000.xyz -o PS2\n$ sagpr_get_kernel -z 2 -ps PS2.npy -ps0 PS0.npy -s PS2_natoms.npy -o kernel2\n\nThe regression can be performed in the same way as before:\n$ sagpr_train -r 2 -reg 1e-8 1e-8 -f coords_1000.xyz -k kernel0.npy kernel2.npy -p alpha -rdm 200 -pr\n\nHere we must specify both the L=0 and L=2 kernels to be used, as well as the regularizations in both cases. The user is encouraged to optimize these regularizations.\n\n2. Zundel Cation - Cartesian Tensor Learning\nNext, we learn the dipole moment of the zundel cation. This has one spherical component, transforming as L=1, so we must first build the respective kernel. Rather than calculating the power spectrum for the entire set of coordinates in one go, we begin by splitting the coordinates into smaller blocks, for each of which we compute the power spectra. To split the coordinates we use the split_dataset script:\n$ cd example/water_zundel\n$ split_dataset.py -f coords_1000.xyz -n 20 -o zundel\n\nThis will create 20 data files containing the coordinates of 50 Zundel cations each. For each of these files we then compute the power spectrum:\n$ for i in {0..19}\n$  do\n$  sagpr_get_PS -lm 1 -f zundel_${i}.xyz -o PS1_${i} > /dev/null &\n$ done\n\nThe power spectra thus created must then be combined together to find the power spectrum for the entire dataset:\n$ rebuild_power_spectrum.py -lm 1 -c coords_1000.xyz -nb 20 -f PS1\n\nThis creates PS1.npy, which contains the full power spectrum. The next step, as usual, is to build the kernel. For this we also build the L=0 power spectrum in order to find the nonlinear L=1 kernel.\n$ sagpr_get_PS -lm 0 -f coords_1000.xyz -o PS0\n$ sagpr_get_kernel -z 2 -ps PS1.npy -ps0 PS0.npy -s PS0_natoms.npy -o kernel1\n\nWe now use the kernel built to perform regression. Rather than do the regression and prediction in one go, we instead demonstrate the generation of an SA-GPR model using sagpr_train and the prediction of the dipole moments using sagpr_prediction. Firstly we train an SA-GPR model:\n$ sagpr_train -r 1 -reg 1e-8 -f coords_1000.xyz -k kernel1.npy -p mu -rdm 200\n\nBecause we have not used the -pr flag here, this code does not give any predictions, it only prints out the details of the model generated (note that these will be printed regardless of whether you use this flag; the -w flag allows you to control the name of these output files). Now, using this model we predict the dipole moments for this system. In addition to the weights generated, we need to know the kernel between the testing and training sets. For this, we can use the following code:\n$ python\n$ >>> import numpy as np\n$ >>> wt = np.load(""weights_1.npy"",allow_pickle=True)\n$ >>> kr = np.load(""kernel1.npy"")\n$ >>> trr = wt[3]\n$ >>> ter = np.setdiff1d(range(1000),trr)\n$ >>> ktest = np.zeros((800,200,3,3),dtype=float)\n$ >>> for i in range(800):\n$ ...     for j in range(200):\n$ ...             ktest[i,j] = kr[ter[i],trr[j]]\n$ ...\n$ >>> np.save(""ker_test.npy"",ktest)\n\nBecause this is quite a contrived example (in this case, it is of course easier not to use sagpr_prediction and just to do the predictions with the regression code), this snippet is not given as a separate script. However, it is important to note that a list of the configurations used in training the model is stored in the third record of the weights.\nNext, we use this kernel to carry out the prediction:\n$ sagpr_prediction -w weights -r 1 -k ker_test.npy -o prediction\n\nUsing the model generated in the previous step, we predict the dipole moments. These are printed in both prediction_cartesian.txt and prediction_L1.txt (note that the latter is not the same as the former, and the order of elements differs due to the definition of the L=1 component). To test the quality of this prediction we must compare these results with the true answers. Although these are tabulated in the output files, we could also use a method such as the following:\n$ python\n$ >>> from ase.io import read\n$ >>> import numpy as np\n$ >>> xyz = read(""coords_1000.xyz"",\':\')\n$ >>> wt = np.load(""weights_1.npy"",allow_pickle=True)\n$ >>> trr = wt[3]\n$ >>> ter = np.setdiff1d(range(1000),trr)\n$ >>> corrfile = open(""compare_cartesian.out"",""w"")\n$ >>> for i in range(len(ter)):\n$ ...     dipole = xyz[ter[i]].info[""mu""]\n$ ...     print (\' \'.join(str(e) for e in list(dipole)),file=corrfile)\n$ ...\n\nThe file compare_cartesian.out contains the correct values of the dipole moments for comparison. Carrying out this comparison with a randomly chosen training set:\n$ paste compare_cartesian.out prediction_cartesian.txt | awk \'BEGIN{err=0.0;n=0}{n++;err += ($1 - $4)**2 + ($2 - $5)**2 + ($3 - $6)**2}END{print (err/n)**0.5}\'\n\nwe find a root mean squared error of 0.003 a.u., which can be compared to the root mean square dipole moment of 0.675 a.u., for an intrinsic error of about 0.5%.\n\n3. Bulk Water - Polarizability and Sparsification\nWe now learn the polarizabilities of bulk water systems. This is a more challenging problem because the large systems mean that we could end up with quite memory-intensive calculations. The solution is to sparsify the power spectra. This means that some small subset of the spherical harmonic components is kept.\nIn order to sparsify our power spectra, we first need some sparsification parameters. To generate sparsified power spectra for the L=0 component:\n$ cd example/water_bulk\n$ sagpr_get_PS -f coords_1000.xyz -lm 0 -p -nc 200 -o PS0\n\nHere we take 200 spherical harmonic components (which is about a ninth as many as the number, 1792, that would be present in the unsparsified power spectrum). It should be noted that no effort has been made here to check on the optimum number of components to be kept, and the user is encouraged to perform this check themselves. Particular attention should be paid to the output of the code, which gives the smallest eigenvalue of the A-matrix used in sparsification. This matrix should be positive definite, so a sparsification that leads to negative eigenvalues -- particularly large negative eigenvalues -- should be treated with suspicion. The list of power spectrum columns retained and the A matrix, which are required for further sparsifying power spectra, are also printed.\nWe now combine this sparsified power spectrum as usual to give a kernel:\n$ sagpr_get_kernel -ps PS0.npy -z 2 -s PS0_natoms.npy -o kernel0\n\nIn order to learn the polarizability, we will also need an L=2 kernel. Because sparsification can take some time, and this part has the potential to be very memory-intensive, instead of using the entire set of coordinates to sparsify we can use some subset of it instead. To use a randomly chosen 500 frames to generate the sparsification details we can use the command:\n$ sagpr_get_PS -f coords_1000.xyz -lm 2 -p -nc 100 -ns 500 -sm \'random\' -o PS2\n\nHere we are decreasing the number of spherical components from 6656 to 100, which will considerably speed up the combination to give a kernel. The details thus generated are then used to sparsify the entire set of coordinates:\n$ sagpr_get_PS -f coords_1000.xyz -lm 2 -p -sf PS2 -o PS2_sparse\n\nNote that we could also, if desired, split this calculation into smaller parts as in the previous example. Now, we build the kernel as before:\n$ sagpr_get_kernel -z 2 -ps PS2_sparse.npy -ps0 PS0.npy -s PS2_sparse_natoms.npy -o kernel2\n\nHaving obtained these kernels, we will build a SA-GPR model to predict the polarizability.\n$ sagpr_train -r 2 -reg 1e-8 1e-5 -f coords_1000.xyz -k kernel0.npy kernel2.npy -p alpha -rdm 500 -pr -t 1.0\n\nThe errors in doing this prediction are quite high, but we could decrease them by retaining more spherical components when sparsifying. Note that the -t 1.0 flag ensures we do not learn the apparent L=1 component of this tensor. We set the threshold for discounting a component at 1.0 atomic units, meaning that we learn the L=0 and L=2, but not the L=1. This threshold should be set according to the error in calculation of the alpha tensor. Note that if we would like to learn this component (i.e. if it it physical), this can be done by computing an L=1 kernel and including this in the arguments, without the threshold flag.\n\n4. Water Monomer - Spherical Tensor Learning\nRather than learning the full polarizability of the water monomers, as in example 1, we could instead learn just the L=2 component. For this, we will rebuild the L=2 kernel centering both on O and on H atoms (unlike in example 1, where we centered only on O atoms):\n$ cd example/water_monomer\n$ sagpr_get_PS -lm 0 -f coords_1000.xyz\n$ sagpr_get_PS -lm 2 -f coords_1000.xyz\n$ sagpr_get_kernel -z 2 -ps PS2.npy -ps0 PS0.npy -s PS2_natoms.npy -o kernel2\n\nBecause we have not specified any centres, the code will take all of the atoms present as centres (i.e., H and O). Note that in this case, we have rebuilt the L=0 power spectrum as well, for creation of the nonlinear kernel. We don\'t actually need this power spectrum, as we could use our old power spectra centered only on O -- so this can be used instead if the user prefers.\nHaving rebuilt kernel2.npy, we will now use it to learn the L=2 component of the polarizability. Firstly we must isolate this component:\n$ sagpr_cart_to_sphr -f coords_1000.xyz -p alpha -r 2 -o processed_coords_1000.xyz\n\nThis command splits the alpha property in coords_1000.xyz into spherical components, and creates processed_coords_1000.xyz containing alpha_L0 and alpha_L2. Next, we can run the regression code with the -sp flag to learn the L=2 spherical component:\n$ sagpr_train -reg 1e-8 -f processed_coords_1000.xyz -p alpha_L2 -r 2 -k kernel2.npy -rdm 200 -pr -sp\n\nThe L=2 error here should be compared to that obtained in example 1.\n\n5. Bulk Water - Environment Sparsification and Ice Tensor Prediction\nWhen training a model to predict the dielectric tensors in bulk water, there is likely to be a fair amount of redundancy: using 1000 configurations with 32 water molecules each, we have 96,000 environments used for training. In addition to sparsification on the spherical components, we can further sparsify on the training environments: this has the potential to save memory both when building the kernels between training and testing systems and when doing the regression.\nWe start with the power spectra generated in example 3, and take 500 environments from each using furthest-point sampling. To generate a list of environments to be retained, we first have to convert the original power spectrum into an environmental power spectrum:\n$ cd example/water_bulk\n$ mv PS2_sparse.npy PS2.npy;mv PS2_sparse_natoms.npy PS2_natoms.npy\n$ get_atomic_power_spectrum.py -lm 0 -p PS0.npy -f coords_1000.xyz -o PS0_sparse_atomic\n$ get_atomic_power_spectrum.py -lm 2 -p PS2.npy -f coords_1000.xyz -o PS2_sparse_atomic\n\nRather than having a row for each molecule, these power spectra have a row for each environment. The next step is to get a list of the 500 furthest-point environments for each power spectrum. Firstly, we have to produce a power spectrum file that has each environment as a separate entry, after which the FPS details can be found:\n$ sagpr_do_env_fps -p PS0_sparse_atomic.npy -n 500 -o FPS_0\n$ sagpr_do_env_fps -p PS2_sparse_atomic.npy -n 500 -o FPS_2\n\nThe next step is to apply these FPS details to get a sparsified power spectrum:\n$ sagpr_apply_env_fps -p PS0_sparse_atomic.npy -sf FPS_0_rows -o PS0_full_sparsified_atomic\n$ sagpr_apply_env_fps -p PS2_sparse_atomic.npy -sf FPS_2_rows -o PS2_full_sparsified_atomic\n\nThese steps take only the furthest-point sampled rows of these two power spectra and produce two outputs which have been sparsified both on the spherical components and on the environments. In order to build a model, we now need to find a number of kernels: namely, the kernels between the sparsified power spectra and themselves, and between the sparsified power spectra and the power spectra that have not been sparsified on environments (this will allow us to build a model where we reduce from the situation with all environments to a situation with fewer environments).\nNow, we build kernels to be used in regression:\n$ sagpr_get_kernel -ps PS0.npy PS0_full_sparsified_atomic.npy -s PS0_natoms.npy NONE -z 2 -o KERNEL_L0_NM\n$ sagpr_get_kernel -ps PS0_full_sparsified_atomic.npy -s NONE -z 2 -o KERNEL_L0_MM\n$ sagpr_get_kernel -ps PS2.npy PS2_full_sparsified_atomic.npy -ps0 PS0.npy PS0_full_sparsified_atomic.npy -s PS2_natoms.npy NONE -z 2 -o KERNEL_L2_NM\n$ sagpr_get_kernel -ps PS2_full_sparsified_atomic.npy -ps0 PS0_full_sparsified_atomic.npy -s NONE -z 2 -o KERNEL_L2_MM\n\nThe regression is then performed to give weights:\n$ sagpr_train -r 2 -reg 1e-8 1e-5 -ftr 1.0 -f coords_1000.xyz -sf KERNEL_L0_NM.npy KERNEL_L0_MM.npy KERNEL_L2_NM.npy KERNEL_L2_MM.npy -p alpha -sel 0 500 -m \'pinv\' -t 1.0\n\nThen, we could use these weights, for example, to predict the properties of the training set:\n$ sagpr_prediction -r 2 -k KERNEL_L0_NM.npy KERNEL_L2_NM.npy -o prediction\n\nProceeding as before, we find the errors to be about 10% of the intrinsic variation of the dataset (a little worse than the unsparsified case, but as before this can be modified by retaining a different number of environments) More interesting, however, is to use this model for extrapolation: that is, to predict the properties of systems that are outside of the training set. To do this, we can use the five ice configurations in ice.xyz. Firstly, we must build the power spectra and the kernels between the training and testing set:\n$ sagpr_get_PS -f ice.xyz -lm 0 -p -sf PS0 -o PS0_ice\n$ sagpr_get_PS -f ice.xyz -lm 2 -p -sf PS2 -o PS2_ice\n$ sagpr_get_kernel -z 2 -ps PS0_ice.npy PS0_full_sparsified_atomic.npy -s PS0_ice_natoms.npy NONE -o KERNEL_L0_ice_train\n$ sagpr_get_kernel -z 2 -ps PS2_ice.npy PS2_full_sparsified_atomic.npy -ps0 PS0_ice.npy PS0_full_sparsified_atomic.npy -s PS0_ice_natoms.npy NONE -o KERNEL_L2_ice_train\n\nWe can then use these kernels to do the prediction:\n$ sagpr_prediction -r 2 -k KERNEL_L0_ice_train.npy KERNEL_L2_ice_train.npy -o prediction_ice\n\n\n6. Water Dimer - Environment Sparsification All-In-One\nThe script src/scripts/train_predict_env_sparse.py is an all-in-one script that takes in a set of power spectra, builds and tests an environmentally-sparsified model for a property. Although this script involves quite a large number of command-line arguments, by putting together a significant part of the workflow in the regression task we should be able to save some time.\nThe only ingredients we need are pre-generated power spectra, which have been sparsified only on features. We will begin by generating these power spectra for the water dimers:\n$ cd example/water_dimer\n$ for lm in 0 1 2 3\n$ do\n$ sagpr_get_PS -f coords_1000.xyz -lm ${lm} -o PS${lm} &\n$ done\n\nWith these power spectra we can learn any of the properties in which we might be interested. Here, we will learn them all. Firstly, the energy:\n$ train_predict_env_sparse.py -p PS0.npy -fr coords_1000.xyz -s PS0_natoms.npy -sm rdm -n 800 -e 500 -z 2 -k 0 -pr potential -reg 1e-7\n\nThis command builds a sparse model for the potential energy of water dimers. -sm rdm -n 800 means that it will take 800 dimers at random as the training set; -e 500 means that of the 2400 environments present we will take 500 of them, and -k 0 means that the kernel coming from the power spectrum in position 0 of the -p PS0.npy argument (i.e., PS0.npy) will be used for prediction. Similarly, we can build models for the dipole moment, polarizability and hyperpolarizability:\n$ train_predict_env_sparse.py -p PS0.npy PS1.npy -fr coords_1000.xyz -s PS1_natoms.npy -sm rdm -n 800 -e 500 -z 2 -k 1 -pr mu -reg 1e-6\n$ train_predict_env_sparse.py -p PS0.npy PS2.npy -fr coords_1000.xyz -s PS2_natoms.npy -sm rdm -n 800 -e 500 -z 2 -k 0 1 -pr alpha -reg 1e-8 1e-5\n$ train_predict_env_sparse.py -p PS0.npy PS1.npy PS3.npy -fr coords_1000.xyz -s PS3_natoms.npy -sm rdm -n 800 -e 500 -z 2 -k 1 2 -pr beta -reg 1e-7 1e-5\n\nNote that the L=0 power spectrum is always specified as the first argument, and that the -k argument denotes which of the power spectra give rise to kernels that will actually be used to build the SA-GPR model (whereas in some cases the L=0 is only used to build a nonlinear kernel).\n\n7. Learning Atomic Properties\nSA-GPR can be used to learn the properties of individual atoms. A dataset in which the water monomers are dressed with the absolute value of the quantum-mechanical force acting on them is given, and we show here how to learn this property for both O and H atoms.\n$ cd example/water_atomic_forces\n\nWe firstly need to find the atomic power spectra for both types of atom individually:\n$ sagpr_get_PS -f coords_800.xyz -lm 0 -o PS0 -a\n\nThis produces the files PS0_atomic_O.npy and PS0_atomic_H.npy, each of which can be used to build atomic kernels:\n$ for atom in O H;do sagpr_get_kernel -ps PS0_atomic_${atom}.npy -s NONE -z 2 -o KER0_atomic_${atom} & done\n\nWe now have the two kernels KER0_atomic_O.npy and KER0_atomic_H.npy. These are all we need to do atomic regression. We now choose a training set using farthest-point sampling:\n$ sagpr_do_env_fps -p PS0_atomic_O.npy -n 800 -o fps_O\n$ sagpr_do_env_fps -p PS0_atomic_H.npy -n 1600 -o fps_H\n\nThese routines give us FPS ordering of the entire set, so we will want to choose some fraction as a training set. Taking 500 atoms for O and 1000 for H, we obtain training set selections:\n$ python\n$ >>> import numpy as np\n$ >>> fpO = np.load(""fps_O_rows.npy"")\n$ >>> fpH = np.load(""fps_H_rows.npy"")\n$ >>> np.save(""selection_O.npy"",fpO[:500])\n$ >>> np.save(""selection_H.npy"",fpO[:1000])\n\nFinally, we do the regression:\n$ sagpr_train -f coords_800.xyz -r 0 -reg 1e-11 -p force -sel selection_O.npy -pr -k KER0_atomic_O.npy -c \'O\'\n$ sagpr_train -f coords_800.xyz -r 0 -reg 1e-11 -p force -sel selection_H.npy -pr -k KER0_atomic_H.npy -c \'H\'\n\nUsing the FPS details generated when making this example, the atomic regression for oxygen gives an RMSE of 0.202945510808 a.u. and the atomic regression for hydrogen gives 0.27160254007 a.u.; these can be compared to the intrinsic deviations within the dataset of 7.20049 a.u. and 5.37668 a.u. respectively (that is, relative errors of 2.8 and 5.1%).\n\n8. Learning Asymmetric Tensors\nThe prediction of asymmetric properties is also possible with this code. To showcase this feature, a dataset has been included that contains a single molecule to which random rotations have been applied both to its coordinates and to its polarizability, and the same molecule to which an asymmetric part has been added to the polarizability before randomly rotating it.\n$ cd example/asymmetry\n\nWe can observe the difference between the two polarizabilities using the command list_spherical_components.py:\n$ list_spherical_components.py -f symmetric.xyz -p alpha -r 2\n$ list_spherical_components.py -f asymmetric.xyz -p alpha -r 2\n\nWe see in the first case that the symmetric polarizability tensor has the familiar L=0 and L=2 spherical components, but that the asymmetric case has an additional L=1 component, described as being imaginary (because the L=1 part of a rank-2 tensor transforms as the imaginary unit times a spherical harmonic). In order to predict the polarizability in the asymmetric case, we will thus have to build L=0, L=1 and L=2 kernels:\n$ for lm in 0 1 2;do sagpr_get_PS -lm ${lm} -o PS${lm} -f asymmetric.xyz;done\n$ for lm in 0 1 2;do sagpr_get_kernel -z 2 -ps PS${lm}.npy -ps0 PS0.npy -s PS0_natoms.npy -o KER${lm};done\n\nHaving built these kernels we can carry out the regression straightforwardly as before:\n$ sagpr_train -r 2 -reg 1e-8 1e-8 1e-8 -f asymmetric.xyz -p alpha -k KER0.npy KER1.npy KER2.npy -sel 0 80 -pr\n\nNote that the relative error in learning the L=0 component is very large (around 100%); this is simply because these coordinates were produced by applying random rigid-body rotations to the molecule. For the same reason, the L=1 and L=2 components are learned with 0% error. Rather than comparing these numbers, we can check on the quality of the prediction by using the prediction_cartesian.txt output file:\n$ cat prediction_cartesian.txt | awk \'BEGIN{n=0}{n++;for (i=1;i<=9;i++){x[i] += $i}}END{for (i=1;i<=9;i++){printf ""%f "",x[i]/n};printf ""\\n""}\' > avg.out;cat avg.out prediction_cartesian.txt | awk \'BEGIN{n=0;std=err=0.0}{if (n==0){n=1;for (i=1;i<=9;i++){x[i]=$i}}else{for (i=1;i<=9;i++){std += ($i - x[i])**2;err += ($i - $(i+9))**2}}}END{print (err/std)**0.5}\';rm avg.out\n\nWe obtain an error of 5e-7% in predicting the asymmetric polarizability tensor. It should be noted that this feature has not yet been tested using data that was not produced by a rigid rotation.\n\n9. Second Hyperpolarizability Learning\nWe next take the learning of the second hyperpolarizability tensor (gamma) of water monomers. The previous incarnation of SA-GPR code was limited to learning tensor orders up to third, so we show here how to deal with a general tensor order. The file water_gamma.xyz is provided with these tensors (computed using a smaller cc-pVDZ basis set than all of the other response properties).\n$ cd example/water_monomer\n\nThe first step is to find which spherical kernels we must produce in order to learn this property.\n$ list_spherical_components.py -f coords_gamma.xyz -p gamma -r 4\n\nWe see that we need to build kernels of order 0, 2 and 4. This can be done with a simple for loop:\n$ for lm in 0 2 4;do sagpr_get_PS -lm ${lm} -f coords_gamma.xyz -o PS${lm} & done\n$ for lm in 0 2 4;do sagpr_get_kernel -z 2 -ps PS${lm}.npy -ps0 PS0.npy -s PS${lm}_natoms.npy -o KER${lm} & done\n\nHaving built these kernels we can now carry out the regression to predict the gamma tensors:\n$ sagpr_train -r 4 -reg 1e-9 1e-9 1e-9 -f coords_gamma.xyz -p gamma -k KER0.npy KER2.npy KER4.npy -sel 0 800 -pr\n\nThe overall error in learning these tensors is 0.457 a.u. (which is 0.25% of the intrinsic deviation of the data).\n\n10. Different Methods for Environmental Sparsification\nTo highlight the different methods for including environmental sparsification in the regression, we will learn the scalar component of the polarizability of the QM7b set (see ref. [2]).\n$ cd example/qm7b\n\nSince we are provided with the full polarizability tensor, we first need to take the trace. Having done so, we will then split the set up into a training set comprising 5400 molecules and a test set containing 1811 molecules.\n$ cartesian_to_spherical.py -f coords.xyz -o coords_trace.xyz -p alpha -r 2\n$ python\n$ >>> from ase.io import read,write\n$ >>> frames = read(""coords_trace.xyz"",\':\')\n$ >>> write(""train.xyz"",frames[:5400])\n$ >>> write(""test.xyz"",frames[5400:])\n\nNext, we get the scalar power spectrum for the training set, sparsified on spherical components.\n$ sagpr_get_PS -f train.xyz -c H C N O S Cl -s H C N O S Cl -lm 0 -nc 600 -o PS0_train\n\nUsing the sparsification details for this set, we get the power spectrum for the testing set.\n$ sagpr_get_PS -f test.xyz -c H C N O S Cl -s H C N O S Cl -lm 0 -sf PS0_train -o PS0_test\n\nTo get an idea of how good our sparsified models are, we will begin by building an unsparsified model. Firstly, we build the kernels and do the regression as usual, then predict on the training set.\n$ sagpr_get_kernel -z 2 -ps PS0_train.npy -s PS0_train_natoms.npy -o K0\n$ sagpr_get_kernel -z 2 -ps PS0_test.npy PS0_train.npy -s PS0_test_natoms.npy PS0_train_natoms.npy -o K0_TT\n$ sagpr_train -r 0 -reg 1e-9 -f train.xyz -p alpha_L0 -k K0.npy -sel 0 5400 -w weights_all_env -perat\n$ sagpr_prediction -r 0 -w weights_all_env -k K0_TT.npy -o prediction_all_env\n\nThe peratom scalar polarizability components are given by test_peratom.txt, and the prediction error from an unsparsified set can be found as:\n$ python\n$ from ase.io import read\n$ frames = read(""test.xyz"",\':\')\n$ >>> fl = open(\'test_peratom.txt\',\'w\')\n$ >>> for i in xrange(len(frames)):\n$ ...     print >> fl, frames[i].info[\'alpha_L0\'] / len(frames[i].get_chemical_symbols())\n$ ...\n$ paste prediction_all_env_L0.txt test_peratom.txt | awk \'BEGIN{m=n=0}{m+=($1-$2)**2;n++}END{print (m/n)**0.5}\'\n\nAn error of 0.033 a.u./atom was found in testing this (the actual value obtained may differ very slightly).\nNext, we build sparsified models. Firstly, we must find atomic power spectra and choose a number of environments. Here we try 1000, 2000 and 5000 environments.\n$ get_atomic_power_spectrum.py -lm 0 -p PS0_train.npy -f train.xyz -o PS0_train_atomic\n$ for env in 1000 2000 5000;do do_fps.py -p PS0_train_atomic.npy -n ${env} -o fps_${env};done\n$ for env in 1000 2000 5000;do apply_fps.py -p PS0_train_atomic.npy -sf fps_${env}_rows -o PS0_train_atomic_${env};done\n\nHaving created the sparsified power spectra, we now build the appropriate kernels.\n$ for env in 1000 2000 5000;do sagpr_get_kernel -z 2 -ps PS0_train.npy PS0_train_atomic_${env}.npy -s PS0_train_natoms.npy NONE -o K0_NM_${env};done\n$ for env in 1000 2000 5000;do sagpr_get_kernel -z 2 -ps PS0_train_atomic_${env}.npy -s NONE -o K0_MM_${env};done\n$ for env in 1000 2000 5000;do sagpr_get_kernel -z 2 -ps PS0_test.npy PS0_train_atomic_${env}.npy -s PS0_test_natoms.npy NONE -o K0_TT_${env};done\n\nWith these kernels, we now perform the regression. There are three possibilities, presented in order; in each case, the regression will be carried out, followed by prediction and finally the error on the testing set will be printed as a function of the number of environments.\nWe begin by using the solve function to perform the regression.\n$ for env in 1000 2000 5000;do sagpr_train -r 0 -reg 1e-8 -f train.xyz -p alpha_L0 -sf K0_NM_${env}.npy K0_MM_${env}.npy -perat -w weights_sparse_solve_${env} -m solve;done\n$ for env in 1000 2000 5000;do sagpr_prediction -r 0 -w weights_sparse_solve_${env} -k K0_TT_${env}.npy -o prediction_sparse_solve_${env};done\n$ for env in 1000 2000 5000;do paste prediction_sparse_solve_${env}_L0.txt test_peratom.txt | awk \'BEGIN{m=n=0}{m+=($1-$2)**2;n++}END{print (m/n)**0.5}\';done\n\nIt should be noted that for the 5000-environment case, sagpr_train gives a warning that the matrix to be inverted is ill-conditioned. This is reflected in the three prediction errors, 0.059 a.u./atom, 0.051 a.u./atom, 0.096 a.u./atom, the latter being much higher than expected. One way to fix this is to tune the regularization: using a value of 1e-5 rather than 1e-8 (the optimum for an unsparsified model) gives errors of 0.058 a.u./atom, 0.047 a.u./atom, 0.036 a.u./atom, with the latter being very close to the unsparsified model\'s prediction error.\nAlternatively, rather than using the solve function we could try using the pinv (pseudoinverse) function:\n$ for env in 1000 2000 5000;do sagpr_train -r 0 -reg 1e-5 -f train.xyz -p alpha_L0 -sf K0_NM_${env}.npy K0_MM_${env}.npy -perat -w weights_sparse_pinv_${env} -m pinv;done\n$ for env in 1000 2000 5000;do sagpr_prediction -r 0 -w weights_sparse_pinv_${env} -k K0_TT_${env}.npy -o prediction_sparse_pinv_${env};done\n$ for env in 1000 2000 5000;do paste prediction_sparse_pinv_${env}_L0.txt test_peratom.txt | awk \'BEGIN{m=n=0}{m+=($1-$2)**2;n++}END{print (m/n)**0.5}\';done\n\nThe pinv function avoids ill-conditioned matrices, but it should be noted that once again the optimum regularization is different from that in the unsparsified model (once again, the errors are 0.058 a.u./atom, 0.047 a.u./atom, 0.036 a.u./atom). However, while this function is more ""forgiving"", and preferable to using solve in sparsification problems, it can be much more expensive.\nAn alternative is to apply a ""jitter"" term, using the solve function but with a diagonal matrix with small magnitude added to the matrix to be inverted, so that it is full-rank:\n$ for env in 1000 2000 5000;do sagpr_train -r 0 -reg 1e-5 -f train.xyz -p alpha_L0 -sf K0_NM_${env}.npy K0_MM_${env}.npy -perat -w weights_sparse_jitter_${env} -m solve -j CHOOSE;done\n$ for env in 1000 2000 5000;do sagpr_prediction -r 0 -w weights_sparse_jitter_${env} -k K0_TT_${env}.npy -o prediction_sparse_jitter_${env};done\n$ for env in 1000 2000 5000;do paste prediction_sparse_jitter_${env}_L0.txt test_peratom.txt | awk \'BEGIN{m=n=0}{m+=($1-$2)**2;n++}END{print (m/n)**0.5}\';done\n\nThe option CHOOSE means that the code will choose a magnitude for this matrix that is as small as possible while still making the resulting matrix full-rank. Alternatively, one can enter their chosen value. The CHOOSE option can make this step quite expensive in its current form, so should be used with care. However, this method may be useful in cases where pinv is very expensive. In this case, we obtain 0.058 a.u./atom, 0.047 a.u./atom, 0.044 a.u./atom. This latter case is a problem largely because in this case the jitter isn\'t really necessary. This should be treated as an experimental feature that may in future become useful.\n\n11. Uncertainty Estimation\nThis example uses three data sets, which will be made available on publication of the relevant paper. Once they are available, they wil be found in example/amino_acid. We begin by calculating the power spectra for the training set, choosing an active set of environments and finding the kernels between the training set and the active set.\n$ cd example/amino_acid\n$ for lm in 0 1;do sagpr_parallel_get_PS -nrun 28 -lm ${lm} -f train.xyz -o PS${lm} -c H C N O S -s H C N O S -n 4 -l 2 -sg 0.23726178 -rs 1 2.91603113 6.08786224 -sm random -nc 400 -ns 1000 -rc 5.0;done\n$ for lm in 0 1;do sagpr_parallel_get_PS -nrun 28 -lm ${lm} -f train.xyz -o PS${lm}_train -c H C N O S -s H C N O S -n 4 -l 2 -sg 0.23726178 -rs 1 2.91603113 6.08786224 -sf PS${lm} -rc 5.0;done\n$ for lm in 0 1;do get_atomic_power_spectrum.py -p PS${lm}_train.npy -f train.xyz -o PS${lm}_train_atomic;done\n$ do_fps.py -p PS1_train_atomic.npy -n 8000 -v | tee fps.out\n$ for lm in 0 1;do apply_fps.py -p PS${lm}_train_atomic.npy -o PS${lm}_train_sparse;done\n$ sagpr_get_kernel -z 2 -ps PS1_train.npy PS1_train_sparse.npy -ps0 PS0_train.npy PS0_train_sparse.npy -s PS1_train_natoms.npy NONE -o K1_NM\n$ sagpr_get_kernel -z 2 -ps PS1_train_sparse.npy -ps0 PS0_train_sparse.npy -s NONE -o K1_MM\n\nNext, we subsample (taking 5000 points per sample and 8 samples):\n$ mkdir np_5000\n$ cd np_5000\n$ python subsample.py -k ../K1_NM.npy -f ../train.xyz -np $(pwd | sed ""s/\\_/ /g"" | awk \'{print $NF}\') -ns 8\n$ multi_train.sh 1 9.694108361689872e-06 ../K1_MM.npy\n\nThis produces 8 weight files, one for each model. The next step is to calibrate the uncertainty estimate, which we do using a validation set.\n$ cd ../\n$ for fl in validation test;do\n$         sagpr_parallel_get_PS -nrun 28 -lm 0 -f ${fl}.xyz -o PS0_${fl} -c H C N O S -s H C N O S -n 6 -l 4 -sg 0.23088253 -rs 1 4.15454532 8.24538508 -sf PS0 -rc 5.0\n$         sagpr_parallel_get_PS -nrun 28 -lm 2 -f ${fl}.xyz -o PS2_${fl} -c H C N O S -s H C N O S -n 4 -l 2 -sg 0.23088253 -rs 1 4.15454532 8.24538508 -sf PS2 -rc 5.0\n$         sagpr_get_kernel -z 2 -ps PS2_${fl}.npy PS2_train_sparse.npy -ps0 PS0_${fl}.npy PS0_train_sparse.npy -s PS2_${fl}_natoms.npy NONE -o K2_${fl}\n$ done\n$ cd np_5000\n$ nsample=$(ls | grep -c WEIGHTS)\n\nNote that the first part also creates the kernels needed for the test set.\n$ for i in $(seq 1 ${nsample});do\n$         sagpr_prediction -r 1 -w WEIGHTS.${i} -k ../K1_validation.npy -o PREDICTION.${i} -sp\n$         cat ../validation.xyz | sed ""s/\\(=\\|\\""\\)/ /g"" | awk \'{if (NF==1){nat=$1}}/Properties/{for (i=1;i<=NF;i++){if ($i==""mu_L1""){printf ""%.16f %.16f %.16f\\n"", $(i+1)/nat,$(i+2)/nat,$(i+3)/nat}}}\' > CALC.${i}_L1.txt\n$         paste PREDICTION.${i}_L1.txt CALC.${i}_L1.txt | awk \'{print $1,$4;print $2,$5;print $3,$6}\' > RESIDUAL.${i}_L1.txt\n$         echo ""Predicted model number ""${i}\n$ done\n\nThis is a version of the script multi_predict.sh, but is written out explicitly here. Having made these predictions, we now calibrate the uncertainty estimation factor alpha:\n$ get_alpha.sh\n\nThis creates a file, alpha.txt, the last line of which is the square of the factor by which the predictions of each individual model must be moved away from the average value.\nFinally, we use the test set to see how good our predictions are, not only of the dipole moment but also of the uncertainty.\n$ cd ../;mkdir test_predictions;cd test_predictions\n$ nmodel=$(ls ../np_5000 | grep -c WEIGHTS)\n$ for i in $(seq 1 ${nmodel});do\n$         sagpr_prediction -r 1 -w ../np_5000/WEIGHTS.${i} -k ../K1_test.npy -o PREDICTION.${1}.${i} -sp\n$ done\n$ for i in PREDICTION.np_5000.*_L1.txt;do cat ${i} | awk \'{print ($1**2 + $2**2 + $3**2)**0.5}\' > $(echo ${i} | sed ""s/L1/NORM/"");done\n$ paste PREDICTION.np_5000.*_NORM.txt | awk \'{n=m=0;for (i=1;i<=NF;i++){n++;m+=$i};printf ""%.16f\\n"",m/n}\' > PREDICTION_MEAN.np_5000_NORM.txt\n$ export alpha=$(tail -n 1 ../np_5000/alpha.txt)\n$ paste PREDICTION_MEAN.np_5000_NORM.txt PREDICTION.np_5000.*_NORM.txt | awk \'BEGIN{al=ENVIRON[""alpha""]**0.5}{printf ""%.16f "",$1;for (i=2;i<=NF;i++){dd=$1 + ($i-$1)*al;printf ""%.16f "",dd};printf ""\\n""}\' | awk \'{l=m=n=0;for (i=1;i<=NF;i++){n++;m+=$i};yavg=(m/n);for (i=2;i<=NF;i++){l+=($i-yavg)**2};printf ""%.16f %.16f\\n"",$1,(l/(n-1))**0.5}\' > PREDICTION_COMMITTEE.np_5000_NORM.txt\n$ cat ../test.xyz | sed ""s/\\(=\\|\\""\\)/ /g"" | awk \'{if (NF==1){nat=$1}}/Properties/{for (i=1;i<=NF;i++){if ($i==""mu_L1""){printf ""%.16f %.16f %.16f\\n"", $(i+1)/nat,$(i+2)/nat,$(i+3)/nat}}}\' | awk \'{print ($1**2 + $2**2 + $3**2)**0.5}\' > CALC_NORM.txt\n$ paste CALC_NORM.txt PREDICTION_COMMITTEE.np_5000_NORM.txt > calc_pred_uncertainty.txt\n\nAs suggested by the name, calc_pred_uncertainty.txt has three columns: the calculated dipole moment norm, the dipole moment norm predicted by the eight models, and the estimated uncertainty from this committee. A good test of whether the model is accurately gauging its uncertainty is to compare the norm of the difference between the first two columns (i.e., the residual) with the uncertainty estimate. If the estimated uncertainty does not match the residual (it likely will not), then it should at least be larger than the residual in the majority of cases, meaning that the model is properly ""cautious"" in its estimates.\n\n12. Application of LODE to the prediction of binding energies\nIn this example, we will learn the binding energy of molecular dimers, in which at least one monomer carries a net charge, using the dataset in example/charged_dimers. Each binding trajectory includes 13 displacements plus the isolated monomers (which have zero binding energy). To compute a multiscale LODE(1,1) representation given by the symmetry-adapted tensor product of atom density and potential features, the -ele flag is used when computing the power spectrum:\n$ sagpr_get_PS -f trajs_with_energies.xyz -sg 0.3 -rc 3.0 -l 4 -n 8 -nn -ele -o LODE\n\nNote that the -nn flag ensures that the power spectrum is not normalized. Setting an angular resolution of -l 4 implies that a linear model for learning the electrostatic energy is mapped to a multipolar expansion of the local electrostatic potential that is implicitly written using spherical multipoles up to L=4. To do so, we first build a linear kernel as the inner product of the LODE(1,1) descriptor:\n$ sagpr_get_kernel -ps LODE.npy -o kernel\n\nWe then carry out a scalar learning exercise, learning the first 300 sets of 15 configurations at different separations and testing the prediction on the remaining 41 sets:\n$ sagpr_train -f trajs_with_energies.xyz -p energy -k kernel.npy -pr -sel 0 4500 -r 0 -reg 1e-6\n\nThis gives an RMSE that is 13% of the standard deviation of the test set.\n\n13. Application of LODE to periodic systems\nThe LODE descriptor can also be calculated for periodic systems (at present, this has only been implemented for orthorhombic cells). To learn the energy of a periodic system we consider random distributions of NaCl at different bulk densities, modelled as fixed point charges. The examples/random_nacl folder contains 2000 frames in coords_with_energies.xyz. The periodic implementation of LODE, activated by the -p flag, relies on Ewald sumation to represent the atomic potential field in terms of a screened, quickly-varying contribution computed in real space and a smooth long-ranged contribution computed in reciprocal space. The width of the Gaussian used to perform the Ewald splitting of the potential is tuned with the -sew flag. To compute the LODE(1,1) representaion coming from a Gaussian density with sigma of 0.3 Angstrom:\n$ sagpr_get_PS -f coords_with_energies.xyz -p -ele -sg 0.3 -rc 2.0 -l 0 -n 8 -sew 1.1 -nn -o LODE_periodic\n\nNote that the radial cutoff of the representation -rc 2.0 is chosen to be smaller than the minimum distance between any pair of ions in the dataset (which is 2.5 Angstrom), while the angular expansion is truncated at -l 0. This is because under these limits the LODE representation is expected to converge to a fixed-point charge model as the Gaussian width of the representation goes to zero. In fact, the descriptor so computed can be used to give negligible error. To see this, we first compute a linear kernel:\n$ sagpr_get_kernel -ps LODE_periodic.npy -o kernel\n\nWe finally carry out a scalar learning exercise, learning the electrostatic energies of 1000 configurations selected at random and testing the predictions on the remaining 1000:\n$ sagpr_train -f coords_with_energies.xyz -p energy -k kernel.npy -pr -rdm 1000 -r 0 -reg 1e-10\n\n\nContact\nd.wilkins@qub.ac.uk\nandrea.grisafi@epfl.ch\n\nContributors\nDavid Wilkins, Andrea Grisafi, Andrea Anelli, Guillaume Fraux, Jigyasa Nigam, Edoardo Baldi, Linnea Folkmann, Michele Ceriotti\n'], 'url_profile': 'https://github.com/dilkins', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 14, 2020', 'Python', 'Updated May 4, 2020', '1', 'C', 'LGPL-3.0 license', 'Updated Feb 12, 2021', 'R', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Dec 17, 2020', 'Python', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '192 contributions\n        in the last year', 'description': [""\nLRpadding\n \nLRpadding is a very short term project to implement a simple padding functionality for logistic regression, allowing the concatenation of arbitrarily many zeros on both the predictors and the response. This is part of a much larger project, this code is entirely for testing and development.\nWithout an intercept, padding in this way would result in no effect (simply multiplying the likelihood by a constant), so we assume we include an intercept as the first column of the model matrix.\nInstallation\nYou can install the current version of LRpadding from GitHub with:\nremotes::install_github('CCGill/LRpadding')\n\nWe make use of the excellent RcppNumerical package (http://cran.r-project.org/package=RcppNumerical), and a simple example version of the RcppNumerical function fastLR, which is built on the L-BFGS algorithm for unconstrained minimization problems based on the LBFGS++ library.\nExample\nThis is an example demonstrating that a logistic_reg implementation on padded response/model agrees with the implementation of the padding. It is based on a unit test for this package. Note that the basic_logistic_reg function is currently identical to the example version of fastLR from the RcppNumerical package, returning only the coefficients, and logistic_reg is identical to the full fastLR function.\nWe also show that our result closely matches the corresponding glm call.\nlibrary(LRpadding)\n## basic example code\nset.seed(42)\n    \n    n = 20000\n    p = 100L\n    x = matrix(rnorm(n * p), n)\n    x = cbind(rep(1,n),scale(x,scale = F,center = T)) ##attach the intercept\n    beta = runif(p+1)\n    xbeta = c(x %*% beta)\n    probs = 1 / (1 + exp(-xbeta))\n    y = rbinom(n, 1, probs)\n    testpadding = 40000L\n  \n    padded_x <- rbind(x,t(matrix( rep.int(c(1,0), c(1,p)), p+1, testpadding)))\n    padded_y <- c(y,rep(0,testpadding))\n    \n    res1 <- logistic_reg(padded_x, padded_y)\n    res2 <- padded_logistic_reg(x,y,padding = testpadding)\n    max(abs(res1$coefficients - res2$coefficients)) ## identical results\n#> [1] 3.166356e-13\n\n    res1$loglikelihood\n#> [1] -13321.19\n    res2$loglikelihood\n#> [1] -13321.19\n    \n    res3 <- basic_logistic_reg(padded_x,padded_y)\n    res4 <- basic_padded_logistic_reg(x,y,padding = testpadding)\n    max(abs(res3 - res4)) ## identical results\n#> [1] 2.273737e-12\n\n    max(abs(res1$coefficients - res3))\n#> [1] 0.0006703128\n    max(abs(res2$coefficients - res4))\n#> [1] 0.0006703128\nA second example, for time.\n    set.seed(42)\n    system.time(res1 <- logistic_reg(padded_x, padded_y)$coefficients )\n#>    user  system elapsed \n#>   0.137   0.001   0.138\n \n    system.time(res2 <- padded_logistic_reg(x,y,padding = testpadding)$coefficients)\n#>    user  system elapsed \n#>   0.042   0.001   0.042\n\n    system.time(res3 <- glm.fit(padded_x,padded_y,family = binomial())$coefficients)\n#>    user  system elapsed \n#>   6.170   0.307   6.550\n\n    max(abs(res1 - res2)) ## identical results\n#> [1] 3.166356e-13\n\n    max(abs(res2 - res3))\n#> [1] 6.953141e-05\n""], 'url_profile': 'https://github.com/CCGill', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 14, 2020', 'Python', 'Updated May 4, 2020', '1', 'C', 'LGPL-3.0 license', 'Updated Feb 12, 2021', 'R', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Dec 17, 2020', 'Python', 'Updated May 4, 2020']}","{'location': 'Dallas, Texas', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abhilampard', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 14, 2020', 'Python', 'Updated May 4, 2020', '1', 'C', 'LGPL-3.0 license', 'Updated Feb 12, 2021', 'R', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Dec 17, 2020', 'Python', 'Updated May 4, 2020']}","{'location': 'new delhi', 'stats_list': [], 'contributions': '321 contributions\n        in the last year', 'description': ['datascience-ml\nadvance house price prediction using advance regression technique kaggle problem\n'], 'url_profile': 'https://github.com/akash8190', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 14, 2020', 'Python', 'Updated May 4, 2020', '1', 'C', 'LGPL-3.0 license', 'Updated Feb 12, 2021', 'R', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Dec 17, 2020', 'Python', 'Updated May 4, 2020']}","{'location': 'Jaypee University of Engineering and Technology,Guna(M.P)', 'stats_list': [], 'contributions': '595 contributions\n        in the last year', 'description': ['Salary-Prediction-Using-Linear-Regression\nMy implementation of Linear Regression model for salary predictions.\n'], 'url_profile': 'https://github.com/rajansh87', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 14, 2020', 'Python', 'Updated May 4, 2020', '1', 'C', 'LGPL-3.0 license', 'Updated Feb 12, 2021', 'R', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Dec 17, 2020', 'Python', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Ruthenium-Price-Predictor\nLinear regression model for Ruthenium prices on certain dates\nQuick guide on how to use this application:\n-Simply call the ""Testing()\' method\n-You will be prompted to enter the input date as year, month and day separately one by one\n'], 'url_profile': 'https://github.com/ZeinaKandil', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 14, 2020', 'Python', 'Updated May 4, 2020', '1', 'C', 'LGPL-3.0 license', 'Updated Feb 12, 2021', 'R', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Dec 17, 2020', 'Python', 'Updated May 4, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '132 contributions\n        in the last year', 'description': ['PySpark_Regression_ML_Model\nRegression Models Using PySpark Package In Hadoop Big Data Environment\nDisclaimer: This code snippet is developed with an intension for generic use only. I hereby declare that the code was not part of any of my professional development work. Also, no sensitive or confidential data sources are used for this development.\nDescription: The repository consists of below list of machine learning models for regression problems:\n1. Linear Regression\n2. GLM\n3. Random Forest\n4. GBM\n\nNote:\n\nThis scripts taking raw csv file as a input source, however Hive integration is possible with very minimal changes\nUser Input Section allows to set all model parameters as well as data source specification. Users dont need to edit anything except this    section. This is a fully automated end to end code.\nThis script also perform basic EDA, Feature Engineering and Random Sampling based on user given test split\nOnly test sample model evaluation metrices are reported\n\nCompatibility: The code is developed and tested on Zeppelin Notebook in Hadoop Big Data Environment using Python 3.7\n'], 'url_profile': 'https://github.com/debasishdutta', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 14, 2020', 'Python', 'Updated May 4, 2020', '1', 'C', 'LGPL-3.0 license', 'Updated Feb 12, 2021', 'R', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Dec 17, 2020', 'Python', 'Updated May 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['statistical-prediction\nLinear and Logistic Regression examples to start learning.\n'], 'url_profile': 'https://github.com/luciananobrega', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Raleigh', 'stats_list': [], 'contributions': '240 contributions\n        in the last year', 'description': ['Reference paper: http://arxiv.org/pdf/1410.1231.pdf\nIn this project, I will be tasked with predicting the price variations of bitcoin, a virtual cryptographic currency. These predictions could be used as the foundation of a bitcoin trading strategy. To make these predictions, you will have to familiarize yourself with a machine learning technique, Bayesian Regression, and implement this technique in Python.\n'], 'url_profile': 'https://github.com/akashsrikanth2310', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Illmitz, Austria', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Predicting SP500 Index\nIn this project, we work with data from the SP500 Index. The file is made available by DataQuest as a csv file containing index prices. Each row in the file contains a daily record of the price of the SP500 Index from 1950 to 2015. The dataset is stored in sphist.csv. Columns:\n\nDate -- The date of the record.\nOpen -- The opening price of the day (when trading starts).\nHigh -- The highest trade price during the day.\nLow -- The lowest trade price during the day.\nClose -- The closing price for the day (when trading is finished).\nVolume -- The number of shares traded.\nAdj Close -- The daily closing price, adjusted retroactively to include any corporate actions.\n\n'], 'url_profile': 'https://github.com/burnier', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Sunnyvale, California', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['DeepLearning_pytorch\nCore Deep Learning Objectives including Classification, Regression, Detection, Segmentation etc.\n'], 'url_profile': 'https://github.com/shruti-jadon', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'College park, Maryland', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['Logistic_Regression\nImplementation of Logistic Regression for binary classification of images\nOverview:\nThis repository contains implementation for Logistic Regression on the given dataset for binary classification. It includes following steps:\n\nLoading necessary packages.\nImplementing utility functions to convert images to dataset.\nImplementing functions for sigmoid activation, forward propogation and backward propagation\nImplementing optimization and prediction functions.\nTraining the model with training dataset.\nTesting the accuracy of model with test dataset.\n\nDependencies\nTo run this project, you need to have deep learning setup in your local machine.\nWindows:\nFollow the instruction here to install conda on windows.\nAfter following the above instructions, you need to install pillow and pytables into the conda deeplearning environment:\n\n\nMake sure deeplearning environment is active:\nactivate deeplearning\n\n\nThen, run following commands:\nconda install pillow\nconda install pytables\nconda install h5py\n\n\nUbuntu\nFollow the steps in this link here to setup a deep learing environment in Ubuntu .\nInstructions to run the project\nAfter setting up deep learning environment, follow these steps to run the .ipynb file that contains code for logistic regression.\n\n\ngit clone https://github.com/nakul3112/Logistic_Regression.git\n\n\nUbuntu: conda activate <environment-name>\nWindows: activate <environment-name>\nReplace the environment name above with the name of your deep learning environment.\n\n\nRun jupyter notebook\njupyter notebook\n\n\nNavigate to the .ipynb file and run the kernel.\n\n\nLearn more about Logistic Regression ?\nThese are few resources that might be helpful for beginners to learn the concepts of logistic regression in machine learning.\n1. Logistic Regression\n'], 'url_profile': 'https://github.com/nakul3112', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': [""house-prices-advanced-regression-techniques\nPredicting house prices using different regression algorithm's\n""], 'url_profile': 'https://github.com/akhil1112', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': [""Supervised Learning in Data Mining\nBoston Housing dataset is used to fit regression models while Bankruptcy dataset is used for classification models.\nRegression Models:\n\nLinear Regression\nRegression Tree\nBagging\nRandom Forest\nBoosting\nGAM\nNeural Network\n\nClassification Models:\n\nLogisitic Regression\nClassification Tree\nGAM\nNeural Network\n\nBoston Housing Data\nExecutive Summary\nThe Boston Housing dataset consisting of 506 observations with 14 variables. The response variable is medv, the median housing price which is continuous in nature. Using a 70-30 sample split we fit multiple models on the training data and compare the In-sample and Out-sample Average Sum Square Errors (ASE). The table below summarizes the methods:\n\n\n\nSr.\nType\nASE\nOut-Sample\n\n\n\n\n1\nLinear Regression\n23.597306\n18.688102\n\n\n2\nRegression Tree\n16.385094\n23.013215\n\n\n3\nBagging\n12.959958\n10.727727\n\n\n4\nRandom Forest\n12.74322\n7.157639\n\n\n5\nBoosting\n2.10690\n6.924797\n\n\n6\nGAM\n8.432493\n16.161948\n\n\n7\nNeural Network\n6.859694\n11.741971\n\n\n\nFor the Linear Regression model we fit using AIC and BIC criterion and try LASSO variable selection. The best MSE and R2adj is found in the AIC model.\nA Regression tree is fit since the response is continuous. An optimal tree with 6 terminal nodes was built.\nAdvanced trees such as Bagging, Random Forest and Boosting are also fit and optimized.\nBoosting shows the best performance in-sample and out-of-sample.\nA General Additive Model is fit with chas and rad as linear terms and splines for all other terms and age, black and zn were not significant so were removed.\nA Neural network with 2 hidden layers each having 3 nodes was found to show least variation on error between train and test set.\nExploratory Data Analysis\nSplit the data as 70% training and 30% testing.\nFor all model training we will stick only to training set.\nLinear Regression\nStepwise with AIC and BIC as criterion and LASSO regression models are fit.\nThe lowest MSE and best R2adj is seen with stepwise AIC regression.\nmedv = 36.51 - 0.57*lstat + 3.72*rm - 0.87*ptratio - 1.7*dis - 17.69*nox + 0.01*black + 0.34*rad + 2.67*chas + 0.05*zn - 0.01*tax - 0.1*crim\nRegression Tree\nThe response is numerical (continuous) so we fit a regression tree.\nPlotting the complexity paramater (cp) we find the lowest cp value within 1 se is 0.03.\nUsing cp = 0.03 to prune the tree we get 6 terminal nodes.\nBagging\nBagging (Bootstrap and Aggregating) helps to improve prediction accuracy.\nIt fits a tree for each bootsrap sample, and then aggregates the predicted values from all trees.\nPlotting the errors against number of trees indicates error flattens after fitting 200 trees.\nRandom Forest\nRandom Forest is similar to bagging except that we randomly select 'm' out of 'p' predictors as candidate variables for each split in each tree.\nIn the case of regression trees the default m = p/3.\nWe fit trees using Random Forest with default 500 trees.\nWe find that lstat and rm are the most important factors.\nPlotting the Out-of_bag error for these 500 trees shows us that the error seems to flatten after 100 trees.\nBoosting Tree\nBoosting sequentially builds a number of small trees, and each time, the response is the residual from last tree.\nWe choose to fit 5000 trees with a depth of 8 splits, shrinkage parameter = 0.01 and doing a 3-fold CV.\nWe find that lstat and rm are the most important factors.\nPlotting the error against trees for the CV error shows optimal trees required to be 1223\nWe fit a second boosting model with 1223 trees to avoid overfitting the data.\nGAM\nGeneralized Additive Models use a sum of non-paramteric functions over each component of X to capture non-linear relationships.\nrad and chas are not included in the spline terms as they are quasi-categorical and categorical respectively.\nEach term was checked for significance at 0.05 level vefore removal and if the edf was ~1 it was moved to be a linear term.\nThe removal of age, black and zn reduced the GCV score to 11.587 and so that model was selected.\nNeural Network\nThe dependent variable is numeric so we set linear.output to be true.\nMultiple models were fit:\n\n1-hidden layer - vary nodes from 1 to 9\n2-hidden layers - Vary nodes from 1 to 9 in first layer and vary nodes from 1 to 5 in second layer\n\nThe model with 3 nodes in the first layer and 3 nodes in second layer had lower out-sample error and lower variance between in-sample and out-sample errors and so it was picked.\nBankruptcy Data\nExecutive Summary\nThe Bankruptcy dataset contains 5436 observations and 13 variables. R1 to R10 are continuous variables contain financial information which will be used as predictor variables, DLRSN is a binary variable where 0 labels not bankrupt, 1 labels bankrupt. Using a 70-30 sample split we fit multiple models on the training data and compare the asymmetric misclassification cost (AMC) using the cut-off probability as 1/36.\nThe table below summarizes the methods:\n\n\n\nSr.\nType\nIn-sample AMC\nOut-Sample AMC\n\n\n\n\n1\nLogistic Regression\n0.6651\n0.6689\n\n\n2\nClassification Tree\n0.5007\n0.6879\n\n\n3\nGAM\n0.5719\n0.6419\n\n\n4\nNeural Network\n0.4896\n0.5984\n\n\n\nFor the logistic regression model AIC criterion was slightly better than BIC and so was picked with out-sample TPR of 96.04%.\nA classification tree with 14 terminal nodes was pruned to have 8 terminal nodes with out-sample TPR of 95.15%.\nThe GAM model was built with R1 and R4 as linear terms and the rest having splines functions which gave out-sample TPR of 95.60%.\nFinally a Neural Network model was fit with one hidden layer having 7 nodes and decay rate of 0.25 to avoid overfitting. The out-sample TPR was found to be 95.60%.\nExploratory Data Analysis\nSplit the data as 70% training and 30% testing.\nFor all model training we will stick only to training set.\nIn the training set 14.4% of the data has DLRSN=1 which are the cases idicating bankruptcy.\nThis implies that the data is imbalanced.\nLogistic Regression\nA full model with variables R1 through R10 was fit as a generalized linear model and another null model was fit with only the intercept.\nThese two models were used as the upper and lower scopes for finding a best model using AIC and BIC criterions.\nAUC for AIC model (0.882) was slightly better that BIC model (0.88) both being higher than industrial standard of 0.7.\nSince this is imbalanced data we plot the precision-recall curve and see the AUC for AIC model (0.5723) is better than AUC for BIC model (0.5718).\nThe asymmetric misclassifaction cost is 0.6652 with:\nAccuracy = 50.46%\nTrue Positive Rate = 96.54%\nFalse Positive Rate = 57.31%\nClassification Tree\nAll the variables from R1 to R10 were used to fit a classification tree as the response is categorical.\nThe tree is allowed to grow till cp=0.0075 and then pruned at 0.01 based on plotcp curve.\nThe asymmetric misclassifaction cost is 0.5007 with:\nAccuracy = 56.19%\nTrue Positive Rate = 98.72%\nFalse Positive Rate = 51.00%\nGAM\nAll the variables R1 to R10 are first fitted with splines which show edf=1 for R1 and R4 so we move those to linear terms.\nThe models deviance explained is low.\nThe asymmetric misclassifaction cost is 0.5718 with:\nAccuracy = 55.32%\nTrue Positive Rate = 97.44%\nFalse Positive Rate = 51.78%\nNeural Network\nThe dependent variable is categorical.\nUsing the nnet function we try different sizes and see that with get good in-sample and out-sample asymetric costs with size=7.\nThe asymmetric misclassifaction cost is 0.4896 with:\nAccuracy = 61.76%\nTrue Positive Rate = 97.81%\nFalse Positive Rate = 44.32%\n""], 'url_profile': 'https://github.com/mhridhay', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': [""Simple-Linear-Regression\nUnderstanding and building a Simple Linear Regression Model\n\nWhen we go about knowing Machine Learning models, one of the first things we generally come across is the Simple Linear Regression. It's the first step into Machine Learning.\nThe term regression was first coined in the 19th century to describe a phenomenon, that the heights of descendants of tall ancestors tend to regress (or approach) towards the normal average height. In other words, regression is the tendency to return to moderation (mean). In statistics, the term is defined as a measure of the relation between an output variable and the input variable(s). Hence, the Linear Regression assumes a linear relationship between the former and the latter.\nDepending upon the number of input variables, Linear Regression can be classified into two categories:\n\nSimple Linear Regression (Single Input Variable)\nMultiple Linear Regression (Multiple Input Variables)\n\n""], 'url_profile': 'https://github.com/datasciencewithsan', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Real-Estate\nReal estate price prediction with different regression models\n'], 'url_profile': 'https://github.com/veerendranallamilli', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Regression_Result\nHere we will apply different regression technique on same dataset.\n'], 'url_profile': 'https://github.com/vcthakur', 'info_list': ['Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '121 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ntentes', 'info_list': ['R', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 6, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'bhubaneswar', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Heart_Disease_Logistic_Regression\nPrediction of Heart Disease Using Logistic Regression\n'], 'url_profile': 'https://github.com/iamchiranjeeb', 'info_list': ['R', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 6, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nperforms simple linear regression over a given dataset using python\n'], 'url_profile': 'https://github.com/Vineeta12345', 'info_list': ['R', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 6, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'Dallas, Texas', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['Text Classification\nImplement and evaluate Naive Bayes and Logistic Regression for text classification for the spam/ham dataset.\nOverview\nMultinomial Naive Bayes algorithm as per http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf and MCAP Logistic Regression algorithm with L2 regularization has been implemented. Improvement over these algorithms by throwing away the stop words and their impact analysis has been done.\nSteps to run\n\nNaive Bayes\n\n\nRun the command python NaiveBayes.py\nThis will run for With and Without Stop Words\n\n\nLogistic Regression\n\n\nRun the command python LogisticRegression.py\nThis will run for With and Without Stop Words\nParameters such as: lambdaVal, totalIterations, learningRate are configurable (at the beginning of the file)\n\n\nAnalysis of both the algorithms with their accuracies have been documented in src\\analysis.pdf\n\n'], 'url_profile': 'https://github.com/amtul-nazneen', 'info_list': ['R', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 6, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['BTL_HocMay\nPredict weather and Evaluate result via LR&Logistic regression\n'], 'url_profile': 'https://github.com/RyanPhuong', 'info_list': ['R', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 6, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'Louisville , Kentucky ', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fmomin01', 'info_list': ['R', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 6, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/LittlePenguin-OvO', 'info_list': ['R', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 6, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\nMultiple Linear Regression model to predict profit of companies.\n'], 'url_profile': 'https://github.com/archith-reddy', 'info_list': ['R', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 6, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'Cincinnati, Ohio', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/digvijaykawale', 'info_list': ['R', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 6, 2020', 'R', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RobertNardello', 'info_list': ['R', 'Updated Jul 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 6, 2020', 'R', 'Updated May 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['PFC-LV\nPrincipal fitted components method with a latent variable for binary responses\nDescription\nWe proposed a joint model for dimension reduction in binary regression using a continuous latent variable to represent an unobserved response underlying the binary response. The minimal sufficient linear reduction is obtained, and an efficient expectation maximization algorithm is developed for carrying out maximum likelihood estimation. The algorithm approximates latent variable integration by using Gauss‚ÄìHermite quadrature.\nUsage\nEMquad_esti_para(X, Y, d, r, B=50, iter.max =30, err.tol = 0.0001)\nArguments\n\nX: design matrix\nY: binary response vector\nd: maximum number of directions to be used in estimating the reduction subspace\nr: degree of polynomial basis function\nB: number of Gauss-Hermite nodes\niter.max: maximum number of iterations within GH-EM algorithm (the default is 30)\nerr.tol: error threshold to stop the algorithm (the default is 0.0001)\n\nValue\n\nMU: estimate of Œº\nGAMMA: estimate of Œì\nBETA: estimate of Œ≤\nDELTA: estimate of Œî\nL: approximate value of the log-likelihood\nnumpar: number of parameters\nbic: value of Bayesian information criterion\n\nExample\n## load the required packages ##\nlibrary(dr) \nlibrary(gam)\nlibrary(ldr)\nlibrary(splines)\nlibrary(mvtnorm)\nlibrary(logitnorm)\nlibrary(fastGHQuad)\nlibrary(randomForest)\n\n## load the required functions ##\nsource(""PFC_LV_function.R"")\n\nset.seed(2020)\nB = 50\nn = 400\nd = 2\np = 10\nr = 3\nGAMMA <- diag(1,p,d)\nDELTA <- diag(1,p,p)\nMU <- as.matrix(rep(0,p),dim=c(p,1))\nBETA <- matrix(c(1,1,0,1,-1,0),ncol=r,byrow = FALSE)\nZERO <- rep(0,p)\n\nR.true <- solve(DELTA) %*% GAMMA\ninv1 <- solve(crossprod(R.true,R.true))\nP1 <- R.true %*% inv1 %*% t(R.true)\n\n## 200 data replications ##\nC = 200\nerr_PFC_O <- err_PFC_LV <- rep(0,C)\nfor(k in 1:C){\n  ## generate data from the joint model ##\n  a_true<-1\n  b_true<-0\n  theta_prior <- rnorm(n,0,1)\n  Pi_theta <- exp(a_true*theta_prior+b_true)/(1+exp(a_true*theta_prior+b_true))\n  X<-matrix(rep(0,n*p),ncol=p)\n  Y<-rep(0,n)\n  for(i in 1:n){\n    X[i,] <- mvrnorm(1,MU + GAMMA %*% BETA %*% (fpi(theta_prior[i],r)),DELTA)\n    Y[i] <- rbinom(1,1,Pi_theta[i])\n  }\n\n  ## gold standard ##\n  fit_gold <- pfc(X, theta_prior, fy = bf(theta_prior,case=""poly"",degree=r), numdir=d, structure=""unstr"")\n  R.pfc_gold <- solve(fit_gold$Deltahat)%*%fit_gold$Gammahat\n  inv_pfc_gold <- solve(crossprod(R.pfc_gold,R.pfc_gold))\n  P_gold <- R.pfc_gold %*% inv_pfc_gold %*% t(R.pfc_gold)\n\n  ## calculate the estimation error of PFC-O ##\n  err_PFC_O[k] <- sqrt(sum(diag(t(P1-P_gold)%*%(P1-P_gold))))\n\n  ## apply PFC-LV ##\n  GHEM_para <- EMquad_esti_para(X, Y, d=d, r=r, B=B, iter.max =30, err.tol = 0.0001)\n  R.new <- solve(GHEM_para$DELTA) %*% GHEM_para$GAMMA\n  inv2 <- solve(crossprod(R.new,R.new))\n  P2<-R.new %*% inv2 %*% t(R.new)\n  \n  ## calculate the estimation error of PFC-LV ##\n  err_PFC_LV[k] <- sqrt(sum(diag(t(P1-P2)%*%(P1-P2))))\n}\n\n'], 'url_profile': 'https://github.com/JunlanLiCC', 'info_list': ['R', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '264 contributions\n        in the last year', 'description': ['Diamond_Predictions\nINTRODUCTION:\nPicked a data set of 54,000 diamonds to predict the continuous variable of price.\nOBJECTIVE:\nTaking a look at the different features such as carat weight, color, cut, and clarity to see how these independent variables impact and influence the target variable of price.\nTHE DATASET:\n‚Ä¢ Kaggle\nSKILLS REQUIRED TO COMPLETE:\nThe skills used to complete this project consisted of working with Python to make visualizations using Pandas and cleaning the data set well.  Also understanding & knowing how to interpret various regression models based on feature engineering & selection.\nWHAT WAS POSTED ON GITHUB:\nOn GitHub I had posted four separate notebooks. One which was consisted of the data collection & cleaning (including visualizations/EDA) ,the other for the different models I used to depict the best predictions,  and finally the ReadMe notebook which is a layout of how my project was presented.\nQUESTIONS I POSED:\nIs there any correlation between price & carat weight?\nIs there any correlation between price & cut of the diamond?\nIs there any correlation between price & color grade of the diamond?\nIs there any correlation between price & clarity of the diamond?\nHow can I use feature engineering to enhance my prediction model values?\nHOW I PUT MY DATA TOGETHER:\nFirst, I gathered a data set of 54,000 different diamonds.  After I gathered the data and cleaned it, I had selected the features from the data in which I thought would most strongly correlate to the ultimate price of the diamond. Next, I did some EDA and decided which features I should include in my models.  Following that, I had split my data into training and testing and analyzed the different values of my R^2 & RMSE (Root Mean Squared Error) for each model. Finally, I compared the different models to see which could predict the best price of the diamonds.\nFUTURE/STEPS I WOULD HAVE DONE:\nThe future steps I would have taken would be to include a Ridge regression model for my data set.  Another goal would have been to find another data set of even more features of diamonds and merge the two & apply more feature engineering & selection from there.\nRECOMMENDATIONS BASED ON ANALYSIS:\nBased on my results from my analysis, I can suggest  that carat weight is the most statistically significant feature in determining the price of a diamond.  There are other important features that can heavily change the total amount of your diamond, however carat is the most influential.  In conclusion, the OLS model of this data set is best represented to predict pricing of your average diamond.\nPRESENTATION LINK:\nhttps://docs.google.com/presentation/d/1J5C9aVBEaC5PkE2vk-u2zqNhYORiG2ELBIQOCTACJTM/edit?usp=sharing\n'], 'url_profile': 'https://github.com/J-Joseph524', 'info_list': ['R', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'US, TN', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['Logistic_Regression\nLogistic Regression, self-implemented for titanic data set (which is included in kaggle.com)\n'], 'url_profile': 'https://github.com/ArazShilabin', 'info_list': ['R', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shubhangperiwal', 'info_list': ['R', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'Kolkata, India', 'stats_list': [], 'contributions': '645 contributions\n        in the last year', 'description': ['linear_regression\nlinear regression problem solving using best fit line\n'], 'url_profile': 'https://github.com/procheta1999', 'info_list': ['R', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'Riyadh, Saudi Arabia', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['Linear Regresion in Python and Excel\nOverview\nExcel and Python are the most common tools for data analysis, and several data analysis tasks can be completed using both of them. Here, I created a linear regression model in Python and in Excel. The model was created using Boston Housing dataset .\nFiles Descriptions\n\nData file Boston_Housing.csv Boston Housing dataset.\nOne notebook file Linear_Regression.ipynb which contains the linear regression python code.\nOne Excel file Boston_Housing_Regression.xls - which contains the linear regression by excel.\n\nNote\nMore explaination about creating a linear regression model in Python and Excel is available in this article.\n'], 'url_profile': 'https://github.com/nadaalay', 'info_list': ['R', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Building linear regression models to predict housing prices.\nFor this project, I used a dataframe consisting of specifications of various homes sold. The end goal was to build a linear regression model that could adequately predict house prices. To execute this, I went through a process of cleaning the data, exploring the data, building visualizations and finally quantifying all the data so it could be used in an ordinary least squares analysis. After completing several analyses I was able to build a model that can correctly predict 73% of the house prices with the given data.\n'], 'url_profile': 'https://github.com/matthewwilson51497', 'info_list': ['R', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['AutomatedEssayGrading\nAutomated essay grader using logistic regression to classify ""good"" writing.\nBasic Structure of the Code:\n\nAll imports of the modules and libraries are located at the top of the code.\nConversion and reading of downloaded files follow, specifically the data set downloaded from Kaggle and the corpus set. We also utilized nltk‚Äôs English corpus and converted it into a list for future word comparison.\nNext, we pre-processed the data by establishing stop words and setting up tokenizers and lemmatizers. We also included our own file called contractions.py that has a dictionary of all the contractions.\nAfter pre-processing, our methods are located, specifically 3 methods: writer_label() which classifies all of the data, spelling_errors() which check for spelling mistakes, and sophisticated() which check for the number of sophisticated words.\nThen, we proceed to transfer the data from our Kaggle dataset into a dataframe that we created.\nAfter creating a dataframe, we start to create our features, including essay length, number of unique words, number of sentences, average sentence length, spelling errors, number of sophisticated words, and number of grammar errors.\nWe converted all the features into numpy arrays and added the rest of these features into the dataframe.\nThe following sections are all scatter plots and correlation coefficients that show the relation between each feature and the label.\nThe final portion includes building the logistic regression model, and testing our model‚Äôs performance.\nAfter running the code, the classification report will be printed which will show the precision, recall, and f-measure scores for the following classes: 0 and 1.\n\nCompliling the Code\n\nDownload EssayGrading.py, contractions.py, and requirements.txt and place into the same project folder.\nGo to python virtual environment (ven) and type ‚Äúpip install -r requirements.txt‚Äù. This will import all the modules and libraries that we used for our project.\nDownload training_set_rel3.tsv and BigCorpus_5000.cvs and place both into your home directory. This is the data set that we used from an online source.\nComments are noted throughout the entire code that explain what each method does, or what the section of code will return.\n\n'], 'url_profile': 'https://github.com/tammieoh', 'info_list': ['R', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '139 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sabato96', 'info_list': ['R', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['exoplanet_gp\nModelling TESS lightcurves and exoplanetary transits with Gaussian Process regression\nWe use a simple harmonic oscillator kernel to model the intrinsic asteroseismic oscillation of the star, in linear combination with a jitter term, with normally distributed prior in log space, to capture misspecified error bars and model misspecification, and a mean constant flux with normal prior centred at zero. The harmonic oscillator has three parameters: the quality factor Q, the central frequency œâ and the amplitude a. Each parameter is given a Gaussian prior in log space.\nTo estimate the power excess of the modes we calculate the Mean Collapsed Correlation (MCC, see Kiefer 2013, Viani et al. 2019) . The MCC metric is convolved with an AstroPy Gaussian 1D Kernel smooth it, and the peak frequency of the smoothed MCC is used as a prior estimate of the dominant frequency œâ.\n'], 'url_profile': 'https://github.com/mercury0100', 'info_list': ['R', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}"
"{'location': 'VA, USA', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['A-B_Testing_Results_and_Regression_Approach\nAn A/B testing analysis and a regression approach project\nFiles:\nab_data.csv\ncountries.csv\nTable of Contents:\nIntroduction\nPart I - Probability\nPart II - A/B Test\nPart III - Regression\nConclusion\n'], 'url_profile': 'https://github.com/uminomuneaki', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'TypeScript', 'MIT license', 'Updated May 22, 2020', 'Python', 'Updated Sep 13, 2020', 'TypeScript', 'MIT license', 'Updated Dec 28, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['LogisticRegression-TFjs\nA from scratch implementation of Logistic Regression in TensorFlowJs. The weights and biases start at 0 originally\nbut as training progresses, they will be optimized via gradient descent\nIn this example, we have a training set of letters that belong to one class or another. After training on those images, we then classify letters as seen in the example below\nTo Run\n\nDownload this code to a folder\nOpen terminal in that folder (ie. cd into this folder)\nSetup all dependencies with npm i\n\nThis will install the project dependencies to node_modules/\nDependencies like: tensorflow.js, nodemon, colors, etc\n\n\nTo run the code, please run:\nnpm start\n\n\nExample\n$ npm start\n\nCreated a model of size [25, 1] with learning rate 9.99999993922529e-9\n=== Cost after iteration 0  : 0.6931471824645996% | Train Accuracy: 50%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n=== Cost after iteration 5  : 0.6931471824645996% | Train Accuracy: 50%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n=== Cost after iteration 10 : 0.6931471824645996% | Train Accuracy: 50%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n=== Cost after iteration 15 : 0.6931471824645996% | Train Accuracy: 50%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n=== Cost after iteration 20 : 0.6931471824645996% | Train Accuracy: 50%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n=== Cost after iteration 25 : 0.6931471824645996% | Train Accuracy: 50%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n=== Cost after iteration 30 : 0.6931471824645996% | Train Accuracy: 50%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n=== Cost after iteration 35 : 0.6931471824645996% | Train Accuracy: 50%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n=== Cost after iteration 40 : 0.6931471824645996% | Train Accuracy: 60.00000238418579%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 0, 0, 0, 1, 0\n=== Cost after iteration 45 : 0.6931471824645996% | Train Accuracy: 60.00000238418579%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 0, 0, 0, 1, 0\n=== Cost after iteration 50 : 0.6931471824645996% | Train Accuracy: 60.00000238418579%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 0, 0, 0, 1, 0\n=== Cost after iteration 55 : 0.6931471824645996% | Train Accuracy: 60.00000238418579%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 0, 0, 0, 1, 0\n.\n.\n.\n=== Cost after iteration 175: 0.6931470632553101% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 180: 0.6931470632553101% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 185: 0.6931470632553101% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 190: 0.6931470632553101% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 195: 0.6931470632553101% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 200: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 205: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 210: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 215: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 220: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 225: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 230: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 235: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 240: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 245: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 250: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 255: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 260: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 265: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 270: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 275: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 280: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 285: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 290: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 295: 0.6931470036506653% | Train Accuracy: 90.00000357627869%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 0, 1, 1, 1\n=== Cost after iteration 300: 0.6931469440460205% | Train Accuracy: 100%\n  Labels Expected: 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n  Labels Guessed : 0, 0, 0, 0, 0, 1, 1, 1, 1, 1\nFinal hyperparams w = [\n   -0.03999998793005943, -3.5762786065873797e-9,\n   0.019999992102384567,   -0.07999997586011887,\n  -0.039999984204769135,    0.01999998651444912,\n   0.019999993965029716,  -0.039999984204769135,\n    0.07999996840953827,  -0.019999993965029716,\n   0.039999980479478836,                      0,\n  -0.039999984204769135,   -2.38418573772492e-9,\n                      0,    0.01999998651444912,\n   0.019999993965029716,   -0.03999998793005943,\n     0.0599999763071537,  -0.019999997690320015,\n  -0.019999993965029716,   0.019999993965029716,\n   0.019999993965029716,   -0.03999998793005943,\n  -0.019999990239739418\n] | b = -3.5762786065873797e-9\n============================\nPredicting on test dataset:\n  Labels Guessed: 1, 0, 1, 1, 0\n  ‚ñà  ‚ñà \n  ‚ñà ‚ñà  \n  ‚ñà‚ñà         ->  1\n  ‚ñà ‚ñà  \n  ‚ñà  ‚ñà  \n\n  ‚ñà    \n  ‚ñà    \n  ‚ñà          ->  0\n  ‚ñà    \n  ‚ñà‚ñà‚ñà‚ñà  \n\n  ‚ñà‚ñà ‚ñà‚ñà\n  ‚ñà ‚ñà ‚ñà\n  ‚ñà ‚ñà ‚ñà      ->  1\n  ‚ñà   ‚ñà\n  ‚ñà   ‚ñà \n\n  ‚ñà‚ñà  ‚ñà\n  ‚ñà‚ñà‚ñà ‚ñà\n  ‚ñà ‚ñà‚ñà‚ñà      ->  1\n  ‚ñà  ‚ñà‚ñà\n  ‚ñà   ‚ñà \n\n   ‚ñà‚ñà‚ñà \n  ‚ñà   ‚ñà\n  ‚ñà   ‚ñà      ->  0\n  ‚ñà   ‚ñà\n   ‚ñà‚ñà‚ñà  \n                                                            \n'], 'url_profile': 'https://github.com/tvdaredevil', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'TypeScript', 'MIT license', 'Updated May 22, 2020', 'Python', 'Updated Sep 13, 2020', 'TypeScript', 'MIT license', 'Updated Dec 28, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020']}","{'location': 'Berkeley, CA', 'stats_list': [], 'contributions': '744 contributions\n        in the last year', 'description': ['Description\nTodo\nPackage Prerequisits\nThe Python packages that are required for the core package are numpy, sympy, scikit-sparse. If you are using anaconda or miniconda python distirbutions in Linux environment, these Python packages can be installed by\n$ sudo conda install -c conda-forge numpy sympy psutil scikit-sparse matplotlib -y\n\nIn additon, if you run the examples, the matplotlib, seaborn and psutil, and ray packages are also needed.\n$ sudo conda install -c conda-forge matplotlib seaborn psutil -y\n\nInstall ray through pip as follows. If neccessary, use the full path to call pip, such as /opt/miniconda/bin/pip.\n$ sudo conda install -c conda-forge pip -y\n$ sudo pip install ray\n\nUsage\n$ python NoiseEstimation.py [options]\n\nCredits\nAuthor:\nSiavash Ameli (University of California, Berkeley)\nCitation:\nAmeli, S. and Shadden. S. C. (2020). Maximum Likelihood Estimation of Variance and Nugget in General Linear Model.\nLicense: GNU General Public License v3.0\n'], 'url_profile': 'https://github.com/ameli', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'TypeScript', 'MIT license', 'Updated May 22, 2020', 'Python', 'Updated Sep 13, 2020', 'TypeScript', 'MIT license', 'Updated Dec 28, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': [""linear-polynomial-regression\nA simple project to show how calculate linear and polynomial regression (third degree) using TensorFlow js.\nClick on the graph to add a point and compare the difference between the two functions.\nLet's play! https://linear-polynomial-regression.herokuapp.com/\n""], 'url_profile': 'https://github.com/ricio91', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'TypeScript', 'MIT license', 'Updated May 22, 2020', 'Python', 'Updated Sep 13, 2020', 'TypeScript', 'MIT license', 'Updated Dec 28, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020']}","{'location': 'Richardson', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sindhun22', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'TypeScript', 'MIT license', 'Updated May 22, 2020', 'Python', 'Updated Sep 13, 2020', 'TypeScript', 'MIT license', 'Updated Dec 28, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020']}","{'location': 'Jaypee University of Engineering and Technology,Guna(M.P)', 'stats_list': [], 'contributions': '595 contributions\n        in the last year', 'description': ['USA-House-Price-Prediction-using-Multi-Linear-Regression\nMy implementation of Multi-Linear Regression to predict house prices.\n'], 'url_profile': 'https://github.com/rajansh87', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'TypeScript', 'MIT license', 'Updated May 22, 2020', 'Python', 'Updated Sep 13, 2020', 'TypeScript', 'MIT license', 'Updated Dec 28, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020']}","{'location': 'new delhi', 'stats_list': [], 'contributions': '321 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akash8190', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'TypeScript', 'MIT license', 'Updated May 22, 2020', 'Python', 'Updated Sep 13, 2020', 'TypeScript', 'MIT license', 'Updated Dec 28, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020']}","{'location': 'Raleigh', 'stats_list': [], 'contributions': '240 contributions\n        in the last year', 'description': ['Predicting-Breast-Cancer-using-Logistic-Regression-from-Scratch-in-python\nPredicting Breast Cancer using Logistic Regression from Scratch in python\nUsing the breast cancer dataset that is available in the sklearn library for processing - The explanation of dataset can be found here : [Reference link :https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)].\nUsing this dataset we implement logistic regression to predict breast cancer.\n'], 'url_profile': 'https://github.com/akashsrikanth2310', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'TypeScript', 'MIT license', 'Updated May 22, 2020', 'Python', 'Updated Sep 13, 2020', 'TypeScript', 'MIT license', 'Updated Dec 28, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mukesh5511', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'TypeScript', 'MIT license', 'Updated May 22, 2020', 'Python', 'Updated Sep 13, 2020', 'TypeScript', 'MIT license', 'Updated Dec 28, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['Air-Quality-Index-Prediction\n\nPrediction of Air Quality Index using Linear Regression\n\nWithin the last few years, an intense curiosity has been progressed by the people in the daily air quality circumstances to which they are encountered. Directed by the growing consciousness of the physical state of air pollution exposure, especially by most sensitive sub‚Äìpopulations such as children and the elderly, short‚Äìterm air pollution forecasts are being accentuated progressively by local authorities. The Air Quality Index (AQI) is the value implemented to estimate the quality of the air at a certain location. The components are estimated with the implementation of the covariance of the input data matrix. Only those elements, having Eigenvalues‚â• 1, were used to anticipate the AQI implementing the linear regression technique.\nMethod\n\nAnalysing the data: Data has been collected from the Real_Combine.csv file and implemented in the estimation of Air Quality Index.\nSeaborn pairplot: Seaborn pairplot has been implemented to plot pairwise relationships from the dataset.The pairplot function generates a grid of axes in such a way that each variable in data will be shared in the y-axis across a single row and in the x-axis across a single column.\nGenerating Heat Map: Heat Map is generated to determine the features which are mostly familiar to the target variable\nGenerating Extra Trees Regressor: This module is imported and generated in the model to determine the significance of each and every particular feature of our dataset.\nImplementation of linear regression: After bringing all the parameters into the learning circumstance, the linear regression is implemented to estimate the Air Quality Index.\nTrain and Test Split: 70% data of the dataset has been utilized for training and 30% has been enabled for the testing process.\nEstimating the coefficients: Keeping the other features fixed,1 single unit increase in T is related with a decrease of 2.690 in AQI PM2.5. Similarly, keeping the other features fixed,1 single unit increase in TM is related with an increase of 0.46 in AQI PM2.5 .\nRegression Evaluation Metrics: Mean Absolute Error (MAE),Mean Squared Error(MSE) and Root Mean Squared error (RSME) are calculated.\nGenerating byte stream file using pickle module :- For further evaluation, the object ‚Äùregression_model.pkl‚Äù file is dumped into byte stream file and then again done ‚Äúunpickling‚Äù to reconvert the analyzed byte stream file into object file.\n\nResult\nThe Mean Absolute Error (MAE) was found to be 44.84 approximately, Mean Squared Error(MSE)was found to be 3687.54 approximately, Root Mean Squared error(RMSE) was found to be 60.73 approximately. The Heatmap has been plotted to predict the familiar features of the target variable using the seaborn library .Pickle module is implemented to convert the regressor file(object)into byte stream and dump function is used to dump the converted byte stream of the file ‚Äùregressoion_model.pkl‚Äù into a file for further evaluation. After the processing and evaluation of the dumped file ,the byte stream file is again converted into object hierarchy using load function.\n'], 'url_profile': 'https://github.com/imkoustav', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'TypeScript', 'MIT license', 'Updated May 22, 2020', 'Python', 'Updated Sep 13, 2020', 'TypeScript', 'MIT license', 'Updated Dec 28, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'R', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020']}"
"{'location': 'Jogeshwari, Mumbai, Maharashtra 400102', 'stats_list': [], 'contributions': '258 contributions\n        in the last year', 'description': ['Ridge-and-Lasso-Regression\nRidge and Lasso Regression using Boston House Prediction\n'], 'url_profile': 'https://github.com/ganesh10-india', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jan 9, 2021', 'Visual Basic .NET', 'Updated May 19, 2020', 'MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'R', 'Updated Jun 16, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Tokyo', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['This repository provides R code implementing robust regression using extremely heavy-tailed error distribution, as proposed by the following paper.\nHamura, Y., Irie, K and Sugasawa, S. (2020). Log-Regularly Varying Scale Mixture of Normals for Robust Regression. https://arxiv.org/abs/2005.02800\nThe repository includes the following files.\n\nEHE-LM-function.R : The script implementing the proposed and alternative methods under linear regression models\nEHE-RI-function.R : The script implementing the proposed and alternative methods under random intercept models\nEHE-sp-function.R : The script implementing the proposed and alternative methods under spatial regression models\nFig.R : The script producing figures for density and cumulative distribution functions\nSim-LM.R : A one-shot simulation under linear regression models\nSim-RI.R : A one-shot simulation under random intercept models\nBoston.R: Illustration using the famous Boston housing data (spatial regression)\nDiabetes.R: Illustration using the famous Diabetes data (linear regression)\n\n'], 'url_profile': 'https://github.com/sshonosuke', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jan 9, 2021', 'Visual Basic .NET', 'Updated May 19, 2020', 'MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'R', 'Updated Jun 16, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SuperAce25', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jan 9, 2021', 'Visual Basic .NET', 'Updated May 19, 2020', 'MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'R', 'Updated Jun 16, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Prashanthsrn', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jan 9, 2021', 'Visual Basic .NET', 'Updated May 19, 2020', 'MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'R', 'Updated Jun 16, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'United States of America', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': [""Stock-Price-Preditction-Using-Regression-and-LSTM\nStock Market Value Prediction In Python Using Regression & Classifiers and LSTM\nFinal project for Hemda Computation Science Course 2020 - Machine Learning\nThis project focuses on the research and implementation of various regressors and classifier vs a neural network\nalgorithm such as an Long Short Term Memory (LSTM) on the topic of predicting stock market values. The subject matter is the performance and final\nability (after being trained) of the different models to predict the close price of stocks.\nTo narrow things down, the algorithm will fixate on Apple Inc.'s stock (AAPL) for training and prediciton.\n\n\nRegressors in quesiton:\n\nSupport Vector Regression\nSGD Regression\nBayesian Regression\nLeast-angle Regression\nAutomatic Relevance Determination Regression (ARD)\nPassive Aggressive Regression\nTheil‚ÄìSen estimator\nLinear regression\n\nThis project is based on this article.\n""], 'url_profile': 'https://github.com/omer1342', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jan 9, 2021', 'Visual Basic .NET', 'Updated May 19, 2020', 'MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'R', 'Updated Jun 16, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Ames Iowa_House Prices Prediction\nSubmission to Kaggle\'s ""House Prices: Advanced Regression Techniques"" competition:\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques\nprediction of the final price of each home from a selection of 79 available explanatory variables via 5 ML algorithms:\nBayesian Ridge regression\nLinear regression\nGradient boosting\nRandom forest\nDecision tree\n'], 'url_profile': 'https://github.com/AmirAvnit', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jan 9, 2021', 'Visual Basic .NET', 'Updated May 19, 2020', 'MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'R', 'Updated Jun 16, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Linear Regression\n'], 'url_profile': 'https://github.com/adityamohapatra1992', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jan 9, 2021', 'Visual Basic .NET', 'Updated May 19, 2020', 'MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'R', 'Updated Jun 16, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['A joint quantile and expected shortfall regression framework\nThis repository contains codes required to replicate the results of the paper A joint quantile and expected shortfall\nregression framework pusblished in Electronic Journal of Statistics.\n'], 'url_profile': 'https://github.com/BayerSe', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jan 9, 2021', 'Visual Basic .NET', 'Updated May 19, 2020', 'MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'R', 'Updated Jun 16, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Learner-X\nIs a simple Linear Regression Machine Learning web app, where one can upload a csv file and have real data predictions with modeled accuracy. Opitmization is in the hands of the user, as long as the 3 parameters are correlated data and the test size is large enough the accuracy can range from 82% - 95%.\nPrerequisites\n-Python 3.8.0\n-Django 3.0\n-Pandas\n-NumPy\n-Sklearn\nInstallation\ninstall python 3 from https://python.org/downloads\npip install -r requirements.txt\nUsage\nCreate a user account and then login, upload csv file per the instructions, set correct parameters for predictions, make sure data is correlated. Set test size and specify what is separating the data i.e. comma, tab or semi-colon\n'], 'url_profile': 'https://github.com/projects-and-algos-april-2020', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jan 9, 2021', 'Visual Basic .NET', 'Updated May 19, 2020', 'MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'R', 'Updated Jun 16, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saloni123-S', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jan 9, 2021', 'Visual Basic .NET', 'Updated May 19, 2020', 'MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', '1', 'R', 'Updated Jun 16, 2020', 'HTML', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['ML-stock-prediction\nStep 1: Start with importing necessary libraries.\nStep 2: Importing the data from NIFTY 50 .\nStep 3: Creating indicators like 10-days moving average, correlation, relative strength index (RSI), the difference between the open price of yesterday and today, difference close price of yesterday and the open price of today, open, high, low, and close price which is used for prediction.\nStep 4: Create the target variable which indicate profit or loss.\nStep5 : Split the dataset into a training dataset and test dataset.\nStep 6: Initiate logistic regression to predict profit or loss.\nStep 7:Predict the class lables using predict function for the test dataset.\nStep 8: Evaluvate the model and find the accuracy. use ‚Äòscore‚Äô function and ‚Äòcrossvalscore‚Äô function for finding accuracy.\nStep 9: Create trading statergy using the model.\n'], 'url_profile': 'https://github.com/Mani-CSEngineer', 'info_list': ['Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AJey1899', 'info_list': ['Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Gradient descent algorith in Linear regression.\nImplementation of gradient descent alogrithm in linear regression.\nTODO:\n\nMathematical explanation in detail.\nExplanation of gradient descent in multivariate linear regression.\nGraph plot of regression model w.r.t test data.\nAnimated graphy of gradient descent in action.\nImplemenation of stocastic gradient descent.\n\n'], 'url_profile': 'https://github.com/vivek2089', 'info_list': ['Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '132 contributions\n        in the last year', 'description': ['Scikit-Learn_Regression_ML_Models\nRegression Models Using Scikit-Learn Package In Python\nDisclaimer: This code snippet is developed with an intension for generic use only. I hereby declare that the code was not part of any of my professional development work. Also, no sensitive or confidential data sources are used for this development.\nDescription: The repository consists of below list of machine learning algorithms for regression problem:\n1. OLS Regression\n2. Lasso Regression\n3. Ridge Regression\n4. ElasticNet Regression\n5. Random Forest\n6. GBM\n7. Xgboost\n\nNote:\n\nThis scripts perform k-fold cross validation with hyper parameter tuning\nUser Input Section allows set all model parameters as well as data source specification. Users dont need to edit anything except this    section. This is a fully automated end to end code\nIt is assumed that input data source is a cleaned i.e. free from outlier and missing values\nRandom Sampling is performed based user given test split and only test sample model evaluation metrices are reported\n\nCompatibility: The code is developed and tested on Jupyter and Spyder Notebook using Python 3.7\n'], 'url_profile': 'https://github.com/debasishdutta', 'info_list': ['Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ianesilaba', 'info_list': ['Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sayantann11', 'info_list': ['Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shababnoor', 'info_list': ['Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'Great Boston Area', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': ['Predict-small-hydro-production-in-California\nProject from 201BUS-212A-1 : Analyzing Big Data II\nUsing regression (multi-linear regression), classification (KNN, Random Forest, Logistic regression, LGBTree), clustering (k-means) models to predict the production of small hydro and try to find out main impactors and certain patterns using R.\nThe data set is from Kaggle https://www.kaggle.com/cheedcheed/california-renewable-production-20102018\n'], 'url_profile': 'https://github.com/menghonghan', 'info_list': ['Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['multiple-regression-for-ML\nMultiple Linear Regression attempts to model the relationship between two or more features and a response by fitting a linear equation to observed data. The steps to perform multiple linear Regression are almost similar to that of simple linear Regression.\n'], 'url_profile': 'https://github.com/sayantann11', 'info_list': ['Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'Vancouver', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tnankie', 'info_list': ['Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Generated files\nThis repository contains generated files and a checksum.\nDo not edit the files in this repository outside of an instance of ServiceNow.\nIf you find yourself unable to import your repository due to the presence of files edited outside an instance of ServiceNow, merge commits that mix files from different revisions, or other data that does not match the checksum, you may recover using either of the following techniques:\n\n\nRemove the problem commits:\n\nClone your repository to a personal computer with the git command line tools installed and open a git command prompt in the repository root\nRun git log and take note of the SHA1s of the problem commits\nBuild revert commits using git revert SHA1 repeatedly, working backward in time, for each commit that introduced changes not generated by a ServiceNow instance\nRun git push\n\n\n\nOverwrite the problem code snapshot with a known good one:\n\nClone your repository to a personal computer with the git command line tools installed and open a git command prompt in the repository root,\nLocate a known good code snapshot and record its SHA1. For this step, git log can be useful.\nRun git reset --hard SHA1 to a commit that was generated by a ServiceNow instance\nRun git reset HEAD{1}\nRun git add -A\nRun git commit\nRun git push\n\n\n\n'], 'url_profile': 'https://github.com/paablo', 'info_list': ['Updated May 20, 2020', 'R', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Applied_Regression\nApplied_Regression_in_R\n'], 'url_profile': 'https://github.com/zhijunm', 'info_list': ['Updated May 20, 2020', 'R', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sk4code', 'info_list': ['Updated May 20, 2020', 'R', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '211 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Raman-rd', 'info_list': ['Updated May 20, 2020', 'R', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': ' Australia', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShashankaRangi', 'info_list': ['Updated May 20, 2020', 'R', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Chandigarh', 'stats_list': [], 'contributions': '121 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dushant01', 'info_list': ['Updated May 20, 2020', 'R', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['titanicRegression\n'], 'url_profile': 'https://github.com/xprimat', 'info_list': ['Updated May 20, 2020', 'R', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Grace-gitau', 'info_list': ['Updated May 20, 2020', 'R', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Banda, Uttar Pradesh', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': [""Logistic-Regression\nIn this notebook, you will learn Logistic Regression, and then, you'll create a model for a telecommunication company, to predict when its customers will leave for a competitor, so that they can take some action to retain the customers.\n""], 'url_profile': 'https://github.com/praphullmaurya', 'info_list': ['Updated May 20, 2020', 'R', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Nigeria', 'stats_list': [], 'contributions': '179 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ScMofeoluwa', 'info_list': ['Updated May 20, 2020', 'R', 'Updated Jun 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '510 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/austinteshuba', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 10, 2020', 'R', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.).\n'], 'url_profile': 'https://github.com/akash16dhotre', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 10, 2020', 'R', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Aish-warya', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 10, 2020', 'R', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'New Delhi,India', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Linear Regression\n'], 'url_profile': 'https://github.com/ashutosh8110', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 10, 2020', 'R', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/songyuwen0808', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 10, 2020', 'R', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['regression_model-\nKNOU\n'], 'url_profile': 'https://github.com/Rina0329', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 10, 2020', 'R', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ps1590', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 10, 2020', 'R', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['polynomial-regression\nIt makes use of a linear regression model to fit the complicated and non-linear functions and datasets. Hence, ""In Polynomial regression, the original features are converted into Polynomial features of required degree (2,3,..,n) and then modeled using a linear model.""\n'], 'url_profile': 'https://github.com/sayantann11', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 10, 2020', 'R', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'Dhaka', 'stats_list': [], 'contributions': '125 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sadat103', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 10, 2020', 'R', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['linear_regression\nBuild an Linear Regression model to predict the real_estate price\n'], 'url_profile': 'https://github.com/tanmayam97', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 10, 2020', 'R', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated May 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Regression-Projects\nProjects where the target variable is continuous\n'], 'url_profile': 'https://github.com/poluvidyasagar', 'info_list': ['Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 4, 2020', 'Updated May 6, 2020', '1', 'Python', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['CourseraRegression\n'], 'url_profile': 'https://github.com/darshanaonline', 'info_list': ['Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 4, 2020', 'Updated May 6, 2020', '1', 'Python', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NabidAlam', 'info_list': ['Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 4, 2020', 'Updated May 6, 2020', '1', 'Python', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/POLAMPALLY', 'info_list': ['Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 4, 2020', 'Updated May 6, 2020', '1', 'Python', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020']}","{'location': 'New Delhi,India', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/ashutosh8110', 'info_list': ['Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 4, 2020', 'Updated May 6, 2020', '1', 'Python', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/finescribe83', 'info_list': ['Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 4, 2020', 'Updated May 6, 2020', '1', 'Python', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tanishka2404', 'info_list': ['Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 4, 2020', 'Updated May 6, 2020', '1', 'Python', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Linear Regression - Fuel Consumption\nThe purpose of the project is to explore linear regression models in scikit-learn and fit a model to predict CO2 emissions from a fuel consumption data from various automobiles. We start by calculating a correlation matrix for the numeric variables in our dataset to check for linearity in our data. A heat map is a good way of visualizing the matrix.\n\nAs seen, data shows high linearity between the explanatory variables (features) and the target variable. Even all features have a strong linear relationship with the target variable, ""CO2EMISSIONS"", we  pick top 3 varaibles as features instead of all of them to prevent overfitting and make a rather generalized model. Hence, ""CYLINDERS"",""ENGINESIZE"", and ""FUELCONSUMPTION_HWY"" are used as features to predict ""CO2EMISSIONS"".\n\n\nWe choose multiple regression models to choose the top performing model.\n\n\n\nModel\nScore\n\n\n\n\nRidge\n0.856\n\n\nLasso\n0.856\n\n\nElasticNet\n0.855\n\n\nLasso_lars\n0.697\n\n\nBayesian_ridge\n0.856\n\n\n\n'], 'url_profile': 'https://github.com/khuzemasunel', 'info_list': ['Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 4, 2020', 'Updated May 6, 2020', '1', 'Python', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['University admissions predictions from two exams\nThis project considers a dataset which include test result of students of two exams.\nIt then considers the training data and considers a set of training examples and uses Regularized Logistic Regression to predict whether a student shoud be given admission in a university or not baed on their marks of the two exams.\nThis code has been written in Octave and can be opened in Octave or Matlab.\nTo run the code simply run ex2_reg.m in either Octave or Matlab.\nOn running it plots the graph and shows which students are eligible to get the admission (inside the decision boundary). This code can be further arranged to give out the exam roll numbers of students who have passed and be displayed in a screen or imported to a file.\n'], 'url_profile': 'https://github.com/RahulPaulML', 'info_list': ['Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 4, 2020', 'Updated May 6, 2020', '1', 'Python', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020']}","{'location': 'Dehradun, India', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['Regression-Algorithms\n'], 'url_profile': 'https://github.com/mudit-agarwal', 'info_list': ['Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 4, 2020', 'Updated May 6, 2020', '1', 'Python', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['elastic-regression\nThis code performs an elastic regression model when phase variation is considered as a noise.\n\nElasticRegressionModel.m is the main file to perform the elastic regression model.\nYou need to download DynamicProgrammingQ.c to run the program (*.mexmaci64 for mac and *mexw64 for window system)\nSimply run Example.m for generating simulated data and run ElasticRegressionModel.m.\n\n'], 'url_profile': 'https://github.com/fdastat', 'info_list': ['C', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 11, 2020', 'C', 'Updated May 6, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Hassen-Bououni', 'info_list': ['C', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 11, 2020', 'C', 'Updated May 6, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Mexico', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': [""logistic-regretion\nLogistic Regression model applied to horse clinic data set.\nSource:\nTable of contents\nIntroduction\nQuick reference\nMethodology\nDataset\nTraining\nResults\nConclusion\nFuture work\nAcknowledgements\nContact\nLicense\n\nIntroduction\nSome regression algorithms can be used to classify. The Logistic regression or also called (Logit Regression) is very used to estimate the probability that an instance belongs to a particular class, that is, if we have a classifier that discriminates for example between whether a horse is going to die or not, by means of the estimate of its probability and it turns out that it is greater than 50% then it belongs to class 1 or if it is less then it is not. This turns the logistic regression into a binary classifier.\nThe data set is divided into two parts, the training set horse-colic.data and the test set horse-colic.test. These were obtained from the data repository of the UCI Machine Learning Repository.\nCreators:\nMary McLeish & Matt Cecile\nDepartment of Computer Science\nUniversity of Guelph\nGuelph, Ontario, Canada N1G 2W1\nmdmcleish '@' water.waterloo.edu\nDonor:\nWill Taylor (taylor '@' pluto.arc.nasa.gov)\nCan obtain in this link: http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/\nThe data set is divided\nData set information:\n2 data files:\n-- horse-colic.data: 300 training instances\n-- horse-colic.test: 68 test instances\nPossible class attributes: 24 (whether lesion is surgical)\n-- others include: 23, 25, 26, and 27\nMany Data types: (continuous, discrete, and nominal)\nQuick reference\n$conda create -n log-resgression\n$conda activate log-resgression\n$conda install -c anaconda jupyter\n$conda install -c anaconda scikit-learn\n$conda install -c anaconda pandas\n$jupyter notebook\n""], 'url_profile': 'https://github.com/Faguilar-V', 'info_list': ['C', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 11, 2020', 'C', 'Updated May 6, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pramod2302', 'info_list': ['C', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 11, 2020', 'C', 'Updated May 6, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/martyraturi', 'info_list': ['C', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 11, 2020', 'C', 'Updated May 6, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/arifRusly', 'info_list': ['C', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 11, 2020', 'C', 'Updated May 6, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'DKI Jakarta, Indonesia', 'stats_list': [], 'contributions': '113 contributions\n        in the last year', 'description': ['Linear-Regression\n-Predicting-Salary-\n-Predicting Goal Scored-\n'], 'url_profile': 'https://github.com/denyfranssitohang', 'info_list': ['C', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 11, 2020', 'C', 'Updated May 6, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['1. Âª∫Á´ãÂ§öÂÖÉÂõûÂΩíÊ®°Âûã‚Äî‚ÄîÊ≥¢Â£´È°øÊàø‰ª∑È¢ÑÊµã\nÊï∞ÊçÆÈõÜ\n‰∏ãËΩΩÈìæÊé•\n\n\n\nÂ±ûÊÄß\nÂê´‰πâ\nÂ±ûÊÄß\nÂê´‰πâ\n\n\n\n\nCRIM\nÂüéÈïá‰∫∫ÂùáÁäØÁΩ™Áéá\nZN\n‰ΩèÂÆÖÁî®Âú∞Ë∂ÖËøá 25000 sq.ft. ÁöÑÊØî‰æã„ÄÇ\n\n\nINDUS\nÂüéÈïáÈùûÈõ∂ÂîÆÂïÜÁî®ÂúüÂú∞ÁöÑÊØî‰æã\nCHAS\nÊü•ÁêÜÊñØÊ≤≥Á©∫ÂèòÈáèÔºàÂ¶ÇÊûúËæπÁïåÊòØÊ≤≥ÊµÅÔºåÂàô‰∏∫1ÔºõÂê¶Âàô‰∏∫0Ôºâ\n\n\nNOX\n‰∏ÄÊ∞ßÂåñÊ∞ÆÊµìÂ∫¶\nRM\n‰ΩèÂÆÖÂπ≥ÂùáÊàøÈó¥Êï∞\n\n\nDIS\nÂà∞Ê≥¢Â£´È°ø‰∫î‰∏™‰∏≠ÂøÉÂå∫ÂüüÁöÑÂä†ÊùÉË∑ùÁ¶ª\nRAD\nËæêÂ∞ÑÊÄßÂÖ¨Ë∑ØÁöÑÊé•ËøëÊåáÊï∞\n\n\nTAX\nÊØè 10000 ÁæéÂÖÉÁöÑÂÖ®ÂÄºË¥¢‰∫ßÁ®éÁéá\nPTRATIO\nÂüéÈïáÂ∏àÁîüÊØî‰æã\n\n\nB\n1000ÔºàBk-0.63Ôºâ^ 2ÔºåÂÖ∂‰∏≠ Bk Êåá‰ª£ÂüéÈïá‰∏≠Èªë‰∫∫ÁöÑÊØî‰æã\nLSTAT\n‰∫∫Âè£‰∏≠Âú∞‰Ωç‰Ωé‰∏ãËÄÖÁöÑÊØî‰æã„ÄÇ\n\n\nMEDV\nËá™‰ΩèÊàøÁöÑÂπ≥ÂùáÊàø‰ª∑Ôºå‰ª•ÂçÉÁæéÂÖÉËÆ°\n\n\n\n\n\n‰ΩøÁî®ÁöÑÁ¨¨‰∏âÊñπÂ∫ì\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nËØªÂèñÂπ∂Â§ÑÁêÜÊï∞ÊçÆ\n\nËØªÂèñÊï∞ÊçÆ\n\ndata = pd.read_csv(\'data/housing.csv\')\n\n‰∏çËÄÉËôëÂüéÈïá‰∫∫ÂùáÁäØÁΩ™ÁéáÔºåÊ®°ÂûãËØÑÂàÜËæÉÈ´ò\nilocÁî®Ê≥ï\n\n# ‰∏çË¶ÅÁ¨¨‰∏ÄÂàóÁöÑÊï∞ÊçÆ\nnew_data = data.iloc[:, 1:]\nÊü•ÁúãÊï∞ÊçÆ\n\nÊü•ÁúãÂ§ÑÁêÜÂêéÁöÑÊï∞ÊçÆÈõÜ\n\n# ÂæóÂà∞Êï∞ÊçÆÈõÜ‰∏îÊü•Áúã\nprint(\'head:\', new_data.head(), \'\\nShape:\', new_data.shape)\n\nÊ£ÄÊü•ÊòØÂê¶Â≠òÂú®Áº∫Â§±ÂÄº\n\n# Áº∫Â§±ÂÄºÊ£ÄÈ™å\nprint(new_data.isnull().sum())\nÊü•ÁúãÊï∞ÊçÆÂàÜÊï£ÊÉÖÂÜµ‚Äî‚ÄîÁªòÂà∂ÁÆ±ÂΩ¢Âõæ\n\nËæìÂá∫Ë°åÊï∞countÔºåÂπ≥ÂùáÂÄºmeanÔºåÊ†áÂáÜÂ∑ÆstdÔºåÊúÄÂ∞èÂÄºminÔºåÊúÄÂ§ßÂÄºmaxÔºå‰∏äÂõõÂàÜ‰ΩçÊï∞75%, ‰∏≠‰ΩçÊï∞50%Ôºå‰∏ãÂõõÂàÜ‰ΩçÊï∞25%\n\nprint(new_data.describe())\n\n\nÁÆ±ÂΩ¢ÂõæÔºàBox-plotÔºâÊòØ‰∏ÄÁßçÁî®‰ΩúÊòæÁ§∫‰∏ÄÁªÑÊï∞ÊçÆÂàÜÊï£ÊÉÖÂÜµËµÑÊñôÁöÑÁªüËÆ°Âõæ„ÄÇ\n\n\nÁÆ±Á∫øÂõæÁöÑÁªòÂà∂ÊñπÊ≥ïÊòØÔºöÂÖàÊâæÂá∫‰∏ÄÁªÑÊï∞ÊçÆÁöÑ‰∏äËæπÁºò„ÄÅ‰∏ãËæπÁºò„ÄÅ‰∏≠‰ΩçÊï∞Âíå‰∏§‰∏™ÂõõÂàÜ‰ΩçÊï∞ÔºõÁÑ∂ÂêéÔºå ËøûÊé•‰∏§‰∏™ÂõõÂàÜ‰ΩçÊï∞ÁîªÂá∫ÁÆ±‰ΩìÔºõÂÜçÂ∞Ü‰∏äËæπÁºòÂíå‰∏ãËæπÁºò‰∏éÁÆ±‰ΩìÁõ∏ËøûÊé•Ôºå‰∏≠‰ΩçÊï∞Âú®ÁÆ±‰Ωì‰∏≠Èó¥„ÄÇ\n\n\n\n\nÁÆ±ÂûãÂõæÁªòÂà∂‰ª£Á†Å\n\nnew_data.boxplot()\nplt.show()\n\nÊï∞ÊçÆÈõÜÂàÜÂâ≤\nÂ∞ÜÂéüÂßãÊï∞ÊçÆÊåâÁÖß2:8ÊØî‰æãÂàÜÂâ≤‰∏∫‚ÄúÊµãËØïÈõÜ‚ÄùÂíå‚ÄúËÆ≠ÁªÉÈõÜ‚Äù\nX_train, X_test, Y_train, Y_test = train_test_split(new_data.iloc[:, :13], new_data.MEDV, train_size=.80)\nÂª∫Á´ãÂ§öÂÖÉÂõûÂΩíÊ®°Âûã\nÊ†πÊçÆËÆ≠ÁªÉÈõÜÂª∫Á´ãÊ®°Âûã\nmodel = LinearRegression()\nmodel.fit(X_train, Y_train)\na = model.intercept_\nb = model.coef_\nprint(""ÊúÄ‰Ω≥ÊãüÂêàÁ∫ø:Êà™Ë∑ù"", a, "",ÂõûÂΩíÁ≥ªÊï∞Ôºö"", b)\n\nscore = model.score(X_test, Y_test)\nprint(score)\nÊúÄ‰Ω≥ÊãüÂêàÁ∫ø:Êà™Ë∑ù 0.0 ,ÂõûÂΩíÁ≥ªÊï∞Ôºö [-1.74325842e-16  1.11629233e-16 -1.79794258e-15  7.04652389e-15\n -2.92277767e-15  2.97853711e-17 -8.23334194e-16  1.17159575e-16\n  1.88696229e-17 -3.41643920e-16 -1.28401929e-17 -5.78208730e-17\n  1.00000000e+00]\n1.0\n\nÊµãËØï\nY_pred = model.predict(X_test)\nprint(Y_pred)\nplt.plot(range(len(Y_pred)), Y_pred, \'b\', label=""predict"")\nplt.show()\nÁîªÂõæË°®Á§∫ÁªìÊûú\nX_train, X_test, Y_train, Y_test = train_test_split(new_data.iloc[:, :13], new_data.MEDV, train_size=.80)\n\nplt.figure()\nplt.plot(range(len(Y_pred)), Y_pred, \'b\', label=""predict"")\nplt.plot(range(len(X_test)), Y_test, \'r\', label=""test"")\nplt.legend(loc=""upper right"")\nplt.xlabel(""the number of MEDV"")\nplt.ylabel(\'value of MEDV\')\nplt.show()\n\n'], 'url_profile': 'https://github.com/BeJane', 'info_list': ['C', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 11, 2020', 'C', 'Updated May 6, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Vancouver, BC', 'stats_list': [], 'contributions': '383 contributions\n        in the last year', 'description': ['Linear-regression\nLearn how to use scikit-learn library to implement Simple linear regression with a data set that is related to fuel consumption and Carbon dioxide emission of cars.\n'], 'url_profile': 'https://github.com/mauedu93', 'info_list': ['C', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 11, 2020', 'C', 'Updated May 6, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Nairobi, Kenya', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Regression-Analysis\nIntroduces the intuition and application of regression analysis using scikit-learn\n'], 'url_profile': 'https://github.com/masakhwe', 'info_list': ['C', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 11, 2020', 'C', 'Updated May 6, 2020', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aiyuswa', 'info_list': ['Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'C++', 'Updated Jun 15, 2020', 'Python', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '192 contributions\n        in the last year', 'description': ['Videogame_Regression\n'], 'url_profile': 'https://github.com/zahmedgit', 'info_list': ['Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'C++', 'Updated Jun 15, 2020', 'Python', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rina0329', 'info_list': ['Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'C++', 'Updated Jun 15, 2020', 'Python', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'Nairobi,Kenya', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Venessachege', 'info_list': ['Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'C++', 'Updated Jun 15, 2020', 'Python', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amitk2587', 'info_list': ['Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'C++', 'Updated Jun 15, 2020', 'Python', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mukul2707', 'info_list': ['Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'C++', 'Updated Jun 15, 2020', 'Python', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': [""Symbolic-Regression\nGoal : Find the best approximation\nContent : 2 class --> Node : Treat the tree of the formula\n\t\t  --> Evolution : Modify the function\n\n\n\nNode : 3 attributs (2 Node* and 1 char*) - include string and vector\n\nPublic :\n\nConstructor : Node(Node* lc, Node* rc, std::string v), copy constructor and destructor\nGetters\n\nNode* left_child()\nNode* right_child()\nstd::string value()\n\n\nSetters\n\nvoid set_left_child (Node* lc)\nvoid set_right_child (Node* rc)\nvoid set_value (std::string s)\n\n\nint node_result(std::vector x) : for have the result of the formula with different value at the end of the tree\nstd::string node_formula() : give the formula of the tree\n\n\nProtected (attributes):\n\nNode* left_child_\nNode* right_child_\nstd::string value_\n\n\n\n\n\nEvolution : 5 attributs (1 Node* and 4 char*) - Use class Node\n\nPublic :\n\nConstructor : Evolution(Node* node, std::string data)\nGetters\n\nNode* root()\nstd::vector<Node*> mutant_children()\nstd::vector<std::vector> data()\nstd::vector fitnesses()\n\n\nSetters\n\nvoid set_node(Node* node)\nvoid set_data(std::string data)\n\n\nstd::vector evolution(int number_of_cycles, int number_of_children)\n\n\nProtected :\n\nstd::vector<Node*> replication_(int number_of_children) : for create number_of_child children to the parent's formula\nvoid mutation_(Node* position, Node* root, int id) : makes a mutation on a precise Node of a future mutant\nvoid insertion_(Node* position, Node* parent, int id) : make the mutation 'insertion' at a given place of the formula\nvoid deletion_(Node* position, Node* parent, int id) : make the mutation 'deletion' at a given place of the formula\nvoid replacement_(Node* position, Node* parent, int id) : make the mutation 'replacement' at a given place of the formula\nvoid replacement_leaf_management_(Node* position, Node* parent, Node* new_node_1, Node* new_node_2, int index_1, int index_2, int n, int id) : Auxiliary function to manage a replacement on a leaf Node\nvoid replacement_and_management_(Node* position, Node* parent, Node* new_node_1, Node* new_node_2, int index_1, int index_2, int n, int id) : Auxiliary function to manage a replacement on an AND Node\nvoid replacement_or_management_(Node* position, Node* parent, Node* new_node_1, Node* new_node_2, int index_1, int index_2, int n, int id) : Auxiliary function to manage a replacement on an OR Node\nvoid replacement_not_management_(Node* position, Node* parent, Node* new_node_1, Node* new_node_2, int index_1, int index_2, int n, int id) : Auxiliary function to manage a replacement on a NOT Node\nfloat compute_fitness_(Node* node) : Computes and returns the fitness for one Node\nvoid compare_fitness_() : Compares all mutants' fitnesses and if one of them has a better fitness than the base Node, it becomes the new base Node for the following cycle\nvoid apoptosis_(Node* node, int id) : Deletes a Node with its children\nNode* get_parent_node_(Node* position, Node* root) : Returns a pointer pointing on the parent of the Node position\nint left_or_right_child_(Node* position, Node* parent) : Returns -1 if the Node position isn't a child, 0 if the Node position is a left child and 1 if the Node position is a right child\nNode* node_at_path_(Node* node, std::string path) : Returns the Node on which the mutation will occur\nstd::string generate_path_() : Generates and returns a randomly generated path to the Node on which the mutation will occur\nstd::vector<std::vector> parse_data_(std::string data_to_parse) : Generates and returns a vector containing the data of the csv file\nvoid generate_used_operands_(Node* root, std::vectorstd::string &sub_used_operands) : Generates a vector of vector of unusable genes for each future mutant (only one copy of the gene for one formula)\nvoid generate_operands_(int number) : Generates a vector of vector of usable genes for each future mutant\nvoid operands_to_used_(int index, int id) : When a gene is used, it is transfered to the corresponding vector of no more usable genes\nvoid used_to_operands_(int index, int id) : When a Node is deleted, its genes are transfered to the corresponding vector of usable genes\nvoid re_operands_to_used_(Node* node, int id) : Same as operands_to_used_ but with a Node as parameter\nAttributes\n\nstd::vector<Node*> mutant_children_ : Vector of mutated clones\nNode* root_ : Base Node from which will be generated the clones to be mutated\nstd::string path_ : The path to the actual Node to mutate\nstd::vector<std::vector> data_ : Vector containing the data of the csv file\nstd::vector fitnesses_ : Vector of computed fitnesses per mutants\n\n\n\n\n\n\n\n""], 'url_profile': 'https://github.com/Dinh-SM', 'info_list': ['Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'C++', 'Updated Jun 15, 2020', 'Python', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zjm0325', 'info_list': ['Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'C++', 'Updated Jun 15, 2020', 'Python', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['LinearRegression\n'], 'url_profile': 'https://github.com/TaranovEV', 'info_list': ['Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'C++', 'Updated Jun 15, 2020', 'Python', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'Burlington, MA', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Polynomial-Regression\nProject Description\nYou have been hired by an ambitious entrepreneur for a new app that aims to help Clemson students make informed decisions concerning studying and drinking beer. Your job is to use meticulously gathered data to create a regression hypothesis function that will predict a student‚Äôs grade point average based on how many minutes per week they study and how many ounces of beer they consume per week.\nYour data is in a file named GPAData.txt that is available on Canvas (beware that Canvas might download it with a different name!). The first line in the file is an Integer value indicating how many lines of data are in the file. Each line after that contains three tab-separated real values that represent minutes studying/week, ounces of beer/week, and semester grade point average.\nThe assignment is to create a polynomial regression solution (y = w0 + w1x1 + w2x2 + w3x1x2 + w4x12 + w5x22). You should randomly divide your data set into a training set (70% of data) and a test set (30% of data).\nDeliverables.\n\n\nProject report that includes : \n‚Ä¢ Problem description \n‚Ä¢ Initial values that you chose for your weights, alpha, and the initial value for J. \n‚Ä¢ Final values for alpha, your weights, how many iterations your learning algorithm went through and your final value of J on your training set. \n‚Ä¢ Include a plot of J (vertical axis) vs. number of iterations (horizontal axis). \n‚Ä¢ If you did feature scaling, describe what you did. \n‚Ä¢ Value of J on your test set.\n\n\nA python file that prompts the user to enter values for minutes spent studying per week and ounces of beer consumed per week, and then predicts their semester GPA (rounded to two decimal places). The program should keep prompting the user until you enter zeros for both values.\n\n\n'], 'url_profile': 'https://github.com/tamannabaig', 'info_list': ['Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'C++', 'Updated Jun 15, 2020', 'Python', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Liner regression model to estimate CO2 emmision.\n\nImplemented Linear Regression model to predict the CO2 emmision for new light-duty vehicles based on its engine size.\nUsed scikit learn library to build regression model.\nPredicted the accuracy using r2-score and MSE loss functions.\nVisualized the results.\n\n'], 'url_profile': 'https://github.com/Pratik0896', 'info_list': ['Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'C++', 'Updated May 7, 2020', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Resturant-Profit-Maximization\nThis project computes the city that a resturant might want to invest in opening an outlet from the data collected by their food trucks across different cities. The data collected is the population of the cities and the respective profits of the food truck in those cities. Linear Regression has been used to analyse and compute the name of the city the CEO of the resturant company should open their outlet in.\nThis code can be run Octave or Matlab by running the file ex1.m and the results with the corresponding graphs has been programmed to be computed, plotted and displayed.\n'], 'url_profile': 'https://github.com/RahulPaulML', 'info_list': ['Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'C++', 'Updated May 7, 2020', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/souviksp', 'info_list': ['Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'C++', 'Updated May 7, 2020', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '789 contributions\n        in the last year', 'description': ['LogisticRegression\npractice for implementation, using dataset from kaggle\n'], 'url_profile': 'https://github.com/KokutoEnma', 'info_list': ['Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'C++', 'Updated May 7, 2020', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Cincinnati, Ohio', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/digvijaykawale', 'info_list': ['Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'C++', 'Updated May 7, 2020', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Logistic-Regression\nThis repository is dedicated to logistics regression. Its simple implementation, coding example with real-life data set and reading material.\n'], 'url_profile': 'https://github.com/gari-sha', 'info_list': ['Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'C++', 'Updated May 7, 2020', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['Linear_Regression\n'], 'url_profile': 'https://github.com/PierreNabil', 'info_list': ['Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'C++', 'Updated May 7, 2020', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['LinearRegression\nIn this a basic of linear regression code implementation in python(jupyterNotebook)\n'], 'url_profile': 'https://github.com/GhayasAhmed786', 'info_list': ['Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'C++', 'Updated May 7, 2020', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/masuun', 'info_list': ['Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'C++', 'Updated May 7, 2020', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '294 contributions\n        in the last year', 'description': ['Logistic-Regression\nThis is a repositary consisting of the template of the code to apply Logistic Regression to evaluate and classify binary data.\n'], 'url_profile': 'https://github.com/mk-gurucharan', 'info_list': ['Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Aug 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'C++', 'Updated May 7, 2020', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}"
"{'location': 'Chandigarh', 'stats_list': [], 'contributions': '320 contributions\n        in the last year', 'description': ['Univariate-Linear-Regression\nLinear Regression with NumPy and Python.\nProject Structure\nThe hands on project on Linear Regression with NumPy and Python is divided into the following tasks:\nTask 1: Introduction and Import Libraries\nTask 2: Load the Data and Libraries\nTask 3: Visualize the Data\nTask 4: Compute the Cost ùêΩ(ùúÉ)\nTask 5: Implement Gradient Descent from scratch in Python\nTask 6: Visualizing the Cost Function J(ùúÉ)\nTask 7: Plotting the Convergence\nTask 8: Training Data with Univariate Linear Regression Fit\nTask 9: Inference using the optimized ùúÉ values\n'], 'url_profile': 'https://github.com/connectaditya', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Oct 19, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Duskey', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Oct 19, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '130 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/Maneenoot', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Oct 19, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/shekhar1990', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Oct 19, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['linearRegression\n'], 'url_profile': 'https://github.com/Brageas', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Oct 19, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dboywjy', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Oct 19, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Lille', 'stats_list': [], 'contributions': '502 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aubryprieur', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Oct 19, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'China', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['linear-Regression\n'], 'url_profile': 'https://github.com/cpup007', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Oct 19, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Brazil', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Logistic_Regression\n'], 'url_profile': 'https://github.com/mvmarcio', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Oct 19, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['LogisticRegression\n'], 'url_profile': 'https://github.com/tanishka2404', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated Oct 19, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/udaybhaskar4592', 'info_list': ['Updated May 10, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Regression-analysis-\n'], 'url_profile': 'https://github.com/AjinkyaSurve240', 'info_list': ['Updated May 10, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Kyiv, Ukraine  ', 'stats_list': [], 'contributions': '343 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ValeriiSielikhov', 'info_list': ['Updated May 10, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['-Credit-Card-Fraud-Detection-\nBy using Logistic Regression with the help of Information Value,Down Sampling the unbalance data using Near Miss function and also use Logit Regression Results for analysis\n'], 'url_profile': 'https://github.com/rahulpawar0712', 'info_list': ['Updated May 10, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kymmie777', 'info_list': ['Updated May 10, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regularized-Linear-Regression\nIn this project we predict the amount of water flowing out of a dam using the change of water level in a reservoir. Regularized Linear Regression is used for training the algorithm.\nThe training set is divided into 3 different sets after random shuffle:\n\nTraining set\nCross validation set\nTest set\n\nThis division was made so that the calculation of the regularization parameter (bias vs variance calculation) can be more efficiently done.\nThe code is written in Octave but can be run in both Octave and Matlab.\nTo run the code simply run the file ex5.m in Octave or Matlab.\n'], 'url_profile': 'https://github.com/RahulPaulML', 'info_list': ['Updated May 10, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Istanbul', 'stats_list': [], 'contributions': '174 contributions\n        in the last year', 'description': ['Logistic-Regression-Naive-Bayesian\n'], 'url_profile': 'https://github.com/ecealptekin', 'info_list': ['Updated May 10, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Kyiv', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Advertising-Revenue-Regression\n'], 'url_profile': 'https://github.com/popovko', 'info_list': ['Updated May 10, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/martyraturi', 'info_list': ['Updated May 10, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['multiple-linear-regression\nmultiple linear regression is a supervised learning technique used to predict a dependent variable based on multiple independent variable.\n'], 'url_profile': 'https://github.com/prathm3sh', 'info_list': ['Updated May 10, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'MATLAB', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 5, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 8, 2020']}"
"{'location': 'Los Angeles', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': [""Deck Of Cards\n\nThis challenge is not complete and is not functional\n\nGet up and running\n\nclone repo\nyarn\nyarn serve\n\nMisc Commands\n\nyarn install : install depenedencies\nyarn serve: run app on localhost:8081\nyarn build: build app for production\nyarn storybook: run living styleguide\nyarn storybook:build: build living styleguide for production\nyarn test:unit: Run snapshot and regression tests\nyarn test:report: Collect test coverage report and view in browser\nyarn test:watch: Update test coverage on the fly\nyarn lint: Lint application with prettier\n\nLiving Style Guide\n\nWe use storybook to bring transparency into the development cycle and to achieve the following:\n\nIncreased velocity\nStreamline the workflow\nBuild components in isolation\nMock hard to reach use cases\nDocument use cases as stories\nShare and reuse everything\nShip with confidence\n\nYou can read more about storybook here.\nStory structure\nEach component should have a story for each state.  For example:\n\ndefault\nEmpty\nNormalized\n\nTesting\n\n\nStorybook\nJest\naddon-storyshots\nstoryshots-puppeteer\n\nWe use the testing pyramid to ensure we deliver high quality user experiences.  We've automated unit and most integration test cases by using jest, storybook and storyshots.  Our stories generate tests to cover:\n\nUnit tests (good coverage)\nIntegration tests, visual regression\nEnd-to-end test, cross-browser visual regression\n\nHaving React coupled with Storybook and Storyshots, unlocks a different model: the Diamond model.\nThe diamond model for your UI/App means: little to zero unit tests, massive amount of integration tests, and zero manual tests.\nWhat changed? Integration tests were avoided in the early days because they had a reputation of running slowly; granted ‚Äî with most technologies this is still very true.\nWith Jest, React, and Storybook/Storyshots, this is (arguably) no longer the case. No longer must you bring up a browser for each test that leaves its traces in your test environment, or have flaky test suites run and fail randomly, using a not-so-smart test runner that forces you to run everything exactly when you didn‚Äôt want to. It‚Äôs an era where frontend tooling really does work, and hard becomes easy.\nStructural Tests\nIf you already write stories for every component, you already are writing tests, and you just don‚Äôt know it yet. Given the thesis above, each of your stories can automatically become a tests:\n\nInput is your story\nProcessing is simply rendering a story (which storybook already does)\nOutput is a generated snapshot\n\nAnd this is what Storyshots does. Storyshots will verify that a React component renders correctly; and if you build multiple stories with a number of different properties then Storyshots can snapshot those as well, and those would be verified on every test run.\n\n\n\nTest Type\nLevel\nSubject\nSolution\nSource\n\n\n\n\nBrowser Regression\nIntegration\nPage/Component\nStoryshots\nStory\n\n\nVisual Regression\nIntegration\nPage/Component\nStoryshots\nStory\n\n\nRender\nIntegration\nPage/Component\nStoryshots\nStory\n\n\nInteraction\nUnit\nComponent\nStoryshots\nUnit Test\n\n\n\nThat said, you should definitely keep your ‚Äúclassic‚Äù unit tests for logic, library and domain model code. All these things you put in /lib, external packages that deal with your domain model and so on.\nYou can read more about it here\nCommands\n\nyarn storybook, starts storybook at localhost:9009\nyarn test:unit, compares updates to image snapshots\nyarn test:report view test coverage report in the browser\nyarn test:watch update test coverage on the fly\n\nrun yarn storybook and yarn test:coverage in seperate terminals to get instant feedback on regression tests\n""], 'url_profile': 'https://github.com/RobbyRob81', 'info_list': ['HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Surabaya, Indonesia', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Regression-Automobile-Time-Series\n'], 'url_profile': 'https://github.com/fahrim27', 'info_list': ['HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['COVID-19-Multilinear-Regression\n3022 Final Project\nIn this repository you will find the final Jupyter NoteBook for my COVID-19 Regression Project. The datasets were huge so they can be found at the links in the notebook but are not uploaded here.\n'], 'url_profile': 'https://github.com/brar9262', 'info_list': ['HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'ALLAPUZHA', 'stats_list': [], 'contributions': '342 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/manisha-jaiswal', 'info_list': ['HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Projects_with_Regression_Models\nThis project is about different analyses using regression algorithms (logistic, linear, ANOVA, Loess...).\n'], 'url_profile': 'https://github.com/lapp03', 'info_list': ['HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': ' Australia', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShashankaRangi', 'info_list': ['HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '271 contributions\n        in the last year', 'description': ['Linear_Regression_Deep_Learning\nThe following repository contains the jupyter notebook and the python file of the implementation of linear regression with deep learning using keras and tensorflow\n'], 'url_profile': 'https://github.com/sayantanmukh050893', 'info_list': ['HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Binary-logistic-Regression\nAnalyzing Hr Data\n'], 'url_profile': 'https://github.com/yhummyengine', 'info_list': ['HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bswssourav', 'info_list': ['HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saloni123-S', 'info_list': ['HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Python', 'Updated May 9, 2020', 'HTML', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}"
"{'location': 'Bandung -Indonesia', 'stats_list': [], 'contributions': '133 contributions\n        in the last year', 'description': ['Flight-Delay-usingRegression\n'], 'url_profile': 'https://github.com/pimens', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['Applying Linear Regression to a Lego dataset\n\nOverview\nUsing a linear regression model, can we help predict the price of a lego set based on specific variables?\nProcess followed\nLibrairies used\n\npandas\nstatsmodels\nscipy.stats\nmatplotlib.pyplot\n\nSteps\n1. Data collection\nDataset imported from : https://github.com/seankross/lego/tree/master/data-tidy\n2. Data cleaning:\n\n\nDeleting 8 columns\n\n\nDeleting ~2000 empty rows\n\n\nReducing the number of themes\n\n\nReplacing ""Year"" by ""Age"" to avoid time series\n\n\nCreating dummies for non-numerical columns\n\n\nChecking if the dataset has duplicates - does not have any\n\n\nConvert numerical columns to be normally distributed using the boxcox method:\n\n\nDistribution in the dataset:\n\nBoth are lognormally distributed.\nDistribution after using the boxcox method with a lambda 0:\n\n\nIdentifying outliers:\n\n\nSolution: creating 2 new columns identifying outliers\n3. Regression analysis: \nDropping 9 more variables with the first analysis based on high pvalues\n 4. Checking the 5 assumptions for linear regression \n\nMulticollinearity\n\n\nDropping 5 more columns, we checked the assumption.\n\nLinearity\n\n\nThe assumption is verified\n\nAutocorrelation\n\nThe Durbin-Watson test shows a positive autocorrelation with a coefficient of 1.11\n\nHomoscedasticity\n\n\nThe assumption is potentially not verified.\n\nExogeneity of residuals\n\n\nThe assumption is not verified as the residuals don\'t follow a normal law according to the Anderson-Darling test (pvalue < 0.05)\nResults\nThe final linear regression model has a high R¬≤, however the 5 assumptions are not verified.\nResult of the OLS model:\n\nThe model equation:\nln(y)= -0.21+1.8xOutliers_pieces+ Œ≤1xThemes+Œ≤2xPackagings+Œ≤3xAvailability+0.68xNumber_pieces\nPlotting the model :\n\nConclusion\nThe model can be used to explain the current prices of these lego sets however is not yet usable for predictions as all assumptions are not checked.\n'], 'url_profile': 'https://github.com/Camillelib', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['-Regression-Analysis-Boston-Dataset-\n'], 'url_profile': 'https://github.com/JNyaata', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': ['Logistic-Regression-Practice\n'], 'url_profile': 'https://github.com/Ernest-Anderson-Hutasoit', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Regression-Decision-Tree\nDecision trees depend on the type of target variable. A decision tree that has a continuous target variable is called a decision tree of a continuous variable. Regression trees are usually used when the target variable is continuous. The following code illustrates the use of decision trees in predicting a set of points and drawing data corresponding to a tree with a depth of 2 and a depth of 5\n'], 'url_profile': 'https://github.com/BayanTurkmaney', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['multiple-linear-regression\n'], 'url_profile': 'https://github.com/Jriki', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/buluofeng2', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'Nepal', 'stats_list': [], 'contributions': '271 contributions\n        in the last year', 'description': ['Requirements:(pip freeze)\njoblib==0.14.1\nnumpy==1.18.4\npandas==1.0.3\npython-dateutil==2.8.1\npytz==2020.1\nscikit-learn==0.22.2.post1\nscipy==1.4.1\nsix==1.14.0\nsklearn==0.0\nOriginal dataset is from kaggle :https://www.kaggle.com/ronitf/heart-disease-uci\nExploratory data analysis is done in EDA folder.\n  \n  Logistic Regression is used as our model and accuracy is obtained to be 85-89%. However mean Cross validation accuracy is found to be 85.863%\nClone this project :Click Here\n'], 'url_profile': 'https://github.com/Aasess', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saloni123-S', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '311 contributions\n        in the last year', 'description': ['Understanding Machine Learning on Regression Problem\nDataset : Medical Cost Personal Datasets\nsource : https://www.kaggle.com/mirichoi0218/insurance\n01 Exploratory Data Analysis\n02 OLS with statsmodel library\n03 Multiple Linear Regression\n04 Polynomial Regression\n05 Support Vector Regression\n06 Decision Tree Regression\n07 Random Forest Regression\n08 XGBoost Regression\n'], 'url_profile': 'https://github.com/WoradeeKongthong', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Python', 'Updated May 6, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 19, 2020']}"
"{'location': 'mumbai', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rashmi3320', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 8, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 6, 2020', 'Clojure', 'Updated May 6, 2020']}","{'location': 'Kampala, Uganda', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nHello there! Thanking you for checking out this repository.\nHere, I demonstrate a simple application of scikit-learn to implement a simple linear regression model\n'], 'url_profile': 'https://github.com/iotim256', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 8, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 6, 2020', 'Clojure', 'Updated May 6, 2020']}","{'location': 'Austin, Texas.', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Regression-ML-Algorithm\n'], 'url_profile': 'https://github.com/saurabhjain0596', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 8, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 6, 2020', 'Clojure', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Stock-Linear-regression\nb√†i t·∫≠p m√¥n machine learning\nmy-investigation: my investigation about relationships between big-5 stocks\nmy-linear-regression : yeilds my best experimental result\n'], 'url_profile': 'https://github.com/not-nam-or-am-i', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 8, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 6, 2020', 'Clojure', 'Updated May 6, 2020']}","{'location': 'Molfetta, BA, Italy', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Wine Dataset with PCA and Linear Regression\nUse of Wine dataset (red+white) using PCA and Logistic Regression in order to predict ""quality"".\nData Distribution of quality Feature after joining white and red wines\n\nVariance distribution after PCA\nAfter PCA we can choose to use the first 8 components only, which explain about 90% of the variance\n\nLinear Regression\n\nTraining set: 70%\nTest set: 30%\n\nConclusions\nNot excellent results for quality prediction on this dataset, with a score of 50%.\n'], 'url_profile': 'https://github.com/gaetanodegennaro', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 8, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 6, 2020', 'Clojure', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['Predicting college admission chances using linear regression\ncollege_admission() in college_admission.py learns the parameters for the linear hypothesis through gradient descent.\n\nGradient descent is implemented from scratch.\nThe data is preprocessed using sklearn.\n\nPassing plot_cost=True to college_admission() plots the training cost as a function of iterations.\nDataset:\nMohan S Acharya, Asfia Armaan, Aneeta S Antony : A Comparison of Regression Models for Prediction of Graduate Admissions, IEEE International Conference on Computational Intelligence in Data Science 2019\n'], 'url_profile': 'https://github.com/simenjh', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 8, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 6, 2020', 'Clojure', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/phuongdongle', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 8, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 6, 2020', 'Clojure', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '190 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chetansy', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 8, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 6, 2020', 'Clojure', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Auto_Regression_Forecasting_Method\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 8, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 6, 2020', 'Clojure', 'Updated May 6, 2020']}","{'location': 'Brooklyn, NY', 'stats_list': [], 'contributions': '1,001 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rschmukler', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 12, 2020', 'MIT license', 'Updated May 8, 2020', 'Updated May 7, 2020', 'Python', 'Updated May 6, 2020', 'Clojure', 'Updated May 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['CFB Win Regression Model\nRepo Guide\n\nACC: All ACC data used in this project in excel format\nB1G10: All B1G 10 data used in this project in excel format\nBIG12: All BIG 12 data used in this project in excel format\nCFB: Saved RDS files and images used throughout project, as well as app.R file\nPAC12: All PAC 12 data used in this project in excel format\nSEC: All SEC data used in this project in excel format\n.gitignore\nREADME.md\nfinal_project.Rmd: R Markdown version of project where data wrangling was done\n\n'], 'url_profile': 'https://github.com/ddiakite1', 'info_list': ['1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Assignment--Linear-Regression\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\nWhich variables are significant in predicting the price of a car.\nHow well those variables describe the price of a car.\n\n'], 'url_profile': 'https://github.com/anikettote', 'info_list': ['1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'London - UK', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PierreElm', 'info_list': ['1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['logistic_regression_social_network-\nDone Logistic Regression and predicted the number of people using the social networking\n'], 'url_profile': 'https://github.com/tanmayam97', 'info_list': ['1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['SVM-machine-for-regression\nSupport vector machines (SVMs) are powerful yet flexible supervised machine learning algorithms which are used both for classification and regression. But generally, they are used in classification problems. ... SVMs have their unique way of implementation as compared to other machine learning algorithms.\n'], 'url_profile': 'https://github.com/sayantann11', 'info_list': ['1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Diabetes-data-logistic-regression\n'], 'url_profile': 'https://github.com/Rajvi-Doshi', 'info_list': ['1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': [""Linear Regression Project\nWe learnt about the New York house prices dataset in the in-class session. We learnt\n\nWhat is linear regression\nHow gradient descent algorithm works to minimize the cost function\nWhat are the assumptions of linear regression\n\nWe also implemented linear regression using sklearn library in which we\n\nFit a linear regression model\nPredicted house prices using the fitted model\nCalculated MSE for predicted and actual house prices\n\nNow, let's take this forward and increase our understanding of linear regression!\nThis assignment is a series of simple tasks, in which we will be fitting a linear regression model on the house pricing data and validating some of the assumptions of linear regression.\nWhy solve this assignment?\nBy the end of this assignment,\n\nYou will be able to confidently train a linear regression model and predict values of the target variables\nYou will have a better understanding about the assumptions of linear regression and how to validate them.\nBy completing this project you have an opportunity to win 800 points!!\n\nOn assumptions of linear regression\nAssumptions of linear regression model play an extremely important role in the model performance and stability. Hence, it is very important to validate these assumptions. Validating these assumptions can give us deeper insights into the kind of data we are dealing with and steps that could be taken to improve the results of the linear model.\nSo, let's get started.\nAbout House Prices dataset\nHere are some of the imports that we will be using throughout the assignment.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\n%matplotlib inline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n""], 'url_profile': 'https://github.com/mhs10031990', 'info_list': ['1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': [""Guided Project - Multivariate regression\nHello Data Scientists,\nLets try our out guided project of predicting students performance based on given features.\nThis is type of multivariate multioutput regression problem and you will be getting chance to showcase your learning experiences gathered so far.\nWe will be presenting multiple approaches to solve the given problem in the real-life way so consider this as simulation to actual data scientists problems available out there.\nPlease feel free to try out other approaches as well if you deem fit, the show cased here will just be the approach.\nMy target here will be  to demonstrate general approach to a problem.\nWhat we have learnt so far..\n\nLinear regression\nFeature Selection\nFeature Engineering\nAdvanced linear regression techniques\n\nDataset\nPredict student performance in secondary education (high school).\nFeatures:\nAttributes for both student-mat.csv (Math course):\n\nschool - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\nsex - student's sex (binary: 'F' - female or 'M' - male)\nage - student's age (numeric: from 15 to 22)\naddress - student's home address type (binary: 'U' - urban or 'R' - rural)\nfamsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\nPstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\nMedu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 √¢‚Ç¨‚Äú 5th to 9th grade, 3 √¢‚Ç¨‚Äú secondary education or 4 √¢‚Ç¨‚Äú higher education)\nFedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 √¢‚Ç¨‚Äú 5th to 9th grade, 3 √¢‚Ç¨‚Äú secondary education or 4 √¢‚Ç¨‚Äú higher education)\nMjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\nFjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\nreason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\nguardian - student's guardian (nominal: 'mother', 'father' or 'other')\ntraveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\nstudytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\nfailures - number of past class failures (numeric: n if 1<=n<3, else 4)\nschoolsup - extra educational support (binary: yes or no)\nfamsup - family educational support (binary: yes or no)\npaid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\nactivities - extra-curricular activities (binary: yes or no)\nnursery - attended nursery school (binary: yes or no)\nhigher - wants to take higher education (binary: yes or no)\ninternet - Internet access at home (binary: yes or no)\nromantic - with a romantic relationship (binary: yes or no)\nfamrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\nfreetime - free time after school (numeric: from 1 - very low to 5 - very high)\ngoout - going out with friends (numeric: from 1 - very low to 5 - very high)\nDalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\nWalc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\nhealth - current health status (numeric: from 1 - very bad to 5 - very good)\nabsences - number of school absences (numeric: from 0 to 93)\n\nThese grades are related with the course subject, Math:\n\nG1 - first period grade (numeric: from 0 to 20)\nG2 - second period grade (numeric: from 0 to 20)\nG3 - final grade (numeric: from 0 to 20, output target)\n\nWhat you will learn solving this ?\n\nLearn systematic approach to select features\nCompare various regression techniques\nEmphasis will be given on correlations between dependant features to take call on approaches\nAlso try out advanced regressions to check what works best for dataset.\n\nGeneral Notes to approach problems are:\n-How to approach a ML problem\n1.import data\n2.missing data\na.remove the missing lines - dangerous\nb.imputation - take mean of column - sklearn.preprocessing.Imputer\n3. convert categorical data\t\n4.splitting datasets - \n5.Feature Scaling\n    a. Standardisation - (x-mean(x))/std_dev(x) \n    b.Normalisation\t\t - (x-min(x))/(max(x)-min(x))\n6.Apply classifier and test on split\n7.Draw conclusions by plottig if required\t\n\nSeems like you are all fired up to put a test to your knowledge.\nLet's get started!\n""], 'url_profile': 'https://github.com/mhs10031990', 'info_list': ['1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\nMultiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. The goal of multiple linear regression (MLR) is to model the linear relationship between the explanatory (independent) variables and response (dependent) variable.\nThe Library i used in my notebook is Scikit-learn.\n'], 'url_profile': 'https://github.com/mshouzebsaleem77', 'info_list': ['1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pramod1109', 'info_list': ['1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}"
"{'location': 'Kolkata', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Linear & Logistic Regression\n'], 'url_profile': 'https://github.com/pranta123456', 'info_list': ['Jupyter Notebook', 'Updated Jun 27, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '211 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Raman-rd', 'info_list': ['Jupyter Notebook', 'Updated Jun 27, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['random-forest-regression\nA Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap Aggregation, commonly known as bagging.\n'], 'url_profile': 'https://github.com/sayantann11', 'info_list': ['Jupyter Notebook', 'Updated Jun 27, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Linear Regression : Important features that measure the concrete compressive strength of a house\nTarget Y(Strength of house)\nConcrete compressive strength [MPa]\nTo predict compressive strengths, we have these features available:\nInput X(Features):\nCement [ùëòùëîùëö3]\nBlast furnace slag [ùëòùëîùëö3]\nFly ask [ùëòùëîùëö3]\nWater [ùëòùëîùëö3]\nPlasticizer [ùëòùëîùëö3]\nCoarse aggregate [ùëòùëîùëö3]\nFine aggregate [ùëòùëîùëö3]\nAge [ùëë]\n'], 'url_profile': 'https://github.com/surya-max', 'info_list': ['Jupyter Notebook', 'Updated Jun 27, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': [""NFL Draft Regression Analysis\nThis repo contains the code, data, outputs and reports (powerpoint and word document) to support my NFL Draft Linear Regression Analysis for Applied Multivariate Data Analysis Project.\nCode\nThe code exists as 4 notebooks all within the 'notebooks' subdirectory.\n\nnfl_draft_data_processing.ipynb - This notebook combines, transforms and normalizes the raw_data to create the data within transformed_data\nnfl_draft_factor_analysis.ipynb - This notebook contains the code to generate the PCA and factor analysis outputs used within the reports\nnfl_draft_regression_analysis.ipynb - This notebook contains the code to generate the linear regression analysis outputs used within the reports\nnfl_draft_eda.ipynb - This notebook contains some additional analysis not included within the report but used to understand the data\n\nReports\n\nnfl_draft_analysis.pptx - A summary of the analysis performed structured as a powerpoint\nnfl_draft_regression_analysis.odt - A summary of the analysis performed structured as a research paper\n\n""], 'url_profile': 'https://github.com/haleysam93', 'info_list': ['Jupyter Notebook', 'Updated Jun 27, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'Burlington, MA', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Classification-Using-Logistic-Regression\nProblem Description and Data Set\nThis Project deals with identifying whether a fish is TigerFish0 or TigerFish1. This time you are to use a logistic regression approach.  The first line in the data file contains a single integer indicating how many sets of labelled data you have to work with. Each line after that contains three tab-separated entries. The first is a float representing the body length in centimeters, followed by a float representing the dorsal fin length in centimeters, then an integer identifying the fish as either TigerFish0 (with a 0) or TigerFish1 (with a 1).\nAssignment\nUsing what you have learned in class and the notes, develop (from scratch in Python, no using a Logistic Regression library function!) a Hypothesis function that will predict type of fish given unseen data. You are free to use any model variation and any testing or training approach we have discussed.\nA pdf report includes:\n‚Ä¢ Problem Description \n‚Ä¢ Description of your data set along with a plot of the data.\n‚Ä¢ A description of your model and testing procedure, including\n‚Ä¢ Initial values that you chose for your weights, alpha, and the initial value for J.\n‚Ä¢ Final values for alpha, your weights, how many iterations your learning algorithm went through and your final value of J on your training set.\n‚Ä¢ Include a plot of J (vertical axis) vs. number of iterations (horizontal axis).\n‚Ä¢ If you did feature scaling, describe what you did.\n‚Ä¢ Value of J on your test set.\n‚Ä¢ A confusion matrix showing your results on the test set.\n‚Ä¢ A description of your final results that includes accuracy, precision, recall and F1 values.\n‚Ä¢ A comparison of your results with a logistic regression approach as compared to your previous k-nearest neighbor approach (including kNN confusion matrix, accuracy, precision, recall and F1 values.\nThe python file that repeatedly prompts for the body length and dorsal fin length (in centimeters) of a candidate fish and prints out the type to the screen. The program should terminate when you enter zero for both values.\n'], 'url_profile': 'https://github.com/tamannabaig', 'info_list': ['Jupyter Notebook', 'Updated Jun 27, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'NYC ', 'stats_list': [], 'contributions': '470 contributions\n        in the last year', 'description': [""Predicting Song Popularity\nThrough linear regression modeling, this project aims to predict the popularity of a song.\nThe Dataset\nThe final dataset used in this project was a compilation of a 2,000 song Spotify dataset sourced from Kaggle along with additional data gathered from API requests to Spotify's API via Spotipy.\nThe target variable assessed was a popularity score for each song.\n\nFrom Spotify's documentation - the popularity score is a calculation of number of plays and how recent those plays were.\nI wanted to see if song and artist elements could predict the popularity score.\n\nThe explantory variables covered a range of characteristics on each song, from specific audio features like BPM, valence and duration, to more categorical data such as, genre, year released and artist follower count.\nEDA\nTarget Distribution:\n\nCorrelation heatmap:\n\nNumerical Feature Distribution:\n\nFeature Engineering - Polynomial Transformation\nAfter my EDA and running a baseline linear regression model, I applied polynomial transformation to the 2nd degree to all of my song audio features. This created interactions among the different song elements, which in hindsight really made sense because it‚Äôs the combination of elements that make up a song. A song is never just one audio feature.\nConclusion\nMy final model which utilized Lasso feature selection on all 85 features (transformed song elements and original features) wasn‚Äôt as predictive as I had hoped, explaining only 28% (R-squared) of the amount of variation in song popularity. However, after analyzing my coefficients, there were a few takeaways to be noted. The following features had the most positive and negative impact on popularity.\nMost positive:\n\nNumber of years since release\nArtist follower count\nDanceability\n\nMost negative:\n\nIndie Genre\nAcousticness & Speechiness\n\nAfter testing out a Decision Tree and Random Forest regressor, I can also conclude that the data is better explained by a non-linear regression model.\nContents of this project repository\n\nData Folder: all csvs stored here and pkled data post cleaning\nNotebooks Folder: all work in progress notebooks - cleaning, EDA, Modeling\nFinal.ipynb - final project notebook\nPresentation - copy of final presentation deck\n\n""], 'url_profile': 'https://github.com/AlisonSalerno', 'info_list': ['Jupyter Notebook', 'Updated Jun 27, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'Nashville, TN', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['regression_base_code\nBase code to be used as template for various regression techniques (OLS, Ridge/Lasso, etc.)\n'], 'url_profile': 'https://github.com/prudolph1', 'info_list': ['Jupyter Notebook', 'Updated Jun 27, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Logistic Regression - Employee Churn\nBuilding a Logistic Regression Model to predict whether company employees were likely to churn or not, including client presentation.\nThe Model includes the use of:\n\nStandardScaler\nGridSearchCV\nPipeLine\n\nand the following libraries:\n\nSKLearn (Logistic Regression)\nSeaborn (Visualization)\n\nMy Model was able to correctly identify churning employees 75% of the time.\n'], 'url_profile': 'https://github.com/SteveGrimm', 'info_list': ['Jupyter Notebook', 'Updated Jun 27, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020', 'R', 'Updated May 4, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['Polynomial-Regression-in-R\n'], 'url_profile': 'https://github.com/rabhadiaavinash', 'info_list': ['Jupyter Notebook', 'Updated Jun 27, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 27, 2020', 'Jupyter Notebook', 'Updated Aug 4, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Sep 4, 2020', 'R', 'Updated May 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Naive approach to predicting fire locations\nMainly as an exercise to analyze the predictive ability of the GFS to determine fire locations\nData\n\nMODIS fire locations - https://firms.modaps.eosdis.nasa.gov/data/active_fire/c6/shapes/zips/MODIS_C6_Global_24h.zip\nGFS Model data - all 2m, 10m, whole atmosphere data - https://nomads.ncep.noaa.gov/cgi-bin/filter_gfs_0p25_1hr.pl?file=gfs.t12z.pgrb2.0p25.anl&lev_10_m_above_ground=on&lev_1-2_m_below_ground=on&lev_2_m_above_ground=on&lev_entire_atmosphere_%5C%28considered_as_a_single_layer%5C%29=on&all_var=on&leftlon=0&rightlon=360&toplat=90&bottomlat=-90&dir=%2Fgfs.20200506%2F12\nMODIS EVI - https://e4ftl01.cr.usgs.gov/MOLT/MOD13Q1.006/2020.04.06/\n\nTechniques\nGoogle Earth Engine API\nhttps://www.earthdatascience.org/tutorials/intro-google-earth-engine-python-api/\nConda stuff\nconda create --name gdal_test_working -c conda-forge python=3.8 gdal\n'], 'url_profile': 'https://github.com/johncel', 'info_list': ['Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Go', 'Updated May 10, 2020', 'C#', 'Updated Oct 31, 2020', '1', 'Python', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': ' Australia', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShashankaRangi', 'info_list': ['Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Go', 'Updated May 10, 2020', 'C#', 'Updated Oct 31, 2020', '1', 'Python', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Logistic-Regression-from-Scratch\n'], 'url_profile': 'https://github.com/manish0718', 'info_list': ['Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Go', 'Updated May 10, 2020', 'C#', 'Updated Oct 31, 2020', '1', 'Python', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '771 contributions\n        in the last year', 'description': [""Correction :\n\ninstaller golang & feh: apt install golang feh\nexport le path\n\nBonus :\n\n\ngraphique :\n\n\nnuage de point\n\n\ncourbe\n\n\naffichage de l'erreur\n\n\ncouleurs\n\n\nrapidite de calcul\n\n\n""], 'url_profile': 'https://github.com/agasparo', 'info_list': ['Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Go', 'Updated May 10, 2020', 'C#', 'Updated Oct 31, 2020', '1', 'Python', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Abyssinia777', 'info_list': ['Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Go', 'Updated May 10, 2020', 'C#', 'Updated Oct 31, 2020', '1', 'Python', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'ALLAPUZHA', 'stats_list': [], 'contributions': '342 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/manisha-jaiswal', 'info_list': ['Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Go', 'Updated May 10, 2020', 'C#', 'Updated Oct 31, 2020', '1', 'Python', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['Linear_regression_pyspark_practice\nIntroducing pyspark and implementing linear regression on the dataset from MLLIB.\n'], 'url_profile': 'https://github.com/vkbandari', 'info_list': ['Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Go', 'Updated May 10, 2020', 'C#', 'Updated Oct 31, 2020', '1', 'Python', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['linear-regression-demo\n'], 'url_profile': 'https://github.com/Dan5762', 'info_list': ['Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Go', 'Updated May 10, 2020', 'C#', 'Updated Oct 31, 2020', '1', 'Python', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'New Delhi,India', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['MultiLinear Regression\n'], 'url_profile': 'https://github.com/ashutosh8110', 'info_list': ['Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Go', 'Updated May 10, 2020', 'C#', 'Updated Oct 31, 2020', '1', 'Python', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '361 contributions\n        in the last year', 'description': ['Brain_age_regression\nIn the first part, we implement a U-net to do the brain segementation. And then use the volumn of each kind of tissue to do the regression. In part2, we apply the PCA to decrease the amount of feature of the grey brain MRI picture and do the regression. In part3, we use the end to end model  and directly predict the age by using the neural network.\nPrerequisites\nBefore running the notebook, you need to install the following package. numpy, torch, pandas, tqdm, sklearn, SimpleITK, ipywidgets, IPython and matplotlib.\n'], 'url_profile': 'https://github.com/chanyikchong', 'info_list': ['Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'Go', 'Updated May 10, 2020', 'C#', 'Updated Oct 31, 2020', '1', 'Python', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['assumptions_of_linear_regression\nThis is a full tutorial on how to check for all the assumptions of a linear regression in R.\n'], 'url_profile': 'https://github.com/jislaaik', 'info_list': ['Updated May 4, 2020', 'HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['multiple-linear-regression\ndata is about start_companies\n'], 'url_profile': 'https://github.com/mp20mp', 'info_list': ['Updated May 4, 2020', 'HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '458 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/xiranwang7', 'info_list': ['Updated May 4, 2020', 'HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TejasSutar01', 'info_list': ['Updated May 4, 2020', 'HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TejasSutar01', 'info_list': ['Updated May 4, 2020', 'HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NaomiMwende', 'info_list': ['Updated May 4, 2020', 'HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['Linear_regression_pyspark_practice\nIntroducing pyspark and implementing linear regression on the dataset from MLLIB.\n'], 'url_profile': 'https://github.com/vkbandari', 'info_list': ['Updated May 4, 2020', 'HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['linear-regression-demo\n'], 'url_profile': 'https://github.com/Dan5762', 'info_list': ['Updated May 4, 2020', 'HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'New Delhi,India', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['MultiLinear Regression\n'], 'url_profile': 'https://github.com/ashutosh8110', 'info_list': ['Updated May 4, 2020', 'HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '361 contributions\n        in the last year', 'description': ['Brain_age_regression\nIn the first part, we implement a U-net to do the brain segementation. And then use the volumn of each kind of tissue to do the regression. In part2, we apply the PCA to decrease the amount of feature of the grey brain MRI picture and do the regression. In part3, we use the end to end model  and directly predict the age by using the neural network.\nPrerequisites\nBefore running the notebook, you need to install the following package. numpy, torch, pandas, tqdm, sklearn, SimpleITK, ipywidgets, IPython and matplotlib.\n'], 'url_profile': 'https://github.com/chanyikchong', 'info_list': ['Updated May 4, 2020', 'HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['multiple-linear-regression\ndata is about start_companies\n'], 'url_profile': 'https://github.com/mp20mp', 'info_list': ['HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'HTML', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '458 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/xiranwang7', 'info_list': ['HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'HTML', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TejasSutar01', 'info_list': ['HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'HTML', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TejasSutar01', 'info_list': ['HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'HTML', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NaomiMwende', 'info_list': ['HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'HTML', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/leskivl', 'info_list': ['HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'HTML', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'BSD, Greater Jakarta, Indonesia', 'stats_list': [], 'contributions': '148 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mnrclab', 'info_list': ['HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'HTML', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Orange County, California', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['WineQuality_LinearRegression\nWINE_preprocessing.ipynb is a script to clean up, visualize, and train simple linear regression onto the wine dataset.\nWINE_MultipleLinearRegression.ipynb trains a multiple linear regression model to predict the target feature (Quality). I account for multicollinearity using VIF scores and use 5-Fold Cross Validation to validate the Mean Squared Error (MSE) of the model.\n'], 'url_profile': 'https://github.com/goldiemalamud', 'info_list': ['HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'HTML', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Home Town: Vietnam', 'stats_list': [], 'contributions': '393 contributions\n        in the last year', 'description': ['Spark SQL: Logistic Regression\n""Repository Description:\n\nThis repository stores the work as part of the SQL Basics for Data Science specialization by University California, Davis. Course URL: https://www.coursera.org/specializations/learn-sql-basics-data-science. Code in this repository is written by myself, Kristen Phan.\n\n\nCaveat:\n\nIf you\'re currently taking the same course, please refrain yourself from checking out this solution as it will be against Coursera\'s Honor Code and won‚Äôt do you any good. Plus, once you\'re worked your heart out and was able to solve a particularly difficult problem, a sense of confidence you gained from such experience is priceless :)\n\n\nAssignment Description:\n\nUse Databricks, a web-based platform for working with Apache Spark, to process fire calls dataset from the San Francisco\'s fire department and build a sklearn logistic regression model to predict the type of calls for incidents in a SQL table.\nNote to notebook on Data Bricks:\nhttps://community.cloud.databricks.com/?o=1571733193913160#notebook/894243193530461/command/894243193530462\n'], 'url_profile': 'https://github.com/kristenphan', 'info_list': ['HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'HTML', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Logistic regression Project\nWelcome! This is new project and new tasks at hand.\nThis is Logistic Regression Project\nWe are going to do task to strengthen whatever we learned in class of Logistic Regression.\nWhat we have learned so far:\n\nWhy Logistic Regression and What is Logistic Regression?\nHypertuning Parameter\nGradient Descent\nEvaluation metrics of Logistic Regression\n\nWhat we are going to do?\n\nYou have above data-set. You need to clean this data first.\nAfter cleaning, you will create a logistic regression model and fit that model on the data.\n\nWhat your will learn by doing this assignment ?\n\nYou will learn and perform the preprocessing steps in efficient way and moreover it will give you an idea as to which steps to inculcate while doing preprocessing.\nYou will learn the importance of scaling and what effect it has on the model.\nYou will learn to build logistic regression model.\n\nDataset\nTo perform Logistic Regression task we will use Loan Prediction dataset.\nThis dataset contains following features:\n\nApplicantIncome\nCoapplicantIncome\nLoan Amount\nLoan Amount term\nCredit History\nProperty_Area\nSelf_Employed\nEducation\nDependents\nMarried\nGender\nLoan_ID\n\nTarget Variable:\n\nLoan Status\n\nDetails information is mentioned in each task.\nBy completing this task you will be earning 250 points in you training. So, GOOD LUCK!\n'], 'url_profile': 'https://github.com/mhs10031990', 'info_list': ['HTML', 'Updated May 7, 2020', 'HTML', 'Updated May 22, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Python', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'HTML', 'Updated May 8, 2020', 'Python', 'Updated May 8, 2020']}"
"{'location': 'Roskilde, Denmark', 'stats_list': [], 'contributions': '427 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/casperbh96', 'info_list': ['Python', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'HTML', 'Updated May 9, 2020', 'Updated Aug 25, 2020', 'C++', 'MIT license', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 30, 2020']}","{'location': 'Cork', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Machine-learning-Regression\nIn this repository I am committing my machine learning Regression projects\nHere I learnt  how to use scikit-learn to implement Simple,Multiple,Ploynomial regression. We download a dataset that is related to fuel consumption and Carbon dioxide emission of cars. Then, we split our data into training and test sets, create a model using training set, Evaluate your model using test set, and finally use model to predict unknown value.\n'], 'url_profile': 'https://github.com/Suhani-SK-Singh', 'info_list': ['Python', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'HTML', 'Updated May 9, 2020', 'Updated Aug 25, 2020', 'C++', 'MIT license', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 30, 2020']}","{'location': 'JAIPUR', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': [""#Logistic Regression on Titanic Dataset\nIn this project I'm attempting to do data analysis on the Titanic Dataset. In the first step I'm doing a very quick data exploration and preprocessing on a visual level, plotting some simple plots to understand the data better. Then I've done some data cleaning and built a Classifier that can predict whether a passenger survived or not.\n""], 'url_profile': 'https://github.com/kushwahvikram15', 'info_list': ['Python', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'HTML', 'Updated May 9, 2020', 'Updated Aug 25, 2020', 'C++', 'MIT license', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '211 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Raman-rd', 'info_list': ['Python', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'HTML', 'Updated May 9, 2020', 'Updated Aug 25, 2020', 'C++', 'MIT license', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/elizacotocea', 'info_list': ['Python', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'HTML', 'Updated May 9, 2020', 'Updated Aug 25, 2020', 'C++', 'MIT license', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': [""mod2linear_regression\nSynopsis\nThe goal of my project was to predict which attributes of a diamond affect the price of the diamond the most and predict the trajectory of the prices of the diamonds. I used data from James Allen online diamond store to gather attributes of their diamonds and also pull their price. Luckily i was able to get a csv file, and import that into a data frame. I needed to clean some of the data to get the variables into useful information. Since i am predicting a diamonds price, and its attributes are based on alot of ordinal values, i needed to change alot of the variables to take into account that it has ordinal value when running linear models. We collected carat, color, clarity, cut, authentication(lab-approved), origin of diamond(lab-grown or naturally farmed), price, shape.\nWhat I Found out\nSo according to the data, the highest contributors to the price of the diamond, was the 4Cs or better known as (carat,color,clarity, and cut) but it seems like the origin of the diamonds whether it was lab-grown or naturally farmed came in second highest contributor to the price. If you take away the 4Cs and origin, the shape of the diamonds really starts to affect the pricing.\nIf i had more time\nI would like to work with figuring out a way to incorporate the diamond colored csv. Originally i wanted to do all types of diamonds, whether it was clearless, pink, blue, canary, and so forth but unfortunatly i didn't have time, because the colored diamonds had more attributes compared to the clearless diamond set.\nMy Files\nmodel_processing.ipynb\nmodel_processing has all my calculations of where I did all my linear regression models and\nmy linear model\nmy lasso model\nmy ridge model\nmy f-test model\nOLS models that so the coef of price/features\nworking_with_data.ipynb\nCleaning/reforming data\ncreating new variables\ncreating plots and charts to analyze data\nChanging existing variables accordingly\nobservation of the data\nheatmaps\ndiamonds_no_color.csv\nwebscrapped csv file of diamonds(clear colored)\ndiamonds_no_color.csv\nrevised csv file of diamonds_no_color to run linear models in model_processing.ipynb\ndiamonds_color.csv\nextra dataset of diamonds with different color\ncould be a future project\nunused csv in model_processing or working_with_data\n""], 'url_profile': 'https://github.com/Kanqaroo', 'info_list': ['Python', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'HTML', 'Updated May 9, 2020', 'Updated Aug 25, 2020', 'C++', 'MIT license', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 30, 2020']}","{'location': '208A, Stanley Tillekerathne Mawatha Nugegoda, Sri Lanka.', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HariharanMiracle', 'info_list': ['Python', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'HTML', 'Updated May 9, 2020', 'Updated Aug 25, 2020', 'C++', 'MIT license', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Applied-Regression-and-Design\nhttp://catalog.illinois.edu/courses-of-instruction/stat/\n'], 'url_profile': 'https://github.com/sroshan2', 'info_list': ['Python', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'HTML', 'Updated May 9, 2020', 'Updated Aug 25, 2020', 'C++', 'MIT license', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 30, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '250 contributions\n        in the last year', 'description': ['MultiLinearRegression\nC++ based Multi Variable Linear Regression with Visualizations and in-depth tweeks;\nPrerequisite\n\nTo work Effectively with this Class use Visual Studio or any Suitable IDE.\nFor Windows: use vcpkg with Visual Studio\nInstall Armadillo (for Linear Algebra) and plplot for Visualization\n\nvcpkg command\n\nvcpkg install armadillo\n\n\nvcpkg install plplot\n\n\nvcpkg integrate install  //for user-wide integration\n\n\nImport the Files into Your Project Project and you are good to go\n\nHow to Use this Class\nThe Class Name is MultiLinearRegression add the Header file MultiLinearRegression.h in your file\n1. Constructors\nfor loading data into the object through file path\n\nMultiLinearRegression model(std::string);\n\nfor loading data into the object through a arma Matrix\n\nMultiLinearRegression model(arma::Mat);\n\n2. load function\nfor loading data into object\n\nmodel.load(std::string);\n\n\nmodel.load(arma::Mat);\n\n3. Spliting Data into Training and Test Data\nBy default 70% of the data is used as training data and rest as Test data\n\nmodel.split();\n\n\nmodel.split(double)\n\n4. Normalize the Data (recommended)\nBy delault the X data is normalized from 0 to 1\n\nmodel.normalize();\n\n\nmodel.normalize(double, double);\n\n5. Fitting the Model into the Trainig Data\nTrain the data from trainig dataset (default), with learing ration (0.1 default)\n\nmodel.fit();\n\n\nmodel(int,double);\n\n6. Error Checking in Hypothesis\nChecks the error of Predicted Y to actual Y from training Data (default)\n\nmodel.fitError();\n\n\nmodel.fitError(int);\n\n7. Predict the Test data set\nReturns the H for Test Data sets(default)\n\nmodel.predict();\n\n\nmodel.predict(int);\n\nReturns the H for Real World Test Data, with normaliztion (0-1 default)\n\nmodel.predict(std::string, double, double);\n\n\nmodel.predict(arma::Mat, double, double);\n\n8. Visualizing the Model for Each Feature\nplots a graph between column (0-indexed) vs Y, with X label, Y label, Graph Title, Type of Graph\n\nmodel.plotModel(int, const char*, const char*, const char*, int);\n\n9. Visualizing the Model for Real World Test cases\n*Plots a grpah between datapoint (strating from 1) vs Y, input data, Y label, Graph Title, lower and upper limit of normalization (0-1 default)\n\nmodel.plot(std::string, const char *, const char *, double, double);\n\n\nmodel.plot(arama::Mat, const char *, const char *, double, double);\n\n'], 'url_profile': 'https://github.com/Shikhar03Stark', 'info_list': ['Python', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'HTML', 'Updated May 9, 2020', 'Updated Aug 25, 2020', 'C++', 'MIT license', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 30, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '829 contributions\n        in the last year', 'description': ['linear-regression-ML\nBoston house prediction\nRequired Notebook\n\nJupiter notepad or Google colab\n\nRequired libries\n\nscikit\nnumpy\npandas\nmatplotlib\nIn this you will predict price of boston house\n\n'], 'url_profile': 'https://github.com/Brillianttyagi', 'info_list': ['Python', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'HTML', 'Updated May 9, 2020', 'Updated Aug 25, 2020', 'C++', 'MIT license', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 30, 2020']}"
"{'location': 'Bras√≠lia', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Tarefa 1: Aprendizado Supervisionado\nAutor: Matheus Jeric√≥ Palhares \nLinkedIn: https://linkedin.com/in/matheusjerico \nGithub: https://github.com/matheusjerico\n1) Tarefa: implementar a fun√ß√£o ‚Äúfit_linear_regression(pontos)‚Äù. Retorne os valores de m e de a.\nVari√°veis:\n\npontos: conjunto de pontos 2D (casos x mortes) que ser√£o clusterizados\nDesafio: pesquise sobre como realizar regress√£o linear para casos com 3 dimens√µes (2 features e 1 target) e implemente a fun√ß√£o fit_linear_regression_3D(pontos).\n\nO seu relat√≥rio ser√° o notebook exportado para um arquivo HTML e deve conter:\n\nUm scatter plot mostrando os pontos e a linha estimada na regress√£o.\nCalcule o R2 e discorra sobre o qu√£o boa foi a sua aproxima√ß√£o, sugerindo poss√≠veis formas de melhorar o seu modelo.\nCompare os seus resultados com os obtidos atrav√©s do sklearn.linear_model.LinearRegression. Eles deveriam ser iguais.\n\n1. Scatter Plot resultante da implementa√ß√£o do Algor√≠timo de Regress√£o Linear\n\n2. R2 Score\n0.9444904768158006\n\nAn√°lise:\n\nO modelo obteve um bom valor de R¬≤.\nR¬≤ = 0.94 significa que o modelo linear explica 94% da vari√¢ncia da vari√°vel dependente a partir do regressores (vari√°veis independentes) inclu√≠das naquele no modelo apresentado linear.\n\nPoss√≠veis melhoras:\n\nAumentar o volume de dados.\nAumentar a quantidade de vari√°veis independentes.\n\n3. Comparando com o modelo do Sklearn\n3.1. RMSE do Algor√≠tmo Scratch\n198.486116140131\n\n3.2. RMSE do Algor√≠tmo Scikit-Learn\n198.48611614013106\n\nAn√°lise:\n\nO resultado obtido atrav√©s da nossa classe de regress√£o linear √© igual ao resultado da classe de regress√£o linear do scikit-learn.\n\n'], 'url_profile': 'https://github.com/matheusjerico', 'info_list': ['HTML', 'Updated May 23, 2020', 'HTML', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jul 1, 2020', 'R', 'Updated Jun 29, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Chandigarh', 'stats_list': [], 'contributions': '320 contributions\n        in the last year', 'description': [""Linear-Regression-Project\nAn Ecommerce company based in New York City that sells clothing online but they also have in-store style and clothing advice sessions. Customers come in to the store, have sessions/meetings with a personal stylist, then they can go home and order either on a mobile app or website for the clothes they want.\nThe company is trying to decide whether to focus their efforts on their mobile app experience or their website. They've hired you on contract to help them figure it out! Let's get started!\nJust follow the steps below to analyze the customer data (it's fake, don't worry I didn't give you real credit card numbers or emails).\n""], 'url_profile': 'https://github.com/connectaditya', 'info_list': ['HTML', 'Updated May 23, 2020', 'HTML', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jul 1, 2020', 'R', 'Updated Jun 29, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': [""Advanced Linear Regression Project\nWelcome to the Advanced linear regression Project!\nIn this project, you will demonstrate what you have learned in this course by conducting an experiment dealing with House Prices.\nWe have seen in the lectures what are the shortcomings of linear regression and how Regularization helps in controlling Overfit and give the better fir to data.\nIn this exercise we will stepwise write to functions to implement our own Regularization algorithm.\nWhat we have learnt so far..\n\nShortcomings of linear regression\nPolynomial Basis Function\nRegularization(L1/L2)\nBias-variance trade-off\n\nDataset\nNow, we will try to implement the Rigde and lasso on the house_prices_multivariate dataset. We have been working with this dataset for some time now. We applied linear regression and Polynomial linear regression to predict the SalePrice.\nFeatures:\n\nTotal BsmtSF : Total square feet of basement area\nLot Area :Lot size in square feet\nStreet : Type of road access to property\nOverallQual :Rates the overall material and finish of the house\nGrLivArea :Above grade (ground) living area square feet\nGarageCars :Size of garage in car capacity\n\nWhat you will learn solving this ?\n\nLearn to Write Sklearn Algorithm for Ridge and Lasso using Cross Validation\nWhen to apply Regularization technique, how it works and prevent under or over fitting\nHow it helps to balance the bais and variance trade-off.\n\n\nBenchmark the performances of linear regression against that of Regularization.\nLearn to use cross validation.\n\nSeems like you are all fired up to put a test to your knowledge.\nLet's get started!\n""], 'url_profile': 'https://github.com/mhs10031990', 'info_list': ['HTML', 'Updated May 23, 2020', 'HTML', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jul 1, 2020', 'R', 'Updated Jun 29, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '274 contributions\n        in the last year', 'description': [""Objective\nThis project is based on a dataset on e-scooter rental demand over 2 years (2011 and 2012), with datetime and weather data. The demand is split between guest users and registered users, while the project's objective is to predict the total (guest + registered) demand.\nKey Requirements\nThis project runs on Python 3.6.8. For libraries and dependencies used, please refer to the requirements.txt in the same directory.\nDirectory for current package\n\nProject Folder\n\nmlp\n\nconfig_create.py -  Generates a models_config.yml file for models' parameters for grid search\npreprocessing.py -  Preprocess input dataset, and generate train and test sets\nmodeling.py -  Script for containing model training and scoring functions \nevaluation.py -  Final script to execute model training and evaluations \nData\n\nmodels_config.yml - config file to be generated\nX_test.pkl - pickled predictors test data to be generated\nX_train.pkl - pickled predictors train data to be generated\ny_test.pkl - pickled target test to be generated\ny_train.pkl - pickled train train data to be generated\n\noutput\n\ngue_model_modelname.pkl - pickled trained best model for guest to be generated\nreg_model_modelname.pkl - pickled trained best model for registered to be generated\nmetrics_reg_gue.csv - csv of metrics for all models on registered and guest to be generated\nmetrics_total.csv - csv of metrics from combined best models for total users to be generated\ntotal_predict.pkl - pickled predicted total users to be generated\n\n\nEDA.ipynb\nrun.sh\nrequirements.txt\nREADME.md\n\n\nModel Design\nThe primary objective is to predict that total users for specific timings. From Exploratory Data Analysis conducted, there are 2 main user groups - registered and guest, each with their own distinct usage behaviours based on time and weather. It is observed that is time based trend - and overall incremental in usage volume from 2011 to 2012 end.\nWith these in mind, it is decided that each user group will be predicted individually, and then aggregated to give the primary target - total users.\nAdditionally, stationarity is introduced to remove the time trend effects, by using the first difference of 24 hour gaps.\nTherefore, the base predictions will be on the first difference values, which will be added to the corresponding actual day before (24hours) user volumes, to retrieve the predicted registered/guest user volumes individually.\n\nThe key predictors (at abstraction) used are:\n\nDay of the week\nHour of day\nMonth\nWeather condition\nFeels-like-temperature\nTemperature\nWindspeed\nPSI\n\nThe cross-validated models in selection are:\n\nLasso\nRandom Forest\nGradientBoosting\nXGBoost\n\n\nCross-validation methodology: Time series split\n\nEvaluation metric : Root mean squared error, r2 on (On registered users and guest users post conversion from predicted first difference, and finally on total users)\nNote: Temperature and feels-like-temperature were observed to be highly collinear, however both were kept as predictors, with the decision to allow freedom for the models to decide the importance of features.\nPipeline Design\nThe pipeline is built to run in 3 main steps:\n\n\nStep 1 - Prepocessing.py\nIt draws the dataset from the target URL, cleans the data, feature engineer and splits the dataset into train and test sets.\n\nData cleaning include correcting erroneous weather data; forward filling missing observations for some dates.\nFeatures engineered include deriving the day of the week and month from the original date feature; applying first difference to 24 observations(24 hours/1 day) earlier; dummy coding categorical features; apply min/max scaling on numeric features\nTrain test split: with data sorted in datetime chronology, the train set is the first 80%, and test set is the last 20%.\nNote: command line configurable with -l / --load (for dataset address), and -ts / --testsize (for test size from dataset, float input)\n\n\n\nStep 2 - config_create.py\nGenerates a .yml file of configurations of parameters grids to be used in grid search. To be used to configure the parameters for grid search.\n\n\nStep 3 - evaluation.py (uses functions from modeling.py) \n\nTrains models and evaluates best models for each target(guest_users, registered_users)\nGenerates a pickled best model each for guest and registered users.\nGenerates predicted value of total_users on the test set.\nGenerates metrics csv files\nNote: command line configurable with -a / --all (run full evaluation process on all models and user group), and -r / --registered (individual model to evaluate for registered, input for -a has to be n for this to work), and -g / --guest (individual model to evaluate for guest, input -a has to be n for this to work)\n\n\n\n\nConclusion\n\nResults: \n\nThe best models for registered and guest users have both been evaluated to be the Random Forest Regressor.\nFor registered users, the resulting r2 and rmse are 0.8149 and approx. 1054 respectively.\nFor guest users, the resulting r2 and rmse are 0.6516 and approx 231 respectively.\nFor the aggregation on total users, the resulting r2 and rmse are 0.8106 and approx 1157 respectively.\n\nLimitations: \n\nUltimately these best models are selected, with random states set to 42. And more effective combination of models can possibly be found with other random states, by virtue of different random starting deciding predictors of trees for random forest, or different random starting weak learners of boosting methods.\nOn the deployment and business side, this model can at best, only predict a day in advance of the next day's demand volume to the hour, since it just predicts the difference and adds on the current day's actual volume, in order to predict each user group volume. Additionally, predicting into the future will also rely on weather forecast, which is not fully reliable, further adding to inaccuracies in the model.\nAppendix\n\nSlides for presentation\nGoogle Colab Notebook for EDA\n\n""], 'url_profile': 'https://github.com/tojhe', 'info_list': ['HTML', 'Updated May 23, 2020', 'HTML', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jul 1, 2020', 'R', 'Updated Jun 29, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['LinearRegression-ApacheSpark\nCreating a simple program with the Linear Regression algorithm to predict how many minutes a flight is delayed. At this simple program, it uses the Apache Sprak framework as the main tool for programming.\n'], 'url_profile': 'https://github.com/harry14jazz', 'info_list': ['HTML', 'Updated May 23, 2020', 'HTML', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jul 1, 2020', 'R', 'Updated Jun 29, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Regression_based_analysis\nThis project analyzes the data obtained from ~2000 customers of Apprentice Chef, a gourmet meal delivery company, by conducting exploratory data analysis, feature engineering, variable selection for modeling and finally building a regression based model to predict revenue of future customers.\n'], 'url_profile': 'https://github.com/BrianJoelDsouza', 'info_list': ['HTML', 'Updated May 23, 2020', 'HTML', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jul 1, 2020', 'R', 'Updated Jun 29, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Kaiserslautern,Germany', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Regression-and-Time-Series\nLeast Square Estimates\nGraphic plots\nConfidence Bands\nNull Hypothesis\nOne Factor Analysis\nTwo Factor Analysis\nGradient Descent\n'], 'url_profile': 'https://github.com/ajaychawda58', 'info_list': ['HTML', 'Updated May 23, 2020', 'HTML', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jul 1, 2020', 'R', 'Updated Jun 29, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Seattle, WA', 'stats_list': [], 'contributions': '206 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jgockley62', 'info_list': ['HTML', 'Updated May 23, 2020', 'HTML', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jul 1, 2020', 'R', 'Updated Jun 29, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Brazil', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['r-symbolic-regression\nThis script plots a symbolic regression model using R and the ggplot2 library.\nFind the full tutorial here, including how to generate the model: https://turingbotsoftware.com/posts/symbolic-regression-r.html\nThis is the final result:\n\n'], 'url_profile': 'https://github.com/turingbotsoftware', 'info_list': ['HTML', 'Updated May 23, 2020', 'HTML', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jul 1, 2020', 'R', 'Updated Jun 29, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nI will explain the working of Simple Linear Regression.\nI will be using Advertising Data by USC Marshall School of Business.\n'], 'url_profile': 'https://github.com/sangeet2893', 'info_list': ['HTML', 'Updated May 23, 2020', 'HTML', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'R', 'Updated Jul 1, 2020', 'R', 'Updated Jun 29, 2020', '1', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Projet-ML-regularized-regression\nProjet M1 Machine Learning\n\n'], 'url_profile': 'https://github.com/nouredinesaleh', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', 'R', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'R', 'Updated May 10, 2020']}","{'location': ' Australia', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShashankaRangi', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', 'R', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'R', 'Updated May 10, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Regression-Case-Study\n'], 'url_profile': 'https://github.com/IAlam0819', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', 'R', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'R', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '287 contributions\n        in the last year', 'description': ['Linear Regression models\nType-SIMPLE LINEAR REGRESSION\n1-Predict Salary of a person\nThis applies simple linear regression algorithm to predicts salary of an employee based on his experience.\nLanguage - PYTHON (done using SKLearn library of python.)\n2-Predict Profit for food truck\nproblem statement-""Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new\noutlet. The chain already has trucks in various cities and you have data for profits and populations from the cities.\nYou would like to use this data to help you select which city to expand to next""\nLanguage-OCTAVE (the algorithm used is GRADIENT DESCENT . all the function to train the algorithm and compute the cost functions\nare written by self)\nType-MULTIPLE LINEAR REGRESSION\n3-Predict Profit for Startups\nproblem statement-"" Given the amount of money spent by a startup on its  Research and development,Aministration,Marketing campaign\nand the state of location of the startup , we have to predict the profit of startup, thereby making a decision about investing in\nstartup easier""\nLanguage - PYTHON (done using SKLearn library of python.).Categorical variable, one hot encoding and dummy variables concepts are also used.\n'], 'url_profile': 'https://github.com/richakbee', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', 'R', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'R', 'Updated May 10, 2020']}","{'location': 'Bangladesh', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['Linear-Regression-With-Python\n'], 'url_profile': 'https://github.com/pieeee', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', 'R', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'R', 'Updated May 10, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Regression Analysis for Apprentice Chef\nAuthor: Arseniy Barmin\nCompany introduction\nApprentice Chef, Inc. is an innovative company with a unique spin on cooking at home. Developed for the busy professional that has little to no skills in the kitchen, they offer a wide selection of daily-prepared gourmet meals delivered directly to your door. Each meal set takes at most 30 minutes to finish cooking at home and also comes with Apprentice Chef\'s award-winning disposable cookware (i.e. pots, pans, baking trays, and utensils), allowing for fast and easy cleanup. Ordering meals is very easy given their user-friendly online platform and mobile app.\nCase context\nAfter 3 years serving customers across the San Francisco Bay Area, the executives at Apprentice Chef have come to realize that over 90% of their revenue comes from customers that have been ordering meal sets for 12 months or less. Given this information, they would like to better understand how much revenue to expect from each customer within their first year of orders.\nThus, they have hired me on a full-time contract to:\n\nAnalyze their data\nDevelop my top insights\nBuild a machine learning model to predict revenue over the first year of each customer‚Äôs life cycle.\n\nIn order to appropriately prepare the data for this analysis, the data science team at Apprentice Chef has queried, sampled, and verified a dataset of approximately 2,000 customers. Each customer met at least one of the following criteria:\n\nAt least one purchase per month for a total of 11 of their first 12 months\nAt least one purchase per quarter and at least 15 purchases throughout their first year\n\nThe data science team assures me that their dataset engineering techniques are statistically sound and represent the true picture of Apprentice Chef‚Äôs customers. To help me in my task, Apprentice Chef, Inc. has provided me with the dataset and additional information about the company.\nBusiness question\nBuild a machine learning model to predict how much revenue to expect over the first year of each customer\'s life cycle.\nHow to download and review my work\nPlease follow the instructions below to download and review my work. The steps below assume that you either have Jupyter Notebook installed on your computer, or are able to open files with the "".ipynb"" extention.\n\nDownload ""Apprentice Chef - Regression Analysis.ipynb"" from this repository.\nOpen ""Apprentice Chef - Regression Analysis.ipynb"", but don\'t run it just yet.\nDownload ""Apprentice_Chef_Dataset.xlsx"" and save it in your working directory.\nAt this time, ""Apprentice Chef - Regression Analysis.ipynb"" will run without errors.\nDownload ""Apprentice_Chef_Data_Dictionary.xlsx"" to learn more about metadata of each feature found in the dataset.\n\nIn case you have any questions\nPlease feel free to reach out to me using my e-mail address below if you have any questions regarding this work.\nE-mail: arseniy.barmin@gmail.com\n'], 'url_profile': 'https://github.com/Arsik36', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', 'R', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'R', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gaoqikai', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', 'R', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'R', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akshayp2020', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', 'R', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'R', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Here Mohsen will share his python code for data generation and training\n'], 'url_profile': 'https://github.com/mohsensadr', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', 'R', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'R', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Analiza i predviƒëanje vremenskih serija primenom linearne regresije\nOvaj repozitorijum sadr≈æi materijale za 13. ƒças predavanja iz predmeta Inteligentni sistemi na temu analize i predviƒëanja vremenskih serija primenom linearne regresije (u programskom jeziku R).\nPredmet se izvodi na Fakultetu organizacionih nauka, Univerzitet u Beogradu.\n'], 'url_profile': 'https://github.com/inteligentni', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 6, 2020', 'R', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', 'R', 'Updated May 10, 2020']}"
"{'location': 'Macei√≥ - Al', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vierrra', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'HTML', 'Updated May 4, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', '1', 'HTML', 'Updated May 9, 2020', '2', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['MiniProject_Logistic_Regression\n'], 'url_profile': 'https://github.com/zosking', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'HTML', 'Updated May 4, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', '1', 'HTML', 'Updated May 9, 2020', '2', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['MiniProject_Linear_Regression\n'], 'url_profile': 'https://github.com/zosking', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'HTML', 'Updated May 4, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', '1', 'HTML', 'Updated May 9, 2020', '2', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'Escondido, CA', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Apolinar97', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'HTML', 'Updated May 4, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', '1', 'HTML', 'Updated May 9, 2020', '2', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'Buenos Aires, Argentina', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['R5-regression-exercise\nLos procesos de analisis y exploracion de datos, asi como la toma de decisiones sobre las pruebas y modelos utilizados, son el resultado de un trabajo en equipo realizado junto a Juan Cruz Alric y Nicolas Buzzano.\nJuan Cruz Alric\nGitHub: https://github.com/Juanchoalric\nNicolas Buzzano\nGitHub: https://github.com/nicolasBuzzano\n'], 'url_profile': 'https://github.com/exequielmoneva', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'HTML', 'Updated May 4, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', '1', 'HTML', 'Updated May 9, 2020', '2', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['simple-linear-regression\nSimple linear regression is a supervised machine learning technique used to predict a dependent variable from a single independent variable.\nAfter understanding the concepts and the code you will have a clear idea about simple linear regression.\nformula used :\ny(DV)=b0+b1*x(IDV)\nwhere,\ny=dependent variable\nb0=constant\nb1=regression coefficient\nx=independent variable\n'], 'url_profile': 'https://github.com/prathm3sh', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'HTML', 'Updated May 4, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', '1', 'HTML', 'Updated May 9, 2020', '2', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'Syracuse, NY', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ezair', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'HTML', 'Updated May 4, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', '1', 'HTML', 'Updated May 9, 2020', '2', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'United Kingdom', 'stats_list': [], 'contributions': '376 contributions\n        in the last year', 'description': ['Covid-19-Testing-Regression\n\nAbout\nA keras regression model to predict the amount of covid-19 tests(Canada) to be done the day after the last date in series.\n'], 'url_profile': 'https://github.com/danlove99', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'HTML', 'Updated May 4, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', '1', 'HTML', 'Updated May 9, 2020', '2', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': '208A, Stanley Tillekerathne Mawatha Nugegoda, Sri Lanka.', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HariharanMiracle', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'HTML', 'Updated May 4, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', '1', 'HTML', 'Updated May 9, 2020', '2', 'Jupyter Notebook', 'Updated May 11, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': [""The-Last-Dance\nIn depth analysis of Michael Jordan's college and professional basketball statistics using predictive models, Seaborn, Pandas, Matplotlib, Plotly, Random Forest Regression.\nGroup Members: Araceli Buenrostro, Stephanie Hidalgo, Brianna John, Daniella DeRose\n""], 'url_profile': 'https://github.com/abuenrostro62', 'info_list': ['Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 6, 2020', 'HTML', 'Updated May 4, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Python', 'Updated May 10, 2020', '1', 'HTML', 'Updated May 9, 2020', '2', 'Jupyter Notebook', 'Updated May 11, 2020']}"
"{'location': 'Alexandria, Egypt', 'stats_list': [], 'contributions': '172 contributions\n        in the last year', 'description': ['Gradient-Descent\nImplementation of mini batch gradient descent Algorithm in finding the min of error function in regression\n'], 'url_profile': 'https://github.com/khadija267', 'info_list': ['Python', 'Updated May 6, 2020', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Apache-2.0 license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Sep 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Updated May 9, 2020', 'Updated May 5, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '224 contributions\n        in the last year', 'description': ['Coding Machine Learning Models from Scratch\n\nEffect of High Dimensionality on Inference in OLS\n\n\nCoefficient value\nStandard error\nSearch for best subset of variables\nEigenvalue of covariance matrix\n\n\nK-Means\n\n'], 'url_profile': 'https://github.com/sapiensvisionem', 'info_list': ['Python', 'Updated May 6, 2020', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Apache-2.0 license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Sep 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Updated May 9, 2020', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '168 contributions\n        in the last year', 'description': ['SalaryPredictionModel-LR-\nSalary prediction model based on years of experience of an employees using Linear Regression Technique\n'], 'url_profile': 'https://github.com/ChandrashekharM3018', 'info_list': ['Python', 'Updated May 6, 2020', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Apache-2.0 license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Sep 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Updated May 9, 2020', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Diabetes\nLogistic Regression algorithm used to predict diabetes for a specific dataset\n'], 'url_profile': 'https://github.com/ShivamHasurkar', 'info_list': ['Python', 'Updated May 6, 2020', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Apache-2.0 license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Sep 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Updated May 9, 2020', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Machine-Learning-for-House-Prediction\nAim:\nBuilding a Knn regression model to forecast the price of house using the house data\nSteps followed :\n1) Saved Price column in a separate vector\n2) Normalized all the columns (except for boolean columns)\n3) Built a function called knn.reg that implements a regression version of kNN that averages the prices of the k nearest neighbors.\n4) Forecasted the price of new home using regression kNN using k = 4\nPlease download the html file in order to see the results of the code \n'], 'url_profile': 'https://github.com/Apoorv27', 'info_list': ['Python', 'Updated May 6, 2020', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Apache-2.0 license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Sep 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Updated May 9, 2020', 'Updated May 5, 2020']}","{'location': 'Vancouver, BC', 'stats_list': [], 'contributions': '203 contributions\n        in the last year', 'description': [""Twitter_Disaster_Classification\nClassify Tweets using various NLP techniques from simple SVC and Logisitic Regression to BERT\nTake a look at my notebook here!\n\nKaggle Competition:\nReal or Not? NLP with Disaster Tweets\nPlaced 443 of 2343 - though from what I've read, the first 200 posts of 100% accuracy are bogus...\nIf anyone has some ideas for how to improve my process (and score!) please pm me! Would love to know how to better fine-tune BERT or combine with an LSTM classifier.\nI can be reached via LinkedIn.\n""], 'url_profile': 'https://github.com/ilanazim', 'info_list': ['Python', 'Updated May 6, 2020', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Apache-2.0 license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Sep 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Updated May 9, 2020', 'Updated May 5, 2020']}","{'location': 'Bialystok, Poland', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Energy_usage\nMachine learning regression model of energy usage at an residential home\nThe main task is finding a dependancy between 3 data types: average month temperature and month number vs. average daily electricity energy usage (given in kWh).\nData set come from the book ""Smoothing Methods in Statistics"" by Jeffrey S. Simonoff\n'], 'url_profile': 'https://github.com/KrzysztofSobota', 'info_list': ['Python', 'Updated May 6, 2020', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Apache-2.0 license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Sep 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Updated May 9, 2020', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '675 contributions\n        in the last year', 'description': [""Predict House Prices\n\n\n\n\n\nTable of contents\n\n\nAbout\n\n\nTechnologies Used\n\n\nResults of the Project\n\n\nInstallation\n\n\nAcknowledgements\n\n\nData Source\n\n\nLicense\n\n\nAbout\n\nIn this project Ames Housing dataset is used, which has 79 features and 1 target variable i.e. SalePrice(house prices) and did feature engineering to see which features are relevant in predicting the house price and created a machine learning model which will take relevant features(selected while feature engineering) as input and give prediction of what might be the price of that house as output.\n\n\nIn this project doing feature engineeringon numerical and categoricaldata and selecting the features that affects the price of the house.\n\n\nAfter scaling and splitting the data into training and testing dataset cross validation is performed and the parameter tuning is done. After analysing the learning curve and then training the model with training data. At last the model is tested on the test data and is evaluated on the basis of mean squared error and r2 score.\n\nTechnologies Used\n\n is used as Programming Language.\n\n\nNumpy is used for the mathematical and data manipulation.\n\n\nPandas is used to analysis and manipulation of data.\n\n\nMatplotlib and Seaborn are used for data visualisation which helped in the analysis of data.\n\n\nSciPy is used to perform statistical operations which is used to deal with outliers.\n\n\nSciki-learn is used for data preprocessing, creating machine learning model and evaluating it, thus creating a pipeline.\n\n\nPipenv is the virtual environment used for the project. Jupyter Notebook is used to for the entire data science and machine learning life cycle.\n\nResults of the Project\nLearning Curve\n\nMetrics Scores\n\nActual VS Prediction\n\nInstallation\nIt is highly recommended to use virtual enviroment for this project to avoid any issues related to dependencies.\nHere pipenv is used for this project.\nThere is a requirements.txt file in 'Predict-House-Prices'/requirements.txt which has all the dependencies for this project.\n\nFirst, start by closing the repository\n\ngit clone https://github.com/AkashSDas/Predict-House-Prices\n\n\nStart by installing pipenv if you don't have it\n\npip install pipenv\n\n\nOnce installed, access the venv folder inside the project folder\n\ncd  'Predict-House-Prices'/venv/\n\n\nCreate the virtual environment\n\npipenv install\n\nThe Pipfile of the project must be for creating replicating project's virtual enviroment.\nThis will install all the dependencies and create a Pipfile.lock (this should not be altered).\n\nEnable the virtual environment\n\npipenv shell\n\n\ndataset, jupyter notebook and model are in 'Predict-House-Prices'/venv/src folder.\n\ncd src/\n\n\nTo start/view the jupyter notebook\n\njupyter noterbook\n\nThis will open a webpage in the browser from there you can click on notebook.ipynb to view it.\nAcknowledgements\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.\nData Source\nThis dataset can found in Kaggle - Source\nLicense\nThis project is licensed under the MIT License - see the MIT LICENSE file for details.\n""], 'url_profile': 'https://github.com/AkashSDas', 'info_list': ['Python', 'Updated May 6, 2020', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Apache-2.0 license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Sep 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Updated May 9, 2020', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['My-Graduation-Capstone-Project\nLink: https://1046040509.wixsite.com/regression\n(Mar 2019 ‚Äì Dec 2019)\nStudent, Supervisor: Prof. Hung-Yi Lu\nl Setting up an interactive website on which visitors can watch tutorials and interact with for better learning of regression analysis models and theories\nl Coding in R (Shiny)\nl Received Certificate of Excellence in departmental 2019 Student Research Project Contest\n'], 'url_profile': 'https://github.com/dannyzzp', 'info_list': ['Python', 'Updated May 6, 2020', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Apache-2.0 license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Sep 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Updated May 9, 2020', 'Updated May 5, 2020']}","{'location': 'Stillwater, OK, USA', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Diabetes_predictive_model\nWe designed, implemented and compared various regression and classification models on Diabetics patient data.\npredictive models used: Multiple Linear Regression (MLR), Naive Bayes, Decision Tree, Logistic Regression, Artificial Neural Networks (ANN).\nTarget Variables: Time in Hospital (Regression), Change in medication (Binary Classification)\nFor more detailed information, review attached PDFs, and presentation files.\nThe Platform used: KNIME (https://www.knime.com/)\nDataset used: Diabetes 130-US hospitals for years 1999-2008 Data Set (http://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008)\nBase Paper on data: Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records (https://www.hindawi.com/journals/bmri/2014/781670/)\nuseful links:\nhttps://en.wikipedia.org/wiki/List_of_ICD-9_codes\nhttps://www.knime.com/learning/cheatsheets\nhttps://docs.knime.com/2019-06/analytics_platform_quickstart_guide/index.html\n'], 'url_profile': 'https://github.com/Reza-Marzban', 'info_list': ['Python', 'Updated May 6, 2020', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'HTML', 'Apache-2.0 license', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Sep 9, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Updated May 9, 2020', 'Updated May 5, 2020']}"
"{'location': 'Toronto, ON', 'stats_list': [], 'contributions': '237 contributions\n        in the last year', 'description': ['unit2_project1\nMy repo for my project for unit 2 covering logistic regression and marketing\n'], 'url_profile': 'https://github.com/SJHH-Nguyen-D', 'info_list': ['GPL-3.0 license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Boston_house_prediction\nThis is a linear regression model to estimate the prices of houses in boston.\n'], 'url_profile': 'https://github.com/PSatwikReddy', 'info_list': ['GPL-3.0 license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['Wine-variety-prediction\nPredictive model for predicting the wine ‚Äúvariety‚Äùusing logistic regression,SVM,Naive Bayes and data visualization\n'], 'url_profile': 'https://github.com/GazalaSayyad', 'info_list': ['GPL-3.0 license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'Karachi, Pakistan', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShizaAbid', 'info_list': ['GPL-3.0 license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'Jaypee University of Engineering and Technology,Guna(M.P)', 'stats_list': [], 'contributions': '595 contributions\n        in the last year', 'description': ['Marks-Prediction-Using-Linear-Regression\nMy implementation of linear regression model to predict marks of student.\n'], 'url_profile': 'https://github.com/rajansh87', 'info_list': ['GPL-3.0 license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'Madrid, Spain', 'stats_list': [], 'contributions': '221 contributions\n        in the last year', 'description': ['Diamonds-Price-Prediction\n1 Intro:\nIn this repository you will find 5 notebooks:\n\nDescriptive Analysis: a detailed descriptive analysis of the dataset\nLinear Models Dummies: the cleaned dataset enconded with dummy variables and trained and tested in linear regression models\nLinear Models Ordinal: the cleaned dataset enconded with ordinal variables and trained and tested in linear regression models\nNon Linear Models Dummies: the cleaned dataset enconded with dummy variables and trained and tested in non linear regression models\nNon Linear Models Ordina: the cleaned dataset enconded with ordinal variables and trained and tested in non linear regression models\n\n2 Goals:\nThe goal of the project is to predict the price of diamonds based in its carat, cut, color, clarity, depth%, table and volume/size. The measure unit is rmse\n3 Steps:\nTo fulfil the previous goals the next steps have been done:\n\nINPUT (2 datasets from Kaggle (1 to train, 1 to test))\nsrc (additional info as diamond images, diamond variables schema and possible models to implement)\nmain (the 5 notebooks explained previously)\nOUTPUT (the predicted price of the diamonds of the best models)\n\n4 Final Output:\nThe final output are the predicted prices of the best models. The best score (in rmse) was achieved with an Extra Trees Model, with a previous dummy encoding (one hot encoding). The score was: rmse = 532.01097\n'], 'url_profile': 'https://github.com/diego-florez', 'info_list': ['GPL-3.0 license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': [""Note: In the GitHub repo, the MNIST training set is not included because the file is too large. Please download the train set from https://pjreddie.com/projects/mnist-in-csv/ into the data folder.\nWelcome to the Machine Learning Algorithms Suite!\nStart the program using the .bat (Windows) or .sh (macOS / Linux) file or run the compare_algorithms.py file\nAt least Python 3.6 needs to be installed and (for Windows users) needs to be included in PATH.\nThis suite contains the following algorithms that you can run:\nRegression (can be visualized with 1D Regression Dataset):\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa01. Linear Regression\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa02. Linear Regression with L2 Regularization\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa03. Kernel Regression with Boxcar Kernel\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa04. Kernel Regression with Linear Kernel\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa05. Kernel Regression with Polynomial Kernel\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa06. Kernel Regression with RBF Kernel\nClassification (can be visualized with 1D Classification Dataset):\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa07. Logistic Regression using Gradient Descent\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa08. Logistic Regression using Newton's Method\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa09. Logistic Regression using Gradient Descent with L2 Regularization\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa010. Logistic Regression using Newton's Method with L2 Regularization\nClassification (can be visualized with 2D Classification Dataset):\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa011. Decision Tree using ID3\nOther Algorithms (cannot be visualized for now):\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa012. K-Means\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa013. K-Nearest Neighbor\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa014. GMM (Gaussian Mixture Models) using Expectation-Maximization\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa015. Neural Network (with one hidden layer)\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa016. Recommender Systems\nExcept for Recommender Systems and Logistic Regression, all algorithms can be run on the full MNIST dataset.\n""], 'url_profile': 'https://github.com/andrewsheng2', 'info_list': ['GPL-3.0 license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'Hyderabad,Telangana,India', 'stats_list': [], 'contributions': '219 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/narsym', 'info_list': ['GPL-3.0 license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['diabetes-pred\nA trial analysis to test my knowledge of the different regression models used to predict data\nUpdates ~ \nAchieved 81% prediction accuracy.\n'], 'url_profile': 'https://github.com/reCursiv3', 'info_list': ['GPL-3.0 license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': '6/Jay aarti Sosayti, Behind Khodiyar Temple ,Juhapura, Jivrajpark, Ahmedabad, Gujrat, India', 'stats_list': [], 'contributions': '256 contributions\n        in the last year', 'description': ['ML-sklearn-basic\nThis repository contains basic linear regression machine learning models with sklearn datasets\n'], 'url_profile': 'https://github.com/sshiv5768', 'info_list': ['GPL-3.0 license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'Python', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kprasertchoang', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 31, 2020', 'MATLAB', 'Updated May 10, 2020', 'MATLAB', 'Updated May 10, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'MIT license', 'Updated Oct 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/LinsamuelATM', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 31, 2020', 'MATLAB', 'Updated May 10, 2020', 'MATLAB', 'Updated May 10, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'MIT license', 'Updated Oct 5, 2020']}","{'location': 'Islamabad, Pakistan', 'stats_list': [], 'contributions': '172 contributions\n        in the last year', 'description': [""Predicting_The_Stock_Market\nNote: You shouldn't make trades with any models developed in this project. Trading stocks has risks, and nothing in this project constitutes stock trading advice.\nIn this project, we worked with data from the S&P500 Index which is a stock market index. The S&P500 Index aggregates the stock prices of 500 large companies. Moreover, we used historical data on the price of the S&P500 Index to make predictions about future prices. Predicting whether an index will go up or down will help us forecast how the stock market as a whole will perform. Since stocks tend to correlate with how well the economy as a whole is performing, it can also help us make economic forecasts.\nThe file that we worked upon is a csv file containing index prices. Each row in the file contains a daily record of the price of the S&P500 Index from 1950 to 2015. The dataset is stored in sphist.csv.\nThe columns of the dataset are:\n\nDate -- The date of the record.\nOpen -- The opening price of the day (when trading starts).\nHigh -- The highest trade price during the day.\nLow -- The lowest trade price during the day.\nClose -- The closing price for the day (when trading is finished).\nVolume -- The number of shares traded.\nAdj Close -- The daily closing price, adjusted retroactively to include any corporate actions. Read more here.\n\n""], 'url_profile': 'https://github.com/syed0019', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 31, 2020', 'MATLAB', 'Updated May 10, 2020', 'MATLAB', 'Updated May 10, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'MIT license', 'Updated Oct 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['Housing_Price_Prediction\nThe idea of this project was to create a predictor on the california housing dataset. The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data.\nThe data\nThe data is comprised of 8 attributes\n\n\nMedInc median income in block\n\n\nHouseAge median house age in block\n\n\nAveRooms average number of rooms\n\n\nAveBedrms average number of bedrooms\n\n\nPopulation block population\n\n\nAveOccup average house occupancy\n\n\nLatitude house block latitude\n\n\nLongitude house block longitude\n\n\nas well as the target, the housing price\nTraining\ntrain.py runs the training for three different models: NN, linear regression and random forest on the scikit-learn california housing dataset:\npython3 train.py\nTraining output example:\n...\n\nTrain Epoch: 150 [0/18576 (0%)] Loss: 0.130984\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:00<00:00, 143.57it/s]\n5\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 102.93it/s]\n        NN\nr2 score: 0.8232532827300671\nMAE score: 0.2833885734455647\n        Linear regressor\nr2 score: 0.6098033978087847\nMAE score: 0.4635741867691994\n        Random forest\nr2 score: 0.8138137169848451\nMAE score: 0.2837869675577879\n...\nThe network\nWe use a stack fully connected layers with ReLU. The r2 score and MAE was used for evaluating the models\nConclusion\nThe neural network trained on the standardized signals gave the best model with an R2 score of 82.4. The models trained on the first two principal componentes gave a poor result even if they accounted for ~96% of the data variance.\n'], 'url_profile': 'https://github.com/mikel-brostrom', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 31, 2020', 'MATLAB', 'Updated May 10, 2020', 'MATLAB', 'Updated May 10, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'MIT license', 'Updated Oct 5, 2020']}","{'location': 'St. Louis, MO', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['Machine_Learning_Labs\nAuthor\nRichard Wu\n2020 May 9\nIntroduction\nThis repository contains many Machine Learning Labs, including Perceptron Learning Algorithm, Hoeffding Bound Experiment, Bias and Variance Decomposition, Gradient Descent for Logistic Regression, with and without regularizer, Ensemble Learning including Random Forest, and AdaBoost.\nFiles\nEach lab is contained in a single folder, with scripts in MATLAB, data files, and a report file. All the details of the experiments can be found at the report.pdf files in each folder.\n'], 'url_profile': 'https://github.com/wuyuanpei', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 31, 2020', 'MATLAB', 'Updated May 10, 2020', 'MATLAB', 'Updated May 10, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'MIT license', 'Updated Oct 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Digit-Classification-Neural-Networks\nDIGIT CLASSIFIER | OCTAVE/MATLAB, LOGISTIC REGRESSION, NEURAL NETWORKS:\ni) Designed a one-vs-all logistic regression classifier and implemented a multi-layered neural network for digit recognition\nii) Achieved 96.5% accuracy with a vectorized implementation of regularized logistic regression and feedforward propagation\n'], 'url_profile': 'https://github.com/hariharankarthik', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 31, 2020', 'MATLAB', 'Updated May 10, 2020', 'MATLAB', 'Updated May 10, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'MIT license', 'Updated Oct 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/michael-abbate', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 31, 2020', 'MATLAB', 'Updated May 10, 2020', 'MATLAB', 'Updated May 10, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'MIT license', 'Updated Oct 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['PredictiveModel\nMachine learning using Linear Regression\nInstal Python libraries Numpy, Pandas and Matplotlib\nimport .csv file to project\n'], 'url_profile': 'https://github.com/MicroTot', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 31, 2020', 'MATLAB', 'Updated May 10, 2020', 'MATLAB', 'Updated May 10, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'MIT license', 'Updated Oct 5, 2020']}","{'location': 'Austin,TX', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': [""Machine Learning Homework Requirements:\nHello! Welcome to the famous Tsukiji fish market of Tokyo, Japan! We came here to collect data on some of the fish they have here\nbut we didn't wake up at 5am for the tuna auction and by the time we showed up they were only left with a few species of fish.\nWe got to work and gathered measurements from a few different species of fish and want you to train a regression model to predict\nthe weight of a fish using some of the features we were able to measure. We have no idea which features will be good predictors.\nWe will hold out 30% of the data before we hand it to you and we will use that csv for scoring.\nHere's what we need from you:\n\nA function that accepts a csv path and returns the predictions of your regression model using our csv. The csv we use will contain all the columns.\nUse a pipenv and scikit learn to submit the final hw. You may use R for model selection.\n\nWith this function and it's output, we will rank the students by how well their model performed on predicting weight based on naive data.\nYour grade will be determined by ranking according to Mean-squared Error.\nIf your function does not return a list of predictions or we cannot compute the accuracy of your model that it will be an automatic F.\n""], 'url_profile': 'https://github.com/sahar3267', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 31, 2020', 'MATLAB', 'Updated May 10, 2020', 'MATLAB', 'Updated May 10, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'MIT license', 'Updated Oct 5, 2020']}","{'location': 'Vancouver, BC', 'stats_list': [], 'contributions': '748 contributions\n        in the last year', 'description': ['ADASPlusML\nPython script for training a multiple linear regression machine learning model to predict the scoring of the ADAS-Cog Plus assessment. ADASPlusML uses Scikit-learn for machine learning and pandas for data processing.\nThe components of the ADAS-Cog Plus used to train the model are the ADAS-Cog Total Score, Trail Making Test Score, Verbal Fluency (Animal), Verbal Fluency (Vegetable), and Digit Symbol Substition Test (DSST) Score.\nThe data used to train the model is used with permission from the UBC Aging, Mobility, and Cognitive Neuroscience Lab. The data is confidential and not provided.\n'], 'url_profile': 'https://github.com/patchan', 'info_list': ['Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 31, 2020', 'MATLAB', 'Updated May 10, 2020', 'MATLAB', 'Updated May 10, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'MIT license', 'Updated Oct 5, 2020']}"
"{'location': 'Washington, D.C.', 'stats_list': [], 'contributions': '517 contributions\n        in the last year', 'description': ['U.S. Crime Analysis\nThis repository includes analysis to predict the crime rates across 50 states using a wide array of machine learning and data science techniques, including, but not limited to:\n\nPrincipal Component Analysis\nDecision Trees\nRandom Forest\nLinear and Logistic Regression\nVariable Selection (LASSO, Ridge, Step, etc.)\nOutlier Detection\n\nThe repository is organized as follows:\n\n\n\nFolder\nDescription\n\n\n\n\nCode\nThis section includes all of the data wrangling and predictive modeling R code used on the U.S. Crime dataset.\n\n\nData\nThis section includes the main dataset used throughout the analyses.\n\n\nViz\nThis section includes all of the main visualizations generated by the R Code in the Code section.\n\n\n\nNote: The data can be found in the data folder as well as at the following location: http://www.statsci.org/data/general/uscrime.txt\nThe description for the dataset can be found at the following location: http://www.statsci.org/data/general/uscrime.html\n'], 'url_profile': 'https://github.com/jschulberg', 'info_list': ['R', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/WinEisEis', 'info_list': ['R', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020']}","{'location': 'Liaoning, China', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dlut-dimt', 'info_list': ['R', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020']}","{'location': 'Toronto, Ontario', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': [""Forest-Fire-Prediction\nForest fires are causing a serious threat to the Tropical forest perseveration. Last year Amazon rainforest noticed the largest fire in years. It is approaching its tipping point, beyond which it will become difficult to save. Hence, predicting the fires would largely help the authorities of not only the Amazon region but also the large forest reserves across the globe.\nI am using the record of last 20 years for the number of forest fires that Brazil has faced every month, using it as a time series to predict the pattern which would include details of forest fires per month for the upcoming 1 years, so that the authorities would have more time to take necessary actions to handle the same, and they would have an estimate of the vastness and frequency of the fires for every state.\nThe dataset seems to have an annual seasonality, with the major forest fires in Sep and Oct. These forest fires are manmade most of the times, mainly because of the loggers that are cutting and burning trees to increase land to pasture their cattle.\nPreprocessing and Feature Engineering\n\nRemoving independent features\nReordering Data so that it can be fed as a time series\nLabel encoding the feature 'month', so that it can be used in timestamp in mm format.\nRemoving null values, since it was just a fraction of the total dataset.\nNormalizing data using MinMaxScaler, as it preserves the shape of original distribution\n\nPrediction Models\n\nARIMA Model\nSVR Model\nHolt Winter's Model\n\nAccuracy Measures\n\nRoot Mean Squared Error\nMean Average Error\n\n\nFor this dataset Holt Winters‚Äô model gives the best result with the least RMSE and MAE values in comparison to the SVR and ARIMA models.\n""], 'url_profile': 'https://github.com/rjagait', 'info_list': ['R', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/likitha4', 'info_list': ['R', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020']}","{'location': 'Atlanta, GA', 'stats_list': [], 'contributions': '203 contributions\n        in the last year', 'description': ['MCMC-Log-Reg\nContains a class that fits logistic regression parameters via a modified Metropolis-Hastings algorithm. An example is contained in a .ipynb file in this repository.\nRequires numpy, scipy, and tqdm libraries.\nPossible inputs are:\n y: numpy array (column vector) of the training labels; length must match the number of rows in X\n\n X: numpy array (matrix format) of independent variables\n\n beta_priors: numpy array of the prior beliefs for each coefficient in the model;\n              must be the same length as the number of columns in X matrix or one more if adding an\n              intercept\n\n prior_stds: numpy array of the standard deviations of the priors;\n             must be the same length as the beta_priors\n\n jumper_stds: numpy array of standard deviations of the jumping distribution for each beta coefficient;\n              must be the same length as beta_priors\n\n num_iter: an int for the number of interations to perform\n\n add_intercept: True (default) if the user wants to add an intercept to X\n\n random_seed: int that sets the random seed for reproducibility\n\n alpha: float on the closed interval 0 to 1:\n        used to create the (1 - alpha)*100% credible interval for the coefficients\n\n burn_in: float on the closed interval 0 to 1:\n          it is the proportion of simulate coefficients to discard as the algorithm searches the parameter space\n\n'], 'url_profile': 'https://github.com/jkclem', 'info_list': ['R', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020']}","{'location': 'Canarias / Madrid', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': ['whitestrap\n\n\n\nThis package presents the White‚Äôs Test of Heterocedasticity and a\nbootstrapped version of it, developed under the methodology of Jeong,\nJ., Lee, K. (1999) (see references for further details).\nInstallation\nYou can install the development version from\nGitHub with:\n# install.packages(""devtools"")\ndevtools::install_github(""jlopezper/whitestrap"")\nExample\nThis is an example which shows you how you can use the white_test and\nwhite_test_boot functions:\nlibrary(whitestrap)\n#> \n#> Please cite as:\n#> Lopez, J. (2020), White\'s test and Bootstrapped White\'s test under the methodology of Jeong, J., Lee, K. (1999) package version 0.0.1\n\nset.seed(123)\n# Let\'s simulate some heteroscedastic data\nn <- 100\ny <- 1:n\nsd <- runif(n, min = 0, max = 4)\nerror <- rnorm(n, 0, sd*y)\nX <- y + error\ndf <- data.frame(y, X)\n# OLS model\nfit <- lm(y ~ X, data = df)\n# White\'s test and Bootstrapped White\'s test\nwhite_test(fit)\n#> White\'s test results\n#> \n#> Null hypothesis: Homoskedasticity of the residuals\n#> Alternative hypothesis: Heteroskedasticity of the residuals\n#> Test Statistic: 12.88\n#> P-value: 0.001597\nwhite_test_boot(fit)\n#> Bootstrapped White\'s test results\n#> \n#> Null hypothesis: Homoskedasticity of the residuals\n#> Alternative hypothesis: Heteroskedasticity of the residuals\n#> Number of bootstrap samples: 1000\n#> Boostrapped Test Statistic: 12.88\n#> P-value: 0.003\nIn either case, the returned object is a list with the value of the\nstatistical test and the p-value of the test. For the bootstrapped\nversion, the number of bootstrap samples is also provided.\nnames(white_test(fit))\n#> [1] ""w_stat""  ""p_value""\nnames(white_test_boot(fit))\n#> [1] ""w_stat""  ""p_value"" ""iters""\nComparison between the original and the bootstrap version\nOne way to compare the results of both tests is through simulations. The\nfollowing plot shows the distribution of 500 simulations where the\np-value of both tests is computed. The data used for this purpose were\nartificially generated to be heterocedastic, so low p-values are\ndesirable.\n\nIf we look at the cumulative distribution functions of both p-value\ndistributions, we see that in small samples the bootstrapped test\nreturns smaller p-values with higher probability.\n\nIn order to check for differences between the two distributions, a\ntwo-sample Kolmogorov‚ÄìSmirnov test is run. In this case, we‚Äôll test\nwhether one distribution stochastically dominates another, so the test\nwill be run for both alternative sides (CDF (BW) > CDF (W) and CDF\n(BW) < CDF (W)). We see from the results that CDF (BW) statistically\noutperforms CDF (W) for samples < 60. No differences are appreciated\nwith samples greater than or equal to 60.\n\nReferences\n\n\nJeong, J., & Lee, K. (1999). Bootstrapped White‚Äôs test for\nheteroskedasticity in regression models. Economics Letters, 63(3),\n261-267.\n\n\nWhite, H. (1980). A Heteroskedasticity-Consistent Covariance Matrix\nEstimator and a Direct Test for Heteroskedasticity. Econometrica,\n48(4), 817-838.\n\n\nWooldridge, Jeffrey M., 1960-. (2012). Introductory econometrics : a\nmodern approach. Mason, Ohio, South-Western Cengage Learning.\n\n\n'], 'url_profile': 'https://github.com/jlopezper', 'info_list': ['R', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': [""Iris-Data-Science\nTy's first model used to predict iris flower species. Using logistic regression and cross-validation.\nSmall dataset with only 150 datapoints\n""], 'url_profile': 'https://github.com/ty-martz', 'info_list': ['R', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '178 contributions\n        in the last year', 'description': ['Electricity-Consumption-Prediction\nPredicting Electricity consumption in the city of Mumbai and Delhi by creating regression models in R\n'], 'url_profile': 'https://github.com/sidh26', 'info_list': ['R', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shubhangperiwal', 'info_list': ['R', 'Updated May 23, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 22, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated Jun 1, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kemalkar', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'C', 'Updated May 7, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Python', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '237 contributions\n        in the last year', 'description': ['Aquatic Animal Classification using Logistic Regression and Support Vector Machines\nThis project is CS-464 Introduction to Machine Learning Spring 2020 Homework 2\nThe models is implemented in MATLAB. You can find homework description, dataset, and my final report in the repository. There are 4 matlab scripts:\nq1_logistic_regression.m\nq1_svm_subclass.m\nq1_svm_superclass.m\nq2_pca_analysis.m\nTo run each scripts, the script should be in the same folder as the data provided for hw2.\nThen you can just Run (F5) the program to execute. It takes some time to execute the program (might change depending on the pc specs).\nAlternatively, I splitted the scripts into sections and write comments regarding what that section is about.\nYou can run each section separetaly. Personally, I believe this way it will be easier to examine codes.\nThe homework is implemented in Matlab r2019b.Therefore some functions to read the data might not be compatible with\nthe earlier versions. Please try to execute the programs using Matlab r2019b.\n'], 'url_profile': 'https://github.com/mmoksas68', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'C', 'Updated May 7, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Python', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['Titanic-Dataset\nThis project consist of Exploratory Data Analysis of the datasets and a followed linear regression model\nSteps for Exploratory Data Analysis\n1)Load and Check Data\n2)Variable Description-\nPassengerId: unique identifier number(id) to each passenger.\nSurvived: passenger survive(1) or died(0)\nPclass: passenger class\nName: name of passenger\nSex: gender of passenger\nAge: age of passenger\nSibSp: number of siblings or spouses\nParch: number of parents or children\nTicket: ticket number\nFare: amount of money spent on ticket\nCabin: cabin category\nEmbarked: port where passenger embarked (C = Cherbourg, Q = Queenstown, S = Southampton)\n3)Basic Data Analysis\n4)Outlier Detection\n\nVisualiztion\n\nin the end linear regression model is there\n'], 'url_profile': 'https://github.com/prithvishkr', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'C', 'Updated May 7, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Python', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cmr378', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'C', 'Updated May 7, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Python', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['heartdiseaselogit\nUsing Logistic Regression to predict 10 year risk of coronary heart disease\nOn Framingham data.\n'], 'url_profile': 'https://github.com/259mit', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'C', 'Updated May 7, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Python', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '135 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ranahamzaintisar1995', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'C', 'Updated May 7, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Python', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Lagos', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['TAXI-OUT-TIME-PREDICTION and TAXI OUT INCIDENT CLASSIFICATION\nPredicting TAXI OUT TIME using simple machine learning regression based models and\nPredicting if a taxi out incident would occur using classification models.\nA taxi out incident is said to occur when taxi out time exceeds the average taxi out time across all airports\nAverage taxi out time across all airports is 18 minutes\n'], 'url_profile': 'https://github.com/Tee-A', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'C', 'Updated May 7, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Python', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'brooklyn, ny', 'stats_list': [], 'contributions': '708 contributions\n        in the last year', 'description': [""Predicting Popularity of an Online News Article\nUsing UCI's Online News Popularity Data Set, I perform statistical tests to measure the significance of certain variables on the number of social media shares an article receives. During my exploratory data analysis, I create graphs that answer some of my objective questions. I then create a linear regression model, from which I can infer the strongest predictors based on the coefficients within the model.\nList of files\n\nfunctions.py - text file with functions for data cleaning and statistical tests.\nonline_news_popularity_final_notebook.ipynb - Jupyter Notebook for data exploration and linear regression modeling.\npresentation.pdf - presentation for Mashable executives with my findings.\narchives folder - Includes scratch notebooks.\ncharts folder - PNG files of insightful charts from the project.\ndata folder - Main CSV file, description of columns, and interaction pickles.\n\nBlog post\n""], 'url_profile': 'https://github.com/p-szymo', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'C', 'Updated May 7, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Python', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Turin,Italy', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['ICT_For_Health\nuse algorithms of machine learning such as decision tree, Regression and k-mean clustering on diseases.\n'], 'url_profile': 'https://github.com/zahra4fact', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'C', 'Updated May 7, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Python', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Lin-Log-Reg-from-Scratch\nlinear and logistic regression implementation in python environment using MNIST dataset\n'], 'url_profile': 'https://github.com/guptabhis', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 10, 2020', 'MATLAB', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'C', 'Updated May 7, 2020', 'R', 'Updated May 5, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Python', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/school-shenanigans', 'info_list': ['Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 7, 2020', 'MATLAB', 'Updated May 8, 2020', 'Python', 'Updated May 27, 2020', '1', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Washington DC', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['dc-housing-inventory\nRegression analysis (Keras) to determine the sale price of a residential unit in Washington D.C.\n'], 'url_profile': 'https://github.com/sandevaj', 'info_list': ['Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 7, 2020', 'MATLAB', 'Updated May 8, 2020', 'Python', 'Updated May 27, 2020', '1', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Islamabad, Pakistan', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['TaxiTripDurationPredictor\nA linear regression model that predicts duration of a taxi trip. This linear-regressor is trained over the provided dataset and the best features are selected to ensure a more accurate fit.\n'], 'url_profile': 'https://github.com/raafey', 'info_list': ['Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 7, 2020', 'MATLAB', 'Updated May 8, 2020', 'Python', 'Updated May 27, 2020', '1', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Leamington Spa', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/leosouliotis', 'info_list': ['Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 7, 2020', 'MATLAB', 'Updated May 8, 2020', 'Python', 'Updated May 27, 2020', '1', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Vancouver, BC', 'stats_list': [], 'contributions': '303 contributions\n        in the last year', 'description': ['Body-Fat-Prediction\nBody Fat Prediction based on Physical Parameters, Kernel Ridge Regression and Support Vector Machine\n'], 'url_profile': 'https://github.com/alireza-alizadegan', 'info_list': ['Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 7, 2020', 'MATLAB', 'Updated May 8, 2020', 'Python', 'Updated May 27, 2020', '1', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Madison, WI, America', 'stats_list': [], 'contributions': '390 contributions\n        in the last year', 'description': ['Covid-19_Data_Linear_Regression\nThis is the final project for CS 540, Spring 2020, UW-Madison.\nLinear regression with macro economic metrics and the rate of increase in the number of cases\nNote that the reason for conducting linear regression is based on the diminishing return phenomena. Since the development of a country is subject to diminishing return, the metrics would hardly have an impact on a variable that exceeds the impact of x on y for function y = kx, where k is a constant. So based on this assumption, the R2 coefficient reflects how much linear between the independent variable and the label, which reflects the importance of that variable.\nFor now, the code is corrected. But the report needs further correction.\n'], 'url_profile': 'https://github.com/xiaoxi-s', 'info_list': ['Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 7, 2020', 'MATLAB', 'Updated May 8, 2020', 'Python', 'Updated May 27, 2020', '1', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '126 contributions\n        in the last year', 'description': ['University-of-Washington\nIt conducts with machine learning specialization including foundations, regression, classification and clustering-retrieval\n'], 'url_profile': 'https://github.com/hrOarr', 'info_list': ['Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 7, 2020', 'MATLAB', 'Updated May 8, 2020', 'Python', 'Updated May 27, 2020', '1', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'Brisbane', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['AsySVR_cost_minimisation\nSupport vector regression with asymmetric loss for optimal electric load forecasting\nThe attached source codes are for the 3-step ahead load forecasting in the two assumed NSW scenarios.\n'], 'url_profile': 'https://github.com/wujrtudou', 'info_list': ['Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 7, 2020', 'MATLAB', 'Updated May 8, 2020', 'Python', 'Updated May 27, 2020', '1', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ANMOLPALIWAL', 'info_list': ['Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 7, 2020', 'MATLAB', 'Updated May 8, 2020', 'Python', 'Updated May 27, 2020', '1', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akhilhumane99', 'info_list': ['Jupyter Notebook', 'Updated May 21, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 7, 2020', 'MATLAB', 'Updated May 8, 2020', 'Python', 'Updated May 27, 2020', '1', 'Jupyter Notebook', 'Updated May 28, 2020', 'R', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Covid-19-India-Linear-Regression-Model\nA basic linear regression model for studying the growth of Covid-19 patients in India\n'], 'url_profile': 'https://github.com/Ryanston', 'info_list': ['Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', '1', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['IBM employee data\nIBM employee data, linear regression model and assumptions model to predict monthly salary.\n'], 'url_profile': 'https://github.com/Mike-Wenzell', 'info_list': ['Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', '1', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Students Performance EDA\nExplatory Data Analysis and Regression to find any differences and causes for differences in scores.\n'], 'url_profile': 'https://github.com/ayaanlehashi', 'info_list': ['Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', '1', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Berkeley', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Wine_Quality_Model_Testing\nTuning hyper-parameters for advanced regression models to predict the quality of red wine.\n'], 'url_profile': 'https://github.com/Tushar0101', 'info_list': ['Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', '1', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/variii', 'info_list': ['Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', '1', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['Unit_2_Project_1\nRepo for Unit 2 Project 1 covering logistic regression and marketing\n'], 'url_profile': 'https://github.com/bonksl', 'info_list': ['Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', '1', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '103 contributions\n        in the last year', 'description': ['Python\nData Preparation :  https://github.com/ChinduSahid/Python/blob/master/Data%20Preperation.ipynb\nFiltering :  https://github.com/ChinduSahid/Python/blob/master/Filtering.ipynb\nSales Analysis : https://github.com/ChinduSahid/Python/blob/master/Sales%20data%20analysis.ipynb\nMarketing campaign Analysis :  https://github.com/ChinduSahid/Python/blob/master/Marketing%20campaign%20analysis.ipynb\nCombining multiple CSV in a folder : https://github.com/ChinduSahid/Python/blob/master/Combining%20data.ipynb\nCustomer Segmentation : https://github.com/ChinduSahid/Python/blob/master/Customer%20Segmentation.ipynb\n'], 'url_profile': 'https://github.com/ChinduSahid', 'info_list': ['Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', '1', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Bangalore,India', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Credit Risk Analytics\nWhen a bank gives a loan to a bad customer who is infact a good customer,results in greater loss to the bank rather than\ndenying a loan to a good customer.\nSo,it would be helpful to identify the characteristics that are indicative of people who are likely to default on loans,\nand then use those characteristics to discriminate between good and bad credit risks\nThe main aim of this project is to predict if a customer will default on loan or not.\nUsed\n1.Logistic regression\n2.Decision tree algorithms\n3.Random Forest to build the model.\nAmong the algorithms Logistic Regression gave the best test accuracy of 72.8%\nAttached the WOE table generated for the categorical variables in the data\n'], 'url_profile': 'https://github.com/SwethaRamanadham', 'info_list': ['Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', '1', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': [""Projects-Python\nPractical demonstration of scikit learn library for building various classification and regression models\nDescription\nThe ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics. In this notebook, we will be covering the steps on how to do Latent Dirichlet Allocation (LDA), which is one of many topic modeling techniques. It was specifically designed for text data. To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up. Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model.\nData set comprises of 20 Newsgroups and using LDA to extract the naturally discussed topics.\nUsing Latent Dirichlet Allocation (LDA) from Gensim package along with the Mallet‚Äôs implementation (via Gensim). Mallet has an efficient implementation of the LDA. It is known to run faster and gives better topics segregation.\nData set\nData can be obtained from : https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json\n""], 'url_profile': 'https://github.com/ankit013', 'info_list': ['Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', '1', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'S√£o Paulo', 'stats_list': [], 'contributions': '196 contributions\n        in the last year', 'description': ['MULTIPLE LINEAR REGRESSION (MLR) - PROFIT STARTUPS\nPython code predicts profits from Startups. It was applied Multiple Linear Regression (MLR).\n'], 'url_profile': 'https://github.com/ricardomotoyama', 'info_list': ['Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jun 16, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 15, 2020', '1', 'Jupyter Notebook', 'Updated Sep 14, 2020', '1', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['ALPhA-O-PPAC-Regression\nEvaluates O-PPAC Dataset using Fully Connected Neural Network and Support Vector Regression Models\nThis repository contains two files called O-PPAC_FCNN and O-PPAC_SVR. Both, at the moment evaluate an O-PPAC dataset that only contains events from 9 different x,y position but all the y positions are 0. Thus, these two programs assume that you have the O-PPAc data file called ""simu_HDF_new.h5"", which was provided by Dr. Yassid Ayyad of the Facility for Rare Isotope Beams at Michigan State University. The FCNN program requires you to have the Tensorflow, Sci-Kit Learn, Numpy, H5py and Matplotlib packages installed. The SVR program requires you to have the Sci-Kit Learn, Numpy, H5py and Matplotlib packages installed.\nThe FCNN program works by running all the sections of code and then allowing the main() function to train an FCNN model using the given dataset, predict, and produce a loss value, Mean Absolute Error, Mean Square Value, Predictions vs True Values graph and a Loss Curve.\nThe SVR programs works by running all the sections of code and then running the main() function will train an SVR model using the given dataset and make predicitons. This program also outputs the Mean Absolute Value and a Predictions vs True Values graph.\n'], 'url_profile': 'https://github.com/TedJYoo', 'info_list': ['Jupyter Notebook', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 5, 2020', '1', 'Python', 'MIT license', 'Updated Dec 18, 2020', '1', 'R', 'Updated Feb 27, 2021', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Dec 25, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['object-detection-\nsingle object detection for electronic cards using transfert learning and regression\nthe dataset used in this project can be download from this link https://www.kaggle.com/tannergi/microcontroller-detection\nthe idea of this project is to provide some simple techniques to use for localization and recognition of one object\nby training using transfer learning with low computational time\nbefore going to more complex project to detect multi-object   using some existing framework such as YOLO ,SSD ...\nOne example of model as shown in the .ipynb file can be used to train from scratch\n'], 'url_profile': 'https://github.com/rd20karim', 'info_list': ['Jupyter Notebook', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 5, 2020', '1', 'Python', 'MIT license', 'Updated Dec 18, 2020', '1', 'R', 'Updated Feb 27, 2021', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Dec 25, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '137 contributions\n        in the last year', 'description': ['Diabetes-and-Heart-Disease-Prediction\nInstallations:\nThe project has been done in the jupyter enviroment with python 3.6. All the package requires are mentioned in requirement.txt\nMotivatoin:\nTo perform analysis on two different disease dataset to predict the chances of having a particular diseases by using different machine learning algorithm and also to check that which particular algorithm performs the best on the given dataset.\nThis project uses KNN, SVM, Logistic Regression and decision tree to predict the output on the given dataset.\nThe dataset has been taken from https://archive.ics.uci.edu/ml/datasets.php for both diabetes and heart disesase.\nSummary\nI found that SVM outperforms the remaining 3 algorithm as it gives an accuracy of 89% on diabetic data set and 80% on heart disease dataset.\n'], 'url_profile': 'https://github.com/piyush9923', 'info_list': ['Jupyter Notebook', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 5, 2020', '1', 'Python', 'MIT license', 'Updated Dec 18, 2020', '1', 'R', 'Updated Feb 27, 2021', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Dec 25, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'Berkeley, CA', 'stats_list': [], 'contributions': '744 contributions\n        in the last year', 'description': ['Orthogonal Functions\nA python package to generate a set of orthogonal functions.\n\nAnaconda package\nPyPi package\nSource code\nDocumentation\nAPI\n\n\nBuild and Test Status\n\n\n\nPlatform\nPython versions\nBuild status\n\n\n\nLinux (Ubuntu)\n2.7, 3.5, 3.6, 3.7, 3.8\n\n\n\nmacOS\n2.7, 3.5, 3.6, 3.7, 3.8\n\n\n\nWindows\n2.7, 3.5, 3.6, 3.7, 3.8\n\n\n\n\n\n\nDescription\nThis package generates a set of orthonormal functions, called \n, based on the set of non-orthonormal functions \n defined by\n\n\nThe orthonormalized functions \n are the linear combination of the functions \n by\n\n\nThe functions \n are orthonormal in the interval \n with respect to the weight function \n. That is,\n\n\nwhere \n is the Kronecker delta function. The orthogonal functions are generated by Gram-Schmidt orthogonalization process. This script produces the symbolic functions using Sympy, a Python computer algebraic package. An application of these orthogonal functions can be found in [Ameli-2020].\n\nInstall\nInstall using either of the following three methods.\n\n1. Install from Anaconda Cloud\n \n\nInstall using Anaconda cloud by\nconda install -c s-ameli orthogonalfunctions\n\n\n2. Install from PyPi\n  \nInstall using the package available on PyPi by\npip install OrtthogonalFunctions\n\n\n3. Install from Source Code\n  \nInstall directly from the source code by\ngit clone https://github.com/ameli/Orthogonal-Functions.git\ncd Orthogonal-Functions\npip install .\n\n\nTesting\nTo test the package, download the source code and use one of the following methods in the directory of the source code:\n\nMethod 1: test locally by:\npython setup.py test\n\n\nMethod 2: test in a virtual environment using tox:\npip install tox\ntox\n\n\n\n\nUsage\nThe package can be used in two ways:\n\n1. Import as a Module\n>>> from OrthogonalFunctions import OrthogonalFunctions\n\n>>> # Create an object\n>>> OF = OrthogonalFunctions()\n\n>>> # Generate Functions\n>>> OF.Process()\n\n>>> # Print Functions\n>>> OF.Print()\n\n>>> # Check mutual orthogonality of Functions\n>>> OF.Check()\n\n>>> # Plot Functions\n>>> OF.Plot()\nThe OrthogonalFunctions also accepts some parameters:\n>>> # Specify any or all of the three parameters below\n>>> OF = OrthogonalFunctions(\n...        StartFunctionIndex=1,\n...        NumFunctions=9,\n...        EndInterval=1)\n\n>>> # The rest is the same as before.\nThe parameters are:\n\nStartFunctionIndex: the index of the starting function, \n. The default is 1.\nNumFunctions: number of orthogonal functions to generate, \n. The default is 9.\nEndInterval: the right interval of orthogonality, \n. The default is 1.\n\n\n2. Use As Standalone Application\nThe standalone application can be executed in the terminal in two ways:\n\nIf you have installed the package, call gen-ortho executable in terminal:\ngen-ortho [options]\n\nThe optional argument [options] will be explained in the next section. When the package OrthogonalFunctions is installed, the executable gen-ortho is located in the /bin directory of the python.\n\nWithout installing the package, the main script of the package can be executed directly from the source code by\n# Download the package\ngit clone https://github.com/ameli/Orthogonal-Functions.git\n\n# Go to the package source directory\ncd OrthogonalFunctions\n\n# Execute the main script of the package\npython -m OrthogonalFunctions [options]\n\n\n\n\nOptional arguments\nWhen the standalone application (the second method in the above) is called, the executable accepts some optional arguments as follows.\n\n\nOption\nDescription\n\n\n\n-h, --help\nPrints a help message.\n\n-v, --version\nPrints version.\n\n-l, --license\nPrints author info, citation and license.\n\n-n, --num-func[=int]\nNumber of orthogonal functions to generate. Positive integer. Default is 9.\n\n-s, --start-func[=int]\nStarting function index. Non-negative integer. Default is 1.\n\n-e, --end-interval[=float]\nEnd of the interval of functions domains. A real number greater than zero. Default is 1.\n\n-c,--check\nChecks orthogonality of generated functions.\n\n-p, --plot\nPlots generated functions, also saves the plot as pdf file in the current directory.\n\n\n\n\nParameters\nThe variables \n, \n, and \n can be set in the script by the following arguments,\n\n\nVariable\nVariable in script\nOption\n\n\n\n\n\nStartFunctionIndex\n-s, or --start-func\n\n\n\nNumFunctions\n-n, or --num-func\n\n\n\nEndInterval\n-e, or --end-interval\n\n\n\n\nExamples\n\nGenerate nine orthogonal functions from index 1 to 9 (defaults)\ngen-ortho\n\n\nGenerate eight orthogonal functions from index 1 to 8\ngen-ortho -n 8\n\n\nGenerate nine orthogonal functions from index 0 to 8\ngen-ortho -s 0\n\n\nGenerate nine orthogonal functions that are orthonormal in the interval [0,10]\ngen-ortho -e 10\n\n\nCheck orthogonality of each two functions, plot the orthonormal functions and save the plot to pdf\ngen-ortho -c -p\n\n\nA complete example:\ngen-ortho -n 9 -s 1 -e 1 -c -p\n\n\n\n\nOutput\n\nDisplays the orthogonal functions as computer algebraic symbolic functions. An example a set of generated functions is shown below.\n\nphi_1(t) =  sqrt(x)\nphi_2(t) =  sqrt(6)*(5*x**(1/3) - 6*sqrt(x))/3\nphi_3(t) =  sqrt(2)*(21*x**(1/4) - 40*x**(1/3) + 20*sqrt(x))/2\nphi_4(t) =  sqrt(10)*(84*x**(1/5) - 210*x**(1/4) + 175*x**(1/3) - 50*sqrt(x))/5\nphi_5(t) =  sqrt(3)*(330*x**(1/6) - 1008*x**(1/5) + 1134*x**(1/4) - 560*x**(1/3) + 105*sqrt(x))/3\nphi_6(t) =  sqrt(14)*(1287*x**(1/7) - 4620*x**(1/6) + 6468*x**(1/5) - 4410*x**(1/4) + 1470*x**(1/3) - 196*sqrt(x))/7\nphi_7(t) =  5005*x**(1/8)/2 - 10296*x**(1/7) + 17160*x**(1/6) - 14784*x**(1/5) + 6930*x**(1/4) - 1680*x**(1/3) + 168*sqrt(x)\nphi_8(t) =  sqrt(2)*(19448*x**(1/9) - 90090*x**(1/8) + 173745*x**(1/7) - 180180*x**(1/6) + 108108*x**(1/5) - 37422*x**(1/4) + 6930*x**(1/3) - 540*sqrt(x))/3\nphi_9(t) =  sqrt(5)*(75582*x**(1/10) - 388960*x**(1/9) + 850850*x**(1/8) - 1029600*x**(1/7) + 750750*x**(1/6) - 336336*x**(1/5) + 90090*x**(1/4) - 13200*x**(1/3) + 825*sqrt(x))/5\n\n\nDisplays readable coefficients, \n and \n of the functions. For instance,\n\n  i      alpha_i                                    a_[ij]\n------  ----------   -----------------------------------------------------------------------\ni = 1:  +sqrt(2/2)   [1                                                                    ]\ni = 2:  -sqrt(2/3)   [6,   -5                                                              ]\ni = 3:  +sqrt(2/4)   [20,  -40,    21                                                      ]\ni = 4:  -sqrt(2/5)   [50,  -175,   210,   -84                                              ]\ni = 5:  +sqrt(2/6)   [105, -560,   1134,  -1008,   330                                     ]\ni = 6:  -sqrt(2/7)   [196, -1470,  4410,  -6468,   4620,   -1287                           ]\ni = 7:  +sqrt(2/8)   [336, -3360,  13860, -29568,  34320,  -20592,   5005                  ]\ni = 8:  -sqrt(2/9)   [540, -6930,  37422, -108108, 180180, -173745,  90090,  -19448        ]\ni = 9:  +sqrt(2/10)  [825, -13200, 90090, -336336, 750750, -1029600, 850850, -388960, 75582]\n\n\nDisplays the matrix of the mutual inner product of functions to check orthogonality (using option -c). An example of the generated matrix of the mutual inner product of functions is shown below.\n\n[[1 0 0 0 0 0 0 0 0]\n [0 1 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 0]\n [0 0 0 1 0 0 0 0 0]\n [0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 0 1 0 0]\n [0 0 0 0 0 0 0 1 0]\n [0 0 0 0 0 0 0 0 1]]\n\n\nPlots the set of functions (using option -p) and saves the plot in the current directory. An example of a generated plot is shown below.\n\n\n\nCitation\n\n\n[Ameli-2020]Ameli, S., and Shadden. S. C. (2020). Interpolating the Trace of the Inverse of Matrix A + t B. arXiv:2009.07385 [math.NA]\n\n\n@misc{AMELI-2020,\n    title={Interpolating the Trace of the Inverse of Matrix $\\mathbf{A} + t \\mathbf{B}$},\n    author={Siavash Ameli and Shawn C. Shadden},\n    year={2020},\n    month = sep,\n    eid = {arXiv:2009.07385},\n    eprint={2009.07385},\n    archivePrefix={arXiv},\n    primaryClass={math.NA},\n    howpublished={\\emph{arXiv}: 2009.07385 [math.NA]},\n}\n\n'], 'url_profile': 'https://github.com/ameli', 'info_list': ['Jupyter Notebook', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 5, 2020', '1', 'Python', 'MIT license', 'Updated Dec 18, 2020', '1', 'R', 'Updated Feb 27, 2021', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Dec 25, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['\n\n\nrisks\nInstallation\nThe development version of the risks package can be installed from\nGitHub using:\n# install.packages(""remotes"")  # The ""remotes"" package needs to be installed\nremotes::install_github(""stopsack/risks"")\nDocumentation\nFull documentation is available at the reference site for the risks\npackage.\n'], 'url_profile': 'https://github.com/stopsack', 'info_list': ['Jupyter Notebook', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 5, 2020', '1', 'Python', 'MIT license', 'Updated Dec 18, 2020', '1', 'R', 'Updated Feb 27, 2021', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Dec 25, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['W-GDipc\nPredicting antifreeze proteins with weighted generalized dipeptide composition and multi-regression feature selection ensemble\n'], 'url_profile': 'https://github.com/Xia-xinnan', 'info_list': ['Jupyter Notebook', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 5, 2020', '1', 'Python', 'MIT license', 'Updated Dec 18, 2020', '1', 'R', 'Updated Feb 27, 2021', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Dec 25, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Coronavirus-Big-Data\n\n\n\n\n\n\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/shayei', 'info_list': ['Jupyter Notebook', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 5, 2020', '1', 'Python', 'MIT license', 'Updated Dec 18, 2020', '1', 'R', 'Updated Feb 27, 2021', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Dec 25, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/samiksha-patil', 'info_list': ['Jupyter Notebook', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 5, 2020', '1', 'Python', 'MIT license', 'Updated Dec 18, 2020', '1', 'R', 'Updated Feb 27, 2021', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Dec 25, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['Spam_Classifier\nDetecting spam and ham messages using Naive Bayes, Logistic Regression and CatBoost.\n'], 'url_profile': 'https://github.com/danielleeco', 'info_list': ['Jupyter Notebook', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 5, 2020', '1', 'Python', 'MIT license', 'Updated Dec 18, 2020', '1', 'R', 'Updated Feb 27, 2021', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Dec 25, 2020', 'Python', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tziojo', 'info_list': ['Jupyter Notebook', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated May 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 5, 2020', '1', 'Python', 'MIT license', 'Updated Dec 18, 2020', '1', 'R', 'Updated Feb 27, 2021', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Dec 25, 2020', 'Python', 'Updated May 10, 2020']}"
"{'location': 'New York, NY', 'stats_list': [], 'contributions': '289 contributions\n        in the last year', 'description': ['IrisSpeciesClassification-LogisticRegression\nIn this project, I implemented a logistic regression model to classify iris species based on their features\n'], 'url_profile': 'https://github.com/cierrajohnsoncarter', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 6, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Kolkata, India', 'stats_list': [], 'contributions': '1,628 contributions\n        in the last year', 'description': ['Salary-Prediction-using-Linear-Regression\n'], 'url_profile': 'https://github.com/ritesh-chafer', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 6, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Cairo , Egypt', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Predicting-House-Prices-with-regression\nSolution for the House prices problem on kaggle\n'], 'url_profile': 'https://github.com/Mokashaa', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 6, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Caracas, Venezuela', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Covid-19 Madrid: Regression Analysis\nRegression Analysis\nRegression is an estimation model used when we want to predict the value of a dependent variable using independent variables.\nWe have then:\nY(X) where:\nY = Dependent value (what we are looking for)\nX = Independent value\nBasically, Y is a function of X, so we need to find the function to estimate propperly the parameter that we are looking for. In the context of the global pandemic: Covid-19, regression analysis can be helpful to determinate interesting parameters to fight against this dangerous virus.\nMadrid, as one of the most affected cities in the world by the Covid-19, have significant datasets to know more about the propagation of the virus and the consequences: as deaths, recovered patients, patients in ICU, and others. Find relations in between these variables, could help us to know which kind of actions could we apply to have a smaller mortality rate.\nTo analyze the data, I started with a Simple-Linear Regression analysis, providing the following results about the relation in between number of cases and number of deaths since Madrid have data to analyze:\nThe data was found here, and give data about all the autonomous communities of Spain, but you can select any other autonomous community of Spain to your analysis using the same code.\nThe data start since 2020-02-21 until 2020-05-03.\nSimple-Linear Regression\nSimple linear regression is used to estimate a dependent variable based on an independent variable. In this case the Number of deaths as function of the Number of cases.\nTo know if a Simple linear regression can be used, we can use scatter plots to know as a graphic, the relation between variables.\nThe Simple-linear regression can be expressed as this equation:\ny = m0 + m1*x1\nIn this equation, y correspond to the dependent variable that we are looking for, and x1 the independent variable. Our goal is to find the parameters m0 and m1 appropiated to find the line that fit better with the growth of the scatter plot.\nSo, we can do a scatter plot in python using the matplotlib.pyplot, with the values of Number of deaths vs Days, and the Number of Cases vs Days obtaining the following result:\n\n\nHow we can see, the behaviour of the both functions is today 2020-05-06 similar to a logarithmic behaviour, and stopped to be an exponential behaviour as the first 40-50 days.\nSo, to start the development of our model, we should start to select only the cases in Madrid, that is easy to do using the library Pandas.\nThis dataset (in this case I have two datasets) must to be divided in two datasets: the training dataset and the testing dataset, in this case I did a 80-20 division.\nThen, we can know if we can really use the simple-linear regression, plotting the relation in between Number of cases and Number of deaths:\n\nHow You can see, the behaviour is more or less linear, so Simple-Linear regression is not a bad option. Also we can calculate the correlation in between this variables, to detect if the model have a linear behaviour\nIn the scatter plot, You can also see, the points used to the training (red) and the points using for testing (green).\nFinally we can find the model, using the scikit-learn from python, is easy to load and build a model importing the linear_model from sklearn. Loading the data from the training set and applying the model we can obtain the parameters, that could provide to us an equation as following:\ny = -373.4907455 + 0.12483009*x\nAs the equation that we are looking for. Plotting this line in the last plot the results are the following\n\nNow we can estimate the accuracy of our regression, for this, we can implement R^2, Residual sum of squares (MSE), and Mean Absolute Error (MAE), also very easy to implement using scikit-learn.\nLoading the testing data to check the accuracy, we find results as this:\nR^2 = 0.98, where 1 is the best\nMAE = 337.63\nMSE = 166028.72\nYou can find the entire python code for this analysis here\n'], 'url_profile': 'https://github.com/Franzmgarcia03', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 6, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TejasSutar01', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 6, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Ankara', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/semihtasbas', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 6, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['decision-tree-model-for-regression and random forest\nPython | Decision Tree Regression using sklearn. Decision Tree is a decision-making tool that uses a flowchart-like tree structure or is a model of decisions and all of their possible results, including outcomes, input costs and utility. Decision-tree algorithm falls under the category of supervised learning algorithms ...\nA Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap Aggregation, commonly known as bagging.\n'], 'url_profile': 'https://github.com/sayantann11', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 6, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['TimeSeries_Regression_Ensembling_Model\nUse time series model and regression model on forecasting Bitcoin price and showcase the power of the combined model of the two\nData\n\nBitcoin data: generate from here\n\nApproach\n\n\nTime sereis model only\n\n\nTime sereis model + regression model\n(regression model > residuals > imput of arima model > combined results from the two to form final predictions)\n\n\nComparison of the two approachs\n\n\nIncluding visualization and arima model (p,d,q) selection\n\n\nShowcase the power of ensembling models, particular in forecasting analysis with apparent trends that can be identified by regressions (linear, poly, etc.). Very welcome for further discussions!\nYili Yu: td821211@gmail.com\n'], 'url_profile': 'https://github.com/yiliyu1211', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 6, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['Regression-Market-Analysis---Structure-Cabling\nAnalyze Structured Cabling Copper Market vs key macroeconomic indicators\nThe project is trying to figure out and analyze the corelation between structured cabling market and key macro economic indicators. For those who is not familiar with structured cabling, here is a good introduction to read https://en.wikipedia.org/wiki/Structured_cabling\nThe data is collected from multiple websites and sources\n\n\n\nIndicators\nSource\nDescription\nUnit\n\n\n\n\nStuctured Cabling Market Sales\nBSRIA Research\nmarket size of stuctured cabling for copper\n$ Million\n\n\nUS Commercial Construction\nUS Concensus Bureau\nUS construction spending\n$ Billion\n\n\nConstruction Spending (Private + Public)\nStatista.com\n***\n$ Billions\n\n\nReal GDP\nthebalance.com\n***\n$ Trillions\n\n\nGDP Growth Rate\nthebalance.com\n***\n% percentage\n\n\nCopper Commodity Price\nfred.stlouisfed.org\nglobal copper price\nMetric Tons\n\n\n\nConclusionÔºö\nBased on p_value output, top 3 indicators of strongest corelations are Copper Commodity Price, Unemployment, Construction Spending\n                                           coef    std err          t      P>|t|      [0.025      0.975]\n\n\nValue of U.S. commercial construction        1.3474      2.024      0.666      0.518      -3.063       5.758\nConstruction Spending (Private + Public)     0.7003      0.284      2.462      0.030       0.080       1.320\nReal GDP                                     6.2764     15.176      0.414      0.686     -26.790      39.342\nGDP Growth Rate                             14.2002      8.859      1.603      0.135      -5.101      33.501\nUnemployment Rate                           30.5962      7.970      3.839      0.002      13.232      47.960\nCopper Commodity Price                       0.0442      0.006      6.911      0.000       0.030       0.058\n'], 'url_profile': 'https://github.com/ShawnLiu119', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 6, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Beijing, China', 'stats_list': [], 'contributions': '368 contributions\n        in the last year', 'description': [""linear-regression-for-score-prediction\n‰ΩøÁî®Â§öÂÖÉÁ∫øÊÄßÂõûÂΩíÊ®°ÂûãÈ¢ÑÊµãÂ≠¶ÁîüÊàêÁª©ÔºåÂàÜÂà´Âú®ËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ‰∏äÂ±ïÁ§∫ÊïàÊûú„ÄÇThe multiple linear regression model was used to predict students' score, and the results were shown in training set and test set respectively.\n""], 'url_profile': 'https://github.com/stxupengyu', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 6, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Python', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020']}"
"{'location': 'ILORIN,NIGERIA', 'stats_list': [], 'contributions': '181 contributions\n        in the last year', 'description': [""ADMISSION-PREDICTION\nThis dataset was built with the purpose of helping students in shortlisting universities with their profiles and it contains several parameters that are considered important during the application for master programs.\nThe predicted output gives them a far idea about their chances for a particular university.\nAfter analyzing the dataset, the linear regression remain the best model, there is a linear relationship between the target and most independent variables and I made sure some of the assumptions of linear regression were respected and the CGPA is the most important score to get admitted.\nFor master program,most university requires students mostly international students to take some test like the GRE test which is a graduate entrance exam, the TOEFL test which is an english language skills and eventually the Cumulative Grade Point Average (CGPA) to evaluate the student's competence.\nHaving a good score in these tests enhance the chance of getting admitted.\nHowever, this dataset's target the chance of getting admitted is ranged from 0 to 1, and transforming the target data and removing some outliers provide an amazing improvement in the model performance.\nFrom the k-fold cross validation standard deviation result, this model perfom well but it can still perfom better maybe with other advanced model such as XGBOOST, Lasso and Ridge regression.\nThank you!\n""], 'url_profile': 'https://github.com/memudualimatou', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Davanagere ,Karanataka ', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vinodrsrs', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RavikanthCh', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '167 contributions\n        in the last year', 'description': ['House_Prices_Advanced_Regression_Techniques\nkaggle: House_Prices_Advanced_Regression_Techniques\n'], 'url_profile': 'https://github.com/yuks0810', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Roorkee, Uttrakhand-247667, India', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/iammmk', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Pune, Maharashtra', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Types-of-Regression-in-ML\nRegression is a technique used to model and analyze the relationships between variables and often times how they contribute and are related to producing a particular outcome together.\nThis repository consist of four common types of regression used in machine learning.\n\nLinear Regression\nPolynomial Regression\nMultiple Linear Regression\nLogistic Regression\n\n'], 'url_profile': 'https://github.com/Pooja-Shivale', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['01-Ames-House-Price-Regression\nPredicting House SalePrice for Ames Dataset from Kaggle\n'], 'url_profile': 'https://github.com/MansoorAB', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TejasSutar01', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['Titanic-Dataset-using-Logistic-Regression\n'], 'url_profile': 'https://github.com/varalakshmiarcot', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['house_prices_advanced_regression_techniques\n'], 'url_profile': 'https://github.com/ezekielolugbami', 'info_list': ['1', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}"
"{'location': 'Hyderabad', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RavikanthCh', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'MIT license', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 12, 2020']}","{'location': 'BSD, Greater Jakarta, Indonesia', 'stats_list': [], 'contributions': '148 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mnrclab', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'MIT license', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 12, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '219 contributions\n        in the last year', 'description': ['COVID-19 Regression Model (India)\nA polynomial regression model to predict the number of infections, recoveries and deaths on a particular day in the future. Currently predicion is based on the Indian dataset. Can be altered according to your prefered dataset. Steps for altering dataset will be present in the README file.\nChangelog\n\n\nv1.2.0\n\nUpdated main code with the changes in the case_time_series.csv dataset format.\nUpdated README.md.\n\n\n\nv1.1.0\n\nAdded auto-update for the dataset. Manual updation not required anymore.\nUpdated README.md.\n\n\n\nv1.0.1\n\nManually updated dataset.\nAdded README.md and LICENSE.\n\n\n\nv1.0.0\n\nProject created and uploaded onto GitHub\n\n\n\nSteps to run\n\n\nPython libraries to be installed to run the program\n\n\nNumPy\n\n\nMatPlotLib\n\n\nPandas\n\n\nSci-Kit Learn\n\n\n\n\nRun Covid19.py\nNOTE\nPlease enter a whole number when prompted to enter a Day i.e. 1,2,3...n.\nDo not enter the date. Please refer the dataset to understand better.\n\n\nAn older version of the dataset is provided for you to better understand the code.\n\n\nAltering the dataset\n\n\nThe current dataset updates automatially. No manual updation required. The link for the complete dataset is provided below. A huge thanks to covid19india.org for providing this awesome dataset.\nhttps://api.covid19india.org/csv/latest/case_time_series.csv\n\n\nIf you wish to add your own dataset, then change the link and alter the dataframe slicing accordingly.\n\n\nFeedback and forking is encouraged.\n\n\nLicense\n¬© MIT Licence\nCreated by Aryan Felix\n'], 'url_profile': 'https://github.com/AryanFelix', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'MIT license', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 12, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Maryam-Ah', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'MIT license', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 12, 2020']}","{'location': 'Rajkot, Gujarat, India', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SohamPatel46', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'MIT license', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abhijeetnagtode', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'MIT license', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 12, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Polynomial-Regression-with-Scikit-learn\nPolynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x).\nIn this notebook, i used scikit-learn library to solve this problem.\nI used some graphical representations to viualize our results.\n'], 'url_profile': 'https://github.com/mshouzebsaleem77', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'MIT license', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/EbruSomuncu', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'MIT license', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': [""House-Prices-Advanced-Regression-Technique\nCompetition Description\nCompetition Link:https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n1)Practice Skills 2)Creative feature engineering 3)Advanced regression techniques like random forest and gradient boosting\nGoal:\nIt is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable.\nMetric:\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\nAcknowledgments:\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.\n""], 'url_profile': 'https://github.com/DineshDhakar1997', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'MIT license', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 12, 2020']}","{'location': 'Phoenix, AZ', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': [""House-Prices-Advanced-Regression-Techniques\n\n\n\n\n\n\nGoal: Predict sales prices and practice feature engineering, RFs, and gradient boosting\nAcknowledgments\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.\nFile descriptions\n\ntrain.csv - the training set\ntest.csv - the test set\ndata_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\nsample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n\nData fields\nHere's a brief version of what you'll find in the data description file.\n\nSalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\nMSSubClass: The building class\nMSZoning: The general zoning classification\nLotFrontage: Linear feet of street connected to property\nLotArea: Lot size in square feet\nStreet: Type of road access\nAlley: Type of alley access\nLotShape: General shape of property\nLandContour: Flatness of the property\nUtilities: Type of utilities available\nLotConfig: Lot configuration\nLandSlope: Slope of property\nNeighborhood: Physical locations within Ames city limits\nCondition1: Proximity to main road or railroad\nCondition2: Proximity to main road or railroad (if a second is present)\nBldgType: Type of dwelling\nHouseStyle: Style of dwelling\nOverallQual: Overall material and finish quality\nOverallCond: Overall condition rating\nYearBuilt: Original construction date\nYearRemodAdd: Remodel date\nRoofStyle: Type of roof\nRoofMatl: Roof material\nExterior1st: Exterior covering on house\nExterior2nd: Exterior covering on house (if more than one material)\nMasVnrType: Masonry veneer type\nMasVnrArea: Masonry veneer area in square feet\nExterQual: Exterior material quality\nExterCond: Present condition of the material on the exterior\nFoundation: Type of foundation\nBsmtQual: Height of the basement\nBsmtCond: General condition of the basement\nBsmtExposure: Walkout or garden level basement walls\nBsmtFinType1: Quality of basement finished area\nBsmtFinSF1: Type 1 finished square feet\nBsmtFinType2: Quality of second finished area (if present)\nBsmtFinSF2: Type 2 finished square feet\nBsmtUnfSF: Unfinished square feet of basement area\nTotalBsmtSF: Total square feet of basement area\nHeating: Type of heating\nHeatingQC: Heating quality and condition\nCentralAir: Central air conditioning\nElectrical: Electrical system\n1stFlrSF: First Floor square feet\n2ndFlrSF: Second floor square feet\nLowQualFinSF: Low quality finished square feet (all floors)\nGrLivArea: Above grade (ground) living area square feet\nBsmtFullBath: Basement full bathrooms\nBsmtHalfBath: Basement half bathrooms\nFullBath: Full bathrooms above grade\nHalfBath: Half baths above grade\nBedroom: Number of bedrooms above basement level\nKitchen: Number of kitchens\nKitchenQual: Kitchen quality\nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\nFunctional: Home functionality rating\nFireplaces: Number of fireplaces\nFireplaceQu: Fireplace quality\nGarageType: Garage location\nGarageYrBlt: Year garage was built\nGarageFinish: Interior finish of the garage\nGarageCars: Size of garage in car capacity\nGarageArea: Size of garage in square feet\nGarageQual: Garage quality\nGarageCond: Garage condition\nPavedDrive: Paved driveway\nWoodDeckSF: Wood deck area in square feet\nOpenPorchSF: Open porch area in square feet\nEnclosedPorch: Enclosed porch area in square feet\n3SsnPorch: Three season porch area in square feet\nScreenPorch: Screen porch area in square feet\nPoolArea: Pool area in square feet\nPoolQC: Pool quality\nFence: Fence quality\nMiscFeature: Miscellaneous feature not covered in other categories\nMiscVal: $Value of miscellaneous feature\nMoSold: Month Sold\nYrSold: Year Sold\nSaleType: Type of sale\nSaleCondition: Condition of sale\n\n""], 'url_profile': 'https://github.com/suroor89', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 12, 2020', 'Python', 'MIT license', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 12, 2020']}"
"{'location': 'Germany', 'stats_list': [], 'contributions': '342 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vikasbhadoria69', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 13, 2020', 'SAS', 'Updated May 8, 2020']}","{'location': 'Alexandria,Egypt', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Predicting House Prices with Regression\nThis model is a project of Predicting House Prices with Regression using TensorFlow course which is a 2-hour long project-based course\nyou will learn the basics of using Keras with TensorFlow as its backend and you will learn to use it to solve a basic regression problem. By the end of this project, you will have created, trained, and evaluated a neural network model that, after the training, will be able to predict house prices with a high degree of accuracy.\nMy Course Certificate\n\n'], 'url_profile': 'https://github.com/AsmaaEssamSultan', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 13, 2020', 'SAS', 'Updated May 8, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '259 contributions\n        in the last year', 'description': ['Linear-Regression-Model-ML-Project\nA linear regression model that takes in any dataset and displays the result in the form of graphs, plots and performance in terms of accuracy and performance of the data. The results displayed are statistical analysis, dataset analysis and results of linear regression which can be saved.\n'], 'url_profile': 'https://github.com/nolita26', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 13, 2020', 'SAS', 'Updated May 8, 2020']}","{'location': 'Semarang, Indonesia', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['Logistic_Regression_Neural_Network_mindset\nWelcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize  cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning.\nInstructions:\n\nDo not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.\n\nYou will learn to:\n\nBuild the general architecture of a learning algorithm, including:\n\nInitializing parameters\nCalculating the cost function and its gradient\nUsing an optimization algorithm (gradient descent)\n\n\nGather all three functions above into a main model function, in the right order.\n\nUpdates\nThis notebook has been updated over the past few months.  The prior version was named ""v5"", and the current versionis now named \'6a\'\nIf you were working on a previous version:\n\nYou can find your prior work by looking in the file directory for the older files (named by version name).\nTo view the file directory, click on the ""Coursera"" icon in the top left corner of this notebook.\nPlease copy your work from the older versions to the new version, in order to submit your work for grading.\n\nList of Updates\n\nForward propagation formula, indexing now starts at 1 instead of 0.\nOptimization function comment now says ""print cost every 100 training iterations"" instead of ""examples"".\nFixed grammar in the comments.\nY_prediction_test variable name is used consistently.\nPlot\'s axis label now says ""iterations (hundred)"" instead of ""iterations"".\nWhen testing the model, the test image is normalized by dividing by 255.\n\n1 - Packages\nFirst, let\'s run the cell below to import all the packages that you will need during this assignment.\n\nnumpy is the fundamental package for scientific computing with Python.\nh5py is a common package to interact with a dataset that is stored on an H5 file.\nmatplotlib is a famous library to plot graphs in Python.\nPIL and scipy are used here to test your model with your own picture at the end.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom lr_utils import load_dataset\n\n%matplotlib inline\n2 - Overview of the Problem set\nProblem Statement: You are given a dataset (""data.h5"") containing:\n- a training set of m_train images labeled as cat (y=1) or non-cat (y=0)\n- a test set of m_test images labeled as cat or non-cat\n- each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\nYou will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\nLet\'s get more familiar with the dataset. Load the data by running the following code.\n# Loading the data (cat/non-cat)\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\nWe added ""_orig"" at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don\'t need any preprocessing).\nEach line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images.\n# Example of a picture\nindex = 20\nplt.imshow(train_set_x_orig[index])\nprint (""y = "" + str(train_set_y[:, index]) + "", it\'s a \'"" + classes[np.squeeze(train_set_y[:, index])].decode(""utf-8"") +  ""\' picture."")\ny = [0], it\'s a \'non-cat\' picture.\n\n\nMany software bugs in deep learning come from having matrix/vector dimensions that don\'t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.\nExercise: Find the values for:\n- m_train (number of training examples)\n- m_test (number of test examples)\n- num_px (= height = width of a training image)\nRemember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0].\n### START CODE HERE ### (‚âà 3 lines of code)\nm_train = train_set_x_orig.shape[0]\nm_test = test_set_x_orig.shape[0]\nnum_px = train_set_x_orig.shape[1]\n### END CODE HERE ###\n\nprint (""Number of training examples: m_train = "" + str(m_train))\nprint (""Number of testing examples: m_test = "" + str(m_test))\nprint (""Height/Width of each image: num_px = "" + str(num_px))\nprint (""Each image is of size: ("" + str(num_px) + "", "" + str(num_px) + "", 3)"")\nprint (""train_set_x shape: "" + str(train_set_x_orig.shape))\nprint (""train_set_y shape: "" + str(train_set_y.shape))\nprint (""test_set_x shape: "" + str(test_set_x_orig.shape))\nprint (""test_set_y shape: "" + str(test_set_y.shape))\nNumber of training examples: m_train = 209\nNumber of testing examples: m_test = 50\nHeight/Width of each image: num_px = 64\nEach image is of size: (64, 64, 3)\ntrain_set_x shape: (209, 64, 64, 3)\ntrain_set_y shape: (1, 209)\ntest_set_x shape: (50, 64, 64, 3)\ntest_set_y shape: (1, 50)\n\nExpected Output for m_train, m_test and num_px:\n\n\n**m_train**\n 209 \n\n\n**m_test**\n 50 \n\n\n**num_px**\n 64 \n\n\nFor convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $$ num_px $$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\nExercise: Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px $$ num_px $$ 3, 1).\nA trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$$c$$d, a) is to use:\nX_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n# Reshape the training and test examples\n\n### START CODE HERE ### (‚âà 2 lines of code)\ntrain_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\ntest_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n### END CODE HERE ###\n\nprint (""train_set_x_flatten shape: "" + str(train_set_x_flatten.shape))\nprint (""train_set_y shape: "" + str(train_set_y.shape))\nprint (""test_set_x_flatten shape: "" + str(test_set_x_flatten.shape))\nprint (""test_set_y shape: "" + str(test_set_y.shape))\nprint (""sanity check after reshaping: "" + str(train_set_x_flatten[0:5,0]))\ntrain_set_x_flatten shape: (12288, 209)\ntrain_set_y shape: (1, 209)\ntest_set_x_flatten shape: (12288, 50)\ntest_set_y shape: (1, 50)\nsanity check after reshaping: [17 31 56 22 33]\n\nExpected Output:\n\n\n**train_set_x_flatten shape**\n (12288, 209)\n\n\n**train_set_y shape**\n(1, 209)\n\n\n**test_set_x_flatten shape**\n(12288, 50)\n\n\n**test_set_y shape**\n(1, 50)\n\n\n**sanity check after reshaping**\n[17 31 56 22 33]\n\n\nTo represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\nOne common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\nLet\'s standardize our dataset.\ntrain_set_x = train_set_x_flatten/255.\ntest_set_x = test_set_x_flatten/255.\n\n**What you need to remember:**\nCommon steps for pre-processing a new dataset are:\n\nFigure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)\nReshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)\n""Standardize"" the data\n\n3 - General Architecture of the learning algorithm\nIt\'s time to design a simple algorithm to distinguish cat images from non-cat images.\nYou will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network!\n\nMathematical expression of the algorithm:\nFor one example $x^{(i)}$:\n$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$\n$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\nThe cost is then computed by summing over all training examples:\n$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\nKey steps:\nIn this exercise, you will carry out the following steps:\n- Initialize the parameters of the model\n- Learn the parameters for the model by minimizing the cost\n- Use the learned parameters to make predictions (on the test set)\n- Analyse the results and conclude\n4 - Building the parts of our algorithm\nThe main steps for building a Neural Network are:\n\nDefine the model structure (such as number of input features)\nInitialize the model\'s parameters\nLoop:\n\nCalculate current loss (forward propagation)\nCalculate current gradient (backward propagation)\nUpdate parameters (gradient descent)\n\n\n\nYou often build 1-3 separately and integrate them into one function we call model().\n4.1 - Helper functions\nExercise: Using your code from ""Python Basics"", implement sigmoid(). As you\'ve seen in the figure above, you need to compute $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ to make predictions. Use np.exp().\n# GRADED FUNCTION: sigmoid\n\ndef sigmoid(z):\n    """"""\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    """"""\n\n    ### START CODE HERE ### (‚âà 1 line of code)\n    s = 1/(1+np.exp(-z))\n    ### END CODE HERE ###\n    \n    return s\nprint (""sigmoid([0, 2]) = "" + str(sigmoid(np.array([0,2]))))\nsigmoid([0, 2]) = [ 0.5         0.88079708]\n\nExpected Output:\n\n\n**sigmoid([0, 2])**\n [ 0.5         0.88079708]\n\n\n4.2 - Initializing parameters\nExercise: Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don\'t know what numpy function to use, look up np.zeros() in the Numpy library\'s documentation.\n# GRADED FUNCTION: initialize_with_zeros\n\ndef initialize_with_zeros(dim):\n    """"""\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias)\n    """"""\n    \n    ### START CODE HERE ### (‚âà 1 line of code)\n    w = np.zeros((dim,1))\n    b = 0\n    ### END CODE HERE ###\n\n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b\ndim = 2\nw, b = initialize_with_zeros(dim)\nprint (""w = "" + str(w))\nprint (""b = "" + str(b))\nw = [[ 0.]\n [ 0.]]\nb = 0\n\nExpected Output:\n\n\n  ** w **  \n [[ 0.]\n [ 0.]] \n\n\n  ** b **  \n 0 \n\n\nFor image inputs, w will be of shape (num_px $\\times$ num_px $\\times$ 3, 1).\n4.3 - Forward and Backward propagation\nNow that your parameters are initialized, you can do the ""forward"" and ""backward"" propagation steps for learning the parameters.\nExercise: Implement a function propagate() that computes the cost function and its gradient.\nHints:\nForward Propagation:\n\nYou get X\nYou compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\nYou calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n\nHere are the two formulas you will be using:\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n# GRADED FUNCTION: propagate\n\ndef propagate(w, b, X, Y):\n    """"""\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true ""label"" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    """"""\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### (‚âà 2 lines of code)\n    A = sigmoid(np.dot(w.T,X)+b)                                    # compute activation\n    cost = 1./(-m)*np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))                                 # compute cost\n    ### END CODE HERE ###\n    \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### (‚âà 2 lines of code)\n    dw = 1./m * np.dot(X, (A-Y).T)\n    db = 1./m * np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {""dw"": dw,\n             ""db"": db}\n    \n    return grads, cost\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\ngrads, cost = propagate(w, b, X, Y)\nprint (""dw = "" + str(grads[""dw""]))\nprint (""db = "" + str(grads[""db""]))\nprint (""cost = "" + str(cost))\ndw = [[ 0.99845601]\n [ 2.39507239]]\ndb = 0.00145557813678\ncost = 5.80154531939\n\nExpected Output:\n\n\n  ** dw **  \n [[ 0.99845601]\n     [ 2.39507239]]\n\n\n  ** db **  \n 0.00145557813678 \n\n\n  ** cost **  \n 5.801545319394553 \n\n\n4.4 - Optimization\n\nYou have initialized your parameters.\nYou are also able to compute a cost function and its gradient.\nNow, you want to update the parameters using gradient descent.\n\nExercise: Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate.\n# GRADED FUNCTION: optimize\n\ndef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    """"""\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true ""label"" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    """"""\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        \n        # Cost and gradient calculation (‚âà 1-4 lines of code)\n        ### START CODE HERE ### \n        grads, cost = propagate(w,b,X,Y)\n        ### END CODE HERE ###\n        \n        # Retrieve derivatives from grads\n        dw = grads[""dw""]\n        db = grads[""db""]\n        \n        # update rule (‚âà 2 lines of code)\n        ### START CODE HERE ###\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        ### END CODE HERE ###\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print (""Cost after iteration %i: %f"" %(i, cost))\n    \n    params = {""w"": w,\n              ""b"": b}\n    \n    grads = {""dw"": dw,\n             ""db"": db}\n    \n    return params, grads, costs\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint (""w = "" + str(params[""w""]))\nprint (""b = "" + str(params[""b""]))\nprint (""dw = "" + str(grads[""dw""]))\nprint (""db = "" + str(grads[""db""]))\nw = [[ 0.19033591]\n [ 0.12259159]]\nb = 1.92535983008\ndw = [[ 0.67752042]\n [ 1.41625495]]\ndb = 0.219194504541\n\nExpected Output:\n<tr>\n   <td> **b** </td>\n   <td> 1.92535983008 </td>\n</tr>\n<tr>\n   <td> **dw** </td>\n   <td> [[ 0.67752042]\n[ 1.41625495]] \n\n\n\n **w** \n[[ 0.19033591]\n [ 0.12259159]] \n\n\n db \n 0.219194504541 \n\n\nExercise: The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function. There are two steps to computing predictions:\n\n\nCalculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n\n\nConvert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this).\n\n\n# GRADED FUNCTION: predict\n\ndef predict(w, b, X):\n    \'\'\'\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    \'\'\'\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector ""A"" predicting the probabilities of a cat being present in the picture\n    ### START CODE HERE ### (‚âà 1 line of code)\n    A = sigmoid(np.dot(w.T, X)+b)\n    ### END CODE HERE ###\n    \n    for i in range(A.shape[1]):\n        \n        # Convert probabilities A[0,i] to actual predictions p[0,i]\n        ### START CODE HERE ### (‚âà 4 lines of code)\n        Y_prediction[0,i] = 1 if A[0,i] > 0.5 else 0\n        ### END CODE HERE ###\n    \n    assert(Y_prediction.shape == (1, m))\n    \n    return Y_prediction\nw = np.array([[0.1124579],[0.23106775]])\nb = -0.3\nX = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\nprint (""predictions = "" + str(predict(w, b, X)))\npredictions = [[ 1.  1.  0.]]\n\nExpected Output:\n\n\n\n             **predictions**\n         \n\n            [[ 1.  1.  0.]]\n         \n\n\n\n**What to remember:**\nYou\'ve implemented several functions that:\n- Initialize (w,b)\n- Optimize the loss iteratively to learn parameters (w,b):\n    - computing the cost and its gradient \n    - updating the parameters using gradient descent\n- Use the learned (w,b) to predict the labels for a given set of examples\n5 - Merge all functions into a model\nYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\nExercise: Implement the model function. Use the following notation:\n- Y_prediction_test for your predictions on the test set\n- Y_prediction_train for your predictions on the train set\n- w, costs, grads for the outputs of optimize()\n# GRADED FUNCTION: model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    """"""\n    Builds the logistic regression model by calling the function you\'ve implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    """"""\n    \n    ### START CODE HERE ###\n    \n    # initialize parameters with zeros (‚âà 1 line of code)\n    w, b = initialize_with_zeros(X_train.shape[0])\n\n    # Gradient descent (‚âà 1 line of code)\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n    \n    # Retrieve parameters w and b from dictionary ""parameters""\n    w = parameters[""w""]\n    b = parameters[""b""]\n    \n    # Predict test/train set examples (‚âà 2 lines of code)\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(""train accuracy: {} %"".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(""test accuracy: {} %"".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {""costs"": costs,\n         ""Y_prediction_test"": Y_prediction_test, \n         ""Y_prediction_train"" : Y_prediction_train, \n         ""w"" : w, \n         ""b"" : b,\n         ""learning_rate"" : learning_rate,\n         ""num_iterations"": num_iterations}\n    \n    return d\nRun the following cell to train your model.\nd = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)\nCost after iteration 0: 0.693147\nCost after iteration 100: 0.584508\nCost after iteration 200: 0.466949\nCost after iteration 300: 0.376007\nCost after iteration 400: 0.331463\nCost after iteration 500: 0.303273\nCost after iteration 600: 0.279880\nCost after iteration 700: 0.260042\nCost after iteration 800: 0.242941\nCost after iteration 900: 0.228004\nCost after iteration 1000: 0.214820\nCost after iteration 1100: 0.203078\nCost after iteration 1200: 0.192544\nCost after iteration 1300: 0.183033\nCost after iteration 1400: 0.174399\nCost after iteration 1500: 0.166521\nCost after iteration 1600: 0.159305\nCost after iteration 1700: 0.152667\nCost after iteration 1800: 0.146542\nCost after iteration 1900: 0.140872\ntrain accuracy: 99.04306220095694 %\ntest accuracy: 70.0 %\n\nExpected Output:\n<tr>\n    <td> **Cost after iteration 0 **  </td> \n    <td> 0.693147 </td>\n</tr>\n  <tr>\n    <td> <center> $\\vdots$ </center> </td> \n    <td> <center> $\\vdots$ </center> </td> \n</tr>  \n<tr>\n    <td> **Train Accuracy**  </td> \n    <td> 99.04306220095694 % </td>\n</tr>\n\n<tr>\n    <td>**Test Accuracy** </td> \n    <td> 70.0 % </td>\n</tr>\n\n\nComment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you\'ll build an even better classifier next week!\nAlso, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the index variable) you can look at predictions on pictures of the test set.\n# Example of a picture that was wrongly classified.\nindex = 21\nplt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\nprint (""y = "" + str(test_set_y[0,index]) + "", you predicted that it is a \\"""" + classes[d[""Y_prediction_test""][0,index]].decode(""utf-8"") +  ""\\"" picture."")\ny = 0, you predicted that it is a ""non-cat"" picture.\n\n\nLet\'s also plot the cost function and the gradients.\n# Plot learning curve (with costs)\ncosts = np.squeeze(d[\'costs\'])\nplt.plot(costs)\nplt.ylabel(\'cost\')\nplt.xlabel(\'iterations (per hundreds)\')\nplt.title(""Learning rate ="" + str(d[""learning_rate""]))\nplt.show()\n\nInterpretation:\nYou can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.\n6 - Further analysis (optional/ungraded exercise)\nCongratulations on building your first image classification model. Let\'s analyze it further, and examine possible choices for the learning rate $\\alpha$.\nChoice of learning rate\nReminder:\nIn order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may ""overshoot"" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That\'s why it is crucial to use a well-tuned learning rate.\nLet\'s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens.\nlearning_rates = [0.01, 0.001, 0.0001]\nmodels = {}\nfor i in learning_rates:\n    print (""learning rate is: "" + str(i))\n    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n    print (\'\\n\' + ""-------------------------------------------------------"" + \'\\n\')\n\nfor i in learning_rates:\n    plt.plot(np.squeeze(models[str(i)][""costs""]), label= str(models[str(i)][""learning_rate""]))\n\nplt.ylabel(\'cost\')\nplt.xlabel(\'iterations (hundreds)\')\n\nlegend = plt.legend(loc=\'upper center\', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor(\'0.90\')\nplt.show()\nlearning rate is: 0.01\ntrain accuracy: 99.52153110047847 %\ntest accuracy: 68.0 %\n\n-------------------------------------------------------\n\nlearning rate is: 0.001\ntrain accuracy: 88.99521531100478 %\ntest accuracy: 64.0 %\n\n-------------------------------------------------------\n\nlearning rate is: 0.0001\ntrain accuracy: 68.42105263157895 %\ntest accuracy: 36.0 %\n\n-------------------------------------------------------\n\n\nInterpretation:\n\nDifferent learning rates give different costs and thus different predictions results.\nIf the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost).\nA lower cost doesn\'t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.\nIn deep learning, we usually recommend that you:\n\nChoose the learning rate that better minimizes the cost function.\nIf your model overfits, use other techniques to reduce overfitting. (We\'ll talk about this in later videos.)\n\n\n\n7 - Test with your own image (optional/ungraded exercise)\nCongratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:\n1. Click on ""File"" in the upper bar of this notebook, then click ""Open"" to go on your Coursera Hub.\n2. Add your image to this Jupyter Notebook\'s directory, in the ""images"" folder\n3. Change your image\'s name in the following code\n4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!\n## START CODE HERE ## (PUT YOUR IMAGE NAME) \nmy_image = ""Denny.jpg""   # change this to the name of your image file \n## END CODE HERE ##\n\n# We preprocess the image to fit your algorithm.\nfname = ""images/"" + my_image\nimage = np.array(ndimage.imread(fname, flatten=False))\nimage = image/255.\nmy_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\nmy_predicted_image = predict(d[""w""], d[""b""], my_image)\n\nplt.imshow(image)\nprint(""y = "" + str(np.squeeze(my_predicted_image)) + "", your algorithm predicts a \\"""" + classes[int(np.squeeze(my_predicted_image)),].decode(""utf-8"") +  ""\\"" picture."")\ny = 0.0, your algorithm predicts a ""non-cat"" picture.\n\n\n\n**What to remember from this assignment:**\n1. Preprocessing the dataset is important.\n2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().\n3. Tuning the learning rate (which is an example of a ""hyperparameter"") can make a big difference to the algorithm. You will see more examples of this later in this course!\nFinally, if you\'d like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:\n- Play with the learning rate and the number of iterations\n- Try different initialization methods and compare the results\n- Test other preprocessings (center the data, or divide each row by its standard deviation)\nBibliography:\n\nhttp://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\nhttps://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c\n\n'], 'url_profile': 'https://github.com/Denysetya1', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 13, 2020', 'SAS', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Regression-predicting-house-prices\nI will be exploring the housing sale prices in King County, USA between the time period May 2014 - May 2015.\nFirstly, I will go through a thorough data exploration to identify most important features and to explore the intercorrelation between features. After that I apply data normalization between varialbes and conduct feature engineering.\n'], 'url_profile': 'https://github.com/SaeedHasanBD', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 13, 2020', 'SAS', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Majd2404', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 13, 2020', 'SAS', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7,853 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NumtraCG', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 13, 2020', 'SAS', 'Updated May 8, 2020']}","{'location': 'Austin, TX', 'stats_list': [], 'contributions': '803 contributions\n        in the last year', 'description': [""Machine Learning Homework Requirements:\nHello! Welcome to the famous Tsukiji fish market of Tokyo, Japan! We came here to collect data on some of the fish they have here\nbut we didn't wake up at 5am for the tuna auction and by the time we showed up they were only left with a few species of fish.\nWe got to work and gathered measurements from a few different species of fish and want you to train a regression model to predict\nthe weight of a fish using some of the features we were able to measure. We have no idea which features will be good predictors.\nWe will hold out 30% of the data before we hand it to you and we will use that csv for scoring.\nHere's what we need from you:\n\nA function that accepts a csv path and returns the predictions of your regression model using our csv. The csv we use will contain all the columns.\nUse a pipenv and scikit learn to submit the final hw. You may use R for model selection.\n\nWith this function and it's output, we will rank the students by how well their model performed on predicting weight based on naive data.\nYour grade will be determined by ranking according to Mean-squared Error.\nIf your function does not return a list of predictions or we cannot compute the accuracy of your model that it will be an automatic F.\n""], 'url_profile': 'https://github.com/sheetalbongale', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 13, 2020', 'SAS', 'Updated May 8, 2020']}","{'location': 'Berkeley California', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': [""Web Differ\n\nFound the article. Kept tab open for a couple days\nDidn't like the node shim and prefer ts so used ts-node\nAdded types\nWanted visual output not objects for diffs, found pixelmatch\nMaking a requirements list\n\nRequirements\n1.0\n\nLogs into console, stores image for 1 page, does diff\n\nReference\nOriginal Article & Dependent Modules\n\nhttps://blog.bitsrc.io/monitor-visual-changes-in-websites-with-puppeteer-and-resemble-js-811437593659\nhttps://github.com/rsmbl/Resemble.js\n\nDev Articles\n\nhttps://github.com/terkelg/prompts\nhttps://help.apify.com/en/articles/1640711-how-to-log-in-to-a-website-using-puppeteer\nhttps://github.com/mapbox/pixelmatch\nhttps://github.com/yahoo/blink-diff\n[D3 & React(https://dev.to/benjaminadk/basketball-stats-through-d3-react-4m10)\n\n""], 'url_profile': 'https://github.com/elvezpablo', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 13, 2020', 'SAS', 'Updated May 8, 2020']}","{'location': 'Dallas', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['SAS (Statistical Analysis System) Project\nGoal of the Project:\nThe goal of the project was to analyze the given information and present the insights along with recommendations to improve sales of the coffee brand called ‚ÄúHills Brothers‚Äù. The interpretations have been drawn by running various models taught in class on the provided data.\nObjective of the project:\n\nAnalyze the effect of various promotion methods using Panel regression.\nAnalyze the effect of change in sales to the change in price for the brand itself (self-price elasticity) and based on the competitor‚Äôs price (cross-price elasticity) and analyze the effect of brand specific and customer specific variables on the choice of Hills brother brand.\nForecasting the sales of Hills Brothers brand for the successive 20 weeks by using Time Series Analysis Method.\n\n'], 'url_profile': 'https://github.com/taniyadhar', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'HTML', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 14, 2020', 'TypeScript', 'Updated May 13, 2020', 'SAS', 'Updated May 8, 2020']}"
"{'location': 'NYC', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['National Park Trails Linear_Regression_Proj\nThe project was to predict the usage(popularity) of National Park Trails given some trail parameters. EDA concluded length, elevation, and type were important criteria in a trails popularity. This repo contains this readme, the final notebook and the csv file of my data. The Final_NB contains my EDA and regression analysis. There is also a presentation.pdf which is a slide summary highlighting key points of my analysis.\n'], 'url_profile': 'https://github.com/halujeff5', 'info_list': ['Jupyter Notebook', 'Updated Nov 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'pune', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['House-price-predication-using-linear-regression-algorithm\n'], 'url_profile': 'https://github.com/shubhampatil22', 'info_list': ['Jupyter Notebook', 'Updated Nov 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/walexbarnes', 'info_list': ['Jupyter Notebook', 'Updated Nov 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Asia95', 'info_list': ['Jupyter Notebook', 'Updated Nov 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Logistic_Regression_with_a_Neural_Network_mindset_\n'], 'url_profile': 'https://github.com/rajmgs', 'info_list': ['Jupyter Notebook', 'Updated Nov 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': ' Australia', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShashankaRangi', 'info_list': ['Jupyter Notebook', 'Updated Nov 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'BSD, Greater Jakarta, Indonesia', 'stats_list': [], 'contributions': '148 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mnrclab', 'info_list': ['Jupyter Notebook', 'Updated Nov 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Quebec City, Quebec, Canada', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Modelling fish physico-thermal habitat selection using functional regression üêü\n\nA scientific research by Jeremie Boudreault, Fateh Chebana, Andr√© St-Hilaire and Normand Bergeron\nThis project is part of my master degree in water sciences at Institut National de la Recherche Scientifique (INRS)\nAll codes and data are made available here under the Creative Common License \nQuestions regarding the code or the data should be sent to Jeremie.Boudreault@ete.inrs.ca\n\nData\nData are from field survey that have been conducted during summer 2017 on the  Sainte-Marguerite river (SMR) :\n\ndata/field/* : contains the raw .xlsx file filled after each day of field work\ndata/* : contains the cleaned and transformed datasets\n\nR codes\nCodes are all from Jeremie Boudreault. They use the R package mgcv to fit generalized additive models (GAM) and of FDboost to fit functional regression models (FRM) :\n\nR/Data_initial_cleaning.R : code to clean the field data spreadsheets and produce more adapted datasets\nR/Data_salmons_lengths.R: code to convert the salmon lengths to number of fry and parr\nR/Data_per_site.R : code to produce the observations at each site (mean value or functional observations)\nR/GAMs_all.R : code to fit several types of GLM/GAM on the data using the mgcv package\nR/GAMs_best.R : among all models, do variable selection to find the best GAM models and save them to out/models\nR/GAMs_predictions : code to calculate the leave-one-out predictions for the GAMs\nR/FRMs_all.R : code to fit several types of FRM on the data using the FDboost package\nR/FRMs_best.R : among all models, select the best FRM models and save them to out/models\nR/FRMs_predictions : code to calculate the leave-one-out predictions for the FRMs\nR/Models_coefficients.R : code to extract the coefficients of the best models\nR/Models_performance.R : compare the results between GAMs and FRMs\n\nOut\nA folder for the results at each part of the coding process :\n\nout/data visualisation/* : raw data visualisation and tables\nout/models/* : fitted final models\nout/coefficients/* : coefficients of the models\nout/predictions/* : leave-one-out predictions and goodness-of-fit\n\n'], 'url_profile': 'https://github.com/jeremieboudreault', 'info_list': ['Jupyter Notebook', 'Updated Nov 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': [""House-Price-Prediction-using-Advanced-Regression\nThis is the solution for Kaggle's house price prediction project for which I have used regression analysis to predict the house prices.\nThis project is divided into two parts 1) Feature Engineering and 2) Model Training\nFor categorical data I have used pd.get_dummies to convert the categorical features into numeric features.\nFor Model training I have used XGBRegressor.\n""], 'url_profile': 'https://github.com/ShivaniGhadigaonkar', 'info_list': ['Jupyter Notebook', 'Updated Nov 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps ‚Äúdays‚Äù uniquely to ‚Äúnumber of sales‚Äù, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Nov 14, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Python', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'R', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression-On-Bill-Authentication-Data\nPython Integrated tools, PyCharm is used to implement the project.\n'], 'url_profile': 'https://github.com/Arifur-ratul', 'info_list': ['1', 'Python', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sean83523', 'info_list': ['1', 'Python', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Logistic-Regression-for-predicting-reactor-operation\nPredicting whether the reactor will operate or fail under the corresponding operating conditions.\nApplied Logistic Regression (Supervised learning model) on given Dataset. This data set describes operating conditions of a reactor and contains class labels\nabout whether the reactor will operate or fail under those operating conditions\n'], 'url_profile': 'https://github.com/shreyanema10', 'info_list': ['1', 'Python', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Linear Regression with NumPy and Python\nIn this project, I am going to focus on three learning objectives:\n\nImplement the gradient descent algorithm from scratch.\nPerform univariate linear regression with Numpy and Python.\nCreate data visualizations and plots using matplotlib.\n\nBy the end of this project, I was able to build linear regression models from scratch using NumPy and Python, without the use of machine learning frameworks such as scikit-learn and statsmodels.\nThe project on Linear Regression with NumPy and Python was divided into the following tasks:\n\n\nTask 1: Introduction and Import Libraries\nIntroduction to the data set and the problem overview.\nImport essential modules and helper functions from NumPy and Matplotlib.\n\n\nTask 2: Load the Data and Libraries\nLoad the dataset using pandas.\nExplore the pandas dataframe using the head() and info() functions.\n\n\nTask 3: Visualize the Data\nUnderstand the data by visualizing it.\nFor this dataset, I will use a scatter plot using Seaborn to visualize the data, since it has only two variables: the         profit and population.\n\n\nTask 4: Compute the Cost ùêΩ(ùúÉ)\nA look at the machinery that powers linear regression: Gradient Descent.\nI want to fit the linear regression parameters ùúÉ to my dataset using gradient descent.\nThe objective of linear regression is to minimize the cost function J(ùúÉ).\nWe can think of the cost as the error my model made in estimating a value.\n\n\nTask 5: Implement Gradient Descent from scratch in Python\nThe parameters of my model are the ùúÉ_j values.\nThese are the values I will adjust to minimize the cost J(ùúÉ).\nOne way to do this is to use the batch gradient descent algorithm.\nIn batch gradient descent, each iteration performs the following update.\nWith each step of gradient descent, the parameters ùúÉ_j come closer to the optimal values that will achieve the lowest           cost J(ùúÉ).\n\n\nTask 6: Visualizing the Cost Function J(ùúÉ)\nTo better understand the cost function J(ùúÉ),I will plot the cost over a 2-dimensional grid of ùúÉ_0 and ùúÉ_1 values.\n\n\nTask 7: Plotting the Convergence\nPlotting how the cost function varies with the number of iterations.\nWhen I ran gradient descent previously, it returns the history of J(ùúÉ) values in a vector ‚Äúcosts‚Äù.\nI will now plot the J values against the number of iterations.\n\n\nTask 8: Training Data with Univariate Linear Regression Fit\nNow that I have correctly implemented and run gradient descent and arrived at the final parameters of my model, I    can use these parameters to plot the linear fit.\n\n\nTask 9: Inference using the optimized ùúÉ values\nIn this final task, I will use my final values for ùúÉ to make predictions on profits in cities of 35,000 and 70,000 people.\n\n\nCertificate details-\nName:Linear Regression with NumPy and Python\nIssuing Organization:Coursera\nIssue Date:May 2020\nExpiration Date:This certification does not expire\nCredential ID:N9EWX5M7FY76\nCredential URL:https://www.coursera.org/account/accomplishments/certificate/N9EWX5M7FY76\n'], 'url_profile': 'https://github.com/WhiteHatM', 'info_list': ['1', 'Python', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pranavseth', 'info_list': ['1', 'Python', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Please read project guidelines file.\n'], 'url_profile': 'https://github.com/Rockysaxena49', 'info_list': ['1', 'Python', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dimasuwandi', 'info_list': ['1', 'Python', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dimasuwandi', 'info_list': ['1', 'Python', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Uttar Pradesh', 'stats_list': [], 'contributions': '540 contributions\n        in the last year', 'description': ['Logistic-regression-on-adult-dataset\n'], 'url_profile': 'https://github.com/Shambhavi-Gupta', 'info_list': ['1', 'Python', 'Updated May 5, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', '1', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Multivariate-Linear-Regression---Boston-Housing-from-scratch\n'], 'url_profile': 'https://github.com/manish0718', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Nov 29, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': [""Bulldoze-price-prediction\nPredicting the Sale Price of Bulldozers using Machine Learning\nIn this notebook, we're going to go through an example machine learning with the goal of predicting the sale price of bulldozers.\n\n\nProblem Defition\nHow well can we predict the future sale price of a bulldozer, given its characteristics and previous examples of how much similar # bulldozers have been sold for?\n\n\nData\nThe data is dowloaded from the Kaggle Bluebook for Bulldozers competition:https://www.kaggle.com/c/bluebook-for-bulldozers/data\n\n\nThere are 3 main datasets:\nTrain.csv is the training set, which contains data through the end of 2011.\nValid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\nTest.csv is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November # 2012. Your score on the test set determines your final rank for the competition.\n3. Evaluation\nThe evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.\nFor more on the evaluation project check: https://www.kaggle.com/c/bluebook-for-bulldozers/overview/evaluation\nNote: The goal for most regression evaluation metrics it to minimize the error.For example, our goal for this project will be to built a # machine learning model which minimises the RMSLE.\n\nFeatures\nKaggle provides a data dictionary detailing all of the features of the dataset.\n\n""], 'url_profile': 'https://github.com/mzarop', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Nov 29, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Kaggle-HousePrices-AdvancedRegressionTechniques\n'], 'url_profile': 'https://github.com/fx757887', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Nov 29, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RenukaShelke', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Nov 29, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated May 9, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Simple-Linear-Regression-with-one-Variable\nA simple python program that implements Linear Regression on a sample dataset. The program uses sklearn.linear_model from the scikit-learn library to import the class LinearRegression.\nA graph is plotted using the matplotlib.pyplot to visually represent the Linear Regression model.\n'], 'url_profile': 'https://github.com/mshouzebsaleem77', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Nov 29, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated May 9, 2020']}","{'location': 'Richmond Va', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GoldinLocks', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Nov 29, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Data-Analysis-of-Kickstarter-Projects\n'], 'url_profile': 'https://github.com/Tanu25', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Nov 29, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['MLB-salary-prediction-using-mulitvariable-regression\nIn this project, I predicted the average amount of salary a MLB team needs to have to gain certain amount of success.\nThis project also helps to understand the correlation between the average salary, league and win percentage .\nFor this project, I have accessed some raw data on relating teams, salary, league , wins and losses in a season.\nThe source of the raw data is attached in the end of the page. The prediction process includes data cleaning, visualization, evaluation, correlation and finally regression\nThe project was created using HTML, CSS, Bootstrap, Flask and Python.\nFor this project I have used various libraries that include pandas, numpy, matplotlib, sklearn, and seaborn.\nwebsite: https://predict-max.herokuapp.com/\n'], 'url_profile': 'https://github.com/Adammanandhar', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Nov 29, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated May 9, 2020']}","{'location': 'chennai', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Linear-regression\nApplication of linear regression on college data in order to know the chances of getting admission in university\n'], 'url_profile': 'https://github.com/roopchandu', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Nov 29, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated May 9, 2020']}","{'location': 'Baltimore ', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Maryland-Crime-trend\nThis project is done in R using regression model to predict the crime rate in every county of Maryland.\nThe purpose of this project was to help government, policy or insurance makers to understand what rules or policies to make based on the crime rate\nin that county.\nThe data is obtained from open Maryland portal : https://opendata.maryland.gov/Public-Safety/Violent-Crime-Property-Crime-by-County-1975-to-Pre/jwfa-fdxs\nThe data is for 42 years from 1975 to 2017.\nThe project deals with cleaning of Data and then using the LMER (linear mixed effect regression model) on the data to predctcrime rate.\nThe equation crime_rate ~ Year + (Year | Jurisdiction) was used. The final result is a map which tells the percentage changes in every couny.\nRed color indicates percentage increase and sky blue decrease.\nSome of the visualizations are done in Tableau\n'], 'url_profile': 'https://github.com/nishigandha2131', 'info_list': ['Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Nov 29, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated May 9, 2020']}"
"{'location': 'India', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['boston-housing-master\nAn Machine Learning project to predict the prices of houses using the Boston Data Set Using Linear Regression.\n'], 'url_profile': 'https://github.com/AnIsH-Coder', 'info_list': ['Updated May 10, 2020', 'R', 'Updated Jun 15, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Samah-Abdelaal', 'info_list': ['Updated May 10, 2020', 'R', 'Updated Jun 15, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 5, 2020']}","{'location': 'bengaluru,india', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['PL-winner-predictor\ninitial days\nWinner predictor based of on the initial performance of the teams, players using regression and classification algorithms\nPL file\n\nchanges made in sub file of spiders \nparse function is used to scrape data and then store in JSON,XML files\n\nContributions required\n\nAny good link for PL scores\nAny good link for PL player stats\n\n'], 'url_profile': 'https://github.com/tantry7', 'info_list': ['Updated May 10, 2020', 'R', 'Updated Jun 15, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 5, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Saravana08', 'info_list': ['Updated May 10, 2020', 'R', 'Updated Jun 15, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '517 contributions\n        in the last year', 'description': ['Predicting Clinical Trial Attributes for Clinical Trial Study documents\nThis repo is part of a larger project in processing clinical trials for faster accumulated knowledge.\nIncluded in this Repo is the jupyter notebook that I have hosted on google Colab as well.\nhttps://colab.research.google.com/drive/1gBHqur01yYWaY3QDChLmC60ZM8ouBs7S\nCurrent Progress:\nCurrent Attributes Supported:\nSample Size\nSample Methodology\nMethod\nFuture Support:\nTopics/Drug/Condition Topic\nRecruiting Periods\nResearchers - in case not in Meta data for Author\nCountries Included\nItems Already in Meta:\nAuthors\nTitle\nCitations\n'], 'url_profile': 'https://github.com/Deamoner', 'info_list': ['Updated May 10, 2020', 'R', 'Updated Jun 15, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Graduate Admission Prediction\nThe dataset used is created for prediction of Graduate Admissions from an Indian perspective.\nThe dataset is acquired from kaggle ""GRADUATE ADMISSION 2"".\nCONTENT FOR THE DATABASE:\nThe dataset contains several parameters which are considered important during the application for Masters Programs.\nThe parameters included are :\n\nGRE Scores ( out of 340 )\nTOEFL Scores ( out of 120 )\nUniversity Rating ( out of 5 )\nStatement of Purpose and Letter of Recommendation Strength ( out of 5 )\nUndergraduate GPA ( out of 10 )\nResearch Experience ( either 0 or 1 )\nChance of Admit ( ranging from 0 to 1 )\n\nCITATION :\nMohan S Acharya, Asfia Armaan, Aneeta S Antony : A Comparison of Regression Models for Prediction of Graduate Admissions,\nIEEE International Conference on Computational Intelligence in Data Science 2019.\nThis dataset is inspired by the UCLA Graduate Dataset.\nThe test scores and GPA are in the older format.\nThe dataset is owned by Mohan S Acharya.\nMODEL :\nThe model is based on multiple Linear Regression\n'], 'url_profile': 'https://github.com/Beenaa99', 'info_list': ['Updated May 10, 2020', 'R', 'Updated Jun 15, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 5, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '195 contributions\n        in the last year', 'description': ['R code for fAPLS\nInstruction\n\nTested at RStudio.\nCreate two folders named after Rimage and figure.\nRun R script main.R.\nR script functions.R contains all the supporting functions.\nSaved R image-files and figures go into Rimage and figure, respectively.\n\nReference\nZhou, Z. (2020). Fast implementation of partial least squares for function-on-function regression.\narXiv:2005.04798\n'], 'url_profile': 'https://github.com/ZhiyangGeeZhou', 'info_list': ['Updated May 10, 2020', 'R', 'Updated Jun 15, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '178 contributions\n        in the last year', 'description': ['Vision-Problems\nPredicting the probability of a person having vision problems by creating a Logistic Regression Model in R and Python\n'], 'url_profile': 'https://github.com/sidh26', 'info_list': ['Updated May 10, 2020', 'R', 'Updated Jun 15, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 5, 2020']}","{'location': 'Washington DC', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['drug-related-deaths-ct\n'], 'url_profile': 'https://github.com/sandevaj', 'info_list': ['Updated May 10, 2020', 'R', 'Updated Jun 15, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Prashanthsrn', 'info_list': ['Updated May 10, 2020', 'R', 'Updated Jun 15, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated May 5, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'MATLAB', 'Updated May 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Prashanthsrn', 'info_list': ['MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021']}","{'location': 'Bengaluru, India', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['social-network-ads-\nAnalyzing social network ads dataset using Logistic Regression, Decision Trees, Random Forests, KNN, SVM, and Naive Bayes.\n'], 'url_profile': 'https://github.com/melvinouseph', 'info_list': ['MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021']}","{'location': 'Goa, India', 'stats_list': [], 'contributions': '800 contributions\n        in the last year', 'description': ['A/B testing Result Analysis\nAnalysed results of an A/B test run by an e-commerce website. Using Hypothesis testing and Regression approach.\n'], 'url_profile': 'https://github.com/mgautam98', 'info_list': ['MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021']}","{'location': 'Toronto, Ontario', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Toronto-break-and-enter-analysis\nUsing R with data of Toronto break and enter from 2014 to 2019, linear regression and statistical inference to suggest better police force allocation.\n'], 'url_profile': 'https://github.com/scr1104', 'info_list': ['MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021']}","{'location': 'Toronto, Ontario', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['car-resell-price\ntrain/test multinomial regression model on car resell price based on open source US car price data from Kaggle.\n'], 'url_profile': 'https://github.com/scr1104', 'info_list': ['MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021']}","{'location': 'mumbai,India', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': [""titanic\na titanic data science challenge from kaggle\nProject Organization\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ Makefile           <- Makefile with commands like `make data` or `make train`\n‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.\n‚îú‚îÄ‚îÄ data\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ external       <- Data from third party sources.\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.\n‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump.\n‚îÇ\n‚îú‚îÄ‚îÄ docs               <- A default Sphinx project; see sphinx-doc.org for details\n‚îÇ\n‚îú‚îÄ‚îÄ models             <- Trained and serialized models, model predictions, or model summaries\n‚îÇ\n‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.\n‚îÇ                         `1.0-jqp-initial-data-exploration`.\n‚îÇ\n‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.\n‚îÇ\n‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting\n‚îÇ\n‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n‚îÇ                         generated with `pip freeze > requirements.txt`\n‚îÇ\n‚îú‚îÄ‚îÄ setup.py           <- makes project pip installable (pip install -e .) so src can be imported\n‚îú‚îÄ‚îÄ src                <- Source code for use in this project.\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ __init__.py    <- Makes src a Python module\n‚îÇ   ‚îÇ\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ data           <- Scripts to download or generate data\n‚îÇ\xa0\xa0 ‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ make_dataset.py\n‚îÇ   ‚îÇ\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ features       <- Scripts to turn raw data into features for modeling\n‚îÇ\xa0\xa0 ‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ build_features.py\n‚îÇ   ‚îÇ\n‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ models         <- Scripts to train models and then use trained models to make\n‚îÇ   ‚îÇ   ‚îÇ                 predictions\n‚îÇ\xa0\xa0 ‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ predict_model.py\n‚îÇ\xa0\xa0 ‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ train_model.py\n‚îÇ   ‚îÇ\n‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ visualization  <- Scripts to create exploratory and results oriented visualizations\n‚îÇ\xa0\xa0     ‚îî‚îÄ‚îÄ visualize.py\n‚îÇ\n‚îî‚îÄ‚îÄ tox.ini            <- tox file with settings for running tox; see tox.testrun.org\n\n\nProject based on the cookiecutter data science project template. #cookiecutterdatascience\n""], 'url_profile': 'https://github.com/pooja97', 'info_list': ['MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021']}","{'location': 'Austin, Tx', 'stats_list': [], 'contributions': '384 contributions\n        in the last year', 'description': [""Machine Learning Homework Requirements:\nHello! Welcome to the famous Tsukiji fish market of Tokyo, Japan! We came here to collect data on some of the fish they have here\nbut we didn't wake up at 5am for the tuna auction and by the time we showed up they were only left with a few species of fish.\nWe got to work and gathered measurements from a few different species of fish and want you to train a regression model to predict\nthe weight of a fish using some of the features we were able to measure. We have no idea which features will be good predictors.\nWe will hold out 30% of the data before we hand it to you and we will use that csv for scoring.\nHere's what we need from you:\n\nA function that accepts a csv path and returns the predictions of your regression model using our csv. The csv we use will contain all the columns.\nUse a pipenv and scikit learn to submit the final hw. You may use R for model selection.\n\nWith this function and it's output, we will rank the students by how well their model performed on predicting weight based on naive data.\nYour grade will be determined by ranking according to Mean-squared Error.\nIf your function does not return a list of predictions or we cannot compute the accuracy of your model that it will be an automatic F. # Fish_weight_prediction_Machine_learning\n""], 'url_profile': 'https://github.com/look4parul', 'info_list': ['MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '129 contributions\n        in the last year', 'description': ['FemEmpMLproj\n'], 'url_profile': 'https://github.com/denizturkcapar', 'info_list': ['MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '694 contributions\n        in the last year', 'description': ['pwned-passwords-project\nEDA of 500,000,000+ leaked passwords from haveibeenpwned, regression model on num of appearances of passwords.\n'], 'url_profile': 'https://github.com/EricB10', 'info_list': ['MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['Predicting-Empty-Shelter-for-the-city-of-Toronto\nPredicting the number of empty rooms in each shelter at the city of Toronto with regression model.\nThe Web App URL: https://smart-city-toronto.herokuapp.com\n'], 'url_profile': 'https://github.com/Maryam-Ah', 'info_list': ['MATLAB', 'Updated May 5, 2020', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2021', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': [""Project_BankPersonalLoan\nThis is a Project to do a Logistic Regression Analysis on the 'Thera Bank Personal Loan Campaign'\n""], 'url_profile': 'https://github.com/manzoorallam', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'R', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '714 contributions\n        in the last year', 'description': ['Time Series Prediction using Recurrent Neural Networks\n1. Auto Regressive Model   \n\nX(t) = a1X(t - 1) + a2X(t - 2) + a3X(t - 3) + U(t)\nwhere U(t) ~ Uniform(0, 0.1)\n\n2. Moving Average Model    \n\nX(t) = U(t) + a1U(t - 1) + a2U(t - 2) + a3U(t - 3) + a4U(t - 4) + a5U(t - 5)\nwhere U(t) ~ Norm(0, 1)\n\n\nNon-Recurrent Model \n\nModeling sequence as a function that maps term index to the term value.\nThe model observes a set of 2500 terms from the sequence and predicts the next 500.\nA simple one-layer perceptron with d hidden layers is used as the model.\n\n\n'], 'url_profile': 'https://github.com/sayarghoshroy', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'R', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Social Media Campaign Analysis\nThe dataset is a sample of the company‚Äôs social media ad-campaign. The company ran three different campaigns and had numerous different advertisements under these three campaigns. The company segmented its customers based on age, gender and interest. It then recorded the response of customers like Clicks, Impressions, Conversions, etc., to validate the effectiveness of a campaign.\nThe analysis focused on 2 main objectives, i.e., to increase the brand awareness by driving more traffic to the website and increase the number of reservations made through the website. Hence, for the project I would be focusing on the some of the key metrics like Total Conversion Rate (TCR), Approved Conversion Rate (ACR), Cost per Click (CPC), Click Through Rate (CTR), Approved Cost per acquisition (ACPA) and Total Cost per acquisition (TCPA). Using the results of these metrics, I identifed the most effective marketing campaign.\nData Source\nKaggle - Sales Conversion Optimization Dataset\nExcel Analysis\nPerformed an analysis in Excel through Pivot Tables and created different KPIs. The data is segmented based on Age Group, Gender and Campaign Ids. Above mentioned metrics are then used on provide an analysis of the most effective campaign based on these segments.\nR Analysis\nPerformed analysis in R looking at different variables, checked for duplicates (Ad_ID ), and created KPIs as mentioned above.\nBuilt the regression model in R, trying to see if there is a causation between predictor variables (xyz_campaign_id, Age, Gender, Spent) and Outcome Variables (Approved Conversion, Total Conversion) simultaneously, to see which variables are important for the company in understanding their areas of improvement. For further analysis,  created 3 interaction terms: (1) Age & Gender, (2) Age & Spent,  (3) Gender and Spent. Consequently validated the result obtained from the model through linear hypothesis testing.\n'], 'url_profile': 'https://github.com/meghnamatai', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'R', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Analyzing-House-Sales-in-King-County-USA\nThis is an analysis of House Sales in King County USA, using Python and libraries such as NumPy, SciPy, and Scikit-Learn. The following analysis will be performed: Model Evaluation Over-fitting, Under-fitting and Model Selection Ridge Regression Grid Search Linear Regressing, Multiple Linear Regression and more Data Analysis Techniques.\n'], 'url_profile': 'https://github.com/AreebMianoor', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'R', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '149 contributions\n        in the last year', 'description': [""Machine-Learning 101\nWe are making a Machine Learning repository where we will upload several datasets and its solution with explanation. Starting from the basic and moving up in difficulty level.\nFocusing on both the classification and regression, we have selected following dataset's on which we would be working on.\nClassification:\n1) Iris ‚úîÔ∏è\n2) Titanic ‚úîÔ∏è\n3) Education dataset ‚úîÔ∏è\n4) MNIST ‚úîÔ∏è\n5) Hand SIGNS ‚úîÔ∏è\nRegression:\n1) Boston housing ‚úîÔ∏è\n2) Red Wine ‚úîÔ∏è\n3) Medical cost personal dataset ‚úîÔ∏è\n4) Car price prediction ‚úîÔ∏è\n5) Human Resource Data Set\n6) New York stock exchange data\n7) Deep fake detection\nClassification\nFirst, if you have a classification problem ‚Äúwhich is predicting the class of a given input‚Äù.\nSlow but accurate\n1)Non-linear SVM\n2)Random Forest\n3)Neural Network (needs a lot of data points)\n4)Gradient Boosting Tree (similar to Random Forest, but easier to overfit)\nFast\n1)Explainable models: Decision Tree and Logistic Regression\n2)Non-explainable Models: Linear SVM and Naive Bayes\nRegression\nIf you have a regression problem ‚Äúwhich is predicting a continuous value like predicting prices of a house given the features of the house like size, number of rooms, etc‚Äù.\nSlow but accurate\n1)Random Forest\n2)Neural Network (needs a lot of data points)\n3)Gradient Boosting Tree (similar to Random Forest, but easier to overfit)\nFast\n1)Decision Tree\n2)Linear Regression\nPull requests always welcomed ! :) \nYou may add your datasets with solutions, or can request us to give their solutions. Happy Coding!! \n""], 'url_profile': 'https://github.com/meetgandhi123', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'R', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Davanagere ,Karanataka ', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vinodrsrs', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'R', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'Brazil', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Neural-Network\nNeural-Network\n'], 'url_profile': 'https://github.com/mcarujo', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'R', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nUnderstand and discuss the assumptions that must be held for least squares regression\nUnderstand linearity, normality and heteroscedasticity assumptions\nIdentify approaches to check for regression assumptions\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper on the topic of ordinary least squares regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions are made. These assumptions define the complete scope of regression analysis and it is mandatory that underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions. We provide an overview in this lesson!\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, Ordinary Least Squares (OLS) will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q‚ÄìQ (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. You will learn about p-values later, but for now, you can remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots for the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'R', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) ‚Äì mean(x*y)) / ( mean (x)^2 ‚Äì mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'R', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'R', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression-On-California-Housing-Test-Data\nPython Integrated tools, PyCharm is used to implement the project.\n'], 'url_profile': 'https://github.com/Arifur-ratul', 'info_list': ['1', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'Hungary', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['CO2-Emission-Modeling-with-Simple-Linear-Regression\n'], 'url_profile': 'https://github.com/tibortuboly', 'info_list': ['1', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps ‚Äúdays‚Äù uniquely to ‚Äúnumber of sales‚Äù, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Project-2-Housing-Prices-Prediction-using-Sklearn-and-LinearRegression\nIntroduction\nIn this project, we will develop and evaluate the performance and the predictive power of a model trained and tested on data collected from houses in Boston‚Äôs suburbs.\nOnce we get a good fit, we will use this model to predict the monetary value of a house located at the Boston‚Äôs area.\nA model like this would be very valuable for a real state agent who could make use of the information provided in a dayly basis.\n\nProblem Statement:\nThe dataset has house prices of the Boston residual areas. The expense of the house varies according to various factors like crime rate, number of rooms, etc. We have to predict prices on the basis of new data.\nRequirements:\n\nJupyter Notebook\n\nBoston-Dataset:\nThe dataset used in this project comes from the UCI Machine Learning Repository. This data was collected in 1978 and each of the 506 entries represents aggregate information about 13 features of homes from various suburbs located in Boston.\nThe features can be summarized as follows:\n\nCRIM: This is the per capita crime rate by town\nZN: This is the proportion of residential land zoned for lots larger than 25,000 sq.ft.\nINDUS: This is the proportion of non-retail business acres per town.\nCHAS: This is the Charles River dummy variable (this is equal to 1 if tract bounds river; 0 otherwise)\nNOX: This is the nitric oxides concentration (parts per 10 million)\nRM: This is the average number of rooms per dwelling\nAGE: This is the proportion of owner-occupied units built prior to 1940\nDIS: This is the weighted distances to five Boston employment centers\nRAD: This is the index of accessibility to radial highways\nTAX: This is the full-value property-tax rate per $10,000\nPTRATIO: This is the pupil-teacher ratio by town\nB: This is calculated as 1000(Bk ‚Äî 0.63)¬≤, where Bk is the proportion of people of African American descent by town\nLSTAT: This is the percentage lower status of the population\n\nImporting the Modules:\n\nSkLearn\nNumpy\nPandas\n\nLinearRegression:\nLinear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting. Different regression models differ based on ‚Äì the kind of relationship between dependent and independent variables, they are considering and the number of independent variables being used.\n\nScore:\nFor this model, the accuracy on the test set is 0.74, which means the model made the right prediction for 74% of the houses in the Boston dataset. We can expect the model to be correct 74% of the time for predicting the new values from different clients.\nSummary:\nThe Boston Housing Prices Prediction problem (and our implementation) is a perfect example to illustrate how a machine learning problem should be approached and how useful the outcome can be to a potential client who wanted to buy a new house.\n'], 'url_profile': 'https://github.com/Yuvrajchopra25', 'info_list': ['1', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon‚Äôs Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It‚Äôs a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a‚Äî\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a‚Äî\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/satyamraj18', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project - Regression Modeling with the Ames Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Ames Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'ames.csv\' as a pandas dataframe\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.style.use(\'seaborn\')\names = pd.read_csv(\'ames.csv\')\n\nsubset = [\'YrSold\', \'MoSold\', \'Fireplaces\', \'TotRmsAbvGrd\', \'GrLivArea\',\n          \'FullBath\', \'YearRemodAdd\', \'YearBuilt\', \'OverallCond\', \'OverallQual\', \'LotArea\', \'SalePrice\']\n\ndata = ames.loc[:, subset]\nThe columns in the Ames housing data represent the dependent and independent variables. We have taken a subset of all columns available to focus on feature interpretation rather than preprocessing steps. The dependent variable here is the sale price of a house SalePrice. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n# You observations here \nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nClearly, the results are not very reliable. The best R-Squared is witnessed with OverallQual, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Ames dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Calgary, Alberta', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['Multiple Linear Regression: Atmospheric CO2 Concentration\nAuthors: Atlanta Liu, Maruthi Mutnuri, Greg Cameron, Edwin Aguirre\nDate: Fall 2020\nStatistical analysis on atmospheric CO2 concentration levels based on worldwide energy consumption, GDP per capita, and global population. One of the major goals of this analysis is to capture the model selection and assumption testing processes involved with linear regressions. The entire project was completed through RStudio.\nProcedure\n\nVariable Selection\nAssumptions:\n\nLinearity\nIndependence\nNormality\nEqual Variance\nMulticolinearity\n\n\nInfluential Points & Outliers\nInterpreting Coefficients\nPredicting future atmospheric CO2 levels\n\n'], 'url_profile': 'https://github.com/AtlantaL', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K ‚Äì 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White‚Äôs Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['4622_ML_finalProject\n'], 'url_profile': 'https://github.com/SHollatz', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Tema-Ghana', 'stats_list': [], 'contributions': '855 contributions\n        in the last year', 'description': ['Cars-Price-Estimation-Using-Linear-Regression-and-Decision-Tree\n'], 'url_profile': 'https://github.com/Ceasar15', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Columbia, SC, USA', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['R programs for the article ""Varying-coefficient regression analysis for pooled biomonitoring.""\nThe required R packges are Rcpp and RcppArmadillo. You can install these two R packages by\n    install.packages(c(""RcppArmadillo"",""Rcpp""))\n\nThe necessary files are ""VCM.cpp"" and ""Functions.R"". Please download the two files to the directory of your R/Rstudio. Use the following codes to source both of them.\n    library(Rcpp)\n    library(RcppArmadillo)\n    sourceCpp(""VCM.cpp"")\n    source(""Functions.R"")\n\nNow you are ready to fit our varying-coefficient model.\nWhen indiviudal-level biomonitoring is used\n    Individual.fit=IndT(u_grid, Y, U, X, W, c(a,b))\n    # u_grid is a grid of u-values\n    # Y: a size-J vector of individual-level measurements\n    # U: a size-J vector of indivdiual-level U-covariates\n    # X: a J*(p+1) matrix of indiviudal-level X-covariates\n    # W: a size-J vector of individual-level sampling weights\n    # (a,b): the cross-validation will search h within the interval (a,b)\n    \n    #The output Individual.fit is a list of the selected bandwidth and the estimates\n    Individual.fit$h # a single value\n    Individual.fit$fit # a (p+1)*length(u_grid) matrix\n    \n    # You can plot the estimates by \n    par(mfrow=c(ceiling(nrow(Individual.fit$fit)/2),2))\n    par(mar=c(4,4,.1,.1))\n    for(k in 1:nrow(Individual.fit$fit))\n    {\n            plot(u_grid,Individual.fit$fit[k,],type=""l"",xlab=""u"",ylab=expression(beta(u)))\n    }\n\nAn example\nDownload the ""individual.csv"" file to the directory. Run the following programs.\n    individual.data=read.csv(""individual.csv"")\n    u_grid=seq(-1.5,1.5,length=400)\n    Y=individual.data$Y\n    U=individual.data$U\n    X=cbind(individual.data$X1,individual.data$X2,individual.data$X3,individual.data$X4)\n    W=individual.data$W\n    Individual.fit=IndT(u_grid,Y,U,X,W,c(0.01,1))\n    Individual.fit$h\n    par(mfrow=c(ceiling(nrow(Individual.fit$fit)/2),2))\n    par(mar=c(4,4,.1,.1))\n    for(k in 1:nrow(Individual.fit$fit))\n    {\n            plot(u_grid,Individual.fit$fit[k,],type=""l"",xlab=""u"",ylab=expression(beta(u)))\n    }\n\nOutput are\n    >  Individual.fit$h\n    [1] 0.5753412\n\n\nWhen randomly pooled biomonitoring is used\n    Random.fit=RT(u_grid,Z,U,X,POOLID,W,c(a,b))\n    # u_grid is a grid of u-values\n    # Z: a size-N vector of pooled-level measurements for each individual,\n    #       if two indiviudals are in the same pool, the corresponding Z-value are the same\n    # U: a size-N vector of indivdiual-level U-covariates\n    # X: a N*(p+1) matrix of indiviudal-level X-covariates\n    # PoolID: a size-N vector of individuals\' pool ID,\n    #       if two individuals are in the same pool, their pool IDs are the same\n    # W: a size-N vector of individual-level sampling weights\n    # (a,b): the cross-validation will search h within the interval (a,b)\n    \n    #The output Random.fit is a list of the selected bandwidth and the estimates\n    Random.fit$h # a single value\n    Random.fit$fit # a (p+1)*length(u_grid) matrix\n    \n    # You can plot the estimates by \n    par(mfrow=c(ceiling(nrow(Random.fit$fit)/2),2))\n    par(mar=c(4,4,.1,.1))\n    for(k in 1:nrow(Random.fit$fit))\n    {\n            plot(u_grid,Random.fit$fit[k,],type=""l"",xlab=""u"",ylab=expression(beta(u)))\n    }\n\nAn example\nDownload the ""random.csv"" file to the directory. Run the following programs.\n    random.data=read.csv(""random.csv"")\n    u_grid=seq(-1.5,1.5,length=400)\n    Z=random.data$Z\n    U=random.data$U\n    X=cbind(random.data$X1,random.data$X2,random.data$X3,random.data$X4)\n    W=random.data$W\n    PoolID=random.data$poolID\n    Random.fit=RT(u_grid,Z,U,X,PoolID,W,c(0.01,1))\n    Random.fit$h\n    par(mfrow=c(ceiling(nrow(Random.fit$fit)/2),2))\n    par(mar=c(4,4,.1,.1))\n    for(k in 1:nrow(Random.fit$fit))\n    {\n            plot(u_grid,Random.fit$fit[k,],type=""l"",xlab=""u"",ylab=expression(beta(u)))\n    }\n\nOutput are\n    >  Random.fit$h\n    [1] 0.7529637\n\n\nWhen homogeneously pooled biomonitoring is used\n    Homogenous.fit=HT(u_grid,Z,U,X,POOLID,W,c(a,b))\n    # u_grid is a grid of u-values\n    # Z: a size-N vector of pooled-level measurements for each individual,\n    #       if two indiviudals are in the same pool, the corresponding Z-value are the same\n    # U: a size-N vector of indivdiual-level U-covariates\n    # X: a N*(p+1) matrix of indiviudal-level X-covariates\n    # PoolID: a size-N vector of individuals\' pool ID,\n    #       if two individuals are in the same pool, their pool IDs are the same\n    # W: a size-N vector of individual-level sampling weights\n    # (a,b): the cross-validation will search h within the interval (a,b)\n    \n    #The output Homogeneous.fit is a list of the selected bandwidth and the estimates\n    Homogeneous.fit$h # a single value\n    Homogeneous.fit$fit # a (p+1)*length(u_grid) matrix\n    \n    # You can plot the estimates by \n    par(mfrow=c(ceiling(nrow(Homogeneous.fit$fit)/2),2))\n    par(mar=c(4,4,.1,.1))\n    for(k in 1:nrow(Homogeneous.fit$fit))\n    {\n            plot(u_grid,Homogeneous.fit$fit[k,],type=""l"",xlab=""u"",ylab=expression(beta(u)))\n    }\n\nAn example\nDownload the ""homogeneous.csv"" file to the directory. Run the following programs.\n    homogeneous.data=read.csv(""homogeneous.csv"")\n    u_grid=seq(-1.5,1.5,length=400)\n    Z=homogeneous.data$Z\n    U=homogeneous.data$U\n    X=cbind(homogeneous.data$X1,homogeneous.data$X2,homogeneous.data$X3,homogeneous.data$X4)\n    W=homogeneous.data$W\n    PoolID=homogeneous.data$poolID\n    Homogeneous.fit=HT(u_grid,Z,U,X,PoolID,W,c(0.01,1))\n    Homogeneous.fit$h\n    par(mfrow=c(ceiling(nrow(Homogeneous.fit$fit)/2),2))\n    par(mar=c(4,4,.1,.1))\n    for(k in 1:nrow(Homogeneous.fit$fit))\n    {\n            plot(u_grid,Homogeneous.fit$fit[k,],type=""l"",xlab=""u"",ylab=expression(beta(u)))\n    }\n\nOutput are\n    >  Homogeneous.fit$h\n    [1] 0.3141536\n\n\n'], 'url_profile': 'https://github.com/Harrindy', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sidharthvats', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ishan-Bhusari-306', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/arvindpatel24', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['CovidMaskN-N\nThis is the logisitic regression image classifier model on the principal of binary classification i.e 0 for non mask and 1 for mask\n'], 'url_profile': 'https://github.com/TarunDhiman1', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shubhangperiwal', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '305 contributions\n        in the last year', 'description': ['Appl-Econ-Assg1\nApplied Econometrics Assignment 1\n(Analysis was done using R).\n\nRun an econometric regression model on your dataset giving proper justification for selection\nof the variables. Interpret the coefficients of your variable appropriately.\nPrepare Graph Matrix for your dataset. Comment on the association between dependent\nvariable & independent variables of your dataset.\nPlot the predict Y and discuss the accuracy of your model.\nFor regression model fit in Question 1, run the tests for checking following OLS assumptions\nand interpret your results.\na. Heteroscedasticity\nb. Multicollinearity\nc. Normality of the error term\nd. Omitted-Variable Bias\nBased on results of Question 4, use the remedies to address the issues identified and alter your\nmodel suitably.\nFor your model, run two joint tests (F-test) giving justification for the same. Interpret the results.\n\n'], 'url_profile': 'https://github.com/siddarthgopalakrishnan', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Tanzania', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Salary_Prediction\nUsing a simple linear regression model to predict the salary of an employee based on the years of experience they have\n'], 'url_profile': 'https://github.com/Rahmahermes', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Visakhapatnam , Andhra Pradesh , India', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['sentiment-analysis-imdb-dataset\nI have classified the reviews given by the audience in IMDB as either a positive review or negative review using Logistic Regression and few NLP tools.\n'], 'url_profile': 'https://github.com/patnaik-sg', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['Winner-Prediction-of-PUBG\nPerforming Exploratory Data Analysis( EDA ) to get some useful insights form the data and Building Regression Model to Predict Winner of the Game PUBG.\nProblem Statement:\nIn a PUBG game, up to 100 players start in each match (matchId). Players (Id) can be on teams (groupId) which get ranked at the end of the game (winPlacePerc) based on how many other teams are still alive when they are eliminated. During the game, players can pick up different amunitions, revive downed-but-not-dead (knocked) teammates, drive vehicles, swim, run, shoot, and experience all of the consequences -- such as falling too far or running themselves over and eliminating themselves.\nThe team at PUBG has made official game data available for the public to explore and scavenge outside of ""The Blue Circle."" This workshop is not an official or affiliated PUBG site. Its based on the data collected by Kaggle and made available through the PUBG Developer API.\nYou are provided with a large number of anonymized PUBG game stats, formatted so that each row contains one player\'s post-game stats. The data comes from matches of all types: solos, duos, squads, and custom; there is no guarantee of there being 100 players per match, nor at most 4 player per group.\nGoal:\nPerform the PUBG data analysis and answer the following questions:\n\n\nDoes killing more people increases the chance of winning the game?\n\n\nUse the correlation between the match winning percentage and number of kills to determine the relationship\nHow do we catch the fraudsters in the game?\n\n\nUse various logical conditions based on game knowledge to determine fraudsters in the game\n\n\nCan we predict the finishing position of a player in the game?\n\n\n\n\nRegression Problem: Train and test a model using regression algorithm to predict the final position of the player at the end of the game. Create a model which predicts players\' finishing placement based on their final stats, on a scale from 1 (first place) to 0 (last place).\nNeeds data or some help you can reach me at:\n\nGmail: shashankshanu1993@gmail.com\nLinkedIn: www.linkedin.com/in/shashank-shanu-425783117\n\n\nCredit: Edureka Team\n'], 'url_profile': 'https://github.com/shashankshanu102', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'India, Hyderabad', 'stats_list': [], 'contributions': '122 contributions\n        in the last year', 'description': ['DataScience-Projects\n'], 'url_profile': 'https://github.com/satishmekkonda', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Toronto, ON', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/haidershoaib', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['Kaggle-BullDozer-price-prediction-\nGiven certain attributes and SaleDates,a Regression model based on RandomForestRegressor is made to predict the prices of Bulldozers in the market sale.\nNumpy,Pandas and Scikit-learn libraries have been used .The project is made using Jupyter Notebook and regression Machine Learning models havebeen used for price prediction.\n'], 'url_profile': 'https://github.com/codewithsupra', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'R', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Python', 'Updated May 10, 2020', 'Python', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/michael-gendy', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 5, 2020', 'R', 'Updated May 4, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Delhi-AQI-Predictor\nDelhi AQI Predictor based on 2018 data . Model is build using Multivariate regression from scratch and also compared with the model build using sklearn\n'], 'url_profile': 'https://github.com/manish0718', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 5, 2020', 'R', 'Updated May 4, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['chazhaobiao-pipei-\nFirst use the library data to solve the coefficients of multiple stepwise regression and verify, and secondly use the image Rrs matching library\n'], 'url_profile': 'https://github.com/Maluoka', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 5, 2020', 'R', 'Updated May 4, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['DataQuest Project 17: Predicting House Prices Using a Linear Regression Model\nThis is the 17th project I completed in the ""Data Analyst in Python"" and ""Data Scientist in Python"" paths from Dataquest. In this project a dataset containing previously sold houses and some of their attributes was used to make predictions on their sale price using a linear regression model.\nThe educational goal of the project was to properly select and engineer the best features and then to use scikit-learn to train and test a linear regression model.\nPrerequisites\nThis project requires both the Jupyter notebook as well as the CSV file containing data on previous house sales.\nTo run this project, you will need Jupyter Notebooks with a python 3 kernel to be installed on your machine. Instructions on how to install Jupyter Notebooks can be found on the Jupyter website.\nThe project itself is written in Python 3 and uses the modules Numpy, matplotlib, sklearn, seaborn and Pandas, to run. These can be installed by runningpip install ` in the console.\n'], 'url_profile': 'https://github.com/jillerhaus', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 5, 2020', 'R', 'Updated May 4, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Akash-ik', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 5, 2020', 'R', 'Updated May 4, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['myLinearPackage\nCreates  a scatterplot  of  each pair  of covariates. Also creates a linear regression of Y on X. Written by Sara Khorramshahgol\n'], 'url_profile': 'https://github.com/Khorramsara', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 5, 2020', 'R', 'Updated May 4, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction\nThis vignette is provided to run the models described in the paper ‚ÄúA\njoint polytomous logistic regression model for the initiation and\ncessation of electronic cigarettes and conventional cigarettes with\ntime-dependent covariates in a longitudinal study‚Äù.\nMethods\nSimulating data\nsource(""model_functions.R"")\n# Parameter specifications \nnT <- 5 # number of time points (including the baseline)\nV <- 4 # number of covariates\nN <- 1000\n\nsimstat <- Initiate(nT=nT, V=V)\nset.seed(2020)\nsData <- SimulateData(param=simstat, N=N)\n\nX <- sData[[""X""]]\nY <- sData[[""Y""]]\nData overview\nsimstat is a list containing parameters like Œªs, Œºs, Œ±s, Œ∑s,\nŒ≤s, Œ≥s, and Œ¥s.\nsimstat\n$lambda\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   NA   -2   -2   -2   -2\n[2,]   NA   -3   -3   -3   -3\n\n$mu\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   NA    1    1    1    1\n[2,]   NA    2    2    2    2\n\n$alpha\n     [,1] [,2] [,3] [,4]\n[1,]  0.3  0.1  0.1  0.1\n[2,]  0.3  0.3  0.1  0.3\n\n$eta\n     [,1] [,2] [,3] [,4]\n[1,] -0.1 -0.1 -0.1 -0.1\n[2,] -0.2 -0.2 -0.2 -0.2\n\n$beta\n[1] 2 2\n\n$gamma\n[1] -2 -2\n\n$delta\n     [,1] [,2]\n[1,]    0    0\n[2,]    0    0\n\nX is a list of N matrices of nT √ó V, each of which contains V\ncovariates over nT time points for one observation.\nhead(X, n = 2)\n[[1]]\n           [,1]          [,2]         [,3]       [,4]\n[1,] -0.2767545 -0.2949347331  0.389957977 -1.0959907\n[2,] -0.1768179 -0.2057567667  0.185015015 -0.9887932\n[3,] -0.2508131 -0.2021924343 -0.005899663 -0.4741751\n[4,] -0.4995336 -0.0006096089 -0.190396962 -0.5540971\n[5,]  0.1635396  0.3973435305  0.033122267  0.2105109\n\n[[2]]\n            [,1]         [,2]       [,3]       [,4]\n[1,] -0.06983043  0.115383532 0.21506862 -0.9518591\n[2,] -0.17221585 -0.201101530 0.05431437 -0.8657853\n[3,]  0.01302561 -0.198498831 0.13416215 -0.6919490\n[4,] -0.01232931  0.002671777 0.15355087 -0.5217994\n[5,]  0.07178421  0.282679259 0.50660175 -0.2592659\n\nY is a list of N matrices of nT √ó V, each of which contains two\noutcomes (0: No; 1: Yes) over nT time points for one observation.\nhead(Y, n = 2)\n[[1]]\n     [,1] [,2]\n[1,]    0    0\n[2,]    0    0\n[3,]    0    0\n[4,]    0    0\n[5,]    0    0\n\n[[2]]\n     [,1] [,2]\n[1,]    0    0\n[2,]    0    0\n[3,]    0    1\n[4,]    1    0\n[5,]    0    0\n\nRunning the joint polytomous logistic regression models\n#res_joint <- Analyze(simstat, X, Y)\nload(""results.rdata"")\nResults\nThe model yields the coefficients and standard errors of all parameters\nlisted by simstat.\n# coefficients \ncoef_joint <- vec2list(res_joint$par)\n# standard errors \nsigma_joint <- vec2list(sqrt(diag(solve(-res_joint$hessian))))\n# create 95% CI plots \n\nhead(coef_joint, n=2)\n$lambda\n     [,1]      [,2]      [,3]      [,4]      [,5]\n[1,]   NA -1.969336 -2.031680 -1.887466 -2.098677\n[2,]   NA -2.964486 -3.091678 -3.007822 -2.839381\n\n$mu\n     [,1]     [,2]     [,3]     [,4]     [,5]\n[1,]   NA 1.290218 0.780491 1.088544 1.158296\n[2,]   NA 1.515351 1.471450 1.528144 1.757900\n\nhead(sigma_joint, n=2)\n$lambda\n     [,1]      [,2]      [,3]      [,4]      [,5]\n[1,]   NA 0.1044587 0.1077947 0.1041738 0.1123711\n[2,]   NA 0.1403005 0.1472668 0.1424860 0.1356227\n\n$mu\n     [,1]      [,2]      [,3]      [,4]      [,5]\n[1,]   NA 0.2205268 0.1849230 0.1791407 0.1813348\n[2,]   NA 0.4192058 0.3364667 0.3307288 0.3304476\n\nWe displayed the odds ratio plots for important variables Œ≤s and Œ≥s.\ndf <- data.frame(type = factor(\n                   rep(c(""Initiation"", ""Cessation""), each = 2), \n                   levels = c(""Initiation"", ""Cessation"")\n                 ),\n                 cig = c(""E-cigarettes"", ""C-cigarettes"", \n                         ""E-cigarettes"", ""C-cigarettes""),\n                 mean = c(coef_joint$beta,\n                          coef_joint$gamma), \n                 sd = c(sigma_joint$beta,\n                        sigma_joint$gamma))\n\ndf$lb <- exp(df$mean + qnorm(0.025)*df$sd)\ndf$ub <- exp(df$mean + qnorm(0.975)*df$sd)\ndf$mean <- exp(df$mean)\n\nggplot(df, aes(x = cig, y = mean)) +\n  geom_point(position=position_dodge(width=0.3), size=2) +\n  geom_hline(yintercept = 1.0, linetype=""dotted"", size=1) + \n  scale_y_continuous(trans=\'log2\', \n                     breaks = c(round(df$lb, 1), \n                                round(df$ub, 1), 1)) +\n  scale_color_manual(values = c(""grey"", ""black"")) +\n  guides(color = guide_legend(reverse = TRUE)) +\n  geom_errorbar(aes(ymin = lb, ymax = ub),\n                position = position_dodge(0.3), width = 0.25) +\n  facet_grid(type~.)+\n  coord_flip() +\n  ylab(""Odds ratio"") +\n  theme_bw()+\n  theme(text = element_text(size=15),\n        axis.text.x = element_text(size=8),\n        panel.grid.minor = element_blank(),\n        legend.position = ""top"", \n        legend.title = element_blank(), \n        axis.title.y = element_blank())\n\n'], 'url_profile': 'https://github.com/USCbiostats', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 5, 2020', 'R', 'Updated May 4, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '675 contributions\n        in the last year', 'description': [""Predict-GDP-of-Canada\n\n\n\n\n\nTable of contents\n\n\nAbout\n\n\nTechnologies Used\n\n\nResults of the Project\n\n\nInstallation\n\n\nData Source\n\n\nLicense\n\n\nAbout\n\nIn this project there are two jupyter notebooks namely from-scratch.ipynb and using-sklearn.ipynb.\n\n\nIn from-scratch.ipynb a linear regression model is built from scratch, using numpy for mathematical operations. This model is then trained with the data to predict Canada's GDP where Year of which we want the GDP is the input data.\n\n\nIn from-scratch.ipynb Gradient Descentand Normal Equation(since the size of data is less than 10,000) are for finding the best parameters and then the model is evaluated using the test data.\n\n\nIn using-sklearn.ipynb sklearn module is used the and machine learning techinques like Cross Validation, Analyzing Learning Curve and Parameter Tunning are used to train the model and then it is evaluated with the test data.\n\nTechnologies Used\n\n is used as Programming Language.\n\n\nNumpy is used for the mathematical and data manipulation.\n\n\nPandas is used to analysis and manipulation of data.\n\n\nMatplotlib and Seaborn are used for data visualisation which helped in the analysis of data.\n\n\nSciki-learn is used for data preprocessing, creating machine learning model and evaluating it, thus creating a pipeline.\n\n\nPipenv is the virtual environment used for the project. Jupyter Notebook is used to for the entire data science and machine learning life cycle.\n\nResults of the Project\n\nResults of from-scratch.ipynb and using-sklearn.ipynb are same i.e. the regression model built using sklearn module and the one built just using numpy gives the same results.\n\nLine Plot\n\nCorrelation Matrix\n\nCross Validation Score\n\nLearning Curve\n\nFitted Line\n\nMetrics Scores\n\nActual VS Prediction\n\nInstallation\nIt is highly recommended to use virtual enviroment for this project to avoid any issues related to dependencies.\nHere pipenv is used for this project.\nThere is a requirements.txt file in 'Predict-GDP-of-Canada'/requirements.txt which has all the dependencies for this project.\n\nFirst, start by closing the repository\n\ngit clone https://github.com/AkashSDas/Predict-GDP-of-Canada\n\n\nStart by installing pipenv if you don't have it\n\npip install pipenv\n\n\nOnce installed, access the venv folder inside the project folder\n\ncd  'Predict-GDP-of-Canada'/venv/\n\n\nCreate the virtual environment\n\npipenv install\n\nThe Pipfile of the project must be for creating replicating project's virtual enviroment.\nThis will install all the dependencies and create a Pipfile.lock (this should not be altered).\n\nEnable the virtual environment\n\npipenv shell\n\n\ndataset, jupyter notebook and model are in 'Predict-GDP-of-Canada'/venv/src folder.\n\ncd src/\n\n\nTo start/view the jupyter notebook\n\njupyter noterbook\n\nThis will open a webpage in the browser from there you can click on notebook.ipynb to view it.\nData Source\nThe source of the data used here is the World Bank national accounts data, and OECD National Accounts data files.\nLicense\nThis project is licensed under the MIT License - see the MIT LICENSE file for details.\n""], 'url_profile': 'https://github.com/AkashSDas', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 5, 2020', 'R', 'Updated May 4, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'kolkata', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['weather_modelling\nA  project on attributing different components of weather with temperature and using a linear regression model to predict the temperature based on a given set of inputs.\n'], 'url_profile': 'https://github.com/N-O-O-B-Coder', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 5, 2020', 'R', 'Updated May 4, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Movie_Genre_Classifier\n'], 'url_profile': 'https://github.com/nosequins', 'info_list': ['Jupyter Notebook', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'HTML', 'Updated May 5, 2020', 'R', 'Updated May 4, 2020', 'HTML', 'Updated May 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Prediction-Model\nUsing Linear regression, random forest, decision tree models to predict the median housing price in any district, given all the other metrics.\n'], 'url_profile': 'https://github.com/RujiaChen', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Galway, Ireland | Pune ,India', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Detection-of-Irony-and-Sarcasm.\nAdvance Natural Language Processing: Detecting Sarcasm and Irony from the Tweets.\nComparing the performance of Logistic Regression against the State of the Art  LSTM Model\n'], 'url_profile': 'https://github.com/saurabhhebbalkar5', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Car_Dataset_Exploration\nI explored a dataset of cars with missing values, playing with different methods (eg basic linear regression and random forests) to fill these values in.\n'], 'url_profile': 'https://github.com/nikulpatel462', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'Dallas Texas', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Data Science challenge Winner\n\nPredicted ""Will Call"" sales for different stores based on historical sales data as well as demographic and Store characteristics\n\nTable of contents\n\nGeneral info\nScreenshots\nTechnologies and Tools\nCode Examples\nStatus\nContact\n\nGeneral info\nIt is a supervise learning problem in which we predicted continuous variable sales. The steps involved are merging of the data, feature preprocessing and engineering, model building and evaluation of the model with the right metric and finally prediction of sales\nScreenshots\nTypes of Data\n\nSales Trend for Given years\n\nsales for given years\n\nRecommended KPIs\n\nPredicted Sales\n\nTechnologies and Tools\n\nPython\nR\nAlteryx\nTableau\n\nCode Examples\n## Data Transformations\n\ntrain_rec <- recipe(Sales ~ ., data = train) %>%\n  step_log(Sales,Foottraffic,Total_Population,Total_HH)%>%\n  prep(data = train,retain = TRUE)\n\ntrain_rec_std <- recipe(Sales ~ ., data = train) %>%\n  step_log(Sales,Foottraffic,Total_Population,Total_HH)%>%\n  step_center(all_numeric(), -all_outcomes()) %>%\n  step_scale(all_numeric(), -all_outcomes())%>%\n  prep(data = train,retain = TRUE)\n\ntrain_tbl <-bake(train_rec, newdata = train)\ntrain_tbl_std <-bake(train_rec_std, newdata = train)\n\ntrain_panel<-pdata.frame(train_tbl,index = c(""Plant"",""FISCAL_YEAR_PERIOD""))\ntrain_panel_std<-pdata.frame(train_tbl_std,index = c(""Plant"",""FISCAL_YEAR_PERIOD""))\n\n## Exponential Time Smoothing\nx <- c(""plant"", ""date"", ""KPI"",""value"")\ncolnames(data)<-x\nfor (kpi in names(storemet)[3:9]){\n  for ( plant in unique(storemet$√Ø..√Ø..Plant)){\n    tsdata<-ts(storemet[storemet$√Ø..√Ø..Plant==plant,kpi],start=c(2013,1),end=c(2014,12),frequency=12)\n    HWmodel<-ets(tsdata,model = ""ZZA"")\n    seas_fcast <- forecast(HWmodel, h=12)\n    data<-rbind(data,data.frame(rep(plant,12),as.yearmon(time(seas_fcast$mean)), rep(kpi,12),as.numeric(seas_fcast$mean)))\n    print(plant)\n    }\n}\n\n## Model Training\nform=Sales~Store_Size+Foottraffic+I(Foottraffic^2)+PhoneCalls+MktgAdopt+WCMTDtoQuota+I(WCMTDtoQuota^2)+VOC+PartsSuppliesMTDtoQuota+WCMTDtoQuota*Store_Size\n\n#OLS model\nplmpooled <- plm(form, data=train_panel, model = ""pooling"")\nsummary(plmpooled)\n\n#Fixed Effect Model\nplmwithin <- plm(form, data=train_panel, model = ""within"")\nsummary(plmwithin)\n\n#Random Effect Model\nplmrandom <- plm(form, data = train_panel, model = ""random"")\nsummary(plmrandom)\n\nStatus\nProject is: finished. We won this data science challenge among 124 teams.\nContact\nIf you loved what you read here and feel like we can collaborate to produce some exciting stuff, or if you\njust want to shoot a question, please feel free to connect with me on\nemail or\nLinkedIn\n'], 'url_profile': 'https://github.com/jainik1392', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Chaitanyakaul97', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['Stock-Market-Prediction-Using-Machine-Learning\nIn this project, the main objective is to find the best stock market prediction system to predict the financial\nprices of the stock market. This project implements a time series analysis approach that combines with\nmachine learning algorithms and deep learning techniques. Algorithms that are regression-based such as\nlinear regression and support vector regression are implemented throughout this project. Moreover, time\nseries forecasting such as Long Short Term Memory and Autoregressive Integrated Moving Average are\nalso used in this experiment. Datasets from Yahoo Finance are collected and preprocessed to be fitted into\nthese models. Historical data includes twelve months and five years of data for Tesco and Sainsbury are\nused for the experiment. In addition, the target variable is the close price since it is determined for profit\nand loss analysis. Hence, the project is attempting to forecast a close price at a certain time based on the\nhistorical stock prices using different kinds of machine learning models.\nPython v3.6.8\nExample result:\nARIMA Model on 5 years Sainsbury data\n\n\nLSTM Model on 5 years Tesco data\n\n\nConclusion:\nBoth LSTM and ARIMA are suitable algorithms for predicting the stock market price based on the historical price data. As other research papers suggested, LSTM and ARIMA do have a lot of potential when dealing with time series forecasting. Hence, these algorithms will be a great asset for traders, investors and analysts to make profitable investment decisions in the stock market. For future work, the idea is to add other variables such as open price, high price, low price, volume and news/sentiment analysis to predict current stock price.\n¬© 2020 Kevin Kurnia Santosa. All rights reserved.\n'], 'url_profile': 'https://github.com/kevinkurniasantosa', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression - Lab\nIntroduction\nRegression analysis forms the basis of machine learning experiments. Understanding regression will help you to get the foundations of most machine learning algorithms. Ever wondered what\'s at the heart of an artificial neural network processing unstructured data like music and graphics? It can be linear regression!\nObjectives\nYou will be able to:\n\nCalculate the slope of a line using standard slope formula\nCalculate the y-intercept using the slope value\nDraw a regression line based on calculated slope and intercept\nPredict the label of a previously unseen data element\n\nLet\'s get started\nA first step towards understanding regression is getting a clear idea about ""linear"" regression and basic linear algebra.\nIn the lesson, we showed the best-fit line\'s slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and  ${S_Y}$ and ${S_X}$ The standard deviation of $x$ and $y$ respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou\'ll use the latter formula in this lab. As in our previous lab, let\'s break down the formula into its parts. First, you\'ll import the required libraries and define some data points to work with. Next, you\'ll use some pre-created toy data in numpy arrays. Let\'s do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use(\'ggplot\')\n\n# Initialize vectors X and Y with given values and create a scatter plot\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot between X and Y and comment on the output\n# Scatter plot\n\n# Your observations about relationship in X and Y \n\n\n\n#\nIn a data analysis context, we can think of these points as two vectors:\n\nvector X: The independent variable or predictor\nvector Y: The dependent variable or target variable\n\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in x and y vectors and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) ‚Äì mean(x*y)) / ( mean (x)^2 ‚Äì mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line\'s y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\n\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting label for new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet\'s try to find a y prediction for a new value of $x = 7$ and unknown $y$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with actual data and regression line\n# Plot as above and show the predicted value\n\nYou now know how to create your own models, which is great, but you still haven\'t answered one very important question: how accurate is our model? This will be discussed next.\nSummary\nIn this lesson, you learned how to draw a best fit line for given data labels and features, by first calculating the slope and intercept. The calculated regression line was then used to predict the label ($\\hat y$-value) of a previously unseen feature ($x$-value). The lesson uses a simple set of data points for demonstration.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['coursera-Neural-Networks-and-Deep-Learning-Logistic_Regression_with_a_Neural_Network_mindset_v6a\n'], 'url_profile': 'https://github.com/DreamyPujara', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', '1', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pramod2302', 'info_list': ['Python', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zzzchaojishuai', 'info_list': ['Python', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lecture, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll go into the more formal notation of logistic regression models. Then, you\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nDescribe the mathematics behind logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(""age"", fontsize=14)\nplt.ylabel(""monthly income"", fontsize=14)\nplt.show()\n<Figure size 800x600 with 1 Axes>\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c = ""black"")\nplt.xlabel(""age"", fontsize=14)\nplt.ylabel(""monthly income"", fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a data set where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin =income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(""age"", fontsize=14)\nplt.ylabel(""monthly income (> or < 4000)"", fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not be exactly desired here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# create linear regression object\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(""age"", fontsize=14)\nplt.ylabel(""monthly income"", fontsize=14)\nplt.plot(age, lin_income, c = ""black"")\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# create logistic regression object\n# solver must be specified to avoid warning, see documentation for more information\n# liblinear was the default solver for previous version of scikit-learn\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n# train the model using the training sets\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n# create the linear predictor\nlin_pred= (age * coef + interc)\n# perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n# sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(""age"", fontsize=14)\nplt.ylabel(""monthly income"", fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c = ""black"")\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2, x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$. $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our data set) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, The outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real data example\nNow you will apply what you have learned to an example using real data.\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(""salaries_final.csv"", index_col = 0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric data type first.\n# convert race and sex using get_dummies \n# age will be ignored by get_dummies because it is numeric, see documentation for more information\nx_feats = [""Race"", ""Sex"", ""Age""]\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n# convert target using get_dummies\ny = pd.get_dummies(salaries[""Target""], dtype=float)\nimport statsmodels.api as sm\n\n# create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n# fit model\nlogit_model = sm.Logit(y.iloc[:,1], X)\n# get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Thu, 25 Jul 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 17:57:43   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\n      LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y.iloc[:,1])\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression and briefly analyzed their output. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nRun a complete regression analysis using python\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1,2,3,4,5,6,7,8,9,10], dtype=np.float64)\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7,853 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NumtraCG', 'info_list': ['Python', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Thailand', 'stats_list': [], 'contributions': '490 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gucino', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Machine-Learning-Multi-linear-Regression-stock-Price-Prediction-multiple-Macro-factors-using-python\nTable of content\n\n\nIntroduction\nExecutive Summary\nMachine Learning Algorithm\nMacro Economic Factors\n\n\nAnalysis And Interpretation\nMultiple Linear Regression\nCollection of Data\nCleaning of Data\nExploratory Data Analysis\nCorrelation\nSplitting Data\nBuilding the Model\nPerform Model Diagnostics\nPredicting\n\n\nConclusion\n\n\nExecutive Summary\nThe project titled ‚ÄúSTOCK PRICE PREDICTION OF BANKS USING MACHINE LEARNING‚Äù the basic problem of investing on stock market is most of the investors are not clear about the fundamental and techniques of investments, they mainly concentrate on risk and return involved in investing and make decisions without the guidance of experts which are thus based on judgments made  on their individual assessments.\nTherefore, the objective here is to understand the basic techniques involved in investing in equity market. Learning the above said objective will give a clear idea of the concept involved in investment decisions.\nTechnical analysis is the study of financial market action. The technical analysts look at the price movement that occur on day to day or week to week or over other constant time period displayed in graphic form, called charts. Technical analysis is a process of analysis is a process of analyzing a security‚Äôs historical prices in an effort to determine probable future prices.\nFrom the study it has been understand  that the price movement of the shares can be determined by using technical analysis and accurate buy and sell decision can be made so that the investors can minimize their risk and maximize the profits also show that all technical analysis indicators can give accurate decisions. The Study show that the historical prices have an impact on future prices, however, even if you are unable to accurately forecast prices, technical analysis can be used to consistently reduce your risks and improve your profits.\nMachine Learning Algorithm\nMultiple Linear Regression\nMultiple Linear Regression is a supervised learning algorithm for finding the existence of an association relationship between a dependent variable and several independent variables\nMultiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. The goal of multiple linear regression (MLR) is to model the linear relationship between the explanatory (independent) variables and response (dependent) variable.\nIn essence, multiple regression is the extension of ordinary least-squares (OLS) regression that involves more than one explanatory variable.\nExplaining Multiple Linear Regression\nA simple linear regression is a function that allows an analyst or statistician to make predictions about one variable based on the information that is known about another variable. Linear regression can only be used when one has two continuous variables an independent variable and a dependent variable. The independent variable is the parameter that is used to calculate the dependent variable or outcome. A multiple regression model extends to several explanatory variables.\nThe multiple regression model is based on the following assumptions:\n‚Ä¢\tThere is a linear relationship between the dependent variables and the independent variables.\n‚Ä¢\tThe independent variables are not too highly correlated with each other.\n‚Ä¢\tyi observations are selected independently and randomly from the population.\n‚Ä¢\tResiduals should be normally distributed with a mean of 0 and variance œÉ.\nThe coefficient of determination (R-squared) is a statistical metric that is used to measure how much of the variation in outcome can be explained by the variation in the independent variables. R2 always increases as more predictors are added to the MLR model even though the predictors may not be related to the outcome variable.\nR2 by itself can\'t thus be used to identify which predictors should be included in a model and which should be excluded. R2 can only be between 0 and 1, where 0 indicates that the outcome cannot be predicted by any of the independent variables and 1 indicates that the outcome can be predicted without error from the independent variables.\nMacro Economic Factors\nGDP\nUnemployment Rate\nInflation Rate\nConsumer Confidence\nConsumer Price Index\nGold price\nYield Value Of 10 Years bond\nCOLLECTING DATA:\nWe used eikon and quandl Sources for collecting the macro details and four banks stock prices of 20 years\nWe used API key and RIC‚Äôs to gets the data from eikon by the below codes and saved the data in the csv files.\nCODE FOR GETTING DATA USING API AND SAVING IT TO CSV FILE\nimport eikon as ek\nimport pandas as pd\nek.set_app_key(\'53f3b6e6a2af4ee987df42330c0fcbf40f75a204\')\nAs we are unable to get the 20 years of data we merged the data by calling it in separate parts\nbank = ek.get_timeseries([""JPM""], start_date = ""1998-12-01"", end_date =""2007-12-21"", interval=""daily"",corax=""adjusted"") # JP Morgan\nbank_1 = ek.get_timeseries([""JPM""], start_date = ""2007-12-22"", end_date =""2019-11-12"", interval=""daily"",corax=""adjusted"")\nbank_jpm_1=pd.merge(bank,bank_1,on=[\'Date\',\'HIGH\', \'CLOSE\', \'LOW\', \'OPEN\', \'COUNT\', \'VOLUME\'],how=""outer"")\nbank_jpm_1.to_csv(""JPM.csv"")\nUnemployment Rate :\nUS_UNEMPLOY = ek.get_timeseries([""USUNR=ECI""], start_date = ""1998-12-01"", end_date = ""2019-11-12"",interval=""monthly"")\nUS_UNEMPLOY.to_csv(""unemployment.csv"")\nInflation Rate:\nUS_INFLATION = ek.get_timeseries([""aUSWOCPIPR""], start_date = ""1998-12-01"", end_date = ""2019-11-12"",interval=""yearly"")\nUS_INFLATION.to_csv(""inflation.csv"")\nConsumer Confidence Rate:\nUS_CC = ek.get_timeseries([""USCONC=ECI""], start_date = ""1998-12-01"", end_date = ""2019-11-12"",interval=""monthly"")\nUS_CC.to_csv(""consumer_confidence.csv"")\nUSA GDP:\nimport quandl\nUSA_GDP = quandl.get(""FRED/GDP"")\nUSA_GDP.to_csv(""gdp.csv"")\nCPI:\nUS_CPI = ek.get_timeseries([""USCPSA=ECI""], start_date = ""1998-12-01"", end_date = ""2019-11-12"",interval=""monthly"")\nUS_CPI.to_csv(""cpi.csv"")\n10Y Bond yield%:\nYield = ek.get_timeseries([""aUSEBM10Y""], start_date = ""1998-12-01"", end_date = ""2019-11-12"",interval=""monthly"")\nYield.to_csv(""yield.csv"")\nGold price world wise:\nGOLD1 = ek.get_timeseries([""XAU=""], start_date = ""1998-12-01"", end_date = ""2007-12-21"",interval=""daily"")\nGOLD2 = ek.get_timeseries([""XAU=""], start_date = ""2007-12-22"", end_date = ""2019-11-12"",interval=""daily"")\nGOLD=pd.merge(GOLD1,GOLD2,on=[\'Date\',\'HIGH\', \'CLOSE\', \'LOW\', \'OPEN\'],how=""outer"")\nGOLD.to_csv(""gold.csv"")\nCLEANING DATA:\nOur data has different granularity so we while merging the data we found lot of NaN values by using interpolate we are able to remove the NaN values and arranged them in equal granularity.\n####function for data massaging\nimport pandas\ndef bank(filename):\nbank=pandas.read_csv(filename,parse_dates=[""Date""],index_col=""Date"")\nbank.drop(columns=[\'HIGH\',\'LOW\', \'OPEN\', \'COUNT\',\'VOLUME\'],inplace=True)\ndt=pandas.date_range(start=""1998-12-01"",end=""2019-11-11"",freq=""D"")\nidx=pandas.DatetimeIndex(dt)\nbank=bank.reindex(idx)\nbank.index.name=\'DATE\'\nreturn(bank)\n###calling Function\nBOA_BANK=bank(""BOA1.csv"")\nBOA_BANK.rename(columns={""CLOSE"":""BOA_CLOSE""},inplace=True)\nUSA GDP:\nusa_gdp=pandas.read_csv(""gdp12.csv"",parse_dates=[""Date""],index_col=""Date"")\nusa_gdp.rename(columns={""Value"":""GDP_VALUE""},inplace=True)\ndt=pandas.date_range(start=""1998-12-01"",end=""2019-11-11"",freq=""D"")\nidx=pandas.DatetimeIndex(dt)\nusa_gdp=usa_gdp.reindex(idx)\nusa_gdp.interpolate(method=""time"",inplace=True)\nUsa Unemployement:\nusa_unemp=pandas.read_csv(""unemployment.csv"",parse_dates=[""Date""],index_col=""Date"")\nusa_unemp.rename(columns={""VALUE"":""UNEMPLO_RATE""},inplace=True)\nUsa Inflation:\nusa_inf=pandas.read_csv(""inflation.csv"",parse_dates=[""Date""],index_col=""Date"")\nusa_inf.rename(columns={""VALUE"":""INFLA_RATE""},inplace=True)\nUsa Consumer Confidence:\nusa_CC=pandas.read_csv(""consumer_confidence.csv"",parse_dates=[""Date""],index_col=""Date"")\nusa_CC.rename(columns={""VALUE"":""CC_VALUE""},inplace=True)\nUSA CPI:\nusa_CPI=pandas.read_csv(""CPI.csv"",parse_dates=[""Date""],index_col=""Date"")\nusa_CPI.rename(columns={""VALUE"":""CPI_VALUE""},inplace=True)\nGOLD:\nGOLD=pandas.read_csv(""gold.csv"",parse_dates=[""Date""],index_col=""Date"")\nGOLD.drop(columns=[\'HIGH\',\'LOW\', \'OPEN\'],inplace=True)\nGOLD.rename(columns={""CLOSE"":""GOLD_CLOSE""},inplace=True)\nYIELD:\nYIELD=pandas.read_csv(""yield.csv"",parse_dates=[""Date""],index_col=""Date"")\nYIELD.rename(columns={""VALUE"":""YIELD_VALUE""},inplace=True)\nMerging Data:\ndata = BOA_BANK.merge(CITY_BANK, how=\'outer\', left_index=True, right_index=True)\ndata=data.merge(WELLS_BANK, how=\'outer\', left_index=True, right_index=True)\ndata=data.merge(JPM_BANK, how=\'outer\', left_index=True, right_index=True)\ndata=data.merge(usa_gdp, how=\'outer\', left_index=True, right_index=True)\ndata=data.merge(usa_unemp, how=\'outer\', left_index=True, right_index=True)\ndata=data.merge(usa_inf, how=\'outer\', left_index=True, right_index=True)\ndata=data.merge(usa_CC, how=\'outer\', left_index=True, right_index=True)\ndata=data.merge(usa_CPI, how=\'outer\', left_index=True, right_index=True)\ndata=data.merge(GOLD, how=\'outer\', left_index=True, right_index=True)\ndata=data.merge(YIELD, how=\'outer\', left_index=True, right_index=True)\nFor Filling The Nan Values:\ndata.interpolate(method=""time"",inplace=True)\nSlicing Data:\ndata=data[\'19990101\':\'20191111\']\ndata.index.name=\'DATE\'\nChecking For NaN values:\ndata.info()\nOUTPUT:\n<class \'pandas.core.frame.DataFrame\'>\nDatetimeIndex: 7620 entries, 1999-01-01 to 2019-11-11\nData columns (total 11 columns):\nBOA_CLOSE       7620 non-null float64\nJPM_CLOSE       7620 non-null float64\nCITY_CLOSE      7620 non-null float64\nWELLS_CLOSE     7620 non-null float64\nINFLA_RATE      7620 non-null float64\nGDP_VALUE       7620 non-null float64\nUNEMPLO_RATE    7620 non-null float64\nCC_VALUE        7620 non-null float64\nCPI_VALUE       7620 non-null float64\nGOLD_CLOSE      7620 non-null float64\nYIELD_VALUE     7620 non-null float64\ndtypes: float64(11)\nmemory usage: 1.0 MB\nVISUALIZING THE DATA:\nWe used pair plot, histograms and scatter plots to visualize the data to know the insights of data.\nBelow are the codes and outputs:\nsns.pairplot(data,kind=\'reg\')\nplt.show()\nSCATTER PLOTS: Below is the code for\nplt.plot(data[""JPM_CLOSE""],color=\'orange\')\nplt.xlabel(""YEAR"")\nplt.ylabel(""PRICE"")\nplt.title(""JPM MORGAN BANK"")\nHISTOGRAM PLOTS: Below is the code for\nplt.hist(data[""JPM_CLOSE""],bins=100,color=\'indigo\')\nplt.xlabel(""JPM_CLOSE_PRICE"")\nplt.ylabel(""FREQUENCY"")\nplt.title(""HISTOGRAM OF JPM BANK"")\nCORRELATION PLOT: Below is the code for correlation\nmask=np.zeros_like(data.corr())\ntraingle=np.triu_indices_from(mask)\nmask[traingle]=True\nplt.figure(figsize=(10,10))\nsns.heatmap(data.corr(),mask=mask,annot=True,annot_kws={""size"":14})\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nsns.set_style(""white"")\nSPLITTING DATA:\nBelow is the code for splitting data.in the below code x is the target value y is the features.used sklearn module for splitting the data into training and testing data in the ratio 80:20.\nfrom sklearn.model_selection import train_test_split\nx2=data[[\'JPM_CLOSE\']]\ny2=data.drop([\'BOA_CLOSE\', \'JPM_CLOSE\', \'CITY_CLOSE\',\'WELLS_CLOSE\'],axis=1)\nx_train2,x_test2,y_train2,y_test2=train_test_split(y2,x2,test_size=0.2,random_state=0)\nBUILDING MODEL:\nUsed multi linear regression in order to predict the model we considered JP Morgan bank Close price as target variable and features as all the macro factors\nMULTI LINEAR REGRESSION MODEL CREATION OF JPM BANK:\nregression2=LinearRegression()\nregression2.fit(x_train2,y_train2)\nprint(pandas.DataFrame({""index"":x_train2.columns,""coeff"":regression2.coef_.tolist()[0]}))\npandas.DataFrame(data=regression2.coef_,index=[\'coefficient\'],columns=x_train2.columns)\nprint(""intercept:"",regression2.intercept_)\nprint(\'R^2 train dataset:\',regression2.score(x_train2,y_train2))\nprint(\'R^2 test dataset:\',regression2.score(x_test2,y_test2))\nOUTPUT:\nindex                  coeff\n0    INFLA_RATE         - 0.941970\n1     GDP_VALUE        0.013303\n2  UNEMPLO_RATE   1.701889\n3      CC_VALUE           0.381072\n4     CPI_VALUE         -0.871155\n5    GOLD_CLOSE        0.004967\n6   YIELD_VALUE         4.657317\nintercept: [-28.65914051]\nR^2 train dataset: 0.9002138646376936\nR^2 test dataset: 0.9067647947946865\nEVALUATING THE MODEL: In this we done some statistical test in order to find out the significance of each independent variable to the target variable so we performed p statistics by using OLS (ordinary Least Square ) method of statsmodels.\nCode:\nimport statsmodels.api as sm\nx2_include_constant=sm.add_constant(x_train2)\nmodel2=sm.OLS(y_train2,x2_include_constant)\nresults2=model2.fit()\nresults2.params\nresults2.pvalues\nprint(pandas.DataFrame({""Coeffiencients"":results2.params,""P-Values"":round(results2.pvalues,2)}))\noutput:\n                   Coeffiencients     P-Values\n\nconst                        -28.659141       0.0\nINFLA_RATE            -0.941970       0.0\nGDP_VALUE             0.013303       0.0\nUNEMPLO_RATE     1.701889       0.0\nCC_VALUE                 0.381072       0.0\nCPI_VALUE               -0.871155       0.0\nGOLD_CLOSE             0.004967       0.0\nYIELD_VALUE             4.657317       0.0\nBy above results we found all the variables are significant to the target value.\nCHECKING FOR MULTI COLLINEARITY:\nVariance inflation Factor (VIF) is used to check the multi co-linearity this can be done by importing variance_inflation_factor from statsmodels in python.\nCODE:\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nVIF=[]\nfor i in range(0, len(x_include_constant.columns)):\nVIF.append(variance_inflation_factor(exog=x_include_constant.values,exog_idx=i))\nprint(VIF)\nprint(pandas.DataFrame({""coeffients"":x_include_constant.columns,""VIF"":np.around(VIF,3)}))\noutput:\ncoeffients               VIF\n0                 const            6185.503\n1           INFLA_RATE        1.968\n2           GDP_VALUE       295.629\n3     UNEMPLO_RATE      10.599\n4              CC_VALUE         12.833\n5             CPI_VALUE       350.044\n6            GOLD_CLOSE     22.206\n7           YIELD_VALUE      11.811\nFor the above output table we observed the VIF values are high for some variable so we will try to drop those variables in the table. So we will drop CPI_VALUE and then run the model below are the results.\nindex                coeff\n0    INFLA_RATE         -0.698328\n1     GDP_VALUE          0.007073\n2  UNEMPLO_RATE   1.957227\n3      CC_VALUE           0.490040\n4    GOLD_CLOSE       -0.003325\n5   YIELD_VALUE         3.654057\nintercept: [-121.04221419]\nR^2 train dataset: 0.8973329178860255\nR^2 test dataset: 0.9034611975914223\nVIF RESULTS AFTER DROPING CPI_VALUE:\nStill we have multi co-linearity as the GOLD _CLOSE is high so we will drop that, and similarly we checked for the rest once also and find and below are the results.\n      index                      coeff\n\n0     GDP_VALUE          0.005197\n1   UNEMPLO_RATE    1.319710\n2      CC_VALUE            0.548225\nintercept:                     [-84.52143208]\nR^2 train dataset:     0.8871161790732169\nR^2 test dataset:       0.8955482922548127\nSo we will continue the model with the above parameters as we considered the VIF limit is less than 5\nRESIDUALS: These are the difference between predicted value and target value.\nBelow code will be used to find the residuals\nCODE:\nresults2.resid\nCo-relation Between y_Train (actual target value) And Predicted y_Train(predicted value)\nCODE:\nact_pred2=pandas.DataFrame({""actual"":y_train2[""JPM_CLOSE""],""predicted"":results2.fittedvalues})\ncorr2=round(act_pred2[""actual""].corr(act_pred2[""predicted""]),2)\nprint(corr2)\noutput:\n0.94\nSo we can say by above result the relation holds well.\nGRAPH OF ACTUAL VS PREDICTION:\nSCATTER PLOT\nplt.figure(figsize=(7,7))\nplt.scatter(x=act_pred2[""actual""],y=act_pred2[""predicted""],color=""red"",alpha=0.6)\nplt.plot(act_pred2[""actual""],act_pred2[""actual""],color=""black"")\nplt.xlabel(""ACTUAL CLOSE VALUES"",fontsize=12)\nplt.ylabel(""PREDICTED CLOSE VALUES"",fontsize=12)\nplt.title(f\'ACTUAL  VS PREDICTED VALUES (corr{corr})\',fontsize=18 )\nplt.show()\noutput:\nRESIDUAL VS PREDICTED VALUES: In general we should have irregularity between these two parameters\nCODE:\nplt.figure(figsize=(7,7))\nplt.scatter(x=results2.fittedvalues,y=results2.resid,color=""navy"",alpha=0.6)\nplt.xlabel(""predicted values"",fontsize=12)\nplt.ylabel(""residuals"",fontsize=12)\nplt.title(""RESIDUALS VS FITTED VALUES"",fontsize=18 )\nplt.show()\noutput:\nDISTRIBUTION OF RESIDUALS:\nCODE:\nresidual_mean2=round(results2.resid.mean(),3)\nresidual_skew2=round(results2.resid.skew(),3)\nplt.figure(figsize=(10,10))\nsns.distplot(results2.resid,color=""blue"")\nplt.title(""RESIDUAL DISTRIBUTION"")\nOUTPUT:\nMEAN SQUARED ERROR:\nCODE:\nprint(results2.mse_resid)\nOUTPUT:\n60.263730672821495\nROOT MEAN SQUARED ERROR:\nCODE:\nRMSE2=np.sqrt(results2.mse_resid)\npandas.DataFrame({""R-Squared"":[results2.rsquared],""Mean Square Error"":[results2.mse_resid],""Root Mean Square"":np.sqrt(results2.mse_resid)},index=[""JPM_CLOSE""])\nOUTPUT:\nR-Squared  Mean        Square Error          Root Mean Square\nJPM_CLOSE                0.887116             60.263731                  7.762972\nNORMAL DISTRIBUTION GRAPH:\nBelow is the figure representing normal distribution graph with standard deviations (1 sigma =68%; 2sigma=95%,3 sigma=100%)\nCODE:\nprint(""one standard deviation:"",np.sqrt(results2.mse_resid)*1)\nprint(""Two standard deviation:"",np.sqrt(results2.mse_resid)*2)\nprint(""Three standard deviation:"",np.sqrt(results2.mse_resid)*3)\nOUTPUT:\none standard deviation: 7.762971768132452\nTwo standard deviation: 15.525943536264904\nThree standard deviation: 23.288915304397356\nPREDICTING THE VALUES WITH ONE STANDARD DEVIATION:\nIn order to do this we defined a function that will plot the graph with predicted, upper, lower limit and actual graph.\nCODE:\ndef predict_Bank_price(dataset):\na=regression2.predict(dataset)\nupper=a+1RMSE2\nlower=a-1RMSE2\npredict=pandas.DataFrame.from_records(a,columns=[""PREDICTED PRICE""])\nupper=pandas.DataFrame.from_records(upper,columns=[""UPPER PRICE""])\nlower=pandas.DataFrame.from_records(lower,columns=[""LOWER PRICE""])\nind=pandas.DataFrame(dataset.index)\nframes=[ind,predict,upper,lower]\ndf=pandas.concat(frames,axis=1)\ndf.set_index([""DATE""],inplace=True)\nprint(df)\nplt.plot(df)\nplt.legend(loc=\'best\')\nplt.show(block=False)\nreturn df\nCalling function and plotting:\nb2=predict_Bank_price(y2)\nplt.figure(figsize=(20,10))\nplt.plot(b2)\nplt.plot(x2)\nplt.title(""JPM_BANK"")\nOUTPUT:\nCONCLUSION:\nThe stock price of   4 listed companies in US stock market has been predicted using multi linear regression. Based on the stock of company either of the models can be used to predict the stock values the accuracy of stock models can be further improved by optimizing the model for relevant indicators and current market trends.\n'], 'url_profile': 'https://github.com/balaji9999', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows √ó 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows √ó 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows √ó 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the final column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nYou will be able to:\n\nImplement logistic regression in scikit-learn\nForm conclusions about the performance of a model\n\nLet's get started!\n#Starter Code\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n#Starter Code\ndf = pd.read_csv('heart.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\nDefine appropriate X and y\nRecall the dataset is whether or not a patient has heart disease and is indicated in the final column labelled 'target'. With that, define appropriate X and y in order to model whether or not a patient has heart disease.\n#Your code here \nX = \ny = \nNormalize the Data\nNormalize the data prior to fitting the model.\n#Your code here\nTrain Test Split\nSplit the data into train and test sets.\n#Your code here\nFit a model\nFit an initial model to the training set. In scikit-learn you do this by first creating an instance of the regression class. From there, then use the fit method from your class instance to fit a model to the training data.\nlogreg = LogisticRegression(fit_intercept = False, C = 1e12) #Starter code\n# Your code here\nPredict\nGenerate predictions for the train and test sets. Use the predict method from the logreg object.\n# Your code here\nInitial Evaluation\nHow many times was the classifier correct for the training set?\n# Your code here\nHow many times was the classifier correct for the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the train and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, splitting into train and test sets, and fitting a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nUnderstand and explain the use cases for linear and multiple regression analyses\nDescribe multiple regression with >1 predictors\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Identification-of-Survival-Rate-of-Prostate-Cancer-using-Survival-Analysis-and-Cox-Regression-\n-Created a ‚ÄúSurvival Rate Prediction‚Äù model using Survival Analysis\n-Identified the risk factors causing Prostate Cancer and investigated in detail the effect of these risk factors upon the Survival Rate using Cox Proportional-Hazards Regression.\n'], 'url_profile': 'https://github.com/vaishnavii28', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nIn this lecture, you'll learn about logistic regression with the scikit-learn package.\nObjectives\nYou will be able to:\n\nUnderstand and implement logistic regression\nCompare testing and training errors\n\nGenerally, the process for implementing logistic regression via scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we failed to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing Logistic Regression in scikit-learn using dummy variables and a proper train-test split.\nStep 1: Import the Data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nStep 2: Define X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows √ó 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df.Survived\nX.head() #Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows √ó 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\nX = X.fillna(value=0) #Fill null values\nfor col in X.columns:\n    X[col] = (X[col]-min(X[col]))/ (max(X[col]) - min(X[col])) #We subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows √ó 153 columns\n\nTrain-Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the regression class. From there, then use the fit method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept = False, C = 1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs on our test set.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n#We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros.\nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct for our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate for our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into train and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real ""learning algorithm"" that aspiring data scientists will come across. It is one of the simplest algorithms to master, but it still requires some mathematical and statistical understanding of the underlying regression process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nDescribe statistical modeling with simple regression\nExplain simple linear regression analysis as solving for straight-line equation: $y=mx+c$\nCalculate the slope and y-intercept given a set of data points\nCalculate a regression line based on calculated slope and intercept\nPredict a target value for a previously unseen input feature, based on model coefficients\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It‚Äôs a statistical methodology that helps estimate the strength and direction of the relationship between two (or more variables). Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows standard parametric assumptions like normality, linearity etc. These will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomenons or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes that data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the maths behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations in to draw a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a‚Äî\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a‚Äî\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression between two variables as a problem of fitting a straight line to best describe the data associations on a 2-dimensional plane.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Python', 'Updated May 9, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Nov 8, 2020', 'R', 'Updated Jun 3, 2020', 'Stata', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Haxe', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Nov 8, 2020', 'R', 'Updated Jun 3, 2020', 'Stata', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Haxe', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nYou were previously given a broad overview of logistic regression. This included two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels.\nObjectives\nYou will be able to:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with regression model parameters\n\nReview\nThe statsmodels example we covered had four essential parts:\n\nImporting the data\nDefining X and y\nFitting the model\nAnalyzing model results\n\nThe corresponding code to these four steps was:\nimport pandas as pd\nimport statsmodels.api as sm\n\n#Step 1: Importing the data\nsalaries = pd.read_csv(""salaries_final.csv"", index_col = 0)\n\n#Step 2: Defining X and y\nx_feats = [""Race"", ""Sex"", ""Age""]\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\ny = pd.get_dummies(salaries[""Target""], dtype=float)\n\n#Step 3: Fitting the model\nX = sm.add_constant(X)\nlogit_model = sm.Logit(y.iloc[:,1], X)\nresult = logit_model.fit()\n\n#Step 4: Analyzing model results\nresult.summary()\n\nMost of this should be fairly familiar to you; importing data with Pandas, initializing a regression object, and calling the fit method of that object. However, step 2 warrants a slightly more in depth explanation.\nRecall that we fit the salary data using Race, Sex, and Age. Since Race and Sex are categorical, we converted them to dummy variables using the get_dummies() method. The get_dummies() method will only convert object and category data types to dummy variables so it is safe to pass Age. Note that we also passed two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the data type of all of the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodel. Finally, note that y itself returns a pandas DataFrame with two columns as y itself was originally a categorical variable. With that, it\'s time to try and define a logistic regression model on your own!\nYour Turn - Step 1: Import the Data\nImport the data stored in the file titanic.csv.\n# Your code here\nStep 2: Define X and Y\nFor your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes it\'s a bit morbid). Follow the programming patterns described above to define X and y.\n# Your code here\nStep 3: Fit the model\nNow with everything in place, initialize a regression object and fit your model!\nWarning: If you receive an error of the form ""LinAlgError: Singular matrix""\nStatsmodels was unable to fit the model due to some Linear Algebra problems. Specifically, the matrix was not invertible due to not being full rank. In layman\'s terms, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n# Your code here\nStep 4: Analyzing results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Your code here\nYour analysis here\nLevel - up\nCreate a new model, this time only using those features you determined were influential based on your analysis in step 4.\n# Your code here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then reviewed interpreting the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Sci-kit learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Nov 8, 2020', 'R', 'Updated Jun 3, 2020', 'Stata', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Haxe', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'Gurgaon, India', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['Predicting-Insurance-Loss\nThis is my Olcademy Internship Assignment, wherein I have to predict insurance loss on a sample dataset. I decided against creating my own dataset. This is because I have no experience in the insurance claims department, and hence, my dataset features may have been irrelevant or unimportant. After some digging around, I came across this excellent dataset for the Allstate Claims Severity Kaggle competition.\nLink: https://www.kaggle.com/c/allstate-claims-severity/data\nSadly, the features remain anonymized. With that being said however, I would much rather build a strong model that takes into consideration the vast amount and complexity of features involved in insurance claim analysis over a weak model trained on a dataset with finite features and limited examples.\nBeyond the preprocessing and data pareparation steps, I had to decide on a model. Given the size of the dataset, my instinct reaction was to go for a neural network. However, due to time constraints, I used XGBoost regression, which delivered satisfactory results.\n'], 'url_profile': 'https://github.com/Akshat2430', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Nov 8, 2020', 'R', 'Updated Jun 3, 2020', 'Stata', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Haxe', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'PUNE', 'stats_list': [], 'contributions': '317 contributions\n        in the last year', 'description': ['A.I--Hit__TARGET\nThis is my First Self_Learning_Game_USING Logistic_Regression and PYGAME\nDescription\nIn this project there is a projectile and a Goal( Block ) on which we are suppose to hit .\nThis block keep appering  at random places on the secreen. The Projectile is provided with the distance from the\ngoal.\nNow its aim is to find the best possible speed which is to be launced at a certain angle (Angle is provided by the user)\nto hit the target.\nFor finding the I have not used any physiscs formula insted used Linear Regression for find it.\n'], 'url_profile': 'https://github.com/Sudhanshu1304', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Nov 8, 2020', 'R', 'Updated Jun 3, 2020', 'Stata', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Haxe', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '240 contributions\n        in the last year', 'description': ['covid-spatial-reg\n'], 'url_profile': 'https://github.com/murray-cecile', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Nov 8, 2020', 'R', 'Updated Jun 3, 2020', 'Stata', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Haxe', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '228 contributions\n        in the last year', 'description': ['SS154---Econometrics-Economic-Systems\nLearn how to use core econometric and statistical methods to construct and validate economic and social models. Students will learn methods to conduct and critique empirical studies in economics and related fields. We apply these techniques to answer questions such as: What are the effects of class size on test scores in California schools? Do cigarette taxes reduce smoking? Do drunk driving laws actually reduce traffic deaths? Do mosquito nets reduce the incidence of malaria? Quantitative approaches used to answer such questions include multiple regression, nonlinear regression and instrumental variables regression.\n'], 'url_profile': 'https://github.com/jasenlo123', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Nov 8, 2020', 'R', 'Updated Jun 3, 2020', 'Stata', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Haxe', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tangj1905', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Nov 8, 2020', 'R', 'Updated Jun 3, 2020', 'Stata', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Haxe', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'UK', 'stats_list': [], 'contributions': '2,833 contributions\n        in the last year', 'description': [""This is a rough and dirty script used to find breaking changes to the haxe compiler\nIt does so by checking out pre-built nightly binaries\nTo use, drop this directory into your local clone of haxe, cd into haxe-bisect-script and run haxe run.hxml\nThe script is meant to be changed by hand, edit Bisect.hx to suit your own problem. I've written it for macOS, you'll need to tweak for other systems\n""], 'url_profile': 'https://github.com/haxiomic', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Nov 8, 2020', 'R', 'Updated Jun 3, 2020', 'Stata', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Haxe', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'Jamshedpur Jharkhand', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['Rossman Store Sales Prediction\nProject Description\nRossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.\nProvided with historical sales data for 1,115 Rossmann stores. The task is to forecast the ""Sales"" column for the test set. Note that some stores in the dataset were temporarily closed for refurbishment.\nProject Overview\n\nExploratory Data Analysis and Visualisation\nFeature Engineering\nMerging train and store dataset to form new train dataset containing all the attributes\nBuilding and Implementing Machine Learning Models\nModel Evaluation\nTest dataset prediction of sales for up to six weeks\n\nResources Used\nPython Version : 3.8\nPackages : pandas, numpy, sklearn, matplotlib, seaborn, plotly, cufflinks\nReference and article : Google\nProject Motivation :Krish Naik\nDataset can be found here: Kaggle\nExploratory Data Analysis :\nI have done indepth analysis by going through each attributes, finding their relations with the store sales, making pivot tables and plotting graphs\nSome of my findings are :\nDays of Week :\n\nTakeaway\n\nThe 7th day of week has very less variability as compare to other days of week\nThe count of 7th days is very less as compare to other days but the average sales and average number of customers are pretty much high.\nOne possible reason could be on sunday customers comes for a specific commodity as an essential need for survival\n\nrunning Promo :\n\nTakeaway\n\nThere is much difference in sale before and after running Promo. It indicates that promo have done a great job in increasing the sale\nNot much noticable difference is seen in number of customers visit to store. Promo idea was not capable to attract new customers but the buying quantity of existing old customers have increased, therefore overall the running of promo worked\n\nState Holidays :\n\nTakeaway\n\nPeople are more often to buy more on Christmas and Easter festival, therefore sales and count of customers visit are more on this seasons\nBut it is clearly seen the variation in public choice is less in these festival as people tend to buy particular range and type of product,where as the opposite behavior is been observed when there is no holiday\n\n\nTakeaway\n\nAs early said Christman and Easter festival season shows high rate of sales\n\n\n\nFeature Engineering\n\nConverted into datetime format containing date information\nMade column of sales per customers\nMade columns which captures all the information of customers based on different categories of stores\nMade columns to show Date information separately\nMade column contains date information in months since the competition was opened\nMade column contains date information in weeks since the promo is running\nGet the dummy varaible of categorical columns\nLog transformation to get the variables distribution as close as Gaussian distribution\n\nBuilding and Implementing Machine Learning Models\nFirst I split the dataset into train and test, then i performed ML model building and trained the model using engineered dataset.\nWith engineered dataset I tried to build Random Forest model beacause the model\'s performance is not affected by an outliers\nModel Evaluation\n\noob score : 0.9228561536464273\nRoot Mean Square Error : 854.7540464572053\n\n5 Important features\n\nPredictions from Model\n\n'], 'url_profile': 'https://github.com/prashantlal56', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Nov 8, 2020', 'R', 'Updated Jun 3, 2020', 'Stata', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Haxe', 'Updated Feb 10, 2021', 'Jupyter Notebook', 'Updated Jul 24, 2020']}"
"{'location': 'Washington DC', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['len-club-loans\n'], 'url_profile': 'https://github.com/sandevaj', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'MATLAB', 'MIT license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Python', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 18, 2020', 'Updated Nov 21, 2020', 'HTML', 'Updated May 11, 2020']}","{'location': 'New York, New York', 'stats_list': [], 'contributions': '1,473 contributions\n        in the last year', 'description': ['Introduction\n\nData Set Information:\nListing of attributes:\n\n\nThe last column >50K, <=50K is the target variable indicating whether the people earn less than or larger than 50K per year\n\n\nage: continuous.\n\n\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n\n\nfnlwgt: continuous.\n\n\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n\n\neducation-num: continuous.\n\n\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n\n\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n\n\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n\n\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n\n\nsex: Female, Male.\n\n\ncapital-gain: continuous.\n\n\ncapital-loss: continuous.\n\n\nhours-per-week: continuous.\n\n\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n\n\n\nExploratory Data Analysis \n\nGender Graph:\nSalary Distribution By Gender: \n                 \n\nCorrelation HeatMap\nCorrelation in DataSet: \n        \n\nConfusionMatrix: \nLogistic Regression Confusion Matrix: \n            \n\n\n'], 'url_profile': 'https://github.com/Arx1971', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'MATLAB', 'MIT license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Python', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 18, 2020', 'Updated Nov 21, 2020', 'HTML', 'Updated May 11, 2020']}","{'location': 'Lahore, Pakistan', 'stats_list': [], 'contributions': '819 contributions\n        in the last year', 'description': ['Bill-Projection\nIntroduction\nThis repository contains two projects\n\nHourly Bill Projection\nMonthly Bill Projection\n\nLinear Regression models are being used for completing the both tasks. Algorithms train on the past data of Date, Time, PV and Temperature and predict the Load accordignly.\nSteps to Run the Code:\nEach sub-directory contains the README file explaining the steps to run the code successfully.\nRequirements:\nMATLAB R2017a (9.3) (64-bit) or higher version should be installed in your machine to run this project successfully.\n'], 'url_profile': 'https://github.com/zainmujahid', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'MATLAB', 'MIT license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Python', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 18, 2020', 'Updated Nov 21, 2020', 'HTML', 'Updated May 11, 2020']}","{'location': 'Nashville', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Fake-News-Detection\nFake news dataset: https://www.uvic.ca/engineering/ece/isot/datasets/index.php\n'], 'url_profile': 'https://github.com/karunagujar13', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'MATLAB', 'MIT license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Python', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 18, 2020', 'Updated Nov 21, 2020', 'HTML', 'Updated May 11, 2020']}","{'location': 'Taipei, Taiwan', 'stats_list': [], 'contributions': '171 contributions\n        in the last year', 'description': ['Audi Used Car Market Analysis\nA market analysis project that can help people gain insights into the used car industry of Audi in Taiwan\nTask\nWrite a web-crawler to gather used car data from the official page of Audi and build regression models to analyze the correlations between variables.\nFor example, analyze the correlation between the mileage and the price of the used car.\nRun Locally\n\nRun this Command: git clone https://github.com/b05702057/Audi-Used-Car-Market-Analysis.git\nChange directory to the where the file ""Audi_Data_Web_Crawler.py"" is saved\nInstall all the toolkits used in this project\nRun this Command: python Audi_Data_Web_Crawler.py\nImport the file ""Audi Used Cars Regression Analysis.R"" into Rstudio and run each command in the file\n\nNote\n\nThe detailed statistical analysis can be seen in the report.\nThe webpage of Audi is constantly revised, and this may cause the web-crawler to break down.\nIf the situation happens, please notify me, and I will fix the problems as soon as possible.\n\n'], 'url_profile': 'https://github.com/b05702057', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'MATLAB', 'MIT license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Python', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 18, 2020', 'Updated Nov 21, 2020', 'HTML', 'Updated May 11, 2020']}","{'location': 'Belo Horizonte, Brasil', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['Predict paid off or defaulted loans\nDevelopment of a classification model to predict customers who will have their loan paid off or defaulted. The KNN, Decision Tree, SVM and Logistic Regression models will be used.\nProject built during the course Machine learning with Python (Coursera).\npt-br\n\nDesenvolvimento de um modelo de classifica√ß√£o para prever os clientes que ter√£o seu empr√©stimo pago ou inadimplente. Ser√£o utilizados os modelos KNN, Decision Tree, SVM e Logistic Regression.\nProjeto construido durante o curso de Machine learning with Python Coursera.\n\nInstallation\nDependencies\nThe dependencies to development this project are below:\npip install scikit-learn\n\nUser installation\nYou need to have the jupyter notebook installed, along with numpy, pandas and matplotlib.\npip install jupyter\n\nDevelopment\nContributions to the project are welcome. The idea was to test the accuracy of some models of the library ** scikit ** - ** learn ** to classify data about people that will have their loan paid off or defaulted.\nThis data set refers to previous loans. The Loan_train.csv dataset includes details for 346 customers whose loans have already been paid or have defaulted.\nSource code\nYou can check the latest sources with the command:\ngit clone https://github.com/joaodias-code/jd-loan-paid-off-project.git\n\n'], 'url_profile': 'https://github.com/joaodias-code', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'MATLAB', 'MIT license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Python', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 18, 2020', 'Updated Nov 21, 2020', 'HTML', 'Updated May 11, 2020']}","{'location': 'Sylhet', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['House-Price-Prediction\nThis is a TensorFlow project where TensorFlow and Keras is used to predict the house price using regression technique. This project used python 3 and these packages: TensorFlow, Pandas, Matplotlib, Scikit-learn.\n'], 'url_profile': 'https://github.com/Jack-Devil', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'MATLAB', 'MIT license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Python', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 18, 2020', 'Updated Nov 21, 2020', 'HTML', 'Updated May 11, 2020']}","{'location': 'Mexico City, Mexico', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['Predicting-Premier-League\n'], 'url_profile': 'https://github.com/pablolopez2733', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'MATLAB', 'MIT license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Python', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 18, 2020', 'Updated Nov 21, 2020', 'HTML', 'Updated May 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['Climate-Visiblity\nEnd to End project,using Machine Learning to build a Regression model to predict the visiblity distance based on given different climatic indicators and Deployed on Pivotal Web Services.\n'], 'url_profile': 'https://github.com/Rajeevkoneru', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'MATLAB', 'MIT license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Python', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 18, 2020', 'Updated Nov 21, 2020', 'HTML', 'Updated May 11, 2020']}","{'location': 'Farmington Hills, Michigan', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': [""Analyze-A-B-Test-Results\nThis project was completed as part of the course requirements of Udacity's Data Analyst Nanodegree certification.\nOverview\nThe project conducted A/B testing of user conversions on an old and new wepage.  Steps included handling mismatched condition and page assignment, removing duplicate ids, hypothesis testing via bootstrapping and standard statistical tests, and multiple regression modelling.\nStatistical Analysis\n. Bootstrapping sampling distributions and p-value calculations\n. Z-core tests\n. Logistic regression\n. Multiple linear regression modelling to assess for interactions of independent variables\nTechnologies Used\n. Python, Numpy, Pandas, Matplotlib, StatsModels, Scipy\n. LaTex\n. Jupyter Notebook\nKey Findings\n. The conversion rate for the new page was not superior to that for the old page\n. The country of the user did not impact the rate of conversion between the new and old page\n""], 'url_profile': 'https://github.com/rohini2615', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 18, 2020', 'MATLAB', 'MIT license', 'Updated May 11, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Python', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 18, 2020', 'Updated Nov 21, 2020', 'HTML', 'Updated May 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['Climate-Visiblity\nEnd to End project,using Machine Learning to build a Regression model to predict the visiblity distance based on given different climatic indicators and Deployed on Pivotal Web Services.\n'], 'url_profile': 'https://github.com/Rajeevkoneru', 'info_list': ['Updated Nov 21, 2020', '1', 'R', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Updated May 29, 2020', 'R', 'Updated May 5, 2020']}","{'location': 'Jaipur, Rajasthan', 'stats_list': [], 'contributions': '582 contributions\n        in the last year', 'description': [""Data Analysis in R\nThis is a comparision program which is written for learning various methods to build a machine learning model. This can be used in Business Data Analysis and in various prediction models in oeder to make them efficient and presentable.\nGoal is to provide a contrast in different models based on the parameters like accuracy, confusion matrix and MSE .\nüî® Basic Working\nWe took two datasets, i.e, House Price and Movie Collection and on these datasets we applied techniques such as LDA, Logistic Regression, Support Vector Machine and Decision Tree.\n\n\nIn House Price Analysis, we have to classify the model such that it presents a view, whether the house listed will be sold in next 3 months or not. For this we employed Linear Discriminant Analysis and Logistic Regression, after which the MSE for both is calculated to examine the best model.\n\n\nMovie Collection, is separated in two parts. The first one tells about the collection a perticular movie made at Box Office and the other part tells that whether the movie won oscar or not. Here SVM is used for modelling and Decision Tree for both classification and regression model is made to enhance the presentation of the model.\n\n\n\n\nHistogram of one observation is shown as to understand the effect of a variable on observational result.\n\nTest, Train and Validation\nSplit data into train, validation and test dataset.These datasets help us find the best model and prevent overfitting.\nFor each hyperparameter combination, train a model with the training dataset, and evaluate the trained model with the validation dataset to provide an unbiased evaluation. Select the best model based on the validation metrics. The test data is used to provide an unbiased evaluation of the final model.\n\n\nA cumulative study of various parameters on the result is shown. Here the features are present in diagonal coulmn and their effect on each of the other variable is observed via scatterplot.\n\nDependencies\n\nPython 3.6\nRStudio\nNumPy\nScikit-learn\nMatplotlib\n\nüì¶ Install\nOpen the code Analysis_LDA.R in Rstudio after downloading both RStudio and 'RStudio for Windows' simply run it. Before this save the csv files in a folder so that you can fetch it by putting the location of saved file in the code.\nSimilarly fetch the files for SVM and Decision Tree model and run the code line by line by pressing Ctrl+Enter, with single or multiple selections.\n""], 'url_profile': 'https://github.com/Hmrinal', 'info_list': ['Updated Nov 21, 2020', '1', 'R', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Updated May 29, 2020', 'R', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Microsoft-stock-price-prediction-using-SVR-and-LR\nIt is a simple ML project. SVR and LR are used as ML models to predict the price of the stock for next 30 days (it can be adjusted).\nRegression models are not generally reliable for price predictions.\n'], 'url_profile': 'https://github.com/AnarHuseynov00', 'info_list': ['Updated Nov 21, 2020', '1', 'R', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Updated May 29, 2020', 'R', 'Updated May 5, 2020']}","{'location': 'Haryana, India', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/arnavobero1', 'info_list': ['Updated Nov 21, 2020', '1', 'R', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Updated May 29, 2020', 'R', 'Updated May 5, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': [""Lending-Club\nThe Lending Club Company is the largest online loan marketplace. This Company has two types of customers i.e. investors and borrowers. Investors are the ones who invest in the lending club and get interest paid from the club. Borrowers are the ones where investment from the Investors is given to the borrowers with a certain interest rate depending on the type of loan and valuation of the borrower. It facilitates personal loans, business loans, and financing of medical procedures. Borrowers can easily access lower interest rate loans through a fast online interface. When the company receives a loan application, the company has to make a decision for loan approval based on the applicant‚Äôs profile. When a person applies for a loan, there are two types of decisions that could be taken by the company:\nLoan accepted: If the company approves the loan, there are 3 possible scenarios described below:\nFully paid: Applicant has fully paid the loan (the principal and the interest rate)\nCurrent: Applicant is in the process of paying the installments, i.e. the tenure of the loan is not yet completed. These candidates are not labeled as 'defaulted'.\nCharged-off: Applicant has not paid the installments in due time for a long period of time, i.e. he/she has defaulted on the loan\nLoan rejected: The Company had rejected the loan (because the candidate does not meet their requirements etc.). Since the loan was rejected, there is no transactional history of those applicants with the company and so this data is not available with the company (and thus in this dataset)\n""], 'url_profile': 'https://github.com/santhoshchakilamcs', 'info_list': ['Updated Nov 21, 2020', '1', 'R', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Updated May 29, 2020', 'R', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AashimaSachdeva', 'info_list': ['Updated Nov 21, 2020', '1', 'R', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Updated May 29, 2020', 'R', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '451 contributions\n        in the last year', 'description': ['BoardGamePriceAnalysis\nUsing the data on the Boardgamegeek.com rankings, as well as data scraped from the web, multilayer perceptron and random forest models were applied to predict the prices of the boardgames. Unfortunately, both models concluded with mean absolute errors around ~¬£9. This is not what I had been hoping for. However, it became clear that whilst the data contained many features, it lacked some of the key information that would indicate the price of the boardgame. Specifically, the quality of game components, location and scale of manufacture. Many boardgames vary wildly in price simply due to production costs. If a game contains many high quality components, it is going to cost more, regardless of its rating or weight on Boardgame geek, or the mechanics employed. Thus, whilst this project would not be suitable for querying whether or not you are getting value for your money - it does a reasonable job at predicting the rough pricing of a game.\nThe boardgame geek reviews data was collated by Jesse van Elteren (https://www.kaggle.com/jvanelteren/boardgamegeek-reviews). I do not own this data, it is owned by BoardGameGeek.\n'], 'url_profile': 'https://github.com/Laurencewm', 'info_list': ['Updated Nov 21, 2020', '1', 'R', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Updated May 29, 2020', 'R', 'Updated May 5, 2020']}","{'location': 'Fort Collins, Colorado, USA', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Airline-Twitter-Sentiment-Analysis\nThis multi-class sentiment analysis was carried out using three classification algorithms: Multinomial Na√Øve Bayes, Support Vector Matrix(SVM), and Random Forest. I also used logistic regression and VADER algorithms.\n'], 'url_profile': 'https://github.com/brungesh', 'info_list': ['Updated Nov 21, 2020', '1', 'R', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Updated May 29, 2020', 'R', 'Updated May 5, 2020']}","{'location': 'Coimbatore, India', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Forest_Fires_Regression\nPython implementation of a regression task, where the aim is to predict the burned area of forest fires, in the northeast region of Portugal, by using meteorological and other data.\nThe dataset can be found at the UCI ML repository.\n\n\nReferences\nP. Cortez and A. Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data. In J. Neves, M. F. Santos and J. Machado Eds., New Trends in Artificial Intelligence, Proceedings of the 13th EPIA 2007 - Portuguese Conference on Artificial Intelligence, December, Guimar√£es, Portugal, pp. 512-523, 2007. APPIA, ISBN-13 978-989-95618-0-9.\n'], 'url_profile': 'https://github.com/kabilanmohanraj', 'info_list': ['Updated Nov 21, 2020', '1', 'R', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Updated May 29, 2020', 'R', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/soliashuptar', 'info_list': ['Updated Nov 21, 2020', '1', 'R', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated May 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', 'Updated May 29, 2020', 'R', 'Updated May 5, 2020']}"
"{'location': 'Berkeley, CA', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['COVID-19 Exploratory Data Analysis\nIntroduction\nThe total number of COVID-19 cases in the US has recently surpassed 1.4 million, with over 80,000 deaths due to the disease or related complications. Though the world‚Äôs foremost institutes are dedicated to researching the virus, we still thought it worth exploring the COVID-19 dataset. Upon some precursory data analysis, we decided to focus our the brunt of our EDA on how COVID-19 affects people of different social status/background. Which social factors are most closely correlated to COVID-19 incidence? If an initial hypothesis that the lower class is more affected by COVID-19 is correct, what are the specific class factors that influence COVID-19 incidence? More specifically, how does SVI (social vulnerability) affect COVID-19 incidence?\nNote: In this project we will be mainly examining COVID-19 in the United States, with parts focusing on California.\nDescription of Data/EDA\nI - SVI and COVID\nSVI (Social Vulnerability Index) is a measure of the ‚Äúresilience of communities when confronted by external stresses on human health, stresses such as natural or human-caused disasters, or disease outbreaks.‚Äù\nA county‚Äôs SVI percentile, given in ‚Äúabridged_couties.csv‚Äù, gives a rough estimate on how vulnerable a county is with respect to the rest of the nation. We merged this into  ‚Äúcounty_info_cases.csv‚Äù to combine county demographic data with COVID-19 case and death data at a county level. We then selected all California counties, and created a pair of new dataframes with SVI percentile data along with case and death data.\nVisualizations\nThe plot_svi_bins() function binned the counties using dataframe methods and then plotted their case and death rates over time, shown below:\n\n\nThe graphs confirm some basic suspicions: the most socially vulnerable counties suffer the largest impact from COVID-19, and the middle 50% of counties have roughly the same COVID-19 incidence. The one surprising feature of these graphs is how quickly the bottom quartile flatlined in both case and death rates, with there being practically no increase in deaths among these counties since mid-April.\nWe briefly examined which factors of SVI had the highest correlation with COVID-19 incidence. This examination can be seen in the SVI portion of our notebook.\nII - Correlation Exploration\nIn addition, we tried to find the specific variables that affect COVID-19 cases/deaths/infections rate. We correlated all appropriate demographic variables (logged and otherwise) with either deaths per capita, cases per capita, or new infections per current infections. Then, we plotted those with the highest correlation. This allowed us to see variables worth exploring more. Here are a subset of the plots with deaths per capita:\n\nWhile performing EDA, we discovered some major issues to be aware of. Many values were zeroes, so performing log transformations were difficult. Cases and demographic county data like the number of hospitals was largely in absolute numbers, which required normalization with the population.\nMethods\nPCA: To further examine our high dimensional data, we decided to use PCA to identify strong associations in the data. Thus, we performed PCA on the cases by day for counties in California, and plotted each county according to PC1, PC2, and PC3.\n\nTo see what PC1 and PC2 were associated with, we then identified clusters in the resulting graph and plotted the cases over time for each cluster. We found that the low PC1 cluster tended to have a higher infection per capita, especially around mid April.\n\n\nResults, Discussion, and Difficulties\nWe would categorize the above methods as a surface level data exploration, as it only looks at potential variables instead of a deep dive. It gives us promising areas of further study, such as a score for minority/language ranking, but also topics that seem to have no useful relation, like longitude. It also shows that the topic is incredibly difficult to study-- not just in the number of variables but also the nature of the disease.\nThe correlation plots have a couple problems: They assume a linear relationship and generally have rather low correlations. To combat the first issue, we also tried it with different log combinations, but we did not try square root or exponentiating. With so many variables tried, it‚Äôs very possible that some are highly correlated due to chance.\nIII - Mortality Prediction\nMany already-existing models already do an excellent job of predicting and visualizing the future of COVID-19, but we nevertheless explored the short-term future of COVID-19‚Äôs mortality rate.\nData Cleaning\nThough we performed our previous analysis at the county-scale, we found that county data was far too messy to warrant usage of a regression model. The vast majority of counties reported few deaths, and at times the cumulative death statistics (which should be strictly increasing) dropped. To obtain more reliable data, we moved to the state level. There was far less demographic data for states available to us, so our goal was not to directly predict future behavior from demographics; instead, our model predicts future mortality rates based on past mortality and incident rates. We do expect demographic and response information to greatly effect the behavior of the time series, but we treat those features as latent variables embedded within the behavior of the time series. With this in mind, we decided to create a moving-window model that predicts COVID-19‚Äôs mortality rate based on a past number of days.\nWe chose mortality rate as our predictive statistic for a number of reasons. First, we make the assumption that the confounds associated with COVID mortality are less pronounced than those associated with its incidence rate. While mortality rates may vary based on  to be the cases rate per day had a heavy dosage of NaN values, so we ultimately took the path of least resistance and analyzed mortality rate over time. This involved scraping GitHub for the cases information each day (done with the function get_day_cases() in utils.py), filtering based on state, and then selecting features to create a training matrix.\nMethods\nLinear Regression: Because we were working at a state scale, we did not have the same access to demographic data as our previous analyses. We decided to take a moving-window approach; calculating the next X days‚Äô mortality rates based off of case and mortality information from the past Y days, using a LinearRegression model from SciKit. The features we ultimately decided to use were the ‚ÄòConfirmed‚Äô, ‚ÄòDeaths‚Äô, ‚ÄòActive‚Äô, ‚ÄòIncident_Rate‚Äô, and ‚ÄòMortality_Rate‚Äô columns (where ‚ÄòMortality_Rate‚Äô was only used for the past Y days) out of get_day_cases(). We achieved a training RMSE of just over .02 using these features, with our cross-validation RMSE coming out to be just over .033 (both values are available in regression.ipynb).\nVisualizations\n\nFor several West coast states, the mortality rate in the short term seems to be decreasing, which reflects where in the curve the west coast is (COVID-19 spread throughout the West Coast sooner than in the Southeast, for example):\n\nIn the Southeast, states are still grappling with increasing cases, and our model predicts that mortality rates still increase in this area of the country.\n'], 'url_profile': 'https://github.com/AndyFlury13', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'C++', 'Updated Aug 28, 2020', 'Stata', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', '2', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Jun 19, 2020', '18', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': [""Cervical-Cancer-ML-Project\nCervical Cancer was once the leading cause for death for women in the United States and continues to be a major global threat. There were an estimated 570, 000 cases of cervical cancer and 311, 000 deaths from the disease in 2018 (Arbyn et al., 2018) (https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(19)30482-6/fulltext). Although the prevalence of cervical cancer has steadily decreased, there remains major challenges in diagnosis and predicting the onset of disease. One such challenge is the financial burden of the multiple screening tests that are required to confirm a cervical cancer diagnosis. As shown in the Recommended Primary HPV Screening Algorithm (Image 1), patients in screening who test positive for risk factors, such as Human Papilloman Virus (HPV), must undergo cytology or colopscopy. Further assessment of risk and a diagnosis of cervical cancer is then confirmed by biopsy. These procedures not only serve as financial and temporal barriers for diagnosis, but also require the onset of disease or risk factors for an accurate diagnosis. What if we could predict the onset or risk of Cervical Cancer and circumvent the aformentioned issues?\nUltimately, this project aims to use machine learning tasks (logistic regression and artificial neural networks (ANN)) to predict onset of cervical cancer from risk factors. Our goal is to accurately classify the onset of cancer from patient records obtained from'Hospital Universitario de Caracas' in Caracas, Venezuela and deposited into the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Cervical+cancer+%28Risk+Factors%29). Additionally, this project explores if recursive feature elimination with cross-validation can be used to identify features of importance to reduce the number of features needed to retain a high accuracy diagnosis for the onset of cancer. Patient datasets are often sparse and missing several feature values. If an equivalent or higher accuracy with fewer features (that are less-invasive)can be obtained, we would begin the steps to combat the aformentioned barriers in cervical cancer diagnosis.\n""], 'url_profile': 'https://github.com/Itsapriori', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'C++', 'Updated Aug 28, 2020', 'Stata', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', '2', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Jun 19, 2020', '18', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/malharb', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'C++', 'Updated Aug 28, 2020', 'Stata', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', '2', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Jun 19, 2020', '18', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Data-Science-Projects\n1. Predict Movie Box Office Revenue with Linear Regression\n2. Python Programming for Data Science and Machine Learning\n3. Optimisation Algorithms and Gradient Descent\n4. Predict House Prices using Multivariable Regression\n5. Naive Bayes Classifier to Filter Spam Emails\n6. Neural Networks Pre-trained Image Classification\n7. Recognise image using Keras and Tensorflow\n8. Classify Handwritten Digits using Tensorflow\n9. Serving a Tensorflow Model through Website\n'], 'url_profile': 'https://github.com/SherBabi', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'C++', 'Updated Aug 28, 2020', 'Stata', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', '2', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Jun 19, 2020', '18', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\nsurbayes\nThe goal of surbayes is to provide tools for Bayesian analysis of the\nseemingly unrelated regression (SUR) model. In particular, we implement\nthe direct Monte Carlo (DMC) approach of Zellner and Ando (2010). We\nalso implement a Gibbs sampler to sample from a power prior on the SUR\nmodel.\nInstallation\nYou can install the released version of surbayes from\nCRAN with:\ninstall.packages(""surbayes"")\nAnd the development version from GitHub with:\n# install.packages(""devtools"")\ndevtools::install_github(""ethan-alt/surbayes"")\nExample\nThis is a basic example which shows you how to sample from the posterior\nlibrary(surbayes)\n## Taken from bayesm package\nM = 10 ## number of samples\nset.seed(66)\n## simulate data from SUR\nbeta1 = c(1,2)\nbeta2 = c(1,-1,-2)\nnobs = 100\nnreg = 2\niota = c(rep(1, nobs))\nX1 = cbind(iota, runif(nobs))\nX2 = cbind(iota, runif(nobs), runif(nobs))\nSigma = matrix(c(0.5, 0.2, 0.2, 0.5), ncol = 2)\nU = chol(Sigma)\nE = matrix( rnorm( 2 * nobs ), ncol = 2) %*% U\ny1 = X1 %*% beta1 + E[,1]\ny2 = X2 %*% beta2 + E[,2]\nX1 = X1[, -1]\nX2 = X2[, -1]\ndata = data.frame(y1, y2, X1, X2)\nnames(data) = c( paste0( \'y\', 1:2 ), paste0(\'x\', 1:(ncol(data) - 2) ))\n## run DMC sampler\nformula.list = list(y1 ~ x1, y2 ~ x2 + x3)\n\n## Fit models\nout_dmc = sur_sample( formula.list, data, M = M )            ## DMC used\n#> Direct Monte Carlo sampling used\nout_powerprior = sur_sample( formula.list, data, M, data )   ## Gibbs used\n#> Gibbs sampling used for power prior model\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'C++', 'Updated Aug 28, 2020', 'Stata', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', '2', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Jun 19, 2020', '18', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['President-Popularity-Functions\nProject made under supervision of Professor Manfred Keil with a team at Claremont McKenna College in 2019. I analyzed the US and Chiina relation using simple panel regression with Stata.\nSteps:\n\nTest unit root with Augmented Dickey-Fuller test\nCreate simple unrestricted model\nSpecify the restricted model\nCalculate Newey‚ÄìWest standard errors\n\n'], 'url_profile': 'https://github.com/rivaldophilip', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'C++', 'Updated Aug 28, 2020', 'Stata', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', '2', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Jun 19, 2020', '18', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Diabetes-prediction-using-machine-learning-algorithms\nThis contains implemented code of diabetes prediction using decision tree,logistic regression model ,random forest model and support vector machine algorithm.It also compares performance evaluation on thee data.\n'], 'url_profile': 'https://github.com/sheerin75', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'C++', 'Updated Aug 28, 2020', 'Stata', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', '2', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Jun 19, 2020', '18', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['Abstract: Through building a logistic regression model to predict heart disease using demographics, behavioural and medical condition features, we‚Äôve learned that holding other features constant, people who had previous had a stroke is almost 2 (1.84) times higher than for people who hadn‚Äôt. The possibility of getting diagnosed with heart disease for males is 62% higher than for females. Increases in age, number of cigarettes smoked per day and systolic blood pressure also increase the possibility of getting diagnosed with heart disease.\n'], 'url_profile': 'https://github.com/yuchaowu-eng', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'C++', 'Updated Aug 28, 2020', 'Stata', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', '2', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Jun 19, 2020', '18', 'Python', 'Updated May 5, 2020']}","{'location': 'Russia', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Goananda', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'C++', 'Updated Aug 28, 2020', 'Stata', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', '2', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Jun 19, 2020', '18', 'Python', 'Updated May 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': [""all-classification-templetes-for-ML\nClassification - Machine Learning This is ‚ÄòClassification‚Äô tutorial which is a part of the Machine Learning course offered by Simplilearn. We will learn Classification algorithms, types of classification algorithms, support vector machines(SVM), Naive Bayes, Decision Tree and Random Forest Classifier in this tutorial.  Objectives Let us look at some of the objectives covered under this section of Machine Learning tutorial.  Define Classification and list its algorithms Describe Logistic Regression and Sigmoid Probability Explain K-Nearest Neighbors and KNN classification Understand Support Vector Machines, Polynomial Kernel, and Kernel Trick Analyze Kernel Support Vector Machines with an example Implement the Na√Øve Bayes Classifier Demonstrate Decision Tree Classifier Describe Random Forest Classifier Classification: Meaning Classification is a type of supervised learning. It specifies the class to which data elements belong to and is best used when the output has finite and discrete values. It predicts a class for an input variable as well.  There are 2 types of Classification:   Binomial Multi-Class Classification: Use Cases Some of the key areas where classification cases are being used:  To find whether an email received is a spam or ham To identify customer segments To find if a bank loan is granted To identify if a kid will pass or fail in an examination Classification: Example Social media sentiment analysis has two potential outcomes, positive or negative, as displayed by the chart given below.  https://www.simplilearn.com/ice9/free_resources_article_thumb/classification-example-machine-learning.JPG  This chart shows the classification of the Iris flower dataset into its three sub-species indicated by codes 0, 1, and 2. https://www.simplilearn.com/ice9/free_resources_article_thumb/iris-flower-dataset-graph.JPG  The test set dots represent the assignment of new test data points to one class or the other based on the trained classifier model. Types of Classification Algorithms Let‚Äôs have a quick look into the types of Classification Algorithm below.  Linear Models Logistic Regression Support Vector Machines Nonlinear models K-nearest Neighbors (KNN) Kernel Support Vector Machines (SVM) Na√Øve Bayes Decision Tree Classification Random Forest Classification Logistic Regression: Meaning Let us understand the Logistic Regression model below.  This refers to a regression model that is used for classification. This method is widely used for binary classification problems. It can also be extended to multi-class classification problems. Here, the dependent variable is categorical: y œµ {0, 1} A binary dependent variable can have only two values, like 0 or 1, win or lose, pass or fail, healthy or sick, etc In this case, you model the probability distribution of output y as 1 or 0. This is called the sigmoid probability (œÉ). If œÉ(Œ∏ Tx) > 0.5, set y = 1, else set y = 0 Unlike Linear Regression (and its Normal Equation solution), there is no closed form solution for finding optimal weights of Logistic Regression. Instead, you must solve this with maximum likelihood estimation (a probability model to detect the maximum likelihood of something happening). It can be used to calculate the probability of a given outcome in a binary model, like the probability of being classified as sick or passing an exam. https://www.simplilearn.com/ice9/free_resources_article_thumb/logistic-regression-example-graph.JPG  Sigmoid Probability The probability in the logistic regression is often represented by the Sigmoid function (also called the logistic function or the S-curve):  https://www.simplilearn.com/ice9/free_resources_article_thumb/sigmoid-function-machine-learning.JPG  In this equation, t represents data values * the number of hours studied and S(t) represents the probability of passing the exam. Assume sigmoid function: https://www.simplilearn.com/ice9/free_resources_article_thumb/sigmoid-probability-machine-learning.JPG  g(z) tends toward 1 as z -> infinity , and g(z) tends toward 0 as z -> infinity  K-nearest Neighbors (KNN) K-nearest Neighbors algorithm is used to assign a data point to clusters based on similarity measurement. It uses a supervised method for classification.  The steps to writing a k-means algorithm are as given below:  https://www.simplilearn.com/ice9/free_resources_article_thumb/knn-distribution-graph-machine-learning.JPG  Choose the number of k and a distance metric. (k = 5 is common) Find k-nearest neighbors of the sample that you want to classify Assign the class label by majority vote. KNN Classification A new input point is classified in the category such that it has the most number of neighbors from that category. For example:  https://www.simplilearn.com/ice9/free_resources_article_thumb/knn-classification-machine-learning.JPG  Classify a patient as high risk or low risk. Mark email as spam or ham. Keen on learning about Classification Algorithms in Machine Learning? Click here!  Support Vector Machine (SVM) Let us understand Support Vector Machine (SVM) in detail below.  SVMs are classification algorithms used to assign data to various classes. They involve detecting hyperplanes which segregate data into classes. SVMs are very versatile and are also capable of performing linear or nonlinear classification, regression, and outlier detection. Once ideal hyperplanes are discovered, new data points can be easily classified. https://www.simplilearn.com/ice9/free_resources_article_thumb/support-vector-machines-graph-machine-learning.JPG  The optimization objective is to find ‚Äúmaximum margin hyperplane‚Äù that is farthest from the closest points in the two classes (these points are called support vectors). In the given figure, the middle line represents the hyperplane. SVM Example Let‚Äôs look at this image below and have an idea about SVM in general.  Hyperplanes with larger margins have lower generalization error. The positive and negative hyperplanes are represented by: https://www.simplilearn.com/ice9/free_resources_article_thumb/positive-negative-hyperplanes-machine-learning.JPG  Classification of any new input sample xtest :  If w0 + wTxtest > 1, the sample xtest is said to be in the class toward the right of the positive hyperplane. If w0 + wTxtest < -1, the sample xtest is said to be in the class toward the left of the negative hyperplane. When you subtract the two equations, you get:  https://www.simplilearn.com/ice9/free_resources_article_thumb/equation-subtraction-machine-learning.JPG  Length of vector w is (L2 norm length):  https://www.simplilearn.com/ice9/free_resources_article_thumb/length-of-vector-machine-learning.JPG  You normalize with the length of w to arrive at:  https://www.simplilearn.com/ice9/free_resources_article_thumb/normalize-equation-machine-learning.JPG  SVM: Hard Margin Classification Given below are some points to understand Hard Margin Classification.  The left side of equation SVM-1 given above can be interpreted as the distance between the positive (+ve) and negative (-ve) hyperplanes; in other words, it is the margin that can be maximized. Hence the objective of the function is to maximize with the constraint that the samples are classified correctly, which is represented as : https://www.simplilearn.com/ice9/free_resources_article_thumb/hard-margin-classification-machine-learning.JPG  This means that you are minimizing ‚Äñw‚Äñ. This also means that all positive samples are on one side of the positive hyperplane and all negative samples are on the other side of the negative hyperplane. This can be written concisely as : https://www.simplilearn.com/ice9/free_resources_article_thumb/hard-margin-classification-formula.JPG  Minimizing ‚Äñw‚Äñ is the same as minimizing. This figure is better as it is differentiable even at w = 0. The approach listed above is called ‚Äúhard margin linear SVM classifier.‚Äù SVM: Soft Margin Classification   Given below are some points to understand Soft Margin Classification.  To allow for linear constraints to be relaxed for nonlinearly separable data, a slack variable is introduced. (i) measures how much ith instance is allowed to violate the margin. The slack variable is simply added to the linear constraints. https://www.simplilearn.com/ice9/free_resources_article_thumb/soft-margin-calculation-machine-learning.JPG  Subject to the above constraints, the new objective to be minimized becomes: https://www.simplilearn.com/ice9/free_resources_article_thumb/soft-margin-calculation-formula.JPG  You have two conflicting objectives now‚Äîminimizing slack variable to reduce margin violations and minimizing to increase the margin. The hyperparameter C allows us to define this trade-off. Large values of C correspond to larger error penalties (so smaller margins), whereas smaller values of C allow for higher misclassification errors and larger margins. https://www.simplilearn.com/ice9/free_resources_article_thumb/machine-learning-certification-video-preview.jpg  SVM: Regularization The concept of C is the reverse of regularization. Higher C means lower regularization, which increases bias and lowers the variance (causing overfitting).  https://www.simplilearn.com/ice9/free_resources_article_thumb/concept-of-c-graph-machine-learning.JPG  IRIS Data Set The Iris dataset contains measurements of 150 IRIS flowers from three different species:  Setosa Versicolor Viriginica Each row represents one sample. Flower measurements in centimeters are stored as columns. These are called features.  IRIS Data Set: SVM Let‚Äôs train an SVM model using sci-kit-learn for the Iris dataset:  https://www.simplilearn.com/ice9/free_resources_article_thumb/svm-model-graph-machine-learning.JPG  Nonlinear SVM Classification There are two ways to solve nonlinear SVMs:  by adding polynomial features by adding similarity features Polynomial features can be added to datasets; in some cases, this can create a linearly separable dataset.  https://www.simplilearn.com/ice9/free_resources_article_thumb/nonlinear-classification-svm-machine-learning.JPG  In the figure on the left, there is only 1 feature x1. This dataset is not linearly separable. If you add x2 = (x1)2 (figure on the right), the data becomes linearly separable. Polynomial Kernel In sci-kit-learn, one can use a Pipeline class for creating polynomial features. Classification results for the Moons dataset are shown in the figure.  https://www.simplilearn.com/ice9/free_resources_article_thumb/polynomial-kernel-machine-learning.JPG  Polynomial Kernel with Kernel Trick Let us look at the image below and understand Kernel Trick in detail.  https://www.simplilearn.com/ice9/free_resources_article_thumb/polynomial-kernel-with-kernel-trick.JPG  For large dimensional datasets, adding too many polynomial features can slow down the model. You can apply a kernel trick with the effect of polynomial features without actually adding them. The code is shown (SVC class) below trains an SVM classifier using a 3rd-degree polynomial kernel but with a kernel trick. https://www.simplilearn.com/ice9/free_resources_article_thumb/polynomial-kernel-equation-machine-learning.JPG  The hyperparameter coefŒ∏ controls the influence of high-degree polynomials.  Kernel SVM Let us understand in detail about Kernel SVM.  Kernel SVMs are used for classification of nonlinear data. In the chart, nonlinear data is projected into a higher dimensional space via a mapping function where it becomes linearly separable. https://www.simplilearn.com/ice9/free_resources_article_thumb/kernel-svm-machine-learning.JPG  In the higher dimension, a linear separating hyperplane can be derived and used for classification.  A reverse projection of the higher dimension back to original feature space takes it back to nonlinear shape.  As mentioned previously, SVMs can be kernelized to solve nonlinear classification problems. You can create a sample dataset for XOR gate (nonlinear problem) from NumPy. 100 samples will be assigned the class sample 1, and 100 samples will be assigned the class label -1. https://www.simplilearn.com/ice9/free_resources_article_thumb/kernel-svm-graph-machine-learning.JPG  As you can see, this data is not linearly separable.  https://www.simplilearn.com/ice9/free_resources_article_thumb/kernel-svm-non-separable.JPG  You now use the kernel trick to classify XOR dataset created earlier.  https://www.simplilearn.com/ice9/free_resources_article_thumb/kernel-svm-xor-machine-learning.JPG  Na√Øve Bayes Classifier What is Naive Bayes Classifier?  Have you ever wondered how your mail provider implements spam filtering or how online news channels perform news text classification or even how companies perform sentiment analysis of their audience on social media? All of this and more are done through a machine learning algorithm called Naive Bayes Classifier.  Naive Bayes Named after Thomas Bayes from the 1700s who first coined this in the Western literature. Naive Bayes classifier works on the principle of conditional probability as given by the Bayes theorem.  Advantages of Naive Bayes Classifier Listed below are six benefits of Naive Bayes Classifier.  Very simple and easy to implement Needs less training data Handles both continuous and discrete data Highly scalable with the number of predictors and data points As it is fast, it can be used in real-time predictions Not sensitive to irrelevant features Bayes Theorem We will understand Bayes Theorem in detail from the points mentioned below.  According to the Bayes model, the conditional probability P(Y|X) can be calculated as:  P(Y|X) = P(X|Y)P(Y) / P(X)  This means you have to estimate a very large number of P(X|Y) probabilities for a relatively small vector space X. For example, for a Boolean Y and 30 possible Boolean attributes in the X vector, you will have to estimate 3 billion probabilities P(X|Y). To make it practical, a Na√Øve Bayes classifier is used, which assumes conditional independence of P(X) to each other, with a given value of Y. This reduces the number of probability estimates to 2*30=60 in the above example. Na√Øve Bayes Classifier for SMS Spam Detection Consider a labeled SMS database having 5574 messages. It has messages as given below:  https://www.simplilearn.com/ice9/free_resources_article_thumb/naive-bayes-spam-machine-learning.JPG  Each message is marked as spam or ham in the data set. Let‚Äôs train a model with Na√Øve Bayes algorithm to detect spam from ham. The message lengths and their frequency (in the training dataset) are as shown below:  https://www.simplilearn.com/ice9/free_resources_article_thumb/naive-bayes-spam-spam-detection.JPG  Analyze the logic you use to train an algorithm to detect spam:  Split each message into individual words/tokens (bag of words). Lemmatize the data (each word takes its base form, like ‚Äúwalking‚Äù or ‚Äúwalked‚Äù is replaced with ‚Äúwalk‚Äù). Convert data to vectors using scikit-learn module CountVectorizer. Run TFIDF to remove common words like ‚Äúis,‚Äù ‚Äúare,‚Äù ‚Äúand.‚Äù Now apply scikit-learn module for Na√Øve Bayes MultinomialNB to get the Spam Detector. This spam detector can then be used to classify a random new message as spam or ham. Next, the accuracy of the spam detector is checked using the Confusion Matrix. For the SMS spam example above, the confusion matrix is shown on the right. Accuracy Rate = Correct / Total = (4827 + 592)/5574 = 97.21% Error Rate = Wrong / Total = (155 + 0)/5574 = 2.78% https://www.simplilearn.com/ice9/free_resources_article_thumb/confusion-matrix-machine-learning.JPG  Although confusion Matrix is useful, some more precise metrics are provided by Precision and Recall.  https://www.simplilearn.com/ice9/free_resources_article_thumb/precision-recall-matrix-machine-learning.JPG  Precision refers to the accuracy of positive predictions.  https://www.simplilearn.com/ice9/free_resources_article_thumb/precision-formula-machine-learning.JPG  Recall refers to the ratio of positive instances that are correctly detected by the classifier (also known as True positive rate or TPR).  https://www.simplilearn.com/ice9/free_resources_article_thumb/recall-formula-machine-learning.JPG  Precision/Recall Trade-off To detect age-appropriate videos for kids, you need high precision (low recall) to ensure that only safe videos make the cut (even though a few safe videos may be left out).  The high recall is needed (low precision is acceptable) in-store surveillance to catch shoplifters; a few false alarms are acceptable, but all shoplifters must be caught.  Learn about Naive Bayes in detail. Click here!  Decision Tree Classifier Some aspects of the Decision Tree Classifier mentioned below are.  Decision Trees (DT) can be used both for classification and regression. The advantage of decision trees is that they require very little data preparation. They do not require feature scaling or centering at all. They are also the fundamental components of Random Forests, one of the most powerful ML algorithms. Unlike Random Forests and Neural Networks (which do black-box modeling), Decision Trees are white box models, which means that inner workings of these models are clearly understood. In the case of classification, the data is segregated based on a series of questions. Any new data point is assigned to the selected leaf node. https://www.simplilearn.com/ice9/free_resources_article_thumb/decision-tree-classifier-machine-learning.JPG  Start at the tree root and split the data on the feature using the decision algorithm, resulting in the largest information gain (IG). This splitting procedure is then repeated in an iterative process at each child node until the leaves are pure. This means that the samples at each node belonging to the same class. In practice, you can set a limit on the depth of the tree to prevent overfitting. The purity is compromised here as the final leaves may still have some impurity. The figure shows the classification of the Iris dataset. https://www.simplilearn.com/ice9/free_resources_article_thumb/decision-tree-classifier-graph.JPG  IRIS Decision Tree Let‚Äôs build a Decision Tree using scikit-learn for the Iris flower dataset and also visualize it using export_graphviz API.  https://www.simplilearn.com/ice9/free_resources_article_thumb/iris-decision-tree-machine-learning.JPG  The output of export_graphviz can be converted into png format:  https://www.simplilearn.com/ice9/free_resources_article_thumb/iris-decision-tree-output.JPG  Sample attribute stands for the number of training instances the node applies to. Value attribute stands for the number of training instances of each class the node applies to. Gini impurity measures the node‚Äôs impurity. A node is ‚Äúpure‚Äù (gini=0) if all training instances it applies to belong to the same class. https://www.simplilearn.com/ice9/free_resources_article_thumb/impurity-formula-machine-learning.JPG  For example, for Versicolor (green color node), the Gini is 1-(0/54)2 -(49/54)2 -(5/54) 2 ‚âà 0.168  https://www.simplilearn.com/ice9/free_resources_article_thumb/iris-decision-tree-sample.JPG  Decision Boundaries Let us learn to create decision boundaries below.  For the first node (depth 0), the solid line splits the data (Iris-Setosa on left). Gini is 0 for Setosa node, so no further split is possible. The second node (depth 1) splits the data into Versicolor and Virginica. If max_depth were set as 3, a third split would happen (vertical dotted line). https://www.simplilearn.com/ice9/free_resources_article_thumb/decision-tree-boundaries.JPG  For a sample with petal length 5 cm and petal width 1.5 cm, the tree traverses to depth 2 left node, so the probability predictions for this sample are 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54), and 9.3% for Iris-Virginica (5/54) CART Training Algorithm Scikit-learn uses Classification and Regression Trees (CART) algorithm to train Decision Trees. CART algorithm:  Split the data into two subsets using a single feature k and threshold tk (example, petal length < ‚Äú2.45 cm‚Äù). This is done recursively for each node. k and tk are chosen such that they produce the purest subsets (weighted by their size). The objective is to minimize the cost function as given below: https://www.simplilearn.com/ice9/free_resources_article_thumb/cart-training-algorithm-machine-learning.JPG  The algorithm stops executing if one of the following situations occurs: max_depth is reached No further splits are found for each node Other hyperparameters may be used to stop the tree: min_samples_split min_samples_leaf min_weight_fraction_leaf max_leaf_nodes Gini Impurity or Entropy Entropy is one more measure of impurity and can be used in place of Gini.  https://www.simplilearn.com/ice9/free_resources_article_thumb/gini-impurity-entrophy.JPG  It is a degree of uncertainty, and Information Gain is the reduction that occurs in entropy as one traverses down the tree. Entropy is zero for a DT node when the node contains instances of only one class. Entropy for depth 2 left node in the example given above is:  https://www.simplilearn.com/ice9/free_resources_article_thumb/entrophy-for-depth-2.JPG  Gini and Entropy both lead to similar trees.  DT: Regularization The following figure shows two decision trees on the moons dataset.  https://www.simplilearn.com/ice9/free_resources_article_thumb/dt-regularization-machine-learning.JPG  The decision tree on the right is restricted by min_samples_leaf = 4. The model on the left is overfitting, while the model on the right generalizes better. Random Forest Classifier Let us have an understanding of Random Forest Classifier below.  A random forest can be considered an ensemble of decision trees (Ensemble learning). Random Forest algorithm: Draw a random bootstrap sample of size n (randomly choose n samples from the training set). Grow a decision tree from the bootstrap sample. At each node, randomly select d features. Split the node using the feature that provides the best split according to the objective function, for instance by maximizing the information gain. Repeat the steps 1 to 2 k times. (k is the number of trees you want to create, using a subset of samples) Aggregate the prediction by each tree for a new data point to assign the class label by majority vote (pick the group selected by the most number of trees and assign new data point to that group). Random Forests are opaque, which means it is difficult to visualize their inner workings. https://www.simplilearn.com/ice9/free_resources_article_thumb/random-forest-classifier-graph.JPG  However, the advantages outweigh their limitations since you do not have to worry about hyperparameters except k, which stands for the number of decision trees to be created from a subset of samples. RF is quite robust to noise from the individual decision trees. Hence, you need not prune individual decision trees. The larger the number of decision trees, the more accurate the Random Forest prediction is. (This, however, comes with higher computation cost). Key Takeaways Let us quickly run through what we have learned so far in this Classification tutorial.  Classification algorithms are supervised learning methods to split data into classes. They can work on Linear Data as well as Nonlinear Data. Logistic Regression can classify data based on weighted parameters and sigmoid conversion to calculate the probability of classes. K-nearest Neighbors (KNN) algorithm uses similar features to classify data. Support Vector Machines (SVMs) classify data by detecting the maximum margin hyperplane between data classes. Na√Øve Bayes, a simplified Bayes Model, can help classify data using conditional probability models. Decision Trees are powerful classifiers and use tree splitting logic until pure or somewhat pure leaf node classes are attained. Random Forests apply Ensemble Learning to Decision Trees for more accurate classification predictions. Conclusion This completes ‚ÄòClassification‚Äô tutorial. In the next tutorial, we will learn 'Unsupervised Learning with Clustering.'\n""], 'url_profile': 'https://github.com/sayantann11', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2021', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Python', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated Jun 27, 2020', 'C++', 'Updated Aug 28, 2020', 'Stata', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', '2', 'Jupyter Notebook', 'Updated Aug 3, 2020', '2', 'Jupyter Notebook', 'Updated Jun 19, 2020', '18', 'Python', 'Updated May 5, 2020']}"
"{'location': 'Tempe', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['ML-Classification-models\nPart of CSE 471 AI assignment where several classification models using Linear, Logistic, Polynomial regressions and neural nets are developed.\nMNIST Fashion is the dataset on which classification is performed using feed-forward neural nets.\n'], 'url_profile': 'https://github.com/binga45', 'info_list': ['Python', 'Updated May 10, 2020', 'Stata', 'Updated May 4, 2020', '3', 'MATLAB', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Jupyter Notebook', 'Updated Aug 20, 2020', '1', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'HTML', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': [""Do female college graduates fare worse in the labour market in America?\nUsing multivariate linear regressions, I investigate whether there is a gender gap among female and male college graduates in America.\nThe original dataset is available for public use at the National Science Foundation's website: https://ncsesdata.nsf.gov/datadownload/?#\nThe do file contains the codes that can be used to replicate my analyses as shared in my article published on Medium.\nHow to use the do file:\n\nDownload the 2017 NSCG dataset from the abovementioned website\nFill in the working directory name (the path where the dataset is saved on your computer)\nRun the codes\n\nNote: I process my tables in Latex, but if you prefer to use Microsoft Word, be sure to change the export file name from .tex to .doc when exporting result tables using esttab\n""], 'url_profile': 'https://github.com/meredithwan', 'info_list': ['Python', 'Updated May 10, 2020', 'Stata', 'Updated May 4, 2020', '3', 'MATLAB', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Jupyter Notebook', 'Updated Aug 20, 2020', '1', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'HTML', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Beijing, China', 'stats_list': [], 'contributions': '368 contributions\n        in the last year', 'description': ['Logistic-Classification-Disease-Diagnosis\n‰ΩøÁî®ÈÄªËæëÂõûÂΩí„ÄÅÁ•ûÁªèÁΩëÁªúÂíåÈöèÊú∫Ê£ÆÊûóÁöÑÂàÜÁ±ªÁÆóÊ≥ïÂÆûÁé∞ÁñæÁóÖËØäÊñ≠ÔºåÂú®ÊµãËØïÈõÜ‰∏äËé∑Âæó‰∫Ü80%ÁöÑacc‰ª•ÂèäF1-score„ÄÇDisease diagnosis is realized by using the classification algorithm of logical regression, neural network and random forest. 80% of ACC and F1 score are obtained in the test set.\n'], 'url_profile': 'https://github.com/stxupengyu', 'info_list': ['Python', 'Updated May 10, 2020', 'Stata', 'Updated May 4, 2020', '3', 'MATLAB', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Jupyter Notebook', 'Updated Aug 20, 2020', '1', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'HTML', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'Gurgaon', 'stats_list': [], 'contributions': '226 contributions\n        in the last year', 'description': ['Analyzing-and-predicting-Bike-rentals\nBike rentals have become very common in the Western world. A customer can simply rent a bike by the hour or day.\nThe aim of this project is to analyze and predict the number of bike rentals recieved by the hour in Washington D.C. district.\nThe analysis identifies the factors influencing bike rentals and finally using these factors, predicts the estimated bike rentals recieved per hour.\nThe dataset can be found at the UCI machine learning repository - Link\nThe notebook contains detailed steps for the same with interactive visualizations. The visualizations made in plotly cannot be rendered on Github and hence it would be advisable to view it on this  Link \n'], 'url_profile': 'https://github.com/rajtulluri', 'info_list': ['Python', 'Updated May 10, 2020', 'Stata', 'Updated May 4, 2020', '3', 'MATLAB', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Jupyter Notebook', 'Updated Aug 20, 2020', '1', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'HTML', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shubhangperiwal', 'info_list': ['Python', 'Updated May 10, 2020', 'Stata', 'Updated May 4, 2020', '3', 'MATLAB', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Jupyter Notebook', 'Updated Aug 20, 2020', '1', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'HTML', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danilmrkov', 'info_list': ['Python', 'Updated May 10, 2020', 'Stata', 'Updated May 4, 2020', '3', 'MATLAB', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Jupyter Notebook', 'Updated Aug 20, 2020', '1', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'HTML', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Scraper: Utilizes a selenium and beautifulsoup to scrape company data from linkedin and crunchbase\nClassification: Classifies investment fit from a VC firm based on their parameters(represented by input data) using decision tree regression model\nML Testing Document: ran a series of boosting combinations to determine the most ideal output and accuracy based on chi-squared\n'], 'url_profile': 'https://github.com/ShyAyyalu', 'info_list': ['Python', 'Updated May 10, 2020', 'Stata', 'Updated May 4, 2020', '3', 'MATLAB', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Jupyter Notebook', 'Updated Aug 20, 2020', '1', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'HTML', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': [""BreastCancerDetection\nHere I have performed  exploratory data analysis on the given dataset and predicted if a person have benign or malignant tumor. I have trained two different models to predict the type of tumor. I have used K-NN and Logistic Regression. With K-NN I have got an accuracy of around 96.50% and with Logistic Regression I have got an accuracy around to 95.90%. The dataset was imbalanced so I have balanced the dataset and it contained some of outliers so I had detected the outliers and removed them. I have performed hyperparameter optimization and found the best value for K in K-NN and best value for C in Logistic Regression.Since the dataset dimmentionality was not very large hence i haven't performed dimentionality reduction.\n""], 'url_profile': 'https://github.com/iamcoolshubham', 'info_list': ['Python', 'Updated May 10, 2020', 'Stata', 'Updated May 4, 2020', '3', 'MATLAB', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Jupyter Notebook', 'Updated Aug 20, 2020', '1', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'HTML', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '346 contributions\n        in the last year', 'description': ['Investigating Early Stages of the COVID-19 Outbreak\nMany reports have indicated that older individuals experience more severe\nsymptoms of COVID-19; in response, this project explores the relationship\nbetween age and survival status of COVID-19 patients. The impact of additional\nfactors, including gender, the country of origin, whether patient is from Wuhan,\nand whether the patient has visited Wuhan, is also explored in this project.\nThe data frame is taken from the early stages of the pandemic, specifically\n1085 observations from 1/20/2020 to 2/15/2020, in which each\nobservation is a case of a patient who tested positive for COVID-19.\nTo gain insight into possible associations between survival status and the\nabove-mentioned variables, statistical methods including bootstrap confidence\nintervals, two-sided hypothesis testing, and logistic regression were employed.\nThe results show that age did bear impact on the survival status of the patient.\nThe gender of the patient and whether they lived in Wuhan did also seem to\ninfluence the survival status of the patient, but on a smaller scale.\n'], 'url_profile': 'https://github.com/JennyHuang19', 'info_list': ['Python', 'Updated May 10, 2020', 'Stata', 'Updated May 4, 2020', '3', 'MATLAB', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Jupyter Notebook', 'Updated Aug 20, 2020', '1', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'HTML', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Banglore-housing-price-prediction.\nThis repository contains a dataset of housing prices of the major technical hub of India i.e. the city of Banglore. We first clean the data, visualize it and then use supervised learning algorithms of Machine Learning for linear regression to predict the house prices.\n'], 'url_profile': 'https://github.com/Not-so-nerd', 'info_list': ['Python', 'Updated May 10, 2020', 'Stata', 'Updated May 4, 2020', '3', 'MATLAB', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Updated May 9, 2020', '1', 'Jupyter Notebook', 'Updated Aug 23, 2020', 'Jupyter Notebook', 'Updated Aug 20, 2020', '1', 'Jupyter Notebook', 'Updated May 14, 2020', '1', 'HTML', 'Updated May 4, 2020', 'Jupyter Notebook', 'Updated May 10, 2020']}"
"{'location': 'Chicago', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Data-Science\nThe repository includes Data Science projects completed by me for educational, academic, self-learning purposes.\nThe projects are mostly written in Python - Jupyter Notebook.\n\n\nThe repository comprises learnings performed on softwares and languages like:\n\nPython - Jupyter Notebook\nR - RStudio\nMicrosoft Excel - VBA\nSQL\n\n\n\nHave completed the following certifications to gain knowledge in Data Science:\n\nDecision Trees, Random Forest, AdaBoost & XGBoost in Python - Udemy\nSQL Masterclass: SQL for Data Analytics - Udemy\nTableau Analyst\nData Analytics Fundamentals - AWS Training\nData Science - Stats, Machine Learning, NLP-Python, R, BigData - Spark\n\n\n\nYou can follow me on LinkedIn for further information.\n'], 'url_profile': 'https://github.com/apatil44', 'info_list': ['1', 'Jupyter Notebook', 'Updated Nov 4, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'C#', 'MIT license', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'JavaScript', 'Updated May 7, 2020', 'C++', 'GPL-3.0 license', 'Updated Nov 1, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'Madrid, Spain', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['dogclothinginAmazon\nMachine Learning application for price estimation and prediction of the best-selling Dog Clothing in Amazon based on their attributes, using Multiple Linear Regression models, kNN, RandomForest, Decision Tree and AdaBoost classifiers with Holdout and Leave One Out validation methods, using Python.\n'], 'url_profile': 'https://github.com/hevi91', 'info_list': ['1', 'Jupyter Notebook', 'Updated Nov 4, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'C#', 'MIT license', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'JavaScript', 'Updated May 7, 2020', 'C++', 'GPL-3.0 license', 'Updated Nov 1, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '582 contributions\n        in the last year', 'description': ['Predicting Stock Movements using News Headlines and News Articles\n\nThe main objective of the system is to analyse the future value of a certain stock of a particular company using the sentiment analysis and to predict whether a particular stock will go up that is whether it will increase or it will go down which means it will decrease on the basis of certain news headline, also detection of fake news and OCR was implemented for providing the user as an option for entering the news headline or a news article, the data we used was DJIA news headlines dataset and five different machine learning algorithms were used ‚Äì Random Forest classifier, Na√Øve Bayes, Decision Tree, Logistic Regression and Support Vector Machine(SVM).\nPredicting stock prices and the status of stock market is a quite strenuous task in itself. Today stock prices of any company so not depend upon the financial factors of the company but also on the various other factors such as socio-economic factors and especially in this century the movement of stock prices are no more only linked with the current economic situation of the country rather the stock prices of the particular day are also directly or indirectly depends on the company related news, natural calamities as well as the political events. The motive of the research is to build a machine learning model which will predict whether the stock price of a company will go up or will go down and the model also predicts the exact stock prices for the next day and the day after based on the today‚Äôs news headlines of the company. We have taken Dow historical stock dataset of the years 2008-2016 which consists of the date, news headline, stock movement labelled as ‚Äò1‚Äô for increment and ‚Äò0‚Äô for decrement. The OCR model was also integrated with this model to make sure if the user is reading a headline on a newspaper or a different language newspaper, he/she should be able to know the price or movement of a stock just by clicking the picture of the headline on a newspaper of any language and upload that picture on the portal.\nAlso the user can have a quick view on the real time stock history or stock prices jut by selecting the ticker and the no of days/months the user wants to see, the model will return real time graph of the selected day/month for stock price of the particular company of which the ticker has been selected. Also the system\nprovides an option to upload the news headlines as well as the whole web-app in three different languages which are English, Hindi and Marathi.\nQuickstart\nClone the git repository:\n$ git clone https://github.com/DeepakT7/Predicting-Stock-Movements-using-News-Headlines-and-News-Articles.git && cd Predicting-Stock-Movements-using-News-Headlines-and-News-Articles\nInstall necessary dependencies\n$ pip install -r requirements.txt\nStart the developement server\n$ python app.py\nOpen the server running on\nhttp://localhost:5000/\nScreenshots\n1. Select a language\n\n2. Prediction through OCR (Optical Character Recognition)\n\n\n3. Prediction through Headline\n\n4. Prediction through URL\n\n5. Real Time Analysis\n\n6. Accuracy Measure\n\n'], 'url_profile': 'https://github.com/DeepakT7', 'info_list': ['1', 'Jupyter Notebook', 'Updated Nov 4, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'C#', 'MIT license', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'JavaScript', 'Updated May 7, 2020', 'C++', 'GPL-3.0 license', 'Updated Nov 1, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'Curitiba, PR', 'stats_list': [], 'contributions': '160 contributions\n        in the last year', 'description': ['Stock Forecast (Machine Learning)\nPrevendo o pre√ßo de a√ß√µes com o framework de aprendizado de m√°quina da Microsoft: O ML.NET!\n\nEsta aplica√ß√£o foi constru√≠da em C#, utilizando o framework chamado ML.NET, fornecido pela Microsoft para trabalhar com algoritmos de aprendizado de m√°quina.\nA aplicativo presente neste reposit√≥rio executa o seguinte fluxo:\n\nL√™ um dataset contendo os dados hist√≥ricos de uma a√ß√£o (aqui utilizei a PETR4 da Petrobr√°s);\nTrata os dados para remover valores vazios e para normatizar tipos primitivos distintos;\nTreina um modelo de previs√£o utilizando um algoritmo de regress√£o;\nSalva o modelo no disco r√≠gido para que ele possa ser utilizado por outras aplica√ß√µes;\nCalcula a previs√£o do pre√ßo de fechamento de uma a√ß√£o com base em novos dados fornecidos pelo usu√°rio.\n\nSaiba Mais\nOs detalhes da constru√ß√£o deste c√≥digo est√£o descritos no artigo: NOME DO ARTIGO\nContato\nQuer tirar d√∫vidas, agradecer, xingar ou me contratar? :P\nEntra em contato pelos links abaixo:\n\nromuloedu@gmail.com\nLinkedIn\n\n'], 'url_profile': 'https://github.com/romuloedu', 'info_list': ['1', 'Jupyter Notebook', 'Updated Nov 4, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'C#', 'MIT license', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'JavaScript', 'Updated May 7, 2020', 'C++', 'GPL-3.0 license', 'Updated Nov 1, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shubhangperiwal', 'info_list': ['1', 'Jupyter Notebook', 'Updated Nov 4, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'C#', 'MIT license', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'JavaScript', 'Updated May 7, 2020', 'C++', 'GPL-3.0 license', 'Updated Nov 1, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'Karachi, Pakistan', 'stats_list': [], 'contributions': '414 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aitezazakhtar', 'info_list': ['1', 'Jupyter Notebook', 'Updated Nov 4, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'C#', 'MIT license', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'JavaScript', 'Updated May 7, 2020', 'C++', 'GPL-3.0 license', 'Updated Nov 1, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '463 contributions\n        in the last year', 'description': ['TensorFlow UI\n\nMachine Learning visualized with tensorflow.js and react.\nIncludes:\n\nLinear\nPolynominial regression\nThird degree regression\n\n'], 'url_profile': 'https://github.com/valuecodes', 'info_list': ['1', 'Jupyter Notebook', 'Updated Nov 4, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'C#', 'MIT license', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'JavaScript', 'Updated May 7, 2020', 'C++', 'GPL-3.0 license', 'Updated Nov 1, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '583 contributions\n        in the last year', 'description': ['ML++ and cppyml\nEfficient C++ implementations of some ML algorithms with Python bindings.\n(C) 2020 Roman Werpachowski.\nLicence\nAvailable under GNU GENERAL PUBLIC LICENSE Version 3.\nDocumentation\nSee here.\n'], 'url_profile': 'https://github.com/romanwerpachowski', 'info_list': ['1', 'Jupyter Notebook', 'Updated Nov 4, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'C#', 'MIT license', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'JavaScript', 'Updated May 7, 2020', 'C++', 'GPL-3.0 license', 'Updated Nov 1, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['AAPL_Analysis\nIn this repo there are used historical data of Apple Inc. (AAPL) to analyse these data structures with some Pandas and Scikit-learn classes.\nIn addition the Streamlit library is applied to show the stock price in a browser as web application.\n'], 'url_profile': 'https://github.com/AvS15', 'info_list': ['1', 'Jupyter Notebook', 'Updated Nov 4, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'C#', 'MIT license', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'JavaScript', 'Updated May 7, 2020', 'C++', 'GPL-3.0 license', 'Updated Nov 1, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}","{'location': 'Taipei, Taiwan', 'stats_list': [], 'contributions': '171 contributions\n        in the last year', 'description': ['Binary Income Prediction\nA prediction of Income done with the generative model and the logistic regression model in machine learning\nTask\nFind the best feature combination by trial and error, normalize the data to speed up the training processm, and use regularizations to avoid overfiitng.\nAlso, compare the results of the generative model and the logistic regression model.\nRun on Colab (cannot save your own version)\n\nAccess the notebook to run the codes: https://colab.research.google.com/drive/197x5KwJ8oB6cwz-0oBMwxxtLI0f4fpD8?usp=sharing\n\nRun on Colab (can save your own version)\n\nRun this command: git clone https://github.com/b05702057/Binary-Income-Prediction-Model.git\nUpload the .ipynb files to your google drive and open it with colab\n\n'], 'url_profile': 'https://github.com/b05702057', 'info_list': ['1', 'Jupyter Notebook', 'Updated Nov 4, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '2', 'Jupyter Notebook', 'Updated May 16, 2020', '1', 'C#', 'MIT license', 'Updated May 4, 2020', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 12, 2020', 'JavaScript', 'Updated May 7, 2020', 'C++', 'GPL-3.0 license', 'Updated Nov 1, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2021', '1', 'Jupyter Notebook', 'Updated Jan 26, 2021']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['covid19_SIRD_risk\nCode provided in phase 1 optimizes SIRD parameters for each US counties COVID-19 data and phase 2 attempts to regress upon the product of these parameters using social and economic features.\n'], 'url_profile': 'https://github.com/alex-seto', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020', 'C++', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'MIT license', 'Updated May 10, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '513 contributions\n        in the last year', 'description': ['Step 1 : Go to https://colab.research.google.com/ (‡¥≤‡¥ø‡¥ô‡µç‡¥ï‡¥ø‡µΩ ‡¥ï‡µç‡¥≤‡¥ø‡¥ï‡µç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï )\nStep 2 : Sign in with your Gmail (‡¥Æ‡µÅ‡¥ï‡¥≥‡¥ø‡µΩ ‡¥ï‡¥æ‡¥£‡µÅ‡¥®‡µç‡¥® ‡¥∏‡µà‡µª ‡¥á‡µª ‡¥¨‡¥ü‡µç‡¥ü‡µ∫ ‡¥Ö‡¥Æ‡µº‡¥§‡µç‡¥§‡¥ø ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥ú‡¥ø‡¥Æ‡µÜ‡¥Ø‡¥ø‡¥≤‡¥ø‡µΩ ‡¥≤‡µã‡¥ó‡¥ø‡µª ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï)\nStep 3: Make sure that you are at https://colab.research.google.com/notebooks/intro.ipynb (‡¥á‡¥™‡µç‡¥™‡µã‡µæ ‡¥§‡¥æ‡¥ô‡µç‡¥ï‡µæ ‡¥à ‡¥™‡µá‡¥ú‡¥ø‡¥≤‡µá‡¥ï‡µç‡¥ï‡µç ‡¥é‡¥§‡µç‡¥§‡¥ø‡¥ö‡µç‡¥ö‡µá‡µº‡¥®‡µç‡¥®‡µÅ ‡¥é‡¥®‡µç‡¥®‡µÅ‡¥±‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï https://colab.research.google.com/notebooks/intro.ipynb)\nStep 4: Click new note book at ""File Menu"" - (‡¥´‡¥Ø‡µΩ ‡¥Æ‡µÜ‡¥®‡µÅ‡¥µ‡¥ø‡µΩ ‡¥®‡µç‡¥Ø‡µÇ ‡¥®‡µã‡¥ü‡µç‡¥ü‡µÅ‡¥¨‡µÅ‡¥ï‡µç‡¥ï‡µç ‡¥ï‡µç‡¥≤‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï )\nVideo : https://www.youtube.com/watch?v=RSc4qOTtdC8\n'], 'url_profile': 'https://github.com/sarincr', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020', 'C++', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'MIT license', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '365 contributions\n        in the last year', 'description': ['robocorpse\nAn AI robot to detect and harvest detect bodies from the aftermath of disasters. Analysing the composition of the environment around the aftermath, the AI will then use regression to determine the date-of-death and how much decomposition is going on to determine the quality of the body. The robot is a mobile construct, mapping out an area and showing the smell concentration map, allowing humanitarian aid to retrieve and identify the deceased, reducing the wasted rescue efforts to find the missing persons.\n'], 'url_profile': 'https://github.com/Iscaraca', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020', 'C++', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'MIT license', 'Updated May 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['canada-covid19-trend-analysis\nThis project performs trend analysis for the COVID-19 pandemic in Canada. The novel dataset provided by JHU CSSE is used through this project. The SIR model is used to get a theoretical feel on the spread of virus depending on several factors. Afterwards, the exponential and logistic functions were fit to the actual data in order to analysis the type of trend the virus is following. The results suggested that the trend fits more into the logistic curve rather than exponential curve. Finally, a linear regression model is suggested and implemented which can be used to perform predictions for different types of cases.\n'], 'url_profile': 'https://github.com/ManpreetNanreh', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020', 'C++', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'MIT license', 'Updated May 10, 2020']}","{'location': 'WA', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['CV-19-Project\nI started this project on my own in April 2020. I wanted to know the ratio of infected per tested cases per area\n(country in this case). I took what I learned in Computational Statics (a lot of R) and applied it to this problem.\nSince I was using Excel to plot my data, I ""factored out"" the countries into their own columns. Unfortunately, the\ndata-set doesn\'t include parameters like health-based investments, tactics, plans, etc. Otherwise, I would have created a\nregression model.\nIf anythin else, this is an example of separating values of a column into their own columns.\n'], 'url_profile': 'https://github.com/paulcollet', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020', 'C++', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'MIT license', 'Updated May 10, 2020']}","{'location': 'Massachusetts', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['MCAS watertown public school score  analysis\nThis project emphasizes an analysis in order to indicate if Watertown public schools have an impact of socio econominc factors such as race and econnomic distress while performing well at the MCAS involving subjets , math , ELA and STE . This is done to obeserve if these scores pf 2018-19 are impacted by such factors and what re the important factors that contribute to that score.\ncleaning the data\nThe student  id and ssid has to  be merged to gauage factors such  as race and economic distress  to form the merged dataset\nAfter that null  values are  categorical  and hence they are replaced by frequenlty occuring values usin DataImputer() and Transformixin\nrather than using one hot encoding\nprediction model creation\nThe models used  in order to  achieve this are linear regression and neural networks as we are dealing with continous  variables\nConclusion:\nIt was found out that important factors were actually the patter innvolved in grading and race and socio- economic disadvantage didnt contibute significantly according to the linear regression model . But it was also known that for every student as a pacific islander they can score a grade of 2 units more in math in comparision to other students and mixed racial students have the ability to secure a grade higher than other race students when it came to english.\n'], 'url_profile': 'https://github.com/ruhika28', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020', 'C++', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'MIT license', 'Updated May 10, 2020']}","{'location': 'Massachusetts', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ruhika28', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020', 'C++', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'MIT license', 'Updated May 10, 2020']}","{'location': 'Jersey City', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['Trends-in-Crime-Rates-Maryland\nCrime rates vary across space and time. The reasons crimes are committed in some places but not others can be difficult to detect because of complex socio-economic factors, but policymakers still need to understand how crime rates are changing from place to place and from time to time to inform their policies.\nMany government statistics, such as crime rates, come from nested datasets. Most US States are divided into counties (Alaska has ‚Äúburrows,‚Äù and Louisiana has ‚Äúparishes‚Äù), and counties and county-level governments can vary within the same state. For example, one county might have a high population density and be urban, whereas a second county might have a low population density and be rural.\nIn this project we will use a form of regression called hierarchical modeling to capture and explore crime statistics collected by the State of Maryland to see if there is a linear trend in violent crime across the state between 1975 and 2016. These data come from the Maryland Statistical Analysis Center.\n'], 'url_profile': 'https://github.com/udayansawant', 'info_list': ['Jupyter Notebook', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020', 'C++', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'R', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Updated May 7, 2020', 'MIT license', 'Updated May 10, 2020']}",,
