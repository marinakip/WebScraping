"{'location': 'Global', 'stats_list': [], 'contributions': '278 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AISPUBLISHING', 'info_list': ['2', 'Jupyter Notebook', 'Updated Nov 28, 2020', '2', 'Python', 'Updated Feb 6, 2020', '2', 'Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Python', 'MIT license', 'Updated Dec 25, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Jun 15, 2020', '4', 'Python', 'MIT license', 'Updated Mar 17, 2020', '4', 'Jupyter Notebook', 'Updated Jun 9, 2020', '2', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'HTML', 'Updated Oct 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mahtab2', 'info_list': ['2', 'Jupyter Notebook', 'Updated Nov 28, 2020', '2', 'Python', 'Updated Feb 6, 2020', '2', 'Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Python', 'MIT license', 'Updated Dec 25, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Jun 15, 2020', '4', 'Python', 'MIT license', 'Updated Mar 17, 2020', '4', 'Jupyter Notebook', 'Updated Jun 9, 2020', '2', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'HTML', 'Updated Oct 3, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Linear Regression project\nA project to find the factors(variables) that affect the pricing of cars using a linear regression model.\n'], 'url_profile': 'https://github.com/giri-sh-irke', 'info_list': ['2', 'Jupyter Notebook', 'Updated Nov 28, 2020', '2', 'Python', 'Updated Feb 6, 2020', '2', 'Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Python', 'MIT license', 'Updated Dec 25, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Jun 15, 2020', '4', 'Python', 'MIT license', 'Updated Mar 17, 2020', '4', 'Jupyter Notebook', 'Updated Jun 9, 2020', '2', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'HTML', 'Updated Oct 3, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '1,335 contributions\n        in the last year', 'description': ['Shabadoo: very easy Bayesian regression.\n\n\n""That\'s the worst name I ever heard.""\n\n\n\n\n\n\nShabadoo is the worst kind of machine learning. It automates nothing; your models will not perform well and it will be your own fault.\n\nBEWARE. Shabadoo is in an open alpha phase. It is authored by someone who does not know how to manage open source projects. Things will change as the author identifies mistakes and corrects (?) them.\n\nShabadoo is for people who want to do Bayesian regression but who do not want to write probabilistic programming code. You only need to assign priors to features and pass your pandas dataframe to a .fit() / .predict() API.\nShabadoo runs on numpyro and is basically a wrapper around the numpyro Bayesian regression tutorial.\n\nQuickstart\n\nInstall\nSpecifying a Shabadoo Bayesian model\nFitting & predicting the model\nInspecting the model\nSaving and recovering a saved model\n\n\nDevelopment\n\nQuickstart\nInstall\npip install shabadoo\nor\npip install git+https://github.com/nolanbconaway/shabadoo\nSpecifying a Shabadoo Bayesian model\nShabadoo was designed to make it as easy as possible to test ideas about features and their priors. Models are defined using a class which contains configuration specifying how the model should behave.\nYou need to define a new class which inherits from one of the Shabadoo models. Currently, Normal, Poisson, and Bernoulli are implemented.\nimport numpy as np\nimport pandas as pd\nfrom numpyro import distributions as dist\nfrom shabadoo import Normal\n\n\n# random number generator seed, to reproduce exactly.\nRNG_KEY = np.array([0, 0])\n\nclass Model(Normal):\n    dv = ""y""\n    features = dict(\n        const=dict(transformer=1, prior=dist.Normal(0, 1)),\n        x=dict(transformer=lambda df: df.x, prior=dist.Normal(0, 1)),\n    )\n\n\ndf = pd.DataFrame(dict(x=[1, 2, 2, 3, 4, 5], y=[1, 2, 3, 4, 3, 5]))\nThe dv attribute specifies the variable you are predicting. features is a dictionary of dictionaries, with one item per feature. Above, two features are defined (const and x). Each feature needs a transformer and a prior.\nThe transformer specifies how to obtain the feature given a source dataframe. The prior specifies your beliefs about the model\'s coefficient for that feature.\nFitting & predicting the model\nShabadoo models implement the well-known .fit / .predict api pattern.\nmodel = Model().fit(df, rng_key=RNG_KEY)\n# sample: 100%|██████████| 1500/1500 [00:04<00:00, 308.01it/s, 7 steps of size 4.17e-01. acc. prob=0.89]\n\nmodel.predict(df)\n\n""""""\n0    1.351874\n1    2.219510\n2    2.219510\n3    3.087146\n4    3.954782\n5    4.822418\n""""""\nCredible Intervals\nUse model.predict(df, ci=True) to obtain a credible interval around the model\'s prediction. This interval accounts for error estimating the model\'s coefficients but does not account for the error around the model\'s point estimate (PRs welcome ya\'ll!).\nmodel.predict(df, ci=True)\n\n""""""\n          y  ci_lower  ci_upper\n0  1.351874  0.730992  1.946659\n1  2.219510  1.753340  2.654678\n2  2.219510  1.753340  2.654678\n3  3.087146  2.663617  3.526434\n4  3.954782  3.401837  4.548420\n5  4.822418  4.047847  5.578753\n""""""\nInspecting the model\nShabadoo\'s model classes come with a number of model inspection methods. It should be easy to understand your model\'s composition and with Shabadoo it is!\nPrint the model formula\nThe average and standard deviation of the MCMC samples are used to provide a rough sense of the coefficient in general.\nprint(model.formula)\n\n""""""\ny = (\n    const * 0.48424(+-0.64618)\n  + x * 0.86764(+-0.21281)\n)\n""""""\nLook at the posterior samples\nSamples from fitted models can be accessed using model.samples (for raw device arrays) and model.samples_df (for a tidy DataFrame).\nmodel.samples[\'x\']\n""""""\nDeviceArray([[0.9443443 , 1.0215557 , 1.0401363 , 1.1768144 , 1.1752374 ,\n...\n""""""\n\nmodel.samples_df.head()\n""""""\n                 const         x\nchain sample                    \n0     0       0.074572  0.944344\n      1       0.214246  1.021556\n      2      -0.172168  1.040136\n      3       0.440978  1.176814\n      4       0.454463  1.175237\n""""""\nMeasure prediction accuracy\nThe Model.metrics() method is packed with functionality. You should not have to write a lot of code to evaluate your model\'s prediction accuracy!\nObtaining aggregate statistics is as easy as:\nmodel.metrics(df)\n\n{\'r\': 0.8646920305474705,\n \'rsq\': 0.7476923076923075,\n \'mae\': 0.5661819464378061,\n \'mape\': 0.21729708806356265}\nFor per-point errors, use aggerrs=False. A pandas dataframe will be returned that you can join on your source data using its index.\nmodel.metrics(df, aggerrs=False)\n\n""""""\n   residual         pe        ape\n0 -0.351874 -35.187366  35.187366\n1 -0.219510 -10.975488  10.975488\n2  0.780490  26.016341  26.016341\n3  0.912854  22.821353  22.821353\n4 -0.954782 -31.826066  31.826066\n5  0.177582   3.551638   3.551638\n""""""\nYou can use grouped_metrics to understand within-group errors. Under the hood, the predicted and actual dv are groupby-aggregated (default sum) and metrics are computed within each group.\ndf[""group""] = [1, 1, 1, 2, 2, 2]\nmodel.grouped_metrics(df, \'group\')\n\n{\'r\': 1.0,\n \'rsq\': 1.0,\n \'mae\': 0.17238043177407247,\n \'mape\': 0.023077819594065668}\nmodel.grouped_metrics(df, ""group"", aggerrs=False)\n\n""""""\n       residual        pe       ape\ngroup                              \n1     -0.209107 -3.485113  3.485113\n2     -0.135654 -1.130450  1.130450\n""""""\nSaving and recovering a saved model\nShabadoo models have to_json and from_dict methods which allow models to be saved and recovered exactly.\nimport json\n\n# export to a JSON string\nmodel_json = model.to_json()\n\n# recover the model\nmodel_recovered = Model.from_dict(json.loads(model_json))\n\n# check the predictions are the same\nmodel_recovered.predict(df).equals(model.predict(df))\nTrue\nDevelopment\nTo get a development installation going, set up a python 3.6 or 3.7 virtualenv however you\'d like and set up an editable installation of Shabadoo like so:\n$ git clone https://github.com/nolanbconaway/shabadoo.git \n$ cd shabadoo\n$ pip install -e .[test]\nYou should be able to run the full test suite via:\n$ tox -e py36  # or py37 if thats what you installed\n'], 'url_profile': 'https://github.com/nolanbconaway', 'info_list': ['2', 'Jupyter Notebook', 'Updated Nov 28, 2020', '2', 'Python', 'Updated Feb 6, 2020', '2', 'Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Python', 'MIT license', 'Updated Dec 25, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Jun 15, 2020', '4', 'Python', 'MIT license', 'Updated Mar 17, 2020', '4', 'Jupyter Notebook', 'Updated Jun 9, 2020', '2', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'HTML', 'Updated Oct 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['2', 'Jupyter Notebook', 'Updated Nov 28, 2020', '2', 'Python', 'Updated Feb 6, 2020', '2', 'Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Python', 'MIT license', 'Updated Dec 25, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Jun 15, 2020', '4', 'Python', 'MIT license', 'Updated Mar 17, 2020', '4', 'Jupyter Notebook', 'Updated Jun 9, 2020', '2', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'HTML', 'Updated Oct 3, 2020']}","{'location': 'Ghaziabad, India', 'stats_list': [], 'contributions': '6,764 contributions\n        in the last year', 'description': ['Logistic Regression\nIn statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1 and the sum adding to one.\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail which is represented by an indicator variable, where the two values are labeled ""0"" and ""1"". In the logistic model, the log-odds (the logarithm of the odds) for the value labeled ""1"" is a linear combination of one or more independent variables (""predictors""); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled ""1"" can vary between 0 (certainly the value ""0"") and 1 (certainly the value ""1""), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio.\nIn a binary logistic regression model, the dependent variable has two levels (categorical). Outputs with more than two values are modeled by multinomial logistic regression and, if the multiple categories are ordered, by ordinal logistic regression (for example the proportional odds ordinal logistic model). The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier. The coefficients are generally not computed by a closed-form expression, unlike linear least squares. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson, beginning in Berkson (1944), where he coined ""logit"".\n'], 'url_profile': 'https://github.com/JayantGoel001', 'info_list': ['2', 'Jupyter Notebook', 'Updated Nov 28, 2020', '2', 'Python', 'Updated Feb 6, 2020', '2', 'Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Python', 'MIT license', 'Updated Dec 25, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Jun 15, 2020', '4', 'Python', 'MIT license', 'Updated Mar 17, 2020', '4', 'Jupyter Notebook', 'Updated Jun 9, 2020', '2', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'HTML', 'Updated Oct 3, 2020']}","{'location': 'Recife, PE - Brazil', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PedroDidier', 'info_list': ['2', 'Jupyter Notebook', 'Updated Nov 28, 2020', '2', 'Python', 'Updated Feb 6, 2020', '2', 'Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Python', 'MIT license', 'Updated Dec 25, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Jun 15, 2020', '4', 'Python', 'MIT license', 'Updated Mar 17, 2020', '4', 'Jupyter Notebook', 'Updated Jun 9, 2020', '2', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'HTML', 'Updated Oct 3, 2020']}","{'location': 'Ghaziabad, India', 'stats_list': [], 'contributions': '6,764 contributions\n        in the last year', 'description': ['LinearRegression\nIn statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.\nIn linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n\nIf the goal is prediction, forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.\nIf the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response.\n\nLinear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the ""lack of fit"" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms ""least squares"" and ""linear model"" are closely linked, they are not synonymous.\nSurface Plots | Data Visualisation\nSurface Plots are used to\n\nVisualise Loss Function in Machine Learning And Deep Learning\nVisualise State or State Value Functions in Reinforcement Learning\n\nAn Efficent Implemenation for Linear Regression using Vectorization\n\nAvoid loops in the implemenation, except gradient descent main loop\nUse numpy functions like np.sum(), np.dot() which are quite fast and already optimised\n\n'], 'url_profile': 'https://github.com/JayantGoel001', 'info_list': ['2', 'Jupyter Notebook', 'Updated Nov 28, 2020', '2', 'Python', 'Updated Feb 6, 2020', '2', 'Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Python', 'MIT license', 'Updated Dec 25, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Jun 15, 2020', '4', 'Python', 'MIT license', 'Updated Mar 17, 2020', '4', 'Jupyter Notebook', 'Updated Jun 9, 2020', '2', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'HTML', 'Updated Oct 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '168 contributions\n        in the last year', 'description': ['linear-regression-using-gradient-descent\n\n'], 'url_profile': 'https://github.com/paramveer1999', 'info_list': ['2', 'Jupyter Notebook', 'Updated Nov 28, 2020', '2', 'Python', 'Updated Feb 6, 2020', '2', 'Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Python', 'MIT license', 'Updated Dec 25, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Jun 15, 2020', '4', 'Python', 'MIT license', 'Updated Mar 17, 2020', '4', 'Jupyter Notebook', 'Updated Jun 9, 2020', '2', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'HTML', 'Updated Oct 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tariqmhmd5', 'info_list': ['2', 'Jupyter Notebook', 'Updated Nov 28, 2020', '2', 'Python', 'Updated Feb 6, 2020', '2', 'Jupyter Notebook', 'Updated Feb 13, 2020', '2', 'Python', 'MIT license', 'Updated Dec 25, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Jun 15, 2020', '4', 'Python', 'MIT license', 'Updated Mar 17, 2020', '4', 'Jupyter Notebook', 'Updated Jun 9, 2020', '2', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'HTML', 'Updated Oct 3, 2020']}"
"{'location': 'Phagwara, Punjab India', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['regression\n\n1_EDA_Correlation - Load Housing dataset and perform EDA and correlation analysis.\n2_LinearRegression_onefeature- perform linear regression and plot line over only one features of Housing datasets.\n3_LinearRegression- perform linear regression on 70% training, 30% testing on housing dataset.\n4_RANSACRegressor-Perform RANSACRegression on housing dataset.\n5_Ridege_Lasso_ElsticNet-perform linear regression varient Ridge, Lasso and Elstic Net.\n6_polynomial regression\nRandom forest and decion tree regression.\n\n'], 'url_profile': 'https://github.com/sanjayksingh012', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'Updated Feb 7, 2020', 'Haskell', 'BSD-3-Clause license', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'Bronxville, NY, USA', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Diamond-Prcie-Predication_python\n'], 'url_profile': 'https://github.com/weideng2019', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'Updated Feb 7, 2020', 'Haskell', 'BSD-3-Clause license', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Priyank0504', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'Updated Feb 7, 2020', 'Haskell', 'BSD-3-Clause license', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['regression\n'], 'url_profile': 'https://github.com/TarekLaj', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'Updated Feb 7, 2020', 'Haskell', 'BSD-3-Clause license', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': ['Regression\nFinds a regression equation using c++ which can be very important for predictions, etc. Also finds the respective errors.\n'], 'url_profile': 'https://github.com/UmarNaeem7', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'Updated Feb 7, 2020', 'Haskell', 'BSD-3-Clause license', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'The Netherlands', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['The granddaddy of Supervised Artificial Intelligence - Regression\n*** The retail market dataset ***\nDescription\nThis is a Haskell implementation of the Regression exercise from the book ""Data Smart"" by John W. Foreman.\nThis implementation uses the provided RetailMart dataset from the book.\nThis implementation is able to produce the following two regressions models by generating two GNUplot scripts:\n\nA linear regression model & ROC curve\nA logistic regression model & ROC curve\n\nAll the meta-data and the actual data points to be plotted are generated in a dataset belonging used by the script to generate the plot.\nDependencies\nSoftware\n\nbed-and-breakfast    <---- matrix library\nstatistics           <---- Only used for calculating the F Test P Value\nfilepath             <---- file paths\ndecimal-arithmetic   <---- To avoid floating point round errors\nbytestring           <---- Used to read in the dataset as a bytestream\noptparse-applicative <---- Applicatives to parse command line arguments\nvector >= 0.11       <---- High speed optimized vector library\ncassava >= 0.5       <---- CSV parser\ngenetic-algorithm    <---- Our own continuous genetic algorithm library (Custom)\n\nTooling\n\nCabal\n\nLinux, use your favorite distro to install, or build cabal from source\nWindows, install the Haskell platform or get a pre-built binary\n\n\nGNUplot version >= 5.2 (required to generate the plots)\n\nHow to execute the program\nIn order to generate a binary and generate the plots, execute the following steps:\n\nClone the genetic-algorithm project and place it at the same folder level as this project.\nCd into the regression project folder cd regression and excute the command cabal new-configure && cabal new-build (this will pull in all dependencies)\n(Optional) if you get a cabal: Could not resolve dependencies:, then execute cabal new-update and then retry step 2\nWe now have a compiled target in the dist-newstyle folder and can generate the datasets and plots using `cabal new-run regression -- --generate data/TrainingSet.csv data/TestSet.csv \n(Optional) if for some reason (Microsoft windows) this does not work, locate the binary (called regression) inside the dist-newstyle and execute `./regression(.exe) --generate data/TrainingSet.csv data/TestSet.csv \nAfter the run, we now have two GNUplot scripts, which can create graph(s) by using GNUplot:\n\ngnuplot -persist <name>_linear_model.plot\ngnuplot -persist <name>_logistic_model.plot\nThese scripts, will use the _linear_model.dat & _logistic_model.dat, which contain the data points and meta-data, used by the plots, as well as the reports of the data, which was also visible in the retailmart spreadsheets in the form of a commented table.\n\n\n\n'], 'url_profile': 'https://github.com/SirJls', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'Updated Feb 7, 2020', 'Haskell', 'BSD-3-Clause license', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gkszg', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'Updated Feb 7, 2020', 'Haskell', 'BSD-3-Clause license', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'brighton, United Kingdom', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/VikramMathur3012', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'Updated Feb 7, 2020', 'Haskell', 'BSD-3-Clause license', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Harikumar97', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'Updated Feb 7, 2020', 'Haskell', 'BSD-3-Clause license', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 26, 2020']}","{'location': 'Charlotte, North Carolina', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tulikaghosh', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Apr 17, 2020', 'C++', 'Updated Feb 7, 2020', 'Haskell', 'BSD-3-Clause license', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Apr 8, 2020', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['Tensorflow-Datasets\nPracticing problems of Classification and Regression on different datasets.\n'], 'url_profile': 'https://github.com/drbilal216', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 12, 2020', '2', 'PHP', 'MIT license', 'Updated Jul 31, 2020', '3', 'R', 'GPL-3.0 license', 'Updated Feb 10, 2020', 'Updated Feb 8, 2020', 'Java', 'Updated Feb 6, 2020', 'MATLAB', 'Updated Feb 7, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'R', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '282 contributions\n        in the last year', 'description': ['API Spec  \nGot API\'s but no time to put automated tests around it? Already have automated tests but are hard to maintain? Here is an effective automated solution that will prevent regression and maintain itself on demand.\nRelease notes\nMajor:\n\nSnapshot testing.\n\nMinor:\n\nUsing \'snapshot pattern\' step defintion produces a regex snapshot.\nCheck multiple json keys in response with one step definition.\n\nPatch:\n\nRunning single scenario does not affect other scenario snapshots.\n\nInstallation\ncomposer require genesis/behat-api-spec\nbehat.yml file\ndefault:\n  suites:\n    default:\n      contexts:\n        - Genesis\\BehatApiSpec\\Context\\ApiSpecContext\n  extensions:\n    Genesis\\BehatApiSpec\\Extension:\n      baseUrl: <Your API Url>\n      specMappings:\n        endpoint: <Namespace to folder to autoload - leading backslash>\nBasic overview\nYou can generate an endpoint file using the --endpoint-generate option. Simply run the following command for an interactive shell:\n./vendor/bin/behat --endpoint-generate\nManually create an endpoint file\n<?php\n\nnamespace ...;\n\nuse Genesis\\BehatApiSpec\\Contracts\\Endpoint;\n\nclass User implements Endpoint\n{\n    public static function getEndpoint(): string\n    {\n        return \'/users\';\n    }\n\n    public static function getRequestHeaders(): array\n    {\n        return [\n            \'accept-language\' => \'en\',\n            \'accept\' => \'text/html\',\n        ];\n    }\n}\nAdd step definition to feature file\nScenario: 200 user response\n    When I make a POST request to the ""User"" endpoint\n    Then I expect a 500 ""User"" response\n    And the response should match the snapshot\n    And I expect the following content in the JSON response:\n       | key1.subkey1 | value1 |\n       | key1.subkey2 | value2 |\nThe When I make a POST request to ""User"" endpoint will initially auto scaffold schema using the response and insert it into the endpoint file you\'ve declared above. On subsequent calls this schema will be used to validate the response, providing protection against regression. A sample schema can be as follows for the response of a GET request with 200 response {""success"": false, ""error"": ""Something went wrong.""}:\n<?php\n\nnamespace ...;\n\nuse Genesis\\BehatApiSpec\\Contracts\\Endpoint;\n\nclass User implements Endpoint\n{\n    ...\n\n    public function getResponseSchema(): array\n    {\n        \'GET\' => [\n            500 => [\n                \'headers\' => [\n                    \'Host\' => [\n                        \'value\' => \'localhost:8090\',\n                        \'type\' => self::TYPE_STRING,\n                    ],\n                    \'Connection\' => [\n                        \'value\' => \'close\',\n                        \'type\' => self::TYPE_STRING,\n                    ],\n                    \'X-Powered-By\' => [\n                        \'value\' => \'PHP/7.2.26-1+ubuntu18.04.1+deb.sury.org+1\',\n                        \'type\' => self::TYPE_STRING,\n                    ],\n                    \'content-type\' => [\n                        \'value\' => \'application/json\',\n                        \'type\' => self::TYPE_STRING,\n                    ],\n                ],\n                \'body\' => [\n                    \'success\' => [\n                        \'type\' => self::TYPE_BOOLEAN,\n                        \'optional\' => false,\n                    ],\n                    \'error\' => [\n                        \'type\' => self::TYPE_STRING,\n                        \'optional\' => false,\n                        \'pattern\' => null,\n                    ],\n                ],\n            ]\n        ]\n    }\n}\nAdjust accordingly.\nSnapshots\nFollowing on from this the And the response should match the snapshot will generate a snapshot automatically storing the response against the scenario title. This will be stored in the same directory as the test. This file should be committed with the code to allow it to be peer reviewed. Upon subsequent requests, the response will be matched with this snapshot, any difference will generate a failure. You have either the option to update the snapshot automatically using the --update-snapshots, -u flag or fix the issue in the API. Any out of date snapshots will be identified and updated with the flag appropriately. Example snapshot:\n<?php return [\n\n    \'500 user response\' =>\n        \'{""success"":false,""error"":""Something went wrong.""}\',\n\n];\nPlaceholders\nAll requests go call on the PlaceHolderService::resolveInString method with the body and url to replace any placeholders (format - {{placeholder_name}}) you may have set using the default preRequestCallable hook which is overridable (See hooks section). To add placeholders, you can use the PlaceholderService like so:\n    public function ...\n    {\n        $value = ...;\n        PlaceholderService::add($name, $value);\n\n        PlaceHolderService::getValue($name); // returns $value;\n    }\nPlaceholders are reset after every scenario to prevent test session bleed. Example usage in feature file:\n    Scenario: 200 user response\n        When I make a POST request to the ""User"" endpoint with body:\n            """"""\n                {""status"": ""{{status_failed}}""}\n            """"""\nIn the above example if you\'ve set PlaceHolderService::add(\'status_failure\', -1) then expect {""status"": ""-1""} to be sent as the body. Note values have to scalar to be part of the body.\nMultiple versions\nYou can set the version of the API to be used from the feature files or by creating a new endpoint file. To set it from the feature file:\n    ...\n    Given I use version ""1"" of the API\n    ...\nThis will allow you to retrieve the version set through the ApiSpecContext::getVersion() method in any file. For example setting it in the Endpoint getRequestHeaders method. The method also accepts a default API version if none is set. The version is also available as a placeholder {{API_VERSION}} placeholder.\nHooks\nPre request and post request hooks can be configured per context configuration in the behat.yml file like so:\n#behat.yml file\n\ndefault:\n  suites:\n    default:\n      contexts:\n        - Genesis\\BehatApiSpec\\Context\\ApiSpecContext:\n            preRequestCallable: \'MyClass::preRequestStaticCallable\'\n            postRequestCallable: \'MyClass::postRequestStaticCallable\'\nGenerating sample requests\nIf you use the step definition When I make a POST request to ""User"" endpoint to send requests to the API, you can use the --sample-request=<format> flag to generate sample requests to execute quickly through the command line. An example would be:\nvendor/bin/behat --sample-request=curl\n    Scenario: 200 user response\n        Given I set the following headers:\n          | content-type | application/json |\n          | accept       | en               |\n        When I make a GET request to ""User"" endpoint\n          │ curl -X GET --header \'content-type: application/json\' --header \'accept: text/html\' --header \'accept-language: en\' \'http://localhost:8090/index.php/users\'\nNote the curl command generated below the step definition.\nStep defintions\nMore step definitions are provided as part of the context file for validation of the API response. Find out using vendor/bin/behat -dl.\n'], 'url_profile': 'https://github.com/forceedge01', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 12, 2020', '2', 'PHP', 'MIT license', 'Updated Jul 31, 2020', '3', 'R', 'GPL-3.0 license', 'Updated Feb 10, 2020', 'Updated Feb 8, 2020', 'Java', 'Updated Feb 6, 2020', 'MATLAB', 'Updated Feb 7, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'R', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020']}","{'location': 'Hyderabad,India', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': [""SoCFourier\nGambian two cohort DNA methylation - season of conception analysis using Fourier regression\nRepo contains source code for analysis described in https://www.biorxiv.org/content/10.1101/777508v1\nMethylation and covariate data from the discovery ('ENID') cohort is available at:\nhttps://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE99863\nData from the replication ('EMPHASIS') cohort will be made available once the results from the main EMPHASIS\nstudy are published.\n\nThe main season of conception analysis is run from: 1.SoCFourier_main_analysis.R\nThis calls functions in:\n\nSoCFourier_modelling_functions.R\nSoCFourier_CpG_annotation_functions.R\nSoCFourier_plot_functions.R\nSoCFourier_stats_functions.R\ngamete_embryo_plot_and_stats_functions.R\n\n\nThe analysis of genetic effects using GEM is run from 2.SoCFourier_GEM_analysis.R\nThis calls functions in:\n\nGEM_analysis_functions.R\nGEM_plot_functions.R\n\n""], 'url_profile': 'https://github.com/Dedaniya08', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 12, 2020', '2', 'PHP', 'MIT license', 'Updated Jul 31, 2020', '3', 'R', 'GPL-3.0 license', 'Updated Feb 10, 2020', 'Updated Feb 8, 2020', 'Java', 'Updated Feb 6, 2020', 'MATLAB', 'Updated Feb 7, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'R', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/moinkhan3094', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 12, 2020', '2', 'PHP', 'MIT license', 'Updated Jul 31, 2020', '3', 'R', 'GPL-3.0 license', 'Updated Feb 10, 2020', 'Updated Feb 8, 2020', 'Java', 'Updated Feb 6, 2020', 'MATLAB', 'Updated Feb 7, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'R', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anoopyourock', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 12, 2020', '2', 'PHP', 'MIT license', 'Updated Jul 31, 2020', '3', 'R', 'GPL-3.0 license', 'Updated Feb 10, 2020', 'Updated Feb 8, 2020', 'Java', 'Updated Feb 6, 2020', 'MATLAB', 'Updated Feb 7, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'R', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mohanasundaram1986', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 12, 2020', '2', 'PHP', 'MIT license', 'Updated Jul 31, 2020', '3', 'R', 'GPL-3.0 license', 'Updated Feb 10, 2020', 'Updated Feb 8, 2020', 'Java', 'Updated Feb 6, 2020', 'MATLAB', 'Updated Feb 7, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'R', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '845 contributions\n        in the last year', 'description': [""linear-Regression\n\nLinear Regression\nLogistic Regression\n\nwhen we are dealing with categorical data( True or False, Yes or No etc) we are using logistice algorithm and\nwhen we are trying to predict result by dependent variable\ny=1/1+e^-x euler's constant = 2.71828\nApllications- Fraud Detection, Credit scoring, Real estate value prediction, Car price prediction\n\nDecision Tree\nSVM\nNaive Bayes\nkNN\nK-Means\nRandom Forest\nDimensionality Reduction Algorithms\nGradient Boosting algorithms\n\n\nGBM\nXGBoost\nLightGBM\nCatBoost\n\n""], 'url_profile': 'https://github.com/monika0123', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 12, 2020', '2', 'PHP', 'MIT license', 'Updated Jul 31, 2020', '3', 'R', 'GPL-3.0 license', 'Updated Feb 10, 2020', 'Updated Feb 8, 2020', 'Java', 'Updated Feb 6, 2020', 'MATLAB', 'Updated Feb 7, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'R', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['Regression-Ridge\nRegression-Ridge: Apply Ridge technique on Regression problem\n'], 'url_profile': 'https://github.com/kesakeerthi', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 12, 2020', '2', 'PHP', 'MIT license', 'Updated Jul 31, 2020', '3', 'R', 'GPL-3.0 license', 'Updated Feb 10, 2020', 'Updated Feb 8, 2020', 'Java', 'Updated Feb 6, 2020', 'MATLAB', 'Updated Feb 7, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'R', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Interactive 3D plot showing regression plane from multiple linear regression\nA multiple linear regression model is created using cigarette data as an example.\nInteractive 3D plot is created using plotly library and saved as a html widget.\nNo analysis is done on the fitted model.\n\n'], 'url_profile': 'https://github.com/Fergal-Stapleton', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 12, 2020', '2', 'PHP', 'MIT license', 'Updated Jul 31, 2020', '3', 'R', 'GPL-3.0 license', 'Updated Feb 10, 2020', 'Updated Feb 8, 2020', 'Java', 'Updated Feb 6, 2020', 'MATLAB', 'Updated Feb 7, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'R', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020']}","{'location': 'Helsinki', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Machine learing regression\nResult\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/canfang-feng', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 12, 2020', '2', 'PHP', 'MIT license', 'Updated Jul 31, 2020', '3', 'R', 'GPL-3.0 license', 'Updated Feb 10, 2020', 'Updated Feb 8, 2020', 'Java', 'Updated Feb 6, 2020', 'MATLAB', 'Updated Feb 7, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'R', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': [""Edit a file, create a new file, and clone from Bitbucket in under 2 minutes\nWhen you're done, you can delete the content in this README and update the file with details for others getting started with your repository.\nWe recommend that you open this README in another tab as you perform the tasks below. You can watch our video for a full demo of all the steps in this tutorial. Open the video in a new tab to avoid leaving Bitbucket.\n\nEdit a file\nYou’ll start by editing this README file to learn how to edit a file in Bitbucket.\n\nClick Source on the left side.\nClick the README.md link from the list of files.\nClick the Edit button.\nDelete the following text: Delete this line to make a change to the README from Bitbucket.\nAfter making your change, click Commit and then Commit again in the dialog. The commit page will open and you’ll see the change you just made.\nGo back to the Source page.\n\n\nCreate a file\nNext, you’ll add a new file to this repository.\n\nClick the New file button at the top of the Source page.\nGive the file a filename of contributors.txt.\nEnter your name in the empty file space.\nClick Commit and then Commit again in the dialog.\nGo back to the Source page.\n\nBefore you move on, go ahead and explore the repository. You've already seen the Source page, but check out the Commits, Branches, and Settings pages.\n\nClone a repository\nUse these steps to clone from SourceTree, our client for using the repository command-line free. Cloning allows you to work on your files locally. If you don't yet have SourceTree, download and install first. If you prefer to clone from the command line, see Clone a repository.\n\nYou’ll see the clone button under the Source heading. Click that button.\nNow click Check out in SourceTree. You may need to create a SourceTree account or log in.\nWhen you see the Clone New dialog in SourceTree, update the destination path and name if you’d like to and then click Clone.\nOpen the directory you just created to see your repository’s files.\n\nNow that you're more familiar with your Bitbucket repository, go ahead and add a new file locally. You can push your change back to Bitbucket with SourceTree, or you can add, commit, and push from the command line.\n""], 'url_profile': 'https://github.com/HayimShaul', 'info_list': ['C++', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Go', 'LGPL-3.0 license', 'Updated Jul 26, 2020', 'R', 'Updated Feb 11, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['regression-works\nlinear regression denemeleri\n'], 'url_profile': 'https://github.com/cansuengineer', 'info_list': ['C++', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Go', 'LGPL-3.0 license', 'Updated Jul 26, 2020', 'R', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Housing Prices\nThe data set for this project has been taken from Kaggle's Housing Data Set Knowledge Competition.\nThe data set is simple. This project aims at predicting house prices (residential) in Ames, Iowa, USA.\n""], 'url_profile': 'https://github.com/atharvamadkar', 'info_list': ['C++', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Go', 'LGPL-3.0 license', 'Updated Jul 26, 2020', 'R', 'Updated Feb 11, 2020']}","{'location': 'Sydney, NSW', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nidhisharma000', 'info_list': ['C++', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Go', 'LGPL-3.0 license', 'Updated Jul 26, 2020', 'R', 'Updated Feb 11, 2020']}","{'location': 'São Paulo', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['WineQuality\nA really simple model using logistic regression\nThe model was adapted to predict if is it (or not) a good wine.\nDownload dataset:\n\nhttps://archive.ics.uci.edu/ml/datasets/Wine+Quality\n\n'], 'url_profile': 'https://github.com/MarceloSantamarco', 'info_list': ['C++', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Go', 'LGPL-3.0 license', 'Updated Jul 26, 2020', 'R', 'Updated Feb 11, 2020']}","{'location': 'Ukraine, Kyiv (hometown is Sumy)', 'stats_list': [], 'contributions': '1,835 contributions\n        in the last year', 'description': ['Cargo check regression report\nThis is a bug report originally found here and reported to cargo team here.\nSteps to reproduce\nMake sure to install these versions of cargo:\n\n\ncargo 1.42.0-nightly (9d32b7b01 2020-01-26) - referred as nightly\n\n\ncargo 1.41.0 (626f0f40e 2019-12-03) - referred as stable\n\n\nrustup default nightly\ncargo check --message-format json > check-1.42.json\n\nrustup default stable\ncargo check --message-format json > check-1.41.json\n\nYou can see the difference in spans for an empty main.rs error message.\nStable version returns no error spans, but nightly one returns an array of one\nspan where line_start and line_end are set to 0, though they are guaranteed\nto be 1-based.\n'], 'url_profile': 'https://github.com/Veetaha', 'info_list': ['C++', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Go', 'LGPL-3.0 license', 'Updated Jul 26, 2020', 'R', 'Updated Feb 11, 2020']}","{'location': 'greater noida', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\nMultiple Linear Regression\n'], 'url_profile': 'https://github.com/kkapasiya', 'info_list': ['C++', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Go', 'LGPL-3.0 license', 'Updated Jul 26, 2020', 'R', 'Updated Feb 11, 2020']}","{'location': 'Pittsburgh, PA', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shiks07', 'info_list': ['C++', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Go', 'LGPL-3.0 license', 'Updated Jul 26, 2020', 'R', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Smilo Integration tests\n\nTest specification\n\nBFT Integration tests\n\nTest specification\n\n'], 'url_profile': 'https://github.com/smilofoundation', 'info_list': ['C++', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Go', 'LGPL-3.0 license', 'Updated Jul 26, 2020', 'R', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['regression\n'], 'url_profile': 'https://github.com/Clorisss', 'info_list': ['C++', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Go', 'LGPL-3.0 license', 'Updated Jul 26, 2020', 'R', 'Updated Feb 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Simple Linear Regression model for beginners.\n'], 'url_profile': 'https://github.com/karakusfurkan', 'info_list': ['Python', 'Updated Jun 29, 2020', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Multi Linear Regression for beginners\n'], 'url_profile': 'https://github.com/karakusfurkan', 'info_list': ['Python', 'Updated Jun 29, 2020', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Logistic-Regression\nSample logistic regression model\n'], 'url_profile': 'https://github.com/RenierVeiga', 'info_list': ['Python', 'Updated Jun 29, 2020', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Pittsburgh, PA', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aghalsas', 'info_list': ['Python', 'Updated Jun 29, 2020', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Los Angeles, CA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sygowda', 'info_list': ['Python', 'Updated Jun 29, 2020', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Pune India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Housing-Prediction-\nHousing Prediction - Linear Regression\n'], 'url_profile': 'https://github.com/kedarjoshi5231', 'info_list': ['Python', 'Updated Jun 29, 2020', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Pittsburgh, PA', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aghalsas', 'info_list': ['Python', 'Updated Jun 29, 2020', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Polynomial Regression in Python for beginners\n'], 'url_profile': 'https://github.com/karakusfurkan', 'info_list': ['Python', 'Updated Jun 29, 2020', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['PSZT_winter_2020\nAuthors: Joanna Kiesiak, Katarzyna Rzeczyca\nProject: Linear regression and MLP comparison.\n'], 'url_profile': 'https://github.com/jkiesiak', 'info_list': ['Python', 'Updated Jun 29, 2020', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}","{'location': 'Istanbul', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gogundur', 'info_list': ['Python', 'Updated Jun 29, 2020', 'Python', 'Updated Jun 29, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated May 4, 2020']}"
"{'location': 'Nigeria', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danieljohnson18', 'info_list': ['Python', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Ruby', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['capstone2-rm\nHola!\nThis is Capstone 2 Project in Data Science Bootcamp hosted by Algoritma Data Science.\nThis R-based project uses several regression model, such as Linear Regression, Decision Tree, and Random Forest, and chooses the best model to be used to predict concrete strength. The conclusion here is Random Forest with number = 9 and repeat = 5 are the best model.\nThank you for seeing this code!\nHope your best comments to be my evaluation!\n'], 'url_profile': 'https://github.com/donigofernando', 'info_list': ['Python', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Ruby', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shubhankit101', 'info_list': ['Python', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Ruby', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'Lille, France', 'stats_list': [], 'contributions': '1,595 contributions\n        in the last year', 'description': ['\nclere: Simultaneous Variables Clustering and Regression\n\n\n\n\n\n\nImplements an empirical Bayes approach for simultaneous variable\nclustering and regression. This version also (re)implements in C++ an R\nscript proposed by Howard Bondell that fits the Pairwise Absolute\nClustering and Sparsity (PACS) methodology (see Sharma et al (2013) doi:\n10.1080/15533174.2012.707849).\nInstallation\nYou can install the released version of clere from\nCRAN with:\ninstall.packages(""clere"")\nAnd the development version from GitHub with:\n# install.packages(""remotes"")\nremotes::install_github(""mcanouil/clere"")\nCiting clere\n\nYengo L, Jacques J, Biernacki C, Canouil M (2016). “Variable Clustering\nin High-Dimensional Linear Regression: The R Package clere.” The R\nJournal, 8(1), 92–106. doi:\n10.32614/RJ-2016-006.\n\n@Article{,\n  title = {{Variable Clustering in High-Dimensional Linear Regression: The R Package clere}},\n  author = {Loïc Yengo and Julien Jacques and Christophe Biernacki and Mickael Canouil},\n  journal = {The R Journal},\n  year = {2016},\n  month = {apr},\n  doi = {10.32614/RJ-2016-006},\n  pages = {92--106},\n  volume = {8},\n  number = {1},\n}\n\nExample\nlibrary(clere)\n\nx <- matrix(rnorm(50 * 100), nrow = 50, ncol = 100)\ny <- rnorm(50)\n\nmodel <- fitClere(y = y, x = x, g = 2, plotit = FALSE)\nmodel\n#>  ~~~ Class: Clere ~~~\n#>  ~ y : [50] -0.3663  1.0417  0.8401  0.6298  1.3977 -0.4709\n#>  ~ x : [50x100]\n#>                  1        2        3        4        5        .    \n#>         1      0.54299  0.54408  1.73588 -0.05461 -0.94133 ........\n#>         2      1.11327 -1.00079 -0.71194 -2.17234  0.38946 ........\n#>         3     -0.97223  0.03499 -1.20295 -1.32578 -1.12280 ........\n#>         4      0.71881 -0.92304  0.22933  1.22511  0.35874 ........\n#>         5     -0.53657  0.01233 -0.72067 -0.10695 -1.71511 ........\n#>         .     ........ ........ ........ ........ ........ ........\n#> \n#>  ~ n : 50\n#>  ~ p : 100\n#>  ~ g : 2\n#>  ~ nItMC : 50\n#>  ~ nItEM : 1000\n#>  ~ nBurn : 200\n#>  ~ dp : 5\n#>  ~ nsamp : 200\n#>  ~ sparse : FALSE\n#>  ~ analysis : ""fit""\n#>  ~ algorithm : ""SEM""\n#>  ~ initialized : FALSE\n#>  ~ maxit : 500\n#>  ~ tol : 0.001\n#>  ~ seed : 945\n#>  ~ b : [2]  0.613709 -0.006548\n#>  ~ pi : [2] 0.01002 0.98998\n#>  ~ sigma2 : 0.5981\n#>  ~ gamma2 : 0.0001097\n#>  ~ intercept : 0.1022\n#>  ~ likelihood : -64.18\n#>  ~ entropy : 0\n#>  ~ P : [100x2]\n#>              Group 1 Group 2\n#>         1       0       1   \n#>         2       0       1   \n#>         3       0       1   \n#>         4       0       1   \n#>         5       0       1   \n#>         .    ....... .......\n#> \n#>  ~ theta : [1000x8]\n#>                intercept    b1        b2        pi1       pi2        .    \n#>          1     -0.03965  -0.02462   0.05342   0.50000   0.50000  .........\n#>          2      0.08769  -0.02585   0.05114   0.53000   0.47000  .........\n#>          3      0.03731  -0.03260   0.05189   0.46000   0.54000  .........\n#>          4     -0.03508  -0.05160   0.05514   0.45000   0.55000  .........\n#>          5     -0.08861  -0.06464   0.05811   0.42000   0.58000  .........\n#>          .     ......... ......... ......... ......... ......... .........\n#> \n#>  ~ Zw : [100x200]\n#>        1 2 3 4 5 .\n#>      1 1 1 1 1 1 .\n#>      2 1 1 1 1 1 .\n#>      3 1 1 1 1 1 .\n#>      4 1 1 1 1 1 .\n#>      5 1 1 1 1 1 .\n#>      . . . . . . .\n#> \n#>  ~ Bw : [100x200]\n#>                     1          2          3          4          5          .     \n#>          1      -7.080e-03 -5.029e-03 -1.654e-02 -9.838e-03 -1.157e-02 ..........\n#>          2       1.664e-03 -3.672e-05 -1.369e-02  5.982e-03 -1.140e-02 ..........\n#>          3      -6.606e-03 -1.330e-02  3.800e-03 -1.147e-02 -1.297e-02 ..........\n#>          4      -8.453e-03  8.423e-03 -1.493e-03  5.931e-03  1.637e-02 ..........\n#>          5      -2.101e-02 -5.158e-03 -7.439e-03 -8.822e-03 -1.320e-02 ..........\n#>          .      .......... .......... .......... .......... .......... ..........\n#> \n#>  ~ Z0 : NA\n#>  ~ message : NA\n\nplot(model)\n\nclus <- clusters(model, threshold = NULL)\nclus\n#>   [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#>  [38] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#>  [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2\n\npredict(model, newx = x + 1)\n#>  [1] -0.41014043  0.58417440  0.07565606  0.12651690  0.10691068 -0.51438081\n#>  [7] -0.53730664 -0.38719372  0.96052339  1.04059573 -0.47278984 -0.54533104\n#> [13] -0.14680892  0.04529841 -0.44803772  0.48753442 -1.03283097 -0.96206984\n#> [19]  0.90252948  0.35887126 -0.59158591  0.27172199  0.73862087 -0.13525905\n#> [25]  1.14287637  0.37955118 -0.21296002 -0.66091713  0.22797485  0.04944170\n#> [31]  0.52612573 -0.15168824 -0.78401104 -0.53532663 -0.44697030  0.19048671\n#> [37]  0.10341728 -0.37691391 -0.69165509  0.52461656  0.60826835  0.01190567\n#> [43] -0.50238925  0.22288924 -0.28840397 -0.43573542 -0.26704384  0.49779102\n#> [49]  0.08028461  0.41752563\n\nsummary(model)\n#>  -------------------------------\n#>  | CLERE | Yengo et al. (2016) |\n#>  -------------------------------\n#> \n#>  Model object for  2 groups of variables ( user-specified )\n#> \n#>  ---\n#>  Estimated parameters using SEM algorithm are\n#>  intercept = 0.1022\n#>  b         =  0.613709   -0.006548\n#>  pi        = 0.01002 0.98998\n#>  sigma2    = 0.5981\n#>  gamma2    = 0.0001097\n#> \n#>  ---\n#>  Log-likelihood =  -64.18 \n#>  Entropy        =  0 \n#>  AIC            =  140.36 \n#>  BIC            =  151.84 \n#>  ICL            =  151.84\nGetting help\nIf you encounter a clear bug, please file a minimal reproducible example\non github.\nFor questions and other discussion, please contact the package\nmaintainer.\n\nPlease note that this project is released with a Contributor Code of\nConduct.\nBy participating in this project you agree to abide by its terms.\n'], 'url_profile': 'https://github.com/mcanouil', 'info_list': ['Python', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Ruby', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/salehgondal', 'info_list': ['Python', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Ruby', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Servizio Stage Testing\nSelenium regression tests for Servizio\nRequires a Ruby installation\nInstall Ruby Version Manager using a Bash shell:\n\\curl -sSL https://get.rvm.io | bash -s stable\'\n\nInstall most recent stable version of Ruby:\nrvm install 2.5.1\n\nRequires Selenium WebDriver installation\nURL for Selenium downloads page:\nhttps://selenium.dev/downloads/\nURL for ChromeDriver (WebDriver for the Google Chrome browser)\nhttps://chromedriver.chromium.org/downloads\nRequired Ruby Gems\nIt is recommended to add these to a gemfile in your local project folder.\nUse the bundler after completing the gemfile.\nSelenium WebDriver:\nhttps://rubygems.org/gems/selenium-webdriver\nrspec:\nhttps://rubygems.org/gems/rspec\nrequire_all:\nhttps://rubygems.org/gems/require_all\nRecommended edits to .bashrc file\nAdd rspec to PATH:\nexport PATH=$PATH: *path to folder where gemfile is located*   \n\nAdd RVM to PATH:\nexport PATH=""$PATH:$HOME/.rvm/bin""\n\nOther Useful Links\nDocs for using Selenium API:\nhttps://selenium.dev/selenium/docs/api/rb/Selenium.html\nRspec expectation cheat sheet:\nhttps://www.rubypigeon.com/posts/rspec-expectations-cheat-sheet/\n'], 'url_profile': 'https://github.com/brad-steveo', 'info_list': ['Python', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Ruby', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'Egypt', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Simple_Linear_Regression\nSimple Linear Regression from Scratch\n'], 'url_profile': 'https://github.com/a7madmostafa', 'info_list': ['Python', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Ruby', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'Berkeley, California', 'stats_list': [], 'contributions': '163 contributions\n        in the last year', 'description': ['correlations\nIntroduction to correlations and regression.\n'], 'url_profile': 'https://github.com/seantrott', 'info_list': ['Python', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Ruby', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': [""Game-Balance-of-Super-Mario-Party\nThis is the main idea: Each one of the 6-faces dice have some common 'resources' that are: - the expected movement in each turn, i.e. the mean of movement faces -the expected coin gain/loss in each turn, i.e. the mean of coin faces -the different options of movement, i.e. the variety of movement faces (due to the many 'special events' squares and different routes, players with more options may be in advantage) The three resources above are favourable to the player, so are to be maximized.\nTo every mean there is a variance associated, which can be favourable (risk-seeking p.o.v.) or unfavorable (risk-adversion p.o.v.).\nAs long as we want the game to be balanced, we can't optimize all the resources values to infinity: there must occur trade-offs between resources so that in improving a resource an other is worsened.\nFor each pair of resources a die can be seen as a market basket and therefore compared with all the others in terms of 'die X is better in this trade-off than die Y'. Therefore for each trade-off we can make a trade-off dice ranking.\nFrom the games we can have an idea of which die is better than the other through the frequency of victories for each character; basically a dice ranking based on victories, or a performance dice ranking.\nThe objective is: Determine which trade-off rankings are significant descriptors of the performance dice-ranking\nFURTHER INFORMATION ABOUT DATA: https://www.kaggle.com/riccardogiussani/super-mario-party-dice\n""], 'url_profile': 'https://github.com/RikJux', 'info_list': ['Python', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Ruby', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'Munich', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Logistic Regression\nLogistic Regression for fraud detection.This repository includes Bilkent University Introduction to Machine Learning course assignment. The problem definition is given below:\nThe dataset contains only numerical input variables which are the result of the PCA transformation, i.e. features V1, V2, ... V28 are the principal components obtained from PCA. The only feature which has not been transformed with PCA is Amount. The Class column is the response variable and it takes value 1 in case of fraud and 0 otherwise.\nThe whole question can be found in cs464_fall19_hw2.pdf file on fourth question.\n'], 'url_profile': 'https://github.com/yusufsamsum', 'info_list': ['Python', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Ruby', 'Updated Oct 29, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'R', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['frols\nForward Regression Orthogonal Least Squares\nNotes and a start at implementing the FROLS algorithm, following\nBillings S.A. ""Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains"". Wiley, 2013\n(e.g. pp. 63)\nFair warning, I have not checked extensively to see if this implementation is correct. In the notebook, I test the algorithm on an example from the book, and it produces a reasonably close result, but not exact.\n'], 'url_profile': 'https://github.com/lkilcommons', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 27, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 9, 2020', 'C', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 28, 2020', '2', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'HTML', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['Linear regression using gradient descent\n'], 'url_profile': 'https://github.com/Ielay', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 27, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 9, 2020', 'C', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 28, 2020', '2', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'HTML', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fivkat', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 27, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 9, 2020', 'C', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 28, 2020', '2', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'HTML', 'Updated Feb 5, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nSimple Linear Regression using Python\n'], 'url_profile': 'https://github.com/mriduldhawan', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 27, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 9, 2020', 'C', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 28, 2020', '2', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'HTML', 'Updated Feb 5, 2020']}","{'location': 'Ukraine', 'stats_list': [], 'contributions': '3,406 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kirushyk', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 27, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 9, 2020', 'C', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 28, 2020', '2', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'HTML', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['logistic-regression\nLogistic regression on Movie dataset :\nThe objective was to determine whether movies will be good or bad based on a variety of metrics prior to its release. We categorized movies that are good as those that received a rating of 6/10 or greater. Next we built a logistic regression model that used budget, runtime, popularity, and month to predict whether a movie would …\n'], 'url_profile': 'https://github.com/balajievr', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 27, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 9, 2020', 'C', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 28, 2020', '2', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'HTML', 'Updated Feb 5, 2020']}","{'location': 'Seattle, WA', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Getting started with Amazon SageMaker\nLab 01: Setting up a Notebook instance\nLab 02: Feature engineering with XGBoost\nLab 03: Regression with Amazon SageMaker built-in algorithm XGBoost\nLab 04: Hyperparameter tuning with Amazon SageMaker\nAdvanced topics\nLab 05: Bring your own custom model with Amazon SageMaker\nLab 06: Debugging XGBoost Training Jobs with Amazon SageMaker Debugger\nLab 07: SageMaker Model Monitor - visualizing monitoring results\nLab 08: Autoscaling Amazon SageMaker endpoints\nBonus\nLab 02a: Feature engineering with scikit-learn\n'], 'url_profile': 'https://github.com/sahays', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 27, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 9, 2020', 'C', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 28, 2020', '2', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'HTML', 'Updated Feb 5, 2020']}","{'location': 'india', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['Housing_price_prediction\nHouse price prediction using different model like Linearn Regression, Decission Tree, Random Forrest\n'], 'url_profile': 'https://github.com/El-Do-RaDo', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 27, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 9, 2020', 'C', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 28, 2020', '2', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'HTML', 'Updated Feb 5, 2020']}","{'location': 'Taiwan', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['house-prices-advanced-regression-techniques\nThis repository is for kaggle competition named house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/UTMOSTOF9', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 27, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 9, 2020', 'C', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 28, 2020', '2', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'HTML', 'Updated Feb 5, 2020']}","{'location': 'Sydney, Australia', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dcastellanogargiulo', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jun 27, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Feb 9, 2020', 'C', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 28, 2020', '2', 'Jupyter Notebook', 'Updated Feb 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 14, 2020', 'HTML', 'Updated Feb 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/soltanimohamed', 'info_list': ['Java', 'Updated Feb 9, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 24, 2020', '1', 'Python', 'Updated May 19, 2020', 'Python', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['126project\nLinear regression on Red Wine Quality\n'], 'url_profile': 'https://github.com/wenhaos99', 'info_list': ['Java', 'Updated Feb 9, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 24, 2020', '1', 'Python', 'Updated May 19, 2020', 'Python', 'Updated Mar 24, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rajeevteejwal', 'info_list': ['Java', 'Updated Feb 9, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 24, 2020', '1', 'Python', 'Updated May 19, 2020', 'Python', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['MLP\nclassification et régression avec un MLP\n'], 'url_profile': 'https://github.com/BEZIA-EEA', 'info_list': ['Java', 'Updated Feb 9, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 24, 2020', '1', 'Python', 'Updated May 19, 2020', 'Python', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hsara-123', 'info_list': ['Java', 'Updated Feb 9, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 24, 2020', '1', 'Python', 'Updated May 19, 2020', 'Python', 'Updated Mar 24, 2020']}","{'location': 'Canada ', 'stats_list': [], 'contributions': '289 contributions\n        in the last year', 'description': ['Linear-regression-ML-price-prediction\nML Linear-regression for housing price prediction\n'], 'url_profile': 'https://github.com/kobi87', 'info_list': ['Java', 'Updated Feb 9, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 24, 2020', '1', 'Python', 'Updated May 19, 2020', 'Python', 'Updated Mar 24, 2020']}","{'location': 'Rochester, NY', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jhordan-McKenzie', 'info_list': ['Java', 'Updated Feb 9, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 24, 2020', '1', 'Python', 'Updated May 19, 2020', 'Python', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Classification Baseline\nThis repository provides a simple logistic regression classification baseline for NLP research in text classification.\nThrough simple commands, one can:\n\nRun random search trials over a variety of LR hyperparameters, including those involving the input representation.\nRun cross-validation/jackknifing if dev set is not available\nRun experiments with (possibly stratified) subsamples of the training data\nParallelize experiments using gnu parallel\nVisualize the effect of individual hyperparameters on classification performance\n\nThis repository just expects a train.jsonl file, in JSON lines format, each line corresponding to the format {""text"":..., ""label"":...}. You can also supply a dev.jsonl file. If you don\'t, we will jackknife the training data and report performance metrics over all splits.\nRun single experiment\npython -m lr.train --train_file data/train.jsonl --dev_file data/dev.jsonl --search_trials 5 --serialization_dir model_logs/lr -o\n\nRun single experiment on (stratified) sampled data\npython -m lr.train --train_file data/train.jsonl --dev_file data/dev.jsonl --search_trials 10 --serialization_dir model_logs/sampled_lr --train_subsample 1000 --stratified -o\n\nRun single jackknifing experiment\npython -m lr.train --train_file data/train.jsonl --search_trials 10 --jackknife_partitions 3 --save_jackknife_partitions --serialization_dir model_logs/jackknife_lr  --stratified --train_subsample 1000 -o\n\nEvaluate on test data\nparallel --ungroup python -m lr.train --train_file data/train.jsonl --dev_file data/dev.jsonl --test_file data/test.jsonl --search_trials 1  --serialization_dir model_logs/ag_lr/exp_{#} --evaluate_on_test -o ::: {1..6}\n\nRun many experiments in parallel\nparallel --ungroup python -m lr.train --train_file data/train.jsonl --dev_file data/dev.jsonl --search_trials 1  --serialization_dir model_logs/parallel_lr/exp_{#}  -o ::: {1..6}\n\nMerge multiple experiment results\npython -m lr.merge --experiments model_logs/parallel_lr/* --output-file model_logs/parallel_lr/master_results.jsonl\n\nVisualize scatterplot of hyperparameter vs performance\npython -m lr.plot --hyperparameter C  --results_file parallel_lr/master_results.jsonl -p dev_f1 \n\nVisualize boxplot of hyperparameter vs performance\npython -m lr.plot --hyperparameter weight --boxplot --results_file parallel_lr/master_results.jsonl -p dev_f1 \n\n'], 'url_profile': 'https://github.com/allenai', 'info_list': ['Java', 'Updated Feb 9, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 24, 2020', '1', 'Python', 'Updated May 19, 2020', 'Python', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Thesis-Machine-Learning-In-Finance\nDerivative pricing using Gaussian process regression obtaining a tremendous speed-up.\nSpecial thanks is given to the authors of the packages Tensorflow (tf), GPyTorch (gpy) and Pymc3 (pym) alleviating the work. When we use one of these\npackages, we write ""_package"".\n'], 'url_profile': 'https://github.com/2290Veusseleir', 'info_list': ['Java', 'Updated Feb 9, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 24, 2020', '1', 'Python', 'Updated May 19, 2020', 'Python', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '199 contributions\n        in the last year', 'description': ['Regression via Custom Gradient Descent Tool\nOverview\nThis is a function implementing a regression model obtained via the application of a custom-built gradient descent method.\n(Note) This function requires Python 3.6.\nHow to Use\nUnivariate Regression Gradient Descent\nLet us first import the required dependencies:\nfrom SimpleRegMod import simply_toy_reg\nfrom sklearn.datasets import make_regression\nfrom CustomGradDescReg import simpreg_custom_graddesc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nLet us generate some random data upon which we will fit our regression model and perform gradient descent:\n(Note): the concatenation of an all ones column at the front of the generated data is meant to maintain the contribution of the intercept as 1.\nX, y = make_regression(n_samples = 100, n_features = 1, noise = 3)\nX_head = np.ones((100,1))\nnew_X = np.concatenate((X_head, X), axis=1)\n\nWe want to learn a univariate regression:\ninputvar = np.array([""x0"", ""x1""])\nparams = np.array([""w0"", ""w1""])\n\nWe can use either stochastic or batch gradient descent. In the former case, we can afford to utilise a relatively larger learning rate. We should however be cautious and not specify too many iterations as this approach learns after each data point:\nres_stoch, err_stoch = simpreg_custom_graddesc(inputvar, params, train_type=""stochastic"",\n                              alpha=0.05, train_dt=new_X, label_dt=y, iter_nb=3)\n\nx_vals = np.arange(len(err_stoch))\n\nplt.plot(x_vals, err_stoch)\n\nplt.title(\'Error Rate Over the Number of Predictions\')\nplt.xlabel(""Nb. of Predictions"", fontsize=8)\nplt.ylabel(""Error Value"", fontsize=8)\nplt.axis(""tight"")\n\nFor the batch gradient descent, the learning rate should be small otherwise it will never converge and reach stratospheric levels of error rate. Furthermore, there should be a more generous number of iterations as this approach learns once after traversing the entire dataset:\nres_batch, err_batch = simpreg_custom_graddesc(inputvar, params, train_type=""batch"",\n                              alpha=0.005, train_dt=new_X, label_dt=y, iter_nb=10)\n\nx_vals = np.arange(len(err_batch))\n\nplt.plot(x_vals, err_batch)\n\nplt.title(\'Error Rate Over the Number of Predictions\')\nplt.xlabel(""Nb. of Predictions"", fontsize=8)\nplt.ylabel(""Error Value"", fontsize=8)\nplt.axis(""tight"")\n\nMultivariate Regression Gradient Descent\nWe have to generate random data of an appropriate dimension for this:\nX, y = make_regression(n_samples = 100, n_features = 2, noise = 3)\nX_head = np.ones((100,1))\nnew_X = np.concatenate((X_head, X), axis=1)\n\ninputvar = np.array([""x0"", ""x1"", ""x2""])\nparams = np.array([""w0"", ""w1"", ""w2""])\n\nThe rest is similar for the stochastic gradient descent:\nres_stoch, err_stoch = simpreg_custom_graddesc(inputvar, params, train_type=""stochastic"",\n                              alpha=0.05, train_dt=new_X, label_dt=y, iter_nb=3)\n\nAnd the batch gradient descent:\nres_batch, err_batch = simpreg_custom_graddesc(inputvar, params, train_type=""batch"",\n                              alpha=0.005, train_dt=new_X, label_dt=y, iter_nb=10)\n\n'], 'url_profile': 'https://github.com/antoine186', 'info_list': ['Java', 'Updated Feb 9, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 24, 2020', '1', 'Python', 'Updated May 19, 2020', 'Python', 'Updated Mar 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/carhulse', 'info_list': ['R', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'PHP', 'Updated Apr 30, 2020', '1', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Johannesburg', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['We will predict employee salaries from different employee characteristics (or features). We are going to use a simple supervised learning technique: linear regression. We want to build a simple model to determine how well Years Worked predicts an employee’s salary.\n'], 'url_profile': 'https://github.com/masaimahapa', 'info_list': ['R', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'PHP', 'Updated Apr 30, 2020', '1', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['BU WordPress Visual Regression assist plugin\nThis plugin is intended to facilitate gathering URLs from a specific WordPress site. The idea is geared towards visual regression testing. It is Initially developed with the use of BackstopJS in mind but can be adapted as it simply helps in the management of the URLs for testing.\nThe name\nThe name of the plugin will most likely change in time as its functionality evolve\nbu-vis-reg\nbu = Boston University\nvis = Visual\nreg = Regression\nRoutes\nIn use\nGet output of all post types URLs\n{yoursite address}/wp-json/visreg/v1/allposts\nUpcoming\nGet output of all category URLs\n{yoursite address}/wp-json/visreg/v1/allcategories\nAdmin page\nThe link to the admin page for this plugin is located under “Tools” in the dashboard menu\nUsage\nFlagging pages manually for custom selection\nOn any page, post, or custom post types, there is a check box on the sidebar to flag a specific page for URL testing. Check the box and hit update if you wish to include such post into the flagged selection.\nURL Request\nIn admin page, select the type of post or content you wish to get the URLs from and hit “Generate list”.\nTo use the list, hit “Export list” and the list of URLs will be available for download as a JSON file.\nHow it works\nThe plugin contains a set of custom API endpoints that helps with sending request via the wp-json option built into WordPress. For more information on wp-json visit the WordPress developer codex.\nThe response is than processes on the client side and generates a list table of published posts that can be later customized.\nWorking features\n\nCopy url request\nQuick filter\nAdvanced filter with date filtering and post-type filtering (one post typa at a time)\n\nNot working features\n\nMultiple post-type filtering\nFlagged filtering\nRandomized selection filtering\n\n'], 'url_profile': 'https://github.com/ekspresyon', 'info_list': ['R', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'PHP', 'Updated Apr 30, 2020', '1', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Bhubneswar,Odisha', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['LINEAR REGRESSION WITH ONE VARIABLE,this model is used to show how the linear regression with one variable is very inaccurate while computing major/huge dataset with many features.\n'], 'url_profile': 'https://github.com/swapnil0070', 'info_list': ['R', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'PHP', 'Updated Apr 30, 2020', '1', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Nigeria', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['linear_regression_scratch\nImplementation of linear regression from scratch\n'], 'url_profile': 'https://github.com/ajatau', 'info_list': ['R', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'PHP', 'Updated Apr 30, 2020', '1', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['QAR-BS\nThis repository contains R codes for the proposed method in \'\'Quantile association regression on bivariate survival data"".\nThe main function, ""qor_bs"", and other required functions can be found in the ""R codes for the proposed method"" folder.\nIn addition, packages ""quantreg"", ""numDeriv"", ""rootSolve"", and ""msm"", are needed in R to conduct the proposed method.\nFinally, one example is provided under the ""Example"" folder for the illustratoin.\n'], 'url_profile': 'https://github.com/lingwanchen', 'info_list': ['R', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'PHP', 'Updated Apr 30, 2020', '1', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['error-metrics\nerror metrics for classification and regression\n'], 'url_profile': 'https://github.com/Appiiee', 'info_list': ['R', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'PHP', 'Updated Apr 30, 2020', '1', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Atlanta, GA', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['nicar_logistic_regression\nNICAR2020 Stats 3: Logistic Regression hands-on class\n'], 'url_profile': 'https://github.com/NewsappAJC', 'info_list': ['R', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'PHP', 'Updated Apr 30, 2020', '1', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '205 contributions\n        in the last year', 'description': ['Linear-Regression-Models\nLinear Regression models implemented in python\n'], 'url_profile': 'https://github.com/AryanP281', 'info_list': ['R', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'PHP', 'Updated Apr 30, 2020', '1', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Regression Project: Ames Housing Price Predictions\n\nContext:\nKaggle competition with the objective of developing a model to predict housing prices in Ames Iowa based on historical data.  Only Regression models were allowed for competition.  Over 100+ DSI students across the nation participated. This model achieved a ranking within the top DSI 5% based on RMSE and is valid for price predictions.\nKey objective:\n\n- Create model to predict housing prices based on historical data\n- Evaluate model\n- Disseminate techniques, methods, results to technical audience and colleagues\n\n\n\nExecutive Summary\n\n\nNormal Distribution is an important factor for machine learning algorithms.  Addressing outliers and applying natural log can assist in normalizing and thereby improving prediction performance\n\n\nIn addition, Understanding Variable Correlation and Feature Engineering can elevate model prediction performance\n\n\nSeasonality affected Ames sales volume but Average Sale Prices remained fairly consistent\n\n\nFor the Ames Housing Dataset Ranking Neighborhoods reduced multicollinearity, improved SalePrice predictions and directly drove a 3000+ reduction in Kaggle RSME score\n\n\nVisualizing and identifying high mean variance within a variable can indicate degree of influence on Target (Sale Price)\n\n\nThe model suggests that Gr Liv Area, Overall Quality, Age, Neighborhood Rank, Bathrooms, Total Sq Ft, Is_New Were Leading Features With Impactful Coefficients\n\n\nThe value of the model is that it is accurate and can facilitate real estate agency decision making and housing price prediction\n\n\nKey recommendation: trial first, then place model into production in order to drive operational/strategic real estate agency decision making.  Over time the model will continue to learn and increasingly become more accurate \n\n\n\n\n'], 'url_profile': 'https://github.com/pabriv', 'info_list': ['R', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'PHP', 'Updated Apr 30, 2020', '1', 'MATLAB', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'MIT license', 'Updated Feb 20, 2020', 'Python', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}"
"{'location': 'Ahmedabad', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Unconstrained_Linear_Regression\nGeneralisation of Linear Regression in python.\nFlask basics for general purpose use in a website.\nFormat to Enter in Postman\n\nEnter the filepath that you want to give in Linear Regression.\nEnter the target that you want to predict.\nExample:\n\n{\n\t""fname"":""train.csv"",\n\t""target"":""SalePrice""\n}\n\n\n'], 'url_profile': 'https://github.com/mahima97', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'JavaScript', 'Updated Jan 22, 2021', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Stockholm, Sweden', 'stats_list': [], 'contributions': '657 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/salrm8', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'JavaScript', 'Updated Jan 22, 2021', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/LeboSeribe', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'JavaScript', 'Updated Jan 22, 2021', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jaypathak25', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'JavaScript', 'Updated Jan 22, 2021', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': [""Web Customer Fingerprints\nLDA customer clustering & future purchase prediction\n(LDA : latent dirichlet allocation)\nA data science adventure by Jim Frank\nE-commerce website data from  Retailrocket recommender system dataset .\n\nTable of Contents\n\nData Science Goals\nThe Data\nEDA And Data Wrangling\nLDA Customer Product Clustering\nPredicting Customer Purchases\nConclusion\n\n\nData Science Goals\nQUESTION:  \nWhich customers did not make a purchase, but were likely to make a purchase soon?\nGOALS:  \n\nLDA customer product relationships quantified by latent grouping probabilities\nProduct purchase prediction with basic dataset features\nAdd latent grouping features to the prediction model\n\nWHY THIS SET OF GOALS?:  \nThe relationships and interactions represented by customer/product interactions may be relevant for useful customer predictions. A perfect case for soft clustering. The anonymized product data means unsupervised grouping is hard to analyze. Improving a prediction model with the customer clusters shows their relevance.\n\nThe Data\nContext\nThe data has been collected from a real-world ecommerce website. It is raw data, i.e. without any content transformations, however, all values are hashed due to confidential issues.\nContent\nBehaviour data consists of 3 event types. Those being views, add to carts, and transactions. These product interactions were collected over a period of 4.5 months in 2015.\nHere's an example of the raw data:\n\n\n\ntimestamp\nvisitorid\nevent\nitemid\ntransactionid\n\n\n\n\n2015-06-01 23:02:12\n257597\nview\n355908\nNaN\n\n\n2015-06-01 23:50:14\n992329\nview\n248676\nNaN\n\n\n\nEDA And Data Wrangling\n\n1,407,580 unique visitors\n2,756,101 total events\n2,664,312 views\n69,332 add to carts\n22,457 transactions\n0.81% events that are transactions\n\nThe data required filtering to be useable for LDA clustering.\n2 < # product connections < 400\nUsing this criteria yields ample quality data.\n\n406,020 unique visitors\n235,061 unique products\n1,587,292 events\n\n\nFigure:  Visitor counts binned on # products interacted with \nLDA Customer Product Clustering\nWhen customers view and purchase products from a particular category of product, they create the ties that LDA finds and groups together. As we are finding customer fingerprints, first lets group our customers and products into 10 groups. Multi-dimensional scaling is done with Jensen-Shannon distance.\n\nFigure:  10 cluster LDA visualization \n\n 10 cluster LDA score. Lower perplexity is better. \nIn the 2-dimension representation of multi-dimensional space, you can see that 3 of the topics group closely to one another. Perhaps two of the groups are redundant. Let's group to 8 clusters and take a look.\n\nFigure:  8 cluster LDA visualization \n\n 8 cluster LDA score. Lower perplexity is better. \nI like the look of this. Quality separation in multi-dimensional space. The anonymized product ID's just means we'll have to improve our logistic prediction to prove the worth of this LDA clustering.\nPredicting Customer Purchases\nHere are the terms of our prediction: With a logistic regression, predict True/False if a visitor has purchased a product or will purchase a product in the near future. The time_hour feature attempts to track customer activity. It is number of hours between earliest and latest events.\n\n\n\nproduct_count\naddtocart\nview\ntime_hour\n\n\n\n\ninteger\ninteger\ninteger\nfloat\n\n\n\nTable:  Customer features used for basic logistic regression. \nData was appropriately sorted, split, and standardized before training the logistic regression predictor.\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n0.87\n77.48\n1.32\n1.02\n\n\n\nTable:  Feature coeffs contributing to odds for purchase \n\n\n\n0.928 acc\n\npredicted yes\n\n\n\n\n\n95923\n7118\n\n\nactual yes\n1122\n11077\n\n\n\nTable:  Train confusion matrix \n\n\n\n0.925 acc\n\npredicted yes\n\n\n\n\n\n23974\n1786\n\n\nactual yes\n795\n7792\n\n\n\nTable:  Test confusion matrix \nPredictions are doing reasonably well for how unsophisticated this model is. Especially considering that it is predicting mainly from one feature, addtocart.\nAdding LDA Features\n\n\n\nX1\nX2\nX3\nX4\nX5\nX6\nX7\nX8\nX9\nX10\nX11\nX12\n\n\n\n\n0.91\n76.14\n1.21\n1.02\n0.11\n0.10\n0.10\n0.10\n0.10\n0.10\n0.10\n0.10\n\n\nproduct_count\naddtocart\nview\ntime_hour\n\n\n\n\n\n\n\n\n\n\n\nTable:  Feature coeffs contributing to odds for purchase \nThe added features are uniformly ineffective.\n\nFigure:  LDA Probability Histogram \nConclusion\nToday we've only had time for one cluster model paired with one regression classifier. Thankfully, there are other paths of investigation to choose from in the data science toolbelt.\n""], 'url_profile': 'https://github.com/truejimfrank', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'JavaScript', 'Updated Jan 22, 2021', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Deepikacmk', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'JavaScript', 'Updated Jan 22, 2021', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Boston House Prices Prediction\n\nTable of Contents\n\nDescription\nTopics Covered\n\n\nDescription\nBoston house prices project is a multivariable regression problem and predict the house prices. Data can be found here https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nTopics Covered\n\nData Preprocessing\nData Visualisations using Matplotlib and seaborn\nDescriptive Statistics, Correlations between features\nData Transformation\nMultivariable Regression using Scikit-Learn, Statsmodels\nP values\nChecking for Multicollinearity\nModel comparison with Baysian Information Criterion (BIC)\n\n\nBack To The Top\n'], 'url_profile': 'https://github.com/gagan1304', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'JavaScript', 'Updated Jan 22, 2021', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['LinearRegressionApp\nThis is an Interactive Linear Regression App! The structure of the app comes from one of the Udemy course projects that I had. As for the linear Regression Project, in stead of having a one-time deployment, I have decided to create a reproducible linear regression app.\nFeel free to play with the codes!\nInstallation\nThis repository is tested on Python 3.5+\nClone the repository\n$ git clone https://github.com/CJtheSloth/LinearRegressionAPP\n$ cd streamlit_LineaarRegression\nInstall the dependencies\n$ pip install pandas streamlit matplotlib\nRun the application on http://localhost:8501/\n$ streamlit run LinearRegression.py\nGeneral Overview of the Iris App\n\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/CJsGit-tech', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'JavaScript', 'Updated Jan 22, 2021', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Helsinki', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['machine learning classification\nResult\n\n\n\n\n'], 'url_profile': 'https://github.com/canfang-feng', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'JavaScript', 'Updated Jan 22, 2021', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shubham-tomar', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 28, 2020', 'JavaScript', 'Updated Jan 22, 2021', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rohitvatwani', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'C++', 'GPL-3.0 license', 'Updated Feb 13, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 16, 2021', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 5, 2020', 'Updated Feb 3, 2020', 'MATLAB', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'Shanghai, China', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': [""Regression(Machine Learning Basic)\nA ML-method-based regression programme.\n注意：\n1、这个项目暂未完成，有意者可以协助完成此项目；\n2、这个项目仅用于学习和研究使用，使用此项目得到的模型数据在未经允许的情况下不得公开发表；\n3、这个项目是在重造车轮。对于ML的应用型初学爱好者而言没有较大意义，仅供了解回归模型训练的工作原理和程序实现；\n4、对于应用型ML开发爱好者，完全可以使用Keras等高度封装的ML代码进行神经网络训练。\n对于回归模型训练，主要包含以下三个步骤：\n1、初始化模型（函数）\n2、选择一个可以评估模型优劣的评估函数（Goodness Function）\n3、通过迭代找到最优的模型（函数）\n（以二维样本空间的一次函数回归为例）\nStep 1：\n\n\n初始化模型参数，这里是指一次回归函数的k和b；\n\n\n假设有 N 个训练样本，每个训练样本可以表示为(x,y0)，将所有的x带入，得到对应的所有y值；\n\n\n这里，所有的y值是指通过初始的模型参数初始化的模型计算得到的y(x)，所有的y0是训练样本的实际值。\n\n\nStep 2：\n\n\n计算上述所有y和y0的方差（Loss Function）\n\n\n一般为了防止过拟合（Overfitting），会让Loss Function正则化（Regularize），具体方式为：设定一个正则化参数λ，再该例中用λ乘以k的平方；\n\n\n注意，λ值并非越大越好，过大的λ会导致欠拟合（Underfitting）。λ一般选择10-1000之间的数字；\n\n\nStep 3：\n\n\n在该例中，寻找最优模型的方式是寻找最优的k和b，即通过Loss Function计算得到的Loss最小；\n\n\n我们定义Loss Function为L，设定一个学习率η，从初始的k和b开始，令L分别对k和b求偏导，得到新的k'和b'，其值分别为k（b）减η乘以L对k（b）的偏导；\n\n\n上述步骤被称为梯度下降（Gradient Descent）。重复梯度下降的步骤直到k和b不再变化或在极其有限的范围内小幅度震荡，此时得到的k和b为最优解；\n\n\n注意，上一步骤中的最优解严格意义上为局部最优解（Local Optimal），然而在类似较为简单的案例中，局部最优解即为全局最优解；\n\n\n更高效的梯度下降方案还有很多，在此不多举例；\n\n\n至此，模型训练初步完成。如果发现训练结果错误率较高或训练效率较低，可以选择修改正则化参数或学习率。\n\n\n以上是最基础的的模型训练和调试方案。\n\n\n""], 'url_profile': 'https://github.com/henryyantq', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'C++', 'GPL-3.0 license', 'Updated Feb 13, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 16, 2021', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 5, 2020', 'Updated Feb 3, 2020', 'MATLAB', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '317 contributions\n        in the last year', 'description': ['Oblique and straight model trees for regression\nRegression trees approximate a function by partitioning the input space and applying an independent regression model on each of the subdivisions.\nA straight tree divides the input space in two at each node by selecting a single feature and splitting at a threshold.\nAn oblique tree divides the input space in two at each node with an hyperplane. This way, the partitioning can accommodate multivariate frontiers and greatly reduce the number of nodes needed.\nExamples\n2D inputs\nBelow is an example for which the training set is provided by the composition of two sigmoid functions. Both model trees tries to reproduce the reference function with a collection of linear regressions. The straight tree can easily model the sigmoid function that is aligned with a single feature but struggle to approximate the multivariate sigmoid function. Conversely, the oblique tree can find a more relevant segmentation which allows a shallower tree to achieve a better match.\n\n\n\nReference function\nStraight tree\nOblique tree\n\n\n\n\n\n\n\n\n\n\n(max_depth=5)\n(max_depth=3)\n\n\n\nBoth resulting trees can be compared using the method Model_tree.diagram():\n\n\nDiagram of the resulting straight tree\n\n\n\nDiagram of the resulting oblique tree\n\nThe source file of the example above is example_2D.py.\n1D inputs\nHere is another example with a single-dimension input space and piecewise second-order regressions given by a model tree, compared to monolithic polynomial regressions of degree 4 or 8:\n\nThe source file of the example above is example_1D.py.\nInstallation\nTo install the module in the user space, run in a terminal:\n$ pip install . --user\nUsage\nThe first argument when initializing a Model_tree object is a boolean specifying if the tree is oblique.\nIf true, the argument split_search let us specify the algorithm to be used in order to find the optimal split at each node. The function to pass has to take an array of data and the cost function as arguments and return the set of parameters for the best hyperplane found, as suggested by the default function CMA_search defined in model_tree.py.\nBy default, split_search=\'cma-es\', resulting in the use of the Covariance Matrix Adaptation Evolution Strategy implemented here.\nTo search for the best split, the cost function to minimize is defined by:\n\n\n\nwhere Li is the loss obtained by the regression model applied to the ni samples of the side i of the current split, while w and b are the coefficients of the hyperplane defining this split. Therefore, the second term of the cost function favors the maximization of the margin between the samples and the hyperplane. The coefficient , by default 0.01, can be changed via the argument margin_coef.\nOtherwise, if the tree is straight, an exhaustive research is performed for every cut possible along every feature. This process can be speed up with the argument search_grid which let us specify an interval number of possible thresholds to skip at a first scan pass. The second scan pass then looks for all thresholds in the best interval found previously.\nWether the tree is oblique or not, the model to use at each terminal node is specified by the argument model. It can be one of the strings \'linear\' or \'polynomial\' in order to use linear or polynomial regressions with L1 and L2 regularizations implemented with the scikit-learn library. Otherwise, a class describing any particular model can be provided as long as it implements the same methods as the classes Linear_regression and Polynomial_regression defined in model_tree.py.\nAny argument can be passed to the model by keyword. For example, to declare an oblique polynomial model tree of degree 3 with a L2 regularization of 0.01 and a maximum depth of 5 nodes, you would write:\ntree = Model_tree( oblique=True, max_depth=5, model=\'polynomial\', degree=3, L2=0.01 )\nThe tree is then trained with the method fit( X, y, verbose=1 ), where X is the array of training data with the shape ( n samples, n features ) and y the targets.\nIf X is a single dimension array, it is considered to be n samples with a single feature.\nIf verbose = 0, no output is given.\nIf verbose = 1, only the outputs from the tree building are displayed.\nIf verbose = 2, outputs from the split search are provided as well.\nThe predictions are obtained with the method predict( X, return_node_id=False ), where X is an array of input data with the shape ( n samples, n features ).\nIf X is a single dimension array, it is considered to be one sample of n features.\nIf return_node_id is True, return a tuple containing first the list of predictions and second a list of the corresponding terminal node numbers.\nArguments for the initialization of a Model_tree object\noblique=False: each split is made according to a scalar threshold on a single feature (straight tree).\noblique=True: splits are defined by a linear combination of all features (hyperplane in the feature space).\nmax_depth: maximum depth of the tree.\nnode_min_samples: minimum number of training samples to be used to constitute and train a terminal node.\nmodel: regression model to use at each terminal node.\nloss_tol: tolerance on the model loss at which to stop splitting.\nsplit_search: function used for searching the oblique split coefficients. If split_search=\'cma-es\', Covariance Matrix Adaptation Evolution Strategy is used (oblique trees only).\nmargin_coef: coefficient used to incite the maximization of the margin with the training samples (oblique trees only).\nsearch_grid: interval number of possible thresholds to skip for the first scan pass of a grid search (straight trees only).\n**model_options: options to be passed to the regression model of each terminal node.\nSaving and loading trained trees\nThe parameters of the current tree can be saved in a YAML file with the method save_tree_params( \'filename\' ) and then restored with the method load_tree_params( \'filename\' ).\nThe parameters can also be extracted as a dictionary with get_tree_params() and set manually with set_tree_params( a_dictionary_of_parameters ).\nUse a trained model tree in a C++ program\nA linear or polynomial model tree trained in python can then be imported in a C++ program thanks to the class templates Linear_model_tree and Polynomial_model_tree defined in cpp/model_tree.cc. To import the parameters from a YAML file, you will need the library yaml-cpp which can be installed on Debian-based systems with:\n$ sudo apt-get install libyaml-cpp-dev\nBelow is an example showing how to use the model tree in C++:\n#include ""model_tree.hh""\n\nint main()\n{\n\t// Declaration of an oblique model tree with second-order regressions:\n\tPolynomial_model_tree<double> tree( ""./mt_params.yaml"", true, 2 );\n\n\t// Declaration of the input (two-dimensional case):\n\tstd::vector<double> input = { -2, 3 };\n\t// Integer used to store the number of the terminal node (optional):\n\tint node_id;\n\n\t// Inference of the model tree output:\n\tdouble output = tree.predict( input, node_id );\n\n\tprintf( ""Output value: %g -- Corresponding node: %d\\n"", output, node_id );\n}\n\n'], 'url_profile': 'https://github.com/Bouty92', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'C++', 'GPL-3.0 license', 'Updated Feb 13, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 16, 2021', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 5, 2020', 'Updated Feb 3, 2020', 'MATLAB', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Stepwise-regression---LASSO---Elastic-Net-regression\nUsing the crime data set uscrime.txt, build a regression model using:\n\nStepwise regression\nLasso\nElastic net\n\n'], 'url_profile': 'https://github.com/ihabselmi', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'C++', 'GPL-3.0 license', 'Updated Feb 13, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 16, 2021', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 5, 2020', 'Updated Feb 3, 2020', 'MATLAB', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'San Francisco Bay Area', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['House_Prices_Predictions_Linear_Regression_Multiple_Regression\n'], 'url_profile': 'https://github.com/chiayunchiang', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'C++', 'GPL-3.0 license', 'Updated Feb 13, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 16, 2021', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 5, 2020', 'Updated Feb 3, 2020', 'MATLAB', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Regression-Tree---Random-Forest---Logistic-Regression\nUsing the GermanCredit data set germancredit.txt from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german\n/ (description at http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29 ),\nuse logistic regression to find a good predictive model for whether credit applicants are good credit risks or not.\nShow your model (factors used and their coefficients), the software output, and the quality of fit.\n'], 'url_profile': 'https://github.com/ihabselmi', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'C++', 'GPL-3.0 license', 'Updated Feb 13, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 16, 2021', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 5, 2020', 'Updated Feb 3, 2020', 'MATLAB', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['regressionmodels\n'], 'url_profile': 'https://github.com/Tocheee', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'C++', 'GPL-3.0 license', 'Updated Feb 13, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 16, 2021', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 5, 2020', 'Updated Feb 3, 2020', 'MATLAB', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['Linear-Regression\nA basic implementation for training a Linear Regression model\nThe octave implementation trains a Linear Regression model using the given dataset.\nThe dataset can be found here: https://www.kaggle.com/mohansacharya/graduate-admissions\nYou can train a model on your own dataset as well.\n'], 'url_profile': 'https://github.com/anishk74', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'C++', 'GPL-3.0 license', 'Updated Feb 13, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 16, 2021', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 5, 2020', 'Updated Feb 3, 2020', 'MATLAB', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'Munich', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Simple Linear Regression Model\nA Simple Linear Regression Model for exchange rate data. This repository includes Bilkent University Introduction to Machine Learning course assignment. The problem definition is given below:\nThe dataset for this question consists of monthly average of USD/TRY exchange rates from November 2014 to August 2019. In addition,\n̈\nyou can also find CPI (Consumer Price Index, or TUFE in Turkish) and unemployment rates of each month\nfrom November 2014 to August 2019. Please read question instructions carefully. Provide proper title, axis labels and legend for each plot requested. You will lose points for unformatted plots. You are not allowed to use any machine learning libraries for any question in this section. The dataset is constructed using the information provided by Central Bank of the Republic of Turkey and Turkish Statistical Institute.\n'], 'url_profile': 'https://github.com/yusufsamsum', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'C++', 'GPL-3.0 license', 'Updated Feb 13, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 16, 2021', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 5, 2020', 'Updated Feb 3, 2020', 'MATLAB', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['nCoV\nnCoV model\n'], 'url_profile': 'https://github.com/odinokov', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'C++', 'GPL-3.0 license', 'Updated Feb 13, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 16, 2021', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 5, 2020', 'Updated Feb 3, 2020', 'MATLAB', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}"
"{'location': 'Zurich, Switzerland', 'stats_list': [], 'contributions': '287 contributions\n        in the last year', 'description': ['Machine-Learning-\nA variety of scripts for regression and classification tasks\n'], 'url_profile': 'https://github.com/Grometton', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'SAS', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'Munich', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Simple Linear Regression Model\nA Simple Linear Regression Model for exchange rate data. This repository includes Bilkent University Introduction to Machine Learning course assignment. The problem definition is given below:\nThe dataset for this question consists of monthly average of USD/TRY exchange rates from November 2014 to August 2019. In addition,\n̈\nyou can also find CPI (Consumer Price Index, or TUFE in Turkish) and unemployment rates of each month\nfrom November 2014 to August 2019. Please read question instructions carefully. Provide proper title, axis labels and legend for each plot requested. You will lose points for unformatted plots. You are not allowed to use any machine learning libraries for any question in this section. The dataset is constructed using the information provided by Central Bank of the Republic of Turkey and Turkish Statistical Institute.\n'], 'url_profile': 'https://github.com/yusufsamsum', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'SAS', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['nCoV\nnCoV model\n'], 'url_profile': 'https://github.com/odinokov', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'SAS', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '318 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jiayilee97', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'SAS', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Mastering-SAS-Programming-Videos\nMaster SAS for Data Analysis, Data Management and Regression.\n\nThis is the code repository for Mastering SAS Programming[Video], published by Packt. It contains all the supporting project files necessary to work through the video course from start to finish.\nAbout the Video Course\nSAS is one of the most popular applications for data analysis and is used widely in enterprises across various sectors such as finance, and healthcare. It holds the highest job market share of all analytics tools available today. If you have a basic SAS programming background and want to get into Data Analysis by mastering SAS and taking your skills to a different level, then this is the right course for you!\nThis course will help you become proficient in using SAS to handle data, build models, and analyze data so you can gain powerful insights quickly and easily. You will start with a quick refresher of basic SAS and then explore advanced statistical concepts (such as clustering and linear/logistic regression), decision trees, and time series analysis in-depth in SAS. You will also master SAS macros, PROC SQL procedures, and advanced SAS procedures so you can use SQL queries to manage and manipulate your data efficiently.\nBy the end of this video, you will be an expert in SAS programming and will have taken your skills to the next level. Also, you will be able to handle and manage your data-related problems very easily in SAS and build statistical models.\nWhat You Will Learn\n\n\nGain an understanding of basic SAS, its GUI, libraries, and importing/exporting data\nUse PROC SQL to fetch data from multiple tables by using JOINS\nExplore the power of SAS macros and optimize code and for faster data manipulation\nGain an in-depth understanding of statistics in SAS to summarize and graph data and also draw inferences from it\nPerform cluster analysis on a dataset by applying K-means clustering to understand the purpose of unsupervised Machine Learning in SAS\nBuild a decision-tree model to visually and explicitly represent decisions and decision making\nApply linear or logistic regression in SAS and build models for predictive analytics on sales data\nBuild a time series model to see patterns in data\nUse ARIMA modeling to build a model and make forecasts\n\nInstructions and Navigation\nAssumed Knowledge\nIf you are a professional statistician or analyst with exposure to the basics of SAS programming and want to be an expert, this course is for you. Experienced SAS professionals who want to harness data science with SAS will also benefit from this course.\nTechnical Requirements\nThis course has the following requirements:\nBasic understanding of SAS programming \nRelated Products\n\n\nSAS in Practice [Video]\n\n\nSAS Programming in 7 Steps [Video]\n\n\nSAS for Finance\n\n\n'], 'url_profile': 'https://github.com/PacktPublishing', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'SAS', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': [""SmallProjects\n\nAn illustration of linear regression use package 'sklearn'\nA Matlab program for K-means clustering\n\n""], 'url_profile': 'https://github.com/zhanjunpeng', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'SAS', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GitNickProgramming', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'SAS', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'Moscow, Russia', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AndreyGorbatov1', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'SAS', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['""# MultivariateLinearRegression""\n'], 'url_profile': 'https://github.com/anishajain22', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'SAS', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Classify Wisconsin Brest Cancer Data Set [1] using logistic regression\n[1] http://mlr.cs.umass.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names\n'], 'url_profile': 'https://github.com/kanarupank', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'SAS', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Feb 20, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-regression-Assumptions\nThis project verify all the assumptions of linear regression\n'], 'url_profile': 'https://github.com/akshatamath', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jul 23, 2020', 'TeX', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 9, 2020', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Supervised Machine Learning Regression\nMachine Learning, Machine Learning - Supervised Machine Learning Regression\n'], 'url_profile': 'https://github.com/BA-Software-Development-Ed', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jul 23, 2020', 'TeX', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 9, 2020', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Random Forest Regression model for beginners in Machine Learning with Python\n'], 'url_profile': 'https://github.com/karakusfurkan', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jul 23, 2020', 'TeX', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 9, 2020', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['MultiLinearRegression-on-Boston-hoousing\nMultiple linear Regression on Boston Housing dataset of sklearn\n'], 'url_profile': 'https://github.com/ArishSayyed', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jul 23, 2020', 'TeX', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 9, 2020', 'Updated Feb 4, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '326 contributions\n        in the last year', 'description': [""Interpolators\nRegression, Interpolation and classifications with GSL, TensorFlow and scikit-learn\nThese are a collection of playgrounds for ML and som comparison on different implementations.\nThe names of the folders specify what the codes are written in. The 'Jupiter' folder contains\nan analysis of Jupiter's atmosphere.\n""], 'url_profile': 'https://github.com/talismanbrandi', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jul 23, 2020', 'TeX', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 9, 2020', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['conference_poster_semi-supervised-regression\nThis is a conference Latex poster on semi-supervised regression.\nThis repository is sorce code for the conference Latex poster. Following is a preview of the poster.\n\n'], 'url_profile': 'https://github.com/zero-cooper', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jul 23, 2020', 'TeX', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 9, 2020', 'Updated Feb 4, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bhumika26', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jul 23, 2020', 'TeX', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 9, 2020', 'Updated Feb 4, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['-image-classification-using-logistic-regression\nUsing Logistic Regression-Gradient Descent model for image recognition\n'], 'url_profile': 'https://github.com/csvinay', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jul 23, 2020', 'TeX', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 9, 2020', 'Updated Feb 4, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '477 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nimatullo', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jul 23, 2020', 'TeX', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 9, 2020', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prerakchintalwar', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jul 23, 2020', 'TeX', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 9, 2020', 'Updated Feb 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prerakchintalwar', 'info_list': ['Updated Feb 4, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'JavaScript', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Aug 18, 2020', 'Updated Jun 26, 2020', 'MATLAB', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Jul 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Gradient-Boosting-Machine\nImplementation of Gradient Boosting for regression and classification\nAlgorithms were implemented according to a paper by Jerome H. Friedman ""Greedy Function Approximation: A Gradient Boosting Machine"" https://statweb.stanford.edu/~jhf/ftp/trebst.pdf\nAlgorithms are:\n\nLS_Boost, optimizing MSE\nL2_TreeBoost, optimizing binary cross-entropy\n\ngradient_boosting.py contains models and module_test.ipynb tests it on a toy dataset.\n'], 'url_profile': 'https://github.com/astraszab', 'info_list': ['Updated Feb 4, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'JavaScript', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Aug 18, 2020', 'Updated Jun 26, 2020', 'MATLAB', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Jul 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Multiple-regression\nto predict medical insurance cost using multiple regression\nThis file has accuracy of 75%\n'], 'url_profile': 'https://github.com/Umama-Alim', 'info_list': ['Updated Feb 4, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'JavaScript', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Aug 18, 2020', 'Updated Jun 26, 2020', 'MATLAB', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Jul 5, 2020']}","{'location': 'Christchurch, New Zealand', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/OstyPowers', 'info_list': ['Updated Feb 4, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'JavaScript', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Aug 18, 2020', 'Updated Jun 26, 2020', 'MATLAB', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Jul 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Credit-Card-Fraud-Detection_Data\nCredit Card Fraud Detection Data_Logistic Regression and Keras\n'], 'url_profile': 'https://github.com/Zeinab-Haroon', 'info_list': ['Updated Feb 4, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'JavaScript', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Aug 18, 2020', 'Updated Jun 26, 2020', 'MATLAB', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Jul 5, 2020']}","{'location': 'Mumbai ', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShubhamSalvi', 'info_list': ['Updated Feb 4, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'JavaScript', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Aug 18, 2020', 'Updated Jun 26, 2020', 'MATLAB', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Jul 5, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Regression Data Modeling using R and Python\nPredicted transfer value of soccer players by creating regression models\nPre-processed the data and Created histograms, boxplots, etc to understand the dataset.\nUsed feature selection to build a backward selection model, forward selection model, etc.\nPerformed Multicollinearity analysis, second-order analysis, Interaction term analysis Residual analysis.\nUsed Cross-validation to determine model accuracy and finally found out the most effective independent variables to determine transfer value.\n'], 'url_profile': 'https://github.com/kaushikbandaru26', 'info_list': ['Updated Feb 4, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'JavaScript', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Aug 18, 2020', 'Updated Jun 26, 2020', 'MATLAB', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Jul 5, 2020']}","{'location': 'Mars', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Logistic-Regression-Regularization\nLogistic Regression and Regularization ex2 from ML course\ncoding in Logistic Regression and Regularization with gradient decent to fit a data set (clasisfication problem)\nusing skeleton code, I added in Logistic Regression and Regularization this was done as part of an assignment that I had been set by done in Octave https://www.coursera.org/learn/machine-learning/\nFeb 2020\n'], 'url_profile': 'https://github.com/biddls', 'info_list': ['Updated Feb 4, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'JavaScript', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Aug 18, 2020', 'Updated Jun 26, 2020', 'MATLAB', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Jul 5, 2020']}","{'location': 'Pittsburgh, PA', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Module 1 Final Project\nData and Objective\nFor this project, we will be working with the King County House Sales dataset. The dataset can be found in the file ""kc_house_data.csv"", in this repo.\nThe description of the column names can be found in the column_names.md file in this repository.\nWe answer 3 questions in this project. 1) Using multivariate ridge regression to predict house prices. 2) Based on your house condition and zipcode what is the best time to sell? 3) Recommend a list of houses based on parameters you choose.\nFiles\nThis project has two notebooks. One is the student.ipynb which has all the analysis and the content of the project in it. The second is the Auxillary.ipynb which has helper functions. The following are the helper functions in Auxillary.ipynb and their functions\nAuxillary.ipynb\nwithnfeatures() - Creates a ridge regression model starting from the most important feature according to the correlation matrix and adding less important features as we go along. Helpful for visualization of importance of features\ncattonum_lin() converting categorical data into numerical one and ""linearizing"" it, i.e. label encoding the categorical labels so that the data falls on a straight line\nfix_skewed() log transforms skewed data for regression\nplot_corr() plots a correlation matrix of given columns\nImages Folder\nUsed for storing images created during analysis\npresentation.pdf\nPDF of non-technical presentation\nBlog\nhttps://aghalsasi-datascience.blogspot.com/2020/01/king-county-housing-beginner-analysis.html\n'], 'url_profile': 'https://github.com/aghalsas', 'info_list': ['Updated Feb 4, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'JavaScript', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Aug 18, 2020', 'Updated Jun 26, 2020', 'MATLAB', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Jul 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Support Vector Regression in Python for Machine Learning Beginners\n'], 'url_profile': 'https://github.com/karakusfurkan', 'info_list': ['Updated Feb 4, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'JavaScript', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Aug 18, 2020', 'Updated Jun 26, 2020', 'MATLAB', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Jul 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['logistic-regression-from-scratch\nLogistic Regression from scratch. Linear classifier. Probabilistic discriminative model.\n'], 'url_profile': 'https://github.com/aymanterra', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 27, 2020', 'R', 'Updated Apr 22, 2020', '2', 'Go', 'Updated Feb 7, 2020']}","{'location': 'Grenoble,France', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NeuralNetworks\nThis repository is for my personal projects in deep learning where I work with  Neural networks for classification, regression, and natural language processing\n'], 'url_profile': 'https://github.com/ProsperJulius', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 27, 2020', 'R', 'Updated Apr 22, 2020', '2', 'Go', 'Updated Feb 7, 2020']}","{'location': 'Orange, CA', 'stats_list': [], 'contributions': '336 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jaliquiel', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 27, 2020', 'R', 'Updated Apr 22, 2020', '2', 'Go', 'Updated Feb 7, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['image-classification-using-logistic-regression\nUsing Logistic Regression-Gradient Descent model for image recognition\n'], 'url_profile': 'https://github.com/RishikaBhatia17', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 27, 2020', 'R', 'Updated Apr 22, 2020', '2', 'Go', 'Updated Feb 7, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['ML-Concrete-compressive-Strength\nThree Models have been deployed on the dataset:\n\nRandom Forest\nNN\nRegression\n\n'], 'url_profile': 'https://github.com/Deepank6', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 27, 2020', 'R', 'Updated Apr 22, 2020', '2', 'Go', 'Updated Feb 7, 2020']}","{'location': 'Portsmouth,England', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/temiloluwadavid', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 27, 2020', 'R', 'Updated Apr 22, 2020', '2', 'Go', 'Updated Feb 7, 2020']}","{'location': 'Philadelphia, PA', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['MultiLinearRegression\nThe goal of this project is to write a custom class that can run multivariate linear regression.\nIn order to assess the efficacy of my custom class, I will compare the performance score to that of sklearn.linear_model.LinearRegression.\nReferences\nMath behind gradient descent:\n\nhttps://www.coursera.org/learn/machine-learning/supplement/aEN5G/gradient-descent-for-multiple-variables\nhttps://mccormickml.com/2014/03/04/gradient-descent-derivation/\n\n'], 'url_profile': 'https://github.com/bpennisi', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 27, 2020', 'R', 'Updated Apr 22, 2020', '2', 'Go', 'Updated Feb 7, 2020']}","{'location': 'Stockholm', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['regression-analysis\nCoursework in Regression Analysis, mostly code for projects in R\n'], 'url_profile': 'https://github.com/lindahlf', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 27, 2020', 'R', 'Updated Apr 22, 2020', '2', 'Go', 'Updated Feb 7, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mikiwieczorek', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 27, 2020', 'R', 'Updated Apr 22, 2020', '2', 'Go', 'Updated Feb 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['benchmarks\nCollection of benchmarks that caused problematic regressions in Go development\n'], 'url_profile': 'https://github.com/dr2chase', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Jun 27, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 27, 2020', 'R', 'Updated Apr 22, 2020', '2', 'Go', 'Updated Feb 7, 2020']}"
"{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '327 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Suneet-M', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'Syracuse , New York', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Linear-regression\nPlotting a graph using linear regression. The data set for predicting CO2 emission is further trained and tested by splitting the dataset in two parts.\n'], 'url_profile': 'https://github.com/SwaraDeshpande', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['logistic_regression\n\nUnderstanding log odds: https://www.youtube.com/watch?v=8nm0G-1uJzA\nPart1 : https://www.youtube.com/watch?v=vN5cNN2-HWE, Part2 : https://www.youtube.com/watch?v=BfKanl1aSG0, Part3 : https://www.youtube.com/watch?v=xxFYro8QuXA\n\n'], 'url_profile': 'https://github.com/abmitra84', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dinfree', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Linear_Regression\nThis project is performing Linear Regression on a Housing and Boston Dataset whose data is in an excel worksheet using python and Jupyter Notebook\n'], 'url_profile': 'https://github.com/Claudine-wangari', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SumitBepari', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Regression-Problems\n'], 'url_profile': 'https://github.com/Angshuman1997', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Regression-model\nUsingcrimedatafromhttp://www.statsci.org/data/general/uscrime.txt (fileuscrime.txt, description at\nhttp://www.statsci.org/data/general/uscrime.html ), use regression to predict the observed crime rate in a city with the following data:\nM = 14.0\nSo = 0\nEd = 10.0\nPo1 = 12.0 Po2 = 15.5\nLF = 0.640\nM.F = 94.0 Pop = 150\nNW = 1.1\nU1 = 0.120\nU2 = 3.6 Wealth = 3200\nIneq = 20.1 Prob = 0.04 Time = 39.0\nShow your model (factors used and their coefficients), the software output, and the quality of fit.\n'], 'url_profile': 'https://github.com/ihabselmi', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Logistic_Regression\nUsing Python to perform Logistic Regression on test and train data following steps given on http://hamelg.blogspot.co.ke/2015/11/python-for-data-analysis-part-28.html\n'], 'url_profile': 'https://github.com/Claudine-wangari', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'chennai', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vinodsml', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}"
"{'location': 'Pune, India', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Abhishek28031995', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'greater noida', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Polynomial-regression\n'], 'url_profile': 'https://github.com/kkapasiya', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/diksha2423', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/faaraan1997', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Prachiiee', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Tokyo ', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sarinash', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AishwaryaLande', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': [""Linear_Regression\nImplementation of Linear Regression in python and application on coursera's introduction to machine learning dataset.\n""], 'url_profile': 'https://github.com/AbdenourCh', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Alex1005594', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}","{'location': 'Dublin', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['logistic_regression\n'], 'url_profile': 'https://github.com/karljmurphy', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Apr 19, 2020']}"
"{'location': 'Bengaluru - India', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': ['Regression-Notes\n\nLinear Regression:\n\n\nRelationship between observers and outcome – straight line.\nRoot mean squared error and Gradient decent is used for best fit line.\nPolynomial regression (not a straight line) is used when relationship between observers and outcome is at nth degree, and method is useful when large number of observations are in curve series.\n\n\n\nR- Squared: R2 tells how line fits the data better than mean.  R2 is percentage of variation explained by the relationship between variables (2 or more in multi regression).  If R2 is 80% then we can say that regression lines are 80% less variation than mean. Also, that relationship between variables waits 80% of variation. And hence, 80% of data is explained by the variable’s relationship.\n\n\nL1 and L2 Regulation:\n\n\n'], 'url_profile': 'https://github.com/RutvijBhutaiya', 'info_list': ['Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['DataScience\nI am able to make the data can tell the story that what are the hidden insights from the data, how useful are them in terms of business value\n'], 'url_profile': 'https://github.com/RaviRyuk', 'info_list': ['Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020']}","{'location': '广州', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['logistic-regression\n常规操作练习\nL1和L2正则化\n损失函数\n评分卡建立\n'], 'url_profile': 'https://github.com/tuqing3', 'info_list': ['Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zamigaliyev', 'info_list': ['Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NDPd-Regression\n'], 'url_profile': 'https://github.com/janaavijit', 'info_list': ['Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['multiple_regression\n'], 'url_profile': 'https://github.com/KALPA2345', 'info_list': ['Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020']}","{'location': 'Islamabad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic-Regression\nIn statistics, the logistic model is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1 and the sum adding to one.\nLogistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\nI have a real world dataset of Weather forcast of Szeged which is third largest city of Hungary.\nThis dataset contain the category values of predicted variable (dependent variable), so I have decided to apply Logistic Regression on it.\nAccording to my working style, I have applied the preprocessing on the dataset which includes;\n\nData Cleaning\nFeatures Representation\nFeature Extraction\nFeature Selection.\n\nDuring preprocessing, I have also used seaborn library and matplotlib library to draw graphs.\nGraphs are very important to gain knowledge about the problem.\n""A picture speaks louder than words""\nFinding corelation is also very important so I cannot ignore this and applied it in this problem.\nAfter finding corelation, i have assigned a threashold of 0.1 and extracted those features who\'s corelation is greater than 0.1.\nI have also detected outliers, removed them.\nThis dataset is mixture of both numerical data and categorical data. For categorical data, encoding is very necessory. So, I have used One Hot Encoding.\nTo calculate Variance Inflation Factor(VIF) of each Iindependent Variable, we need to ignore the real Dependent Variable and make each Independent Variable our target feature. For this purpose we perform Auxillary Regression.\nAfter applying regression model, it is time to check whether the model is good or not.\nI have applied the following error finding techniques:\n\nR-Square value\nRoot Mean Square Error\nMean Absolute Error\nRoot Mean Absolute Error\nTraining and Test Score\n\nall the above error finding techniques shows the goodness of the model which mean model is good.\nRegularization of the model is important, so I have used Cross Validation method, then applied the regression equation and find out a expected result.\n'], 'url_profile': 'https://github.com/buikhizarkhan', 'info_list': ['Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/manishphatak', 'info_list': ['Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shreya-totla', 'info_list': ['Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['regression-samples\n'], 'url_profile': 'https://github.com/drmikedcook', 'info_list': ['Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020']}"
"{'location': 'Xuzhou City, Jiangsu Province，China', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic-Regression\na practice for  Kaggle\n'], 'url_profile': 'https://github.com/lyd3', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Performing basic Linear Regression on set of data\n'], 'url_profile': 'https://github.com/nitinkmr333', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shreya-totla', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['regression-samples\n'], 'url_profile': 'https://github.com/drmikedcook', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mriduldhawan', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Eric-Liu16', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'Islamabad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression\nIn statistics, linear regression is a linear approach to modeling the relationship between a scalar response and one or more explanatory variables. The case of one explanatory variable is called simple linear regression.\nI have develop a linear regression model with the dataset of real estate which includes 8 attributes.\nI have overview the dataset with some of the techniques and graph-plots and find out the Corelaton between every independent and dependent variable and choose the highest corelation holding attribute.\nThen I find out the slop and intercept values which will be using in the linear regression equation and then find out the equation.\nI have also determine the R-Square value which was very good.\n'], 'url_profile': 'https://github.com/buikhizarkhan', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['LogisticRegression\nLogistic regression ML technique used on banking data to predict whether users will take term deposit or not.\nResources\nhttps://upxacademy.learnyst.com/ (Project materials)\nhttps://upxacademy.com/\n'], 'url_profile': 'https://github.com/acharyakarthikeya85', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['LinearRegression\nMachine Learning on Boston Housing Price prediction using Supervised Linear Regression.\nResources\nhttps://upxacademy.learnyst.com/ (Project materials)\nhttps://upxacademy.com/\n'], 'url_profile': 'https://github.com/acharyakarthikeya85', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'China', 'stats_list': [], 'contributions': '180 contributions\n        in the last year', 'description': ['Linear-Regression\nRY first machine learning task\n'], 'url_profile': 'https://github.com/nmrenyi', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'Updated Jun 15, 2020', 'Python', 'Updated Feb 6, 2020']}"
"{'location': 'Xuzhou City, Jiangsu Province，China', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic-Regression\na practice for  Kaggle\n'], 'url_profile': 'https://github.com/lyd3', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'MATLAB', 'Updated Oct 15, 2020', 'Updated Feb 7, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Performing basic Linear Regression on set of data\n'], 'url_profile': 'https://github.com/nitinkmr333', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'MATLAB', 'Updated Oct 15, 2020', 'Updated Feb 7, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['Logistics-Regression\n'], 'url_profile': 'https://github.com/shantanudn', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'MATLAB', 'Updated Oct 15, 2020', 'Updated Feb 7, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Regression-problem\n'], 'url_profile': 'https://github.com/Muzamil001', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'MATLAB', 'Updated Oct 15, 2020', 'Updated Feb 7, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dhara11-p', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'MATLAB', 'Updated Oct 15, 2020', 'Updated Feb 7, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mosquitoCat', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'MATLAB', 'Updated Oct 15, 2020', 'Updated Feb 7, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Linear Regression for single feature\nThese codes are programmed in octave/matlab language\ncomputeCost.m is the program to compute the cost function\nplotData.m is the program to convert the given data into the graphical figure\ngraientDecent.m is the program to check if our cost function is minimizing in all the steps\nex1.m is the combined program of all the programs mentioned above\nex1data1.txt is the data in the form of a text file\n'], 'url_profile': 'https://github.com/VishalGohelishere', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'MATLAB', 'Updated Oct 15, 2020', 'Updated Feb 7, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'Cambridge, Massachusetts from Kigali, Rwanda', 'stats_list': [], 'contributions': '1,023 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/heyaudace', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'MATLAB', 'Updated Oct 15, 2020', 'Updated Feb 7, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Debashish96', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'MATLAB', 'Updated Oct 15, 2020', 'Updated Feb 7, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/guptavdivya', 'info_list': ['Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'MATLAB', 'Updated Oct 15, 2020', 'Updated Feb 7, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/preethyann', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/neha0587', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shauryas123', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'London, ON, Canada', 'stats_list': [], 'contributions': '123 contributions\n        in the last year', 'description': ['Linear and Polynomial Regression \n\n\n\n\n\n\n\nTrain Error = 0.924\nTest Error = 1.144\n'], 'url_profile': 'https://github.com/nikiibayat', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Buffalo,NY', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['LogisticRegression\nLogisticRegression using Breast Cancer Data\n'], 'url_profile': 'https://github.com/pavithrun1997', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alphapri', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Malaysia', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sitisalwani', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Russia', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/qztr', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Predict-Housing-Prices\nProject for predicting housing prices in the Kaggle Housing Prices: Advanced Regression Techniques competition (https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\nThis is a feature changing branch, specifically for creating new features (ithink)\n'], 'url_profile': 'https://github.com/ihr0008', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Alwar, (RAJ.) INDIA', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['Regression and Classification code\nPython codes for\nLinear regression with gradient descent,\nCollaborative filtering,\nLinear regression with random data,\nNaive Bayes on Movie review etc\nNote: (For Naive Bayes on movie review) This zip file having all the data for both positive and negative reviews (both 700-700) and this python file too. The path in the code file for reading positive and negative data files is given with assumption that both folders are in current directory. So please keep all three ( pos_folder, neg_folder, python_file) at one place.\n'], 'url_profile': 'https://github.com/Heartbeat143', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 27, 2020']}"
"{'location': 'Irvine', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': [""Superbowl-Score-Prediction\nThis model was a boosted forest ensemble of my ensemble model via Poisson Regression and my partner's Poisson Regression model\n""], 'url_profile': 'https://github.com/kelandrin', 'info_list': ['Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Apr 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Graduate_Admission\nA mini project to analyse the Graduate admission dataset and to train a regression model(Simple linear regression) and a classification algorithm(Decision tree).\n'], 'url_profile': 'https://github.com/NitinCJ', 'info_list': ['Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Apr 12, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Student-Grade-Predictor-\nLinear regressor project in python\n'], 'url_profile': 'https://github.com/abubakaar', 'info_list': ['Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Apr 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['\nPlaying with statistics in dataframes and testing out predictions with Linear Regression\nv.1 2020/2/7\nArmani Chien\n'], 'url_profile': 'https://github.com/ArmaniChi', 'info_list': ['Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Apr 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Regression-With-Neural-Network\n'], 'url_profile': 'https://github.com/AryamanShaan', 'info_list': ['Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Apr 12, 2020']}","{'location': 'Saudi Arabia', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['walmart_sales_regression\n""\nYou are provided with historical sales data for 45 Walmart stores located in different regions. Each store contains a number of departments, and you are tasked with predicting the department-wide sales for each store.\nIn addition, Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete/ideal historical data.\n""\nTaken from Kaggle Competetion\nNotebook Outline\n\nImport\nData Main Aspects\nPreprocessing\nEDA\nFeature Selection\nEDA - Extention\nDimensionality Reduction Trial\nModeling and Model-Driven Optimization\nQuality Metric Summary\n\nFuture Improvements\n\nData-Driven model optimization by focusing on the outliers\nUsing a recurrent neural network\nTrying some Auto Regressive and moving average models\n\nClick on walmart_sales_regression.html HTML file, Click Download, in the redirected page, Save the page in your desktop, open it from your PC so that it renders HTML and CSS markdowns in the browser. And Enjoy!\n'], 'url_profile': 'https://github.com/YousefGh', 'info_list': ['Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Apr 12, 2020']}","{'location': 'Mexico', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['This Jupyter provides visual feedback on a Linear Regression model fitting the data\nusing batch Gradient Descent.\n\n'], 'url_profile': 'https://github.com/Seriozha', 'info_list': ['Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Apr 12, 2020']}","{'location': 'Japan', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic-Regression-with-R-\n'], 'url_profile': 'https://github.com/deafrhino', 'info_list': ['Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Apr 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Regression Analysis Mooc\nhttps://www.icourse163.org/course/LIXIN-1207403805\n'], 'url_profile': 'https://github.com/LekaiSong', 'info_list': ['Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Apr 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '103 contributions\n        in the last year', 'description': ['Linear_Regression_GUI\nA Simple GUI application to predict the linear relationship in the given Sample data\nPrerequiste\nPython3\nInterface\n\nAbout\nThis is linear Regression algorithm which helps in predicting the value of a dependent varibale by using the relationship given in the\nsample data.\nIt also uses Tkinter to create an ""Python GUI Script"" to accept the data from the user and predict the values with the help of the Linear_regression_Algorithm\nUser can also use multivariable parameters to predict the value.\nThis model has been tested to predict the Weather condition for a sample data that uses various parameters.\n'], 'url_profile': 'https://github.com/varunsly', 'info_list': ['Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 23, 2020', '1', 'Python', 'Updated Apr 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': [""Housing-Prices-Advanced-Regression\nIn order to understand the data, look at each variable and try to understand their meaning and relevance to this problem.\nVariable - Variable name. Type - Identification of the variables' type. There are two possible values for this field: 'numerical' or 'categorical'. By 'numerical' we mean variables for which the values are numbers, and by 'categorical' we mean variables for which the values are categories. Segment - Identification of the variables' segment. We can define three possible segments: building, space or location. When we say 'building', we mean a variable that relates to the physical characteristics of the building (e.g. 'OverallQual'). When we say 'space', we mean a variable that reports space properties of the house (e.g. 'TotalBsmtSF'). Finally, when we say a 'location', we mean a variable that gives information about the place where the house is located (e.g. 'Neighborhood'). Expectation - Our expectation about the variable influence in 'SalePrice'. We can use a categorical scale with 'High', 'Medium' and 'Low' as possible values. Conclusion - Our conclusions about the importance of the variable, after we give a quick look at the data. We can keep with the same categorical scale as in 'Expectation'. Comments - Any general comments that occured to us. While 'Type' and 'Segment' is just for possible future reference, the column 'Expectation' is important because it will help us develop a 'sixth sense'. To fill this column, we should read the description of all the variables and, one by one, ask ourselves:\nDo we think about this variable when we are buying a house? (e.g. When we think about the house of our dreams, do we care about its 'Masonry veneer type'?). If so, how important would this variable be? (e.g. What is the impact of having 'Excellent' material on the exterior instead of 'Poor'? And of having 'Excellent' instead of 'Good'?). Is this information already described in any other variable? (e.g. If 'LandContour' gives the flatness of the property, do we really need to know the 'LandSlope'?). After this daunting exercise, we can filter the spreadsheet and look carefully to the variables with 'High' 'Expectation'. Then, we can rush into some scatter plots between those variables and 'SalePrice', filling in the 'Conclusion' column which is just the correction of our expectations.\n""], 'url_profile': 'https://github.com/ramziash', 'info_list': ['Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Dec 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Kotlin', 'Apache-2.0 license', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '402 contributions\n        in the last year', 'description': [""Training a semantic classifier with Reddit data\nThe following code pulls the 1000 newest posts from each designated subreddit,\nembeds each post using Tf-idf, and predicts the forum using logistic regression.\nReproducible\nAll data and parameters used to create the model are loaded into mongoDB.\nThe entire pipeline is also saves as a pickle file. TFIDF_SVD_LRCV.pkl\nThe pipeline:\n\n# Text vectorizer\nvector = TfidfVectorizer(min_df=2,\n                         max_features=3200,\n                         preprocessor=preprocessor)\n\n\n# Principal component analyzer\ndecomp = TruncatedSVD(n_iter=8,\n                      random_state=0,\n                      n_components=1000)\n\n\n# Regression model\nmodel = LogisticRegressionCV(Cs=8, \n                             cv=5, \n                             n_jobs=-1,\n                             max_iter=200,\n                             solver='saga',\n                             random_state=0)\n\n\nResults\nClassification report\n                 precision    recall  f1-score   support\n\nmachinelearning       0.72      0.87      0.79        78\n    datascience       0.74      0.70      0.72        64\n     conspiracy       0.88      0.85      0.86        41\n      astrology       0.94      0.92      0.93        53\n      chemistry       0.78      0.79      0.79        72\n       starwars       0.79      0.87      0.82        38\n        biology       0.58      0.62      0.60        50\n        physics       1.00      0.42      0.59        12\n         gaming       0.96      0.69      0.80        32\n          vegan       0.88      0.79      0.84        29\n\n       accuracy                           0.78       469\n      macro avg       0.83      0.75      0.77       469\n   weighted avg       0.80      0.78      0.78       469\n\n\nConfusion Matrix\n\n""], 'url_profile': 'https://github.com/Schlam', 'info_list': ['Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Dec 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Kotlin', 'Apache-2.0 license', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Pune, India', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Linear-Regression-Using-Tensorflow\nThis is a Linear Regression model built using tensorlfow.\n'], 'url_profile': 'https://github.com/rohit-thakur12', 'info_list': ['Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Dec 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Kotlin', 'Apache-2.0 license', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/itsdevbrat', 'info_list': ['Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Dec 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Kotlin', 'Apache-2.0 license', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': [""Python-Regression-Analysis\nThis repository contains different code and examples for performing securities regression analysis with Python. All work is done in a Jupyter notebook, using several plugins such as: pandas, plotlib and numpy. All financial data is retreived from Yahoo Finance's API\n""], 'url_profile': 'https://github.com/DimiGod', 'info_list': ['Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Dec 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Kotlin', 'Apache-2.0 license', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SNathJr', 'info_list': ['Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Dec 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Kotlin', 'Apache-2.0 license', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['ML_LinearRegression\n'], 'url_profile': 'https://github.com/MingjuiLee', 'info_list': ['Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Dec 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Kotlin', 'Apache-2.0 license', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['1.数据集\n使用的数据集为SST2，该数据集收集了一些影评中的句子，每个句子都有标注，好评被标注为 1，差评标注为 0。\n2. 目标\n创建一个分类器，它的输入是一句话（即类似于数据集中的句子），并输出一个 1（表示这句话体现出了积极的情感）或是 0（表示这句话体现出了消极的情感）。整个模型由两个子模型组成：\n\nDistilBERT 先对句子进行处理，并将它提取到的信息传给下个模型。DistilBERT 是 BERT 的缩小版，它是由 HuggingFace 的一个团队开发并开源的。它是一种更轻量级并且运行速度更快的模型，同时基本达到了 BERT 原有的性能。\n另外一个模型，是 scikit learn 中的 Logistic 回归模型，它会接收 DistilBERT 处理后的结果，并将句子分类为积极或消极（0 或 1）。\n\n尽管整个模型包含了两个子模型，但是我们只需要训练 logistic 回归模型。至于 DistillBERT，我们会直接使用已经在英文上预训练好的模型。然而，这个模型不需要被训练，也不需要微调，就可以进行句子分类。BERT 训练时的一般目标让我们已经有一定的句子分类能力了，尤其是 BERT 对于第一个位置的输出（也就是与 [CLS] 对应的输出）。我觉得这主要得益于 BERT 的第二个训练目标——次句预测（Next sentence classification）。该目标似乎使得它第一个位置的输出封装了句子级的信息。Transformers 库给我们提供了 DistilBERT 的一种实现以及预训练好的模型。\n3. 环境\n\ntorch\ntransformers(pytorch transformer)\npython 3.x\n\n4. 流程\n(1) 将句子分词（包括subword）并进行编码（索引）和padding\n(2) 将输入向量传递给 DistilBert 之后的工作方式就跟在 BERT 中一样，每个输入的词都会得到一个由 768 个浮点数组成的输出向量。\n\n由于这是个句子分类任务，我们只关心第一个向量（与 [CLS] 对应的向量）。该向量就是我们输入给 logistic 回归模型的向量。\n\n5.参考\n\nhttps://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb\nBert快速入门指南\n\n'], 'url_profile': 'https://github.com/orangerfun', 'info_list': ['Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Dec 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Kotlin', 'Apache-2.0 license', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Tokyo', 'stats_list': [], 'contributions': '807 contributions\n        in the last year', 'description': ['VisualRegressionTestingSample\nDroidKaigi2020: Androidでもビジュアルリグレッションテストをはじめよう のサンプルアプリです\nスライド\nhttps://speakerdeck.com/keidroid/droidkaigi2020-androiddemobiziyuaruriguretusiyontesutowohazimeyou\n概要\n以下の環境で動作確認しています。\n\nmacOS 10.15\nAndroid Studio 3.5\nデバイス: 3.7inch WVGA(480x800) API26\n\nreg-suitのコマンドは実行できる前提となっており、別途行う必要があります。\n\nhttps://github.com/reg-viz/reg-suit#getting-started\n\nライブラリ\n以下ライブラリを使用しています\nSpoon\nCopyright 2013 Square, Inc.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\ngradle-spoon-plugin\nCopyright (C) 2017 Jared Burrows\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nFalcon\nCopyright 2015 Josef Raska\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nFacebook Screenshot Tests for Android\nCopyright Facebook, Inc. and its affiliates\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nFirebase Snapshotter\nCopyright Google Inc.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nreg-suit\nThe MIT License (MIT)\n\nCopyright 2017 Quramy\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n'], 'url_profile': 'https://github.com/keidroid', 'info_list': ['Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Dec 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Kotlin', 'Apache-2.0 license', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Bangalore, India ', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Linear_regression_3D\n'], 'url_profile': 'https://github.com/akshita2k', 'info_list': ['Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Dec 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Kotlin', 'Apache-2.0 license', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}"
"{'location': 'Greater Noida', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Simple-Linear-Regression-model\nIn this model, we will predict the salaries of employees.\nHere, we have a data set of employees. the data set contains the data of 30 employees. which is split into a 1/3 ratio of training and testing data i.e 20 data will be used in training set and 10 data will be in the test set.\nfor a better understanding of the model, we have also visualized the training and the test graph separately in the code.\n'], 'url_profile': 'https://github.com/poojatripathi06', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': [""Credit_Card_Spent\nBusiness Problem:\nOne of the global banks would like to understand what factors driving credit card spend are. The bank want use these insights to calculate credit limit. In order to solve the problem, the bank conducted survey of 5000 customers and collected data.\nThe objective of this case study is to understand what's driving the total spend (Primary Card + Secondary card). Given the factors, predict credit limit for the new applicants\n""], 'url_profile': 'https://github.com/manishani99', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AishwaryaLande', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'Chennai,Tamilnadu', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/VinodhkumarBaskaran', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ramziash', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Cai-Yuan', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'greater noida', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['linear-regression-multivariate\n'], 'url_profile': 'https://github.com/kkapasiya', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nayanika-g', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'Islamabad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Multiple-Linear-Regression\nIn statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\nMultiple Linear Regression (MLR) method helps in establishing correlation between the independent and dependent variables.\nI have develop a multiple linear regression on a real world dataset which is about car pricing prediction.\nFirstly, I have find out the statistics of every atrribute and the null values(missing value) in the dataset.\nThen, find out the relation between every attritubes by using seaborn graphs and matplotlib.\nThis dataset is mixture of both numerical data and categorical data. For categorical data, encoding is very necessory.\nSo, I have used One Hot Encoding.\nFinding corelation is also very important so I cannot ignore this and applied it in this problem.\nAfter finding corelation, i have assigned a threashold of 0.3 such that I find out the attributes who's corelation is greater than 0.3.\nThen, I have develop a multiple linear regression with only those attributes which are highly corelated with the predicted variable.\nWe get a multiple regression model. Now I find out the following to find out the goodness of my model;\n\nR-Square value\nRoot Mean Square Error\nMean Absolute Err\nThe Model Score\n\nall the above error finding techniques shows the goodness of the model which mean model is good.\n""], 'url_profile': 'https://github.com/buikhizarkhan', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ntokozomfene', 'info_list': ['Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020']}"
"{'location': 'United Kingdom', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/stevenlsenior', 'info_list': ['Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'Santiago, Chile', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juanmadepalacios', 'info_list': ['Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amarquezmazzeo', 'info_list': ['Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\n'], 'url_profile': 'https://github.com/shrikantagrawal', 'info_list': ['Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'Nigeria', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danieljohnson18', 'info_list': ['Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'Paris 17', 'stats_list': [], 'contributions': '352 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ppichier', 'info_list': ['Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'Jakarta', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Logistic-Regression-Binary-Classification\nExample of Supervised Machine Learning (Classification) using Logistic Regression in Python\nFor more details please visit:\nResearch Gate: https://www.researchgate.net/publication/338735685_IMPLEMENTING_EXTREME_GRADIENT_BOOSTING_XGBOOST_CLASSIFIER_TO_IMPROVE_CUSTOMER_CHURN_PREDICTION\n'], 'url_profile': 'https://github.com/iqbalhanif', 'info_list': ['Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Bigmart_sales_analyses_regression\n'], 'url_profile': 'https://github.com/rajat2341', 'info_list': ['Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jRayze', 'info_list': ['Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': ['Explored and created new features from the data. I analyze customer data from an E-commerce company to evaluate if focusing on mobile app experience or their website would be better for the yearly amount spent by customers. I used Pandas for data analysis and Matplotlib/seaborn for data visualization. I fitted a linear regression model and used it to predict the yearly amount spent by customers. Finally, I Evaluated the predictions and residuals.\nUdemy course - Python for Data Science and Machine Learning Bootcamp\n'], 'url_profile': 'https://github.com/nrepesh', 'info_list': ['Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020']}"
"{'location': 'Pune', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\n'], 'url_profile': 'https://github.com/shrikantagrawal', 'info_list': ['Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'Nigeria', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danieljohnson18', 'info_list': ['Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'Paris 17', 'stats_list': [], 'contributions': '352 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ppichier', 'info_list': ['Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'Jakarta', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Logistic-Regression-Binary-Classification\nExample of Supervised Machine Learning (Classification) using Logistic Regression in Python\nFor more details please visit:\nResearch Gate: https://www.researchgate.net/publication/338735685_IMPLEMENTING_EXTREME_GRADIENT_BOOSTING_XGBOOST_CLASSIFIER_TO_IMPROVE_CUSTOMER_CHURN_PREDICTION\n'], 'url_profile': 'https://github.com/iqbalhanif', 'info_list': ['Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SNathJr', 'info_list': ['Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'Santiago, Chile', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juanmadepalacios', 'info_list': ['Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['logistic_regression_project\nNote: We\'ve adapted this Mini Project from Lab 5 in the CS109 course. Please feel free to check out the original lab, both for more exercises, as well as solutions.\nWe turn our attention to classification. Classification tries to predict, which of a small set of classes, an observation belongs to. Mathematically, the aim is to find $y$, a label based on knowing a feature vector $\\x$. For instance, consider predicting gender from seeing a person\'s face, something we do fairly well as humans. To have a machine do this well, we would typically feed the machine a bunch of images of people which have been labelled ""male"" or ""female"" (the training set), and have it learn the gender of the person in the image from the labels and the features used to determine gender. Then, given a new photo, the trained algorithm returns us the gender of the person in the photo.\nThere are different ways of making classifications. One idea is shown schematically in the image below, where we find a line that divides ""things"" of two different types in a 2-dimensional feature space. The classification show in the figure below is an example of a maximum-margin classifier where construct a decision boundary that is far as possible away from both classes of points. The fact that a line can be drawn to separate the two classes makes the problem linearly separable. Support Vector Machines (SVM) are an example of a maximum-margin classifier.\n'], 'url_profile': 'https://github.com/kumarravindra', 'info_list': ['Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'chennai', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vinodsml', 'info_list': ['Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Bengaluru-House-Pricing-Dataset\nRequired data is provided in repository.\nBasic linear regression model implemented for regression and rmse is considered as evaluation metric\n'], 'url_profile': 'https://github.com/Sasidhart97', 'info_list': ['Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\n'], 'url_profile': 'https://github.com/shantanudn', 'info_list': ['Python', 'Updated Feb 8, 2020', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020']}"
"{'location': 'Bangalore, India ', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['diabetes_linear_regression\n'], 'url_profile': 'https://github.com/akshita2k', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020', '1', 'R', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Java', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['logistic_regression_project\nNote: We\'ve adapted this Mini Project from Lab 5 in the CS109 course. Please feel free to check out the original lab, both for more exercises, as well as solutions.\nWe turn our attention to classification. Classification tries to predict, which of a small set of classes, an observation belongs to. Mathematically, the aim is to find $y$, a label based on knowing a feature vector $\\x$. For instance, consider predicting gender from seeing a person\'s face, something we do fairly well as humans. To have a machine do this well, we would typically feed the machine a bunch of images of people which have been labelled ""male"" or ""female"" (the training set), and have it learn the gender of the person in the image from the labels and the features used to determine gender. Then, given a new photo, the trained algorithm returns us the gender of the person in the photo.\nThere are different ways of making classifications. One idea is shown schematically in the image below, where we find a line that divides ""things"" of two different types in a 2-dimensional feature space. The classification show in the figure below is an example of a maximum-margin classifier where construct a decision boundary that is far as possible away from both classes of points. The fact that a line can be drawn to separate the two classes makes the problem linearly separable. Support Vector Machines (SVM) are an example of a maximum-margin classifier.\n'], 'url_profile': 'https://github.com/kumarravindra', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020', '1', 'R', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Java', 'Updated Feb 3, 2020']}","{'location': 'chennai', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vinodsml', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020', '1', 'R', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Java', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Bengaluru-House-Pricing-Dataset\nRequired data is provided in repository.\nBasic linear regression model implemented for regression and rmse is considered as evaluation metric\n'], 'url_profile': 'https://github.com/Sasidhart97', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020', '1', 'R', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Java', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\n'], 'url_profile': 'https://github.com/shantanudn', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020', '1', 'R', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Java', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/andruuhurst', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020', '1', 'R', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Java', 'Updated Feb 3, 2020']}","{'location': 'Beijing, China', 'stats_list': [], 'contributions': '368 contributions\n        in the last year', 'description': ['logistic-regression-variable-selection\nUse logistic regression to predict default events. We first discretize the continuous variables, then encode them with one hot code. Finally, we use RFE in the sklearn library to select the variables and get 11 significant features. Finally, the precision, recall and F1 score of 0.75 + are obtained on the test set.\n\n\n\n\n使用logistic回归预测违约事件。我们首先对连续性变量进行离散化，之后进行one-hot编码，最后利用sklearn库中的RFE进行变量选择，得到结果显著的11个特征。最后在测试集上获得了0.75+的precision,recall and F1-score.\n'], 'url_profile': 'https://github.com/stxupengyu', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020', '1', 'R', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Java', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SNathJr', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020', '1', 'R', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Java', 'Updated Feb 3, 2020']}","{'location': 'Santiago, Chile', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juanmadepalacios', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020', '1', 'R', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Java', 'Updated Feb 3, 2020']}","{'location': 'Chandigarh', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prince3103', 'info_list': ['1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Feb 8, 2020', '1', 'R', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 4, 2020', 'Java', 'Updated Feb 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Radial Basis Function Regression\n'], 'url_profile': 'https://github.com/AbChatt', 'info_list': ['Python', 'MIT license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pulengmoru', 'info_list': ['Python', 'MIT license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '516 contributions\n        in the last year', 'description': [' STAT-420-Assignments \nINSTALLATION:\n\nDownload the R program and open in R studio and the related dataset\nRun the program in R studio\n\n'], 'url_profile': 'https://github.com/bsathyamur', 'info_list': ['Python', 'MIT license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['sum_regularized_regression\n'], 'url_profile': 'https://github.com/ai-se', 'info_list': ['Python', 'MIT license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear_Regression_Using_Fuelconsumption\n'], 'url_profile': 'https://github.com/shamhussain', 'info_list': ['Python', 'MIT license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KEC79', 'info_list': ['Python', 'MIT license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/companion-cube-lead', 'info_list': ['Python', 'MIT license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020']}","{'location': 'College Park, MD, US', 'stats_list': [], 'contributions': '379 contributions\n        in the last year', 'description': ['deadCellsRegression_pythonic\npythonic version of deadCellsRegression. Uses keras instead of TMVA.\n'], 'url_profile': 'https://github.com/chrispap95', 'info_list': ['Python', 'MIT license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020']}","{'location': 'Dallas', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/swapnilbansal1', 'info_list': ['Python', 'MIT license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['Multi-Linear-Regression\n'], 'url_profile': 'https://github.com/shantanudn', 'info_list': ['Python', 'MIT license', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Image on Scalar Regression Program\nShiny app for viewing brain image data and fitting image on scalar regression/ Jian Kang\nUse this program for free online at Image on Scalar Regression Program\nDeveloped at the University of Michigan Department of Biostatistics\n'], 'url_profile': 'https://github.com/umich-biostatistics', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Python', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['sum_regularized_regression\n'], 'url_profile': 'https://github.com/ai-se', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Python', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear_Regression_Using_Fuelconsumption\n'], 'url_profile': 'https://github.com/shamhussain', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Python', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KEC79', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Python', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/companion-cube-lead', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Python', 'Updated Mar 9, 2020']}","{'location': 'College Park, MD, US', 'stats_list': [], 'contributions': '379 contributions\n        in the last year', 'description': ['deadCellsRegression_pythonic\npythonic version of deadCellsRegression. Uses keras instead of TMVA.\n'], 'url_profile': 'https://github.com/chrispap95', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Python', 'Updated Mar 9, 2020']}","{'location': 'Dallas', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/swapnilbansal1', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Python', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['Multi-Linear-Regression\n'], 'url_profile': 'https://github.com/shantanudn', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Python', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jatin-658', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Python', 'Updated Mar 9, 2020']}","{'location': 'Kyiv', 'stats_list': [], 'contributions': '358 contributions\n        in the last year', 'description': ['ft_linear_regression\n42 ML project\nsetting up\npip3 install --user virtualenv\npython3 -m virtualenv -p $(which python3) env/\nsource env/bin/activate\npip install -r requirements.txt\n\n'], 'url_profile': 'https://github.com/akorunska', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'JavaScript', 'Updated Feb 9, 2020', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'Python', 'Updated Mar 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Image on Scalar Regression Program\nShiny app for viewing brain image data and fitting image on scalar regression/ Jian Kang\nUse this program for free online at Image on Scalar Regression Program\nDeveloped at the University of Michigan Department of Biostatistics\n'], 'url_profile': 'https://github.com/umich-biostatistics', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'MIT license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': [""Ames, Iowa Housing Analysis\nOverview\nProblem Statement\n\nWhat are the factors that affect Ames’ housing price in the past?\nHow accurately could we predict the trend of housing market using existing information and past data?\n\nPrimary Objective\nUse existing data and sales price to train a predictive model and predict sales price for houses with unknown sales price\nDataset\ndataset overview\nThe dataset includes housing sales data in Ames Iowa from 2006 to 2010. The data has 81 columns and 2051 rows in train dataset and 80 columns and 878 rows in test dataset\ndataset source\nThe raw data is from Kaggle competition. The dataset could be accessed at https://www.kaggle.com/c/dsi-us-10-project-2-regression-challenge/data\nEDA\nData Cleaning\n\nDrop all the columns with null values more than 1000\nFill in columns with null values based on columns' categories or relative scales\nClear outliers with exceptionally large values such as 'Ground Living Area'\n\nVisualization\n\nCheck overall distribution of Sales Price\nVisualize distribution of sales price by Neighborhood based on neighborhood category, year built, and overall quality\n\n\nPackage used: Matplotlib.pyplot, seaborn\n\nModel Selection\nModel 1: use only numeric variables\nMethod used:\n\nLinear Regression\nLasso\nRidge\nRidge CV\nElastic Net\n\nConclusion:\nIn Model 1, I selected all numeric variable from the dataset into the model. For each method, I split data into 70% training and 30% testing, and set cross validation to 5 times per method. By measuring RMSE, R2 score for both training and testing, I select the method with lowest RMSE and highest R2 testing with smallest difference between R2 training and testing. I conclude that Ridge CV has the most accurate prediction.\nModel 2: use high correlated numeric variables and add interaction and sqaured terms\nMethod used:\n\nLinear Regression\nLasso\nRidge\nRidge CV\n\nConclusions:\nIn Model 2, I select only high correlated numeric variables from model 1. I used a heatmap to visualize all variables' correlation with sales price, and selected variables with absolute values of correlation higher than 0.5. Through the correlation analysis, I found out some predictor variables have high correlations with each other, so I added their interaction terms to the model. Also, by plotting residuls from model 1, I found out that the plot shows a pattern of a parabola shape. As a result, I added all included variables' squared value into the model as well. Ridge CV continues to be the best fit method. However, compare to Model 1, the accuracy of prediction in Model 2 merely increased very little.\nModel 3: Model 2 improvement with high correlated scaled classification variables\nMethod used:\n\nLinear Regression\nRidge CV\nPipeline gridsearch\n\nConclusion:\nIn Model 3, I turned ordinal classification variables to numeirc scale, which turned them into numeric variables. Then I conducted correlation analysis on all numeric variables again and select high correlated variables with sale price. (correlation > 0.5), repeat the process for model 2. Ridge CV and Pipeline gridsearch both have the highest R2 score and low RMSE while the difference between R2 training and R2 testing is small. The prediction result also improved significantly.\nModel 4: Model 3 improvement with scaled Neighborhood and log sale price\nMethod used:\n\nLinear Regression\nRidge CV\n\nConclusion:\nIn Model 4, I added log value on sales price to make its distribution closer to fit normality. Then based on the median sale price, I scaled all Neighborhood from the highest median sales price to lowest median sales price. The prediction result was a bit worse than model 3 pipeline, so I might included some low correlated variables in the model.\nSummary\n\nModel 3 with pipeline method has the highest accuracy of predicting sales price\nThe prediction model tends to undervalue extremely expensive houses\n\nSlide Link\nhttps://drive.google.com/file/d/1PUx15Evh_m84E4RLdQKiFr5rn-FC4JBl/view?usp=sharing\names_housing_price\n""], 'url_profile': 'https://github.com/kevinlu1996', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'MIT license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated May 6, 2020']}","{'location': 'Goderich, Ontario', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Linear and Polynomial Regression\nA small project where I perform linear and polynomial regression on a dataset of linear data.\nGetting Started\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\nInstalling\n$ pip3 install virtual env\n$ virtualenv -p python3 env\n$ source env/bin/activate\n$ pip3 install requirements.txt\n$ python3 regression.ipynb\n\nIf you want to use jupyter notebook\n$ jupyter notebook\n\nThank you\n'], 'url_profile': 'https://github.com/jacobprouse', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'MIT license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Polynomial Regression Python\n'], 'url_profile': 'https://github.com/AbChatt', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'MIT license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated May 6, 2020']}","{'location': 'Nigeria', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danieljohnson18', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'MIT license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated May 6, 2020']}","{'location': 'Nigeria', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danieljohnson18', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'MIT license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/deathwing573', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'MIT license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['1_SpuriousRegression\nThis is an R program that seeks to reporduce the set p-value, using data generated at random\n'], 'url_profile': 'https://github.com/Eikonomics', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'MIT license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated May 6, 2020']}","{'location': 'Johannesburg', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ThembaMbulwana', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'MIT license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated May 6, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cchicote', 'info_list': ['1', 'R', 'Updated May 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'MIT license', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated May 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Project-on-Logistic-Regression\nAnalysed the data of a group of people using R to predict if a person will have a heart attack or not. The dataset was taken from kaggle.com .\n'], 'url_profile': 'https://github.com/w1ndwatcher', 'info_list': ['1', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/benvneb', 'info_list': ['1', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AishwaryaLande', 'info_list': ['1', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['PySpark_Model_Regression\nWill help you get started using Apache Spark’s spark.ml Linear Regression for predicting\n'], 'url_profile': 'https://github.com/DwarakanadhKopuri', 'info_list': ['1', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '788 contributions\n        in the last year', 'description': ['logistic_regression_API\n\n\n\n'], 'url_profile': 'https://github.com/NoriKaneshige', 'info_list': ['1', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['An Introduction to Machine Learning  with Scikit-Learn.\nThis story represents an easy path for beginners to built an Regression ML model using Scikit-Learn Framework. Here,  a Home Loan Rate Calculator is built using Freddie Mac Single Family Home Loan data. Similar approach can be used for building a Regression Model using Scikit-Learn.\nTools/Software Used:\nService\u200a-\u200aJupyter Notebook.\nStorage\u200a-\u200aPC.\nLanguage\u200a-\u200aPython.\nChecklist Followed for Building an ML\xa0Model:\nTaking a look at the Big Picture.\nImporting and Selecting Valid Data.\nData Analysis and Visualization.\nData Preparation Pipeline.\nTrying Different Models and Selecting a Model.\nHyperparameter Tuning and Feature Selection and Final Pipeline.\nFinal Model Training and Evaluation.\nSolution Presentation.\nBatch Deploy and Monitor.\nPlease Visit https://medium.com/@abhilash.mohapatra25/an-introduction-to-machine-learning-with-scikit-learn-9f72812dd413 for more details.\n'], 'url_profile': 'https://github.com/abhilash499', 'info_list': ['1', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zamigaliyev', 'info_list': ['1', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression_weight_prediction\nDevelop regression model to predict the weight of a person by knowing the height and gender of the person\n'], 'url_profile': 'https://github.com/rishabht95', 'info_list': ['1', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Regularized Text Logistic Regression\nIntroduction\nTo repository is used to host the source code used in the paper Regularized Text Logistic Regression: Key Words Detection and Sentiment Classification for Online Reviews. The code contains two parts, one for Restaurant dataset and one for Hotel dataset.\nData\nThe Restaurant data is contained in the restaurant folder. For Hotel dataset, please refer to the reference in the paper.\nCode\nFor each experimentation, two files are used. The file working.R contains the high level workflow, with all the detailed implementation in the file helper.R.\n'], 'url_profile': 'https://github.com/jackliu333', 'info_list': ['1', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Bangalore, India ', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Boston_housing_polynomial_regression-\n'], 'url_profile': 'https://github.com/akshita2k', 'info_list': ['1', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020']}"
"{'location': 'Santiago, Chile', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juanmadepalacios', 'info_list': ['R', 'Updated Feb 5, 2020', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'TypeScript', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shariff94', 'info_list': ['R', 'Updated Feb 5, 2020', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'TypeScript', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['RA-Logistics-Regression\n'], 'url_profile': 'https://github.com/chaipi-chaya', 'info_list': ['R', 'Updated Feb 5, 2020', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'TypeScript', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['PCA---Regression-Model\nUsing the same crime data set uscrime.txt, apply Principal Component Analysis and then create a regression model\nusing the first few principal components. Specify your new model in terms of the original variables\n(not the principal components).\n'], 'url_profile': 'https://github.com/ihabselmi', 'info_list': ['R', 'Updated Feb 5, 2020', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'TypeScript', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tuanhm11299', 'info_list': ['R', 'Updated Feb 5, 2020', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'TypeScript', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'Amsterdam, NL', 'stats_list': [], 'contributions': '3,435 contributions\n        in the last year', 'description': ['Issue\nReproduces issue documented here: https://github.com/nestjs/nest/issues/4017.\nRunning\n\nRun NATS server locally\n\n$ docker run -p 4222:4222 -ti nats:latest\n\nRun Nest server\n\n$ npm start\n\n\nRun test script\n\n$ npm run reproduce-issue\n\n'], 'url_profile': 'https://github.com/rhlsthrm', 'info_list': ['R', 'Updated Feb 5, 2020', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'TypeScript', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Prediction using Linear Regression in 10 lines of code.\nProject 1 Udacity nano degree\n\nIf one is familiar with the concept of linear regreation. The libraries used are pandas, dask, numpy, sklearn and matplotlib. This exercise will walk you through a clean and simple implementation of a linear regreation to predict some values for next year of stackoverflow survey answers. All data is included as .csv files, as well as the code step by step, with annotations and comments.\n'], 'url_profile': 'https://github.com/maycy92', 'info_list': ['R', 'Updated Feb 5, 2020', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'TypeScript', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['IGB-Greyhound-Position-Prediction\nThe objective of this project is to predict the position of the greyhound based on the other fields and estimated time taken by the greyhound to finish the race.\nThe workflow of the project is as follows:\n\nAcquire data:\nMake http requests to the website, to download the html data\nData Cleaning:\nData is found under various tags in the HTML file. So the data is extracted from the file and cleaned. This finally gives us the data we can work on.\nGreyhound Position Prediction:\nWe are predicting the top 3 Greyhound in the race using other columns. This problem is a classification problem. The prediction is done using ANN and CNN. The ANN was giving an accuracy of 87% and CNN 76%.\nEstimating the race finish time of Greyhound:\nWe are estimating the race finish time of Greyhound using ANN.\n\nTechnologies Used\nLibraries:\n\npandas\nurllib3\nKeras\nNumpy\nsklearn\nmatplotlib.pyplot\n\nRequirements:\n\nPython 3.6\n\nData Source:\nhttps://www.igb.ie/\n'], 'url_profile': 'https://github.com/krshubham12', 'info_list': ['R', 'Updated Feb 5, 2020', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'TypeScript', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'Sri Lanka', 'stats_list': [], 'contributions': '421 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sanushi-Salgado', 'info_list': ['R', 'Updated Feb 5, 2020', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'TypeScript', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['GradeEstimator\nsimply feed in a .csv file following the layout of the supplied one and change the separating value (line 10).\n'], 'url_profile': 'https://github.com/KeenanShropshire', 'info_list': ['R', 'Updated Feb 5, 2020', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'TypeScript', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', '2', 'Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 6, 2020']}"
"{'location': 'Tempe', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Handwritten Digit Recognition using TensorFlow and MNIST dataset\nTo execute the code, in ubuntu, you can simply run following commands:\nsudo apt-get install python3\nsudo apt-get install python3-pip\nsudo apt-get install python3-tk\npip3 install -r requirements.txt\nTo use TensorBoard in Linux:\nIn the directory where you run the python script, type following command:\ntensorboard --logdir .\nThen you can view the graph in following website:\nlocalhost:6006\n'], 'url_profile': 'https://github.com/arunimamookherjee', 'info_list': ['Python', 'Updated Feb 7, 2020', 'HTML', 'Updated May 10, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['STAT425_APD\nRepository for STAT 425 at the University of Illinois - Applied Regression and Design\n'], 'url_profile': 'https://github.com/joshjanda1', 'info_list': ['Python', 'Updated Feb 7, 2020', 'HTML', 'Updated May 10, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Google-ML-course\nSome code examples of linear regressor, logistic regressor, DNN, multiclass DNN, embedding .\nUsing tensorflow API.\nAll of those are derived from Google Machine Learning Course .\nTo see the whole course, please refer to https://developers.google.com/machine-learning/ .\n'], 'url_profile': 'https://github.com/2015211289', 'info_list': ['Python', 'Updated Feb 7, 2020', 'HTML', 'Updated May 10, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sneha-jain-n', 'info_list': ['Python', 'Updated Feb 7, 2020', 'HTML', 'Updated May 10, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'San Antonio, TX, USA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pdimitroulis', 'info_list': ['Python', 'Updated Feb 7, 2020', 'HTML', 'Updated May 10, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['TAS\nThis repository contains the source code and data to regenerate the transformation activity scores using ordered logistic regression model with random effects from our paper about FGFR variants.\nSupplementary Code\n\nI. Takeda-Nakamura et al., ""Title TBD"", in submission.\n\nContents\n\nOverview\nSystem Requirements\nInstructions for Use\n\nOverview\nWe developed an ordered logistic regression model with random effects for evakuating analyzing the the transformation activity of FGFR variants. We performed four experimental batches of the focus formation assay and also four experimental batches of the low-serum cell proliferation assay. All the experimental results of both assays were scored in four classes. Due to differences in culture conditions, batch-to-batch variations in the obtained scores were observed. We defined transformation activity scores (TASs) by integrating all the experimental results and set for four classes commensurate with the scores of each assay. To calculate TASs with batch-to-batch adjustment, we utilized the ordered logistic regression model with random effects.\nSystem Requirements\nHardware Requirements\nThe scripts requires only a standard computer with enough RAM to support the operations defined by a user. For minimal performance, this will be a computer with about 8 GB of RAM. For optimal performance, we recommend a computer with the following specs:\nRAM: 32+ GB\nCPU: 4+ cores, 4.0+ GHz/core\nThe runtimes below are generated using a computer with the recommended specs (32 GB RAM, 4 cores@4.2 GHz) and internet of speed 100 Mbps.\nSoftware Requirements\nR and Rstan\nThis script files runs on R and Rstan for Windows, Mac, or Linux, which requires the R version 3.4.0 or later. For install instructions, visit the RStan Getting Started website at\nhttps://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\nPackage dependencies\nUsers should install the following packages prior to use the scripts, from an R terminal:\ninstall.packages(c(\'reshape2\', \'rstan\', \'tidyverse\', \'shinystan\'))\n\nwhich will install in about 5 minutes on a recommended machine.\nPackage Versions\nThis  with all packages in their latest versions as they appear on CRAN on April 13, 2019. Users can check CRAN snapshot for details. The versions of software are, specifically:\n""rstan version: 2.18.2""\n""tidyverse version: 1.2.1""\n""reshape2 version: 1.4.3""\n""shinystan version: 2.5.0""\n\nIf you are having an issue that you believe to be tied to software versioning issues, please drop us an Issue.\nInstructions for Use\nPlease put all the files in the same directory, and set the working directory appropriately.\nReproducibility\nAll the code and data are available in TAS.Rmd and Data.csv to reproduce TASs in our paper. Each Bayesian inference takes about 10-20 minutes on a recommended machine.\nInference results will be saved as FGFR-FFA_score.tsv.\nTAS.pdf generated from TAS.Rmd includes all scripts with information.\n├── README.md\n├── TAS.Rmd\n├── Dada.csv\n└── FGFR-FFA_score.tsv\n\n0 directories, 4 files\n\n'], 'url_profile': 'https://github.com/ikegami-tky', 'info_list': ['Python', 'Updated Feb 7, 2020', 'HTML', 'Updated May 10, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Junfeizhang2010', 'info_list': ['Python', 'Updated Feb 7, 2020', 'HTML', 'Updated May 10, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['boston-housing\nPredicting housing prices from boston house datasets using linear regression model\n'], 'url_profile': 'https://github.com/pprerna27', 'info_list': ['Python', 'Updated Feb 7, 2020', 'HTML', 'Updated May 10, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'Sydney', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mkhalai', 'info_list': ['Python', 'Updated Feb 7, 2020', 'HTML', 'Updated May 10, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'Brazil', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['edxlinear\nScripts used  on edx module 7 of data science professional certificate - linear regression\n'], 'url_profile': 'https://github.com/wmoreiraa', 'info_list': ['Python', 'Updated Feb 7, 2020', 'HTML', 'Updated May 10, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Junfeizhang2010', 'info_list': ['Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'C++', 'Updated Feb 18, 2020', 'JavaScript', 'MIT license', 'Updated Aug 19, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['boston-housing\nPredicting housing prices from boston house datasets using linear regression model\n'], 'url_profile': 'https://github.com/pprerna27', 'info_list': ['Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'C++', 'Updated Feb 18, 2020', 'JavaScript', 'MIT license', 'Updated Aug 19, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 5, 2020']}","{'location': 'Sydney', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mkhalai', 'info_list': ['Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'C++', 'Updated Feb 18, 2020', 'JavaScript', 'MIT license', 'Updated Aug 19, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 5, 2020']}","{'location': 'Brazil', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['edxlinear\nScripts used  on edx module 7 of data science professional certificate - linear regression\n'], 'url_profile': 'https://github.com/wmoreiraa', 'info_list': ['Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'C++', 'Updated Feb 18, 2020', 'JavaScript', 'MIT license', 'Updated Aug 19, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 5, 2020']}","{'location': 'Guwahati', 'stats_list': [], 'contributions': '368 contributions\n        in the last year', 'description': ['Predicting-Bikes-On-Rent\n   \nApplying different ML algorithms like Linear Regression, Decision tree  and Random forest on the data set bike_rental_hour.csv and predicting the number of bikes on rent according to the given time, day, month, etc.\n'], 'url_profile': 'https://github.com/pcsingh', 'info_list': ['Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'C++', 'Updated Feb 18, 2020', 'JavaScript', 'MIT license', 'Updated Aug 19, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 5, 2020']}","{'location': 'Dallas, Texas', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Machine Learning Performance Test\n'], 'url_profile': 'https://github.com/michaeldannunzio', 'info_list': ['Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'C++', 'Updated Feb 18, 2020', 'JavaScript', 'MIT license', 'Updated Aug 19, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['tensorflow-classify-cardiovascular-disease\nTraining regression models using the tensorflow.js library. The model is used to predict cardiovascular diseases.\n'], 'url_profile': 'https://github.com/anomerova', 'info_list': ['Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'C++', 'Updated Feb 18, 2020', 'JavaScript', 'MIT license', 'Updated Aug 19, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 5, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['New York AirBnB Regression Analysis and Modelling\nRPubs: https://rpubs.com/SayakChakraborty/NewYorkAirBnB_Modelling\nThe Data was collected from Kaggle. In this project we present to you exploratory data analysis, visualizations and modelling of New York Airbnb data. Airbnb, Inc.is an online marketplace for arranging or offering lodging, primarily homestays, or tourism experiences. The company does not own any of the real estate listings, nor does it host events; it acts as a broker, receiving commissions from each booking. Revenue for Airbnb comes from its guests and hosts: hosts are charged 3% of the value of the booking, while guests are charged 6%-12% per the nature of the booking. Airbnb market is quite blooming in New York city (NYC) which had more than 48,000 listings as of August-2019 calendar year.\nWe focus on New York City’s data as we wish to perform an in-depth analysis on one of the most densely populated cities in the world.\nIn this project, we also try to predict the factors that affect the pricing of the airbnbs around New York. This includes creating different kind of models, model specification, transformation, variable selection and many more.\nWe carried out the project in the following steps:\n\nData Cleaning and Preparation\nData Visualization\nModelling and Model Checking\nFinalising the Model\nPrediction using the Final Model.\n\n'], 'url_profile': 'https://github.com/sayakc027', 'info_list': ['Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'C++', 'Updated Feb 18, 2020', 'JavaScript', 'MIT license', 'Updated Aug 19, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 5, 2020']}","{'location': 'Copenhagen ', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['edX-course-Python-for-Data-Science-final-project\nThis is my final project submitted for the course ""Python for data science"" run by edX, online learning initiative of UC San Diego. Have a look at my presentation for more info!\n'], 'url_profile': 'https://github.com/Meghdad-DTU', 'info_list': ['Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'C++', 'Updated Feb 18, 2020', 'JavaScript', 'MIT license', 'Updated Aug 19, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['famTEMsel (R package version 0.1.0)\nFunctional Additive models for Treatment Effect-Modifier Selection\nAn implementation of a functional additive regression model which is uniquely modified and constrained to model nonlinear interaction effects between a categorical treatment variable and a potentially large number of functional/scalar pretreatment covariates on their effects on a scalar-valued outcome. The model generalizes functional additive models by incorporating treatment-specific components into additive effect components, however, a structural constraint is imposed on the treatment-specific components, to give a class of orthogonal main and interaction effect additive models. If primary interest is in interactions, one can avoid estimating main effects, obviating the need to specify their form and thereby avoiding the issue of model misspecification. Utilizing this orthogonality, one can effectively conduct treatment effect-modifier variable selection. The selected covariates can be used to make individualized treatment recommendations. We refer to Park, Petkova, Tarpey, and Ogden (2020) doi:10.1016/j.jspi.2019.05.008 and Park, Petkova, Tarpey, and Ogden (2020) ""Constrained functional additive models for estimating interactions between a treatment and functional covariates"" (pre-print) for detail of the method. The main function of this package is famTEMsel().\nDescription\n\nfamTEMsel - famTEMsel main function\ncv.famTEMsel - famTEMsel cross-validation function for tuning parameter selection\npredict_famTEMsel - famTEMsel prediction function\nmake_ITR_famTEMsel - make individualized treatment recommendations (ITRs) based on a famTEMsel object\nplot_famTEMsel -  plot component functions from a famTEMsel object\n\nTo run:\nTo install an R package, start by installing the ""devtools"" package (from CRAN). On R, type:\ninstall.packages(""devtools"")  # install the devtools package from CRAN\nlibrary(devtools)\n\nTo install the ""famTEMsel"" package from github, type:\ndevtools::install_github(""syhyunpark/famTEMsel"")  # install the famTEMsel package from github \nlibrary(famTEMsel)       # load the famTEMsel package to R \n\nTo see some of the example codes appearing in the ""help"" menu, type:\n?famTEMsel   \n?cv.famTEMsel\n\n'], 'url_profile': 'https://github.com/syhyunpark', 'info_list': ['Updated Feb 8, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'C++', 'Updated Feb 18, 2020', 'JavaScript', 'MIT license', 'Updated Aug 19, 2020', 'HTML', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 5, 2020']}"
"{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '366 contributions\n        in the last year', 'description': ['cat-classifier-from-scratch\nImplementation of the logistic regression algorithm for classifying pictures as containing cat or not\n'], 'url_profile': 'https://github.com/TeyimPila', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Java', 'Updated Mar 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Mar 9, 2020']}","{'location': 'Montreal, Quebec', 'stats_list': [], 'contributions': '644 contributions\n        in the last year', 'description': ['Machine-Learning\n'], 'url_profile': 'https://github.com/AymenRumi', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Java', 'Updated Mar 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Mar 9, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['python_classifier\nThis notebook uses historical dataset from previous loan applications to create a loan classifier, through different algorithms:\n\nKNN\nDecision Tree\nSVM\nLogistic Regression\n\nThe following metrics are used to report the accuracy of each classifier:\n\nJaccard index\nF1-score\nLogLoass\n\nSummary of steps involved:\n1- Feature selection and preprocessing of training dataset\n2- Split training dataset into train/test.\n3- Oversampling imbalanced class on training dataset.\n4- Creation of models with training data.\n5 -Evaluate and choose the best hyperparameters in each model\n5- Preprocessing of test dataset\n6- Predictions using test dataset in each model\nThis project is part of the course ""Machine Learning with Python"" by IBM\n'], 'url_profile': 'https://github.com/jesusmariog7', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Java', 'Updated Mar 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Mar 9, 2020']}","{'location': 'DELHI, INDIA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['CREDIT_LOAN_DEFAULT\nINTRODUCTION:\nIts a training project and this file consist of python coding that contains the prediction of CREDIT_LOAN_DEFAULTERS.\nfirst, you need to download both the CSV Files and another file DICTIONARY which contains the detail of the columns\nPACKAGE:\nnumpy;\nmatplotlib;\npandas\nseaborn;\nsklearn;\nscipy\nALGORITHM:\nLogistic Regression.\nCONCLUSION:\nPREDICTING LOAN DEFAULTERS AND VISUALIZES DATA BY APPLYING ALGORITHM WHICH GIVES THE BEST ACCURACY TO THE MODEL AT THE CORE RATE OF 74%. AT LAST YOU WILL SEE THE FACTOR INPUTS WHICH GIVES THE LOAN DEFAULTERS VARIED TO COLUMNS IN THE MODEL.\n'], 'url_profile': 'https://github.com/Shifali58', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Java', 'Updated Mar 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Adil-EL', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Java', 'Updated Mar 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['\nThis projects shows how natural language processing algorithms can be used to determine emails as spam or not spam. This is a type of supervised learning. The data is a text corpus of emails either labeled as spam or not spam. The text in the labeled emails provides the features to classify an email as spam or not spam.\n\n\nEmail consists of course of text. The differentiation between spam or not spam is made on basis of the words written in the email text. Not every word of course works as a feature. Some stop words like “is” or “there” maybe given in both emails. Only special kind of words are associated with spam or not spam. The words are the key unit used in the analysis by the algorithm. \n\n\nWords cannot be parsed in efficient way by computers. The algorithms need numbers for successful parsing. Words are therefore transformed into vectors in higher dimensional spaces. The length and direction of those vectors determines the vectors and the similarities between different vectors supports the assumption of relationships. Before a vector can be created from a word it is necessary to segment the text into words in a process of tokenization. After transformation different kind of feature selection techniques like counting or other statistics to make the association between the label and the word occurrences. The predictions are evaluated and the results interpreted.\n\n \n\n\n\n'], 'url_profile': 'https://github.com/RolfChung', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Java', 'Updated Mar 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Mar 9, 2020']}","{'location': 'Tehran', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['predict_profits_for_foodTrunk\nit is a linear regression with one variable problem solved by a single neuron\nits answer of first assignment of coursera course Machine learning Andrew Ng in python\nat first data plotted because it has only one variable it is easily possible\nthen paramethers learned with gradien descent\nat the end,prediction line plotted.\n'], 'url_profile': 'https://github.com/mahsa-meymari', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Java', 'Updated Mar 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Bulldozer-kaggle-competition-beginner-code\nAs a first project, I decided to quickly tackle an old kaggle regression competition.\n'], 'url_profile': 'https://github.com/Chongz-28', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Java', 'Updated Mar 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SongXueZhi', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Java', 'Updated Mar 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Mar 9, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': [""globe-trotting\nAbout\nThis project includes a web scraper to retrieve box scores of international basketball games from https://basketball.realgm.com/international/, and tools to train machine learning models to predict game outcomes.\nInstall\n\nClone the repository\nCreate a virtual environment (if you want) virtualenv my_env source /my_env/bin/activate\npip install -r requirements.txt\n\nTools\nboxScoreScraper.py\nRunning boxScoreScraper from the command line will create a .csv in the working directory with stats about every basketball game played between 'start-date' and 'end-date'. As of right now, these dates are hardcoded in at the top of the script.\naddColumns.py\nThis script has a positional argument, target, which takes a .csv. The script adds two new columns to the dataset, one with the difference between the home and away team final scores, and another with the total points scored. If the optional argument --out is specified, the new dataset will be saved to that file, instead of overwriting the target file.\npointTotals.py\nTrains a regression model to predict the total points scored in a given game. Takes a positional argument target which should be a .csv containing the training data.\nThis is currently incomplete. Right now, it trains on an arbitrary set of features that don't reflect real world conditions, and no thought has been put into various ways to reshape the data before training.\n""], 'url_profile': 'https://github.com/rabisnath', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Oct 16, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Java', 'Updated Mar 22, 2020', '1', 'Python', 'Apache-2.0 license', 'Updated Mar 9, 2020']}"
"{'location': 'Paris', 'stats_list': [], 'contributions': '274 contributions\n        in the last year', 'description': ['first-order-methods\nFirst order methods for regression models. Assignment for the MAP569 Machine Learning 2 course.\n'], 'url_profile': 'https://github.com/mmcenta', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'R', 'Updated Mar 3, 2021', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Machine Learning and Artificial Intelligence\nPython Project implements linear and ridge regression methods and neural network training.\n'], 'url_profile': 'https://github.com/MajdGhazaleh', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'R', 'Updated Mar 3, 2021', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['CS559---Machine-Learning-Assignment-1\nAssignment 1 for CS 559 (ML: fundamentals and application) Probability Theory, Regression, KNN and LDA\n'], 'url_profile': 'https://github.com/rpatel1291', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'R', 'Updated Mar 3, 2021', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Copenhagen ', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['Classify-song-genres-from-audio-data\nDecision tree, Random forest, SVM, Logestic regression and KNN were used to classify song genres.\n'], 'url_profile': 'https://github.com/Meghdad-DTU', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'R', 'Updated Mar 3, 2021', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Gaussian01', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'R', 'Updated Mar 3, 2021', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'South Africa', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['PolynomialReggression\nIdentifying relationships using Polynomial regression to predict, train, and plot results\n'], 'url_profile': 'https://github.com/dillon-DL', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'R', 'Updated Mar 3, 2021', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Jaipur, Rajasthan', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['Coronavirus-infectent-prediction-using-linear-regression-model\nEDA on COVID-19 dataset and Prediction of COVID-19 infectants using Linear Regression model.\nPrediction of total number of infected people in next coming 1 month.\n'], 'url_profile': 'https://github.com/balandhanka', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'R', 'Updated Mar 3, 2021', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Palo Alto, CA', 'stats_list': [], 'contributions': '436 contributions\n        in the last year', 'description': ['README\nThis provides code to reproduce all simulation experiments and figures of the paper:\n\nIgnatiadis, N., Saha, S., Sun D. L., & Muralidharan, O. (2019).  Empirical Bayes mean estimation with nonparametric errors via order statistic regression. [arXiv]\n\nThere are two files:\naurora_iso.R\nThis file provides code to reproduce Figures 1 and 7.\nTo run the code, the following R packages (available on CRAN are required):\n\nREBayes (for this, also the Mosek solver is required; we used Version 8)\nfdrtool\ndeconvolveR\ngrid\ngridExtra\ntidyverse\ngtable\ncowplot\n\naurora_simulations.R\nThis file provides code to reproduce Figures 3,4,5 and 6. The packages used here are:\n\ntidyverse\ncowplot\nparallel\n\nWIP\nThe repository is currently being updated to wrap the different methods and simulations in a standalone R package.  This will be documented shortly, however for now it may be installed as follows:\ndevtools::install_github(""nignatiadis/AuroraPaper"", subdir=""AuroraR"")\n'], 'url_profile': 'https://github.com/nignatiadis', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'R', 'Updated Mar 3, 2021', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'Turkey', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Basic-Machine-Learning-Project-With-Python\nBeginner or Elementary projects in Python about Regression ,Classification and data processing\n'], 'url_profile': 'https://github.com/tgbaozkn', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'R', 'Updated Mar 3, 2021', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}","{'location': 'DELHI, INDIA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['SUV_PURCHASE_PREDICTION\nINTRODUCTION:\nIts a training project and this file consist of python coding that contains the prediction of SUV_PURCHASE_PREDICTION.\nfirst, you need to download  the CSV File.\nPACKAGE:\nnumpy;\nmatplotlib;\npandas\nseaborn;\nsklearn;\nscipy\nALGORITHM:\nLogistic Regression.\nCONCLUSION:\nPREDICTING SUV_PURCHASE AND VISUALIZES DATA BY APPLYING ALGORITHM WHICH GIVES THE BEST ACCURACY TO THE MODEL AT THE CORE RATE OF 83%. AT LAST YOU WILL SEE THE CONFUSION METRICS WHICH SHOWS THE ACTUAL AND PREDICTED PURCHASE.\n'], 'url_profile': 'https://github.com/Shifali58', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Aug 2, 2020', 'R', 'Updated Mar 3, 2021', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['House-pricing-ML\nMachine learning linear regression for the House Pricing competition on Kaggle\n'], 'url_profile': 'https://github.com/SSLotfi', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'MIT license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '174 contributions\n        in the last year', 'description': ['CarPricePrediction\nRMSE : 0.0779 \nr2 score : 0.8615 \nmodel : Linear Regression\n'], 'url_profile': 'https://github.com/mehdi1514', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'MIT license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': [""digitRecogniser-KNN-LR\nKNN and Logistic Regression models to detect images which contain the digit 5 or not.\nModels are trained on the MNIST image database imported directly from the scikit-learn module.\nDataset used = 'mnist_784'.\nThe dataset contains 70000 images of digits. 10000 of the 28 x 28 px images are used to train and validate the models.\nThe data used are the intensities of 784 pixels.\n""], 'url_profile': 'https://github.com/Harshith-H', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'MIT license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'Lahore,Pakistan', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Price-prediction\nPredicting  Price of Ames housing data using Principal component analysis and Regression\nSteps wise\n\n\nEDA expolatory data analysis\nTotal number of attributes equals 81, of which 36 is quantitative, 43 categorical + Id and SalePrice.\nQuantitative: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath,\nBsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr,\nLotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond,\nOverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\nQualitative: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1,\nCondition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional,\nGarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour,\nLandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl,\nRoofStyle, SaleCondition, SaleType, Street, Utilities,\nwe can intially see that our SalesPrice in left skewed which Indicates that the majority of prices\nare below the average price.\n\n\nMissing Values\nWe should drop features like;\nPoolQC\nMiscFeature\nAlley\nFence\nFireplace Qu.\nHaving missing values more than 1000 variables and the major cause of outliers.\nSecondly Replacing missing value with mode of Qualitative data\nMszoning\nElectrical\nSaleType\nExterior1st\nExterior2nd\nKitchenQual\nfilling zero for numerical data\nLastly Filling missing values with subgrouping lotfrontage and neighborhood and filing median against it of each class.\n\n\nNormality and standerdizing\nSome features of data shows high skewness In this case,the square-root transformation may help to make the variances more\nconstant throughout the study area and often makes the data appear normally distributed as well.\n\n\nPrincipal component analysis (PCA):is a statistical procedure that uses an orthogonal transformation to convert a set of\nobservations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal\ncomponents.\n\n\nUsing Advanced regression techniques to predict and analysing the rsquare value and mean square residual error.\n\n\n'], 'url_profile': 'https://github.com/usmanfarooq619', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'MIT license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['ECE 276A: Stop Sign Detection\nStop sign detection algorithm using logistic regression for color segmentation and OpenCV for shape recognition\nCode:\nLogisticRegression.ipynb - program used to train Logistic Regression model and get model parameters.\nstop_sign_detector.py - program used to segment images and find bounding boxes of stop sign like regions.\ngetROI.py - program used to make binary mask labels for training and validation sets.\nReport.pdf - project report containing examples of segmentation and bounding boxes.\nResults:\nExample 1:\n\nExample 2:\n\nExample 3:\n\n'], 'url_profile': 'https://github.com/jamessalem', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'MIT license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'Northwest Indiana', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Weight_Predictor\nUsing Linear Regression in the Scikit Learn Library to predict peoples' weight\n""], 'url_profile': 'https://github.com/DrueStaples08', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'MIT license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'Roorkee, Uttarakhand.', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['House_Price_Prediction\nSource : https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/Shreyansh1610', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'MIT license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'Bay Area', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['Linear Regression Project\n\nscrappy_linreg.py\nscrappy_linreg_in_action.py\n\nPredicting a Car\'s MPG based on Weight\n\nIn this project, I created my own linear regression model and used it to build a model\nthat will predict a car\'s mpg based on its weight\nThe linear regression model finds the best fit line of the dataset and makes predictions based off of it\nscrappy_linreg.py also ultilizes a R squared function where one could find the R squared value of the line\nDATASET link: https://archive.ics.uci.edu/ml/datasets/Auto+MPG\n\nAbstract\n\n\nFuture directions\n\nBuild a model that can more accurately model unknown data\nFinding a way to use the model with uneven X and Y\'s (more info in ""scrappy_linreg.py"").\n\n'], 'url_profile': 'https://github.com/choucurtis987', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'MIT license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': ['MTSGL\nMulti-task regression with sparse groupe regularization using a proximal ADMM. This Python module was created as part of\na class project for STATS 606 at the University of Michigan during the winter 2020 term by Simon Fontaine, Jinming Li\nand Yang Li.\nWe consider the following optimization problem\n\\[\n\\text{minimize}\\beta\n\\sum{k=1}^K L(Y^{(k)},X^{(k)}\\beta^{(k)})\n\n\\lambda P_{q,\\alpha}(\\beta),\n\\]\n\nwhere\n\\[\nP_{q,\\alpha}(\\beta)\n= \\sum_{j=1}^p \\alpha\\Vert\\beta\\Vert_1\n\n(1-\\alpha)\\Vert\\beta\\Vert_q,\n\\]\n\nfor (q \\in {2,\\infty}), (\\alpha\\in[0,1]) and (\\lambda>0$).\nInstallation\nExample\nReferences\nAuthors\nSimon Fontaine, simfont@umich.edu\nJinming Li, lijinmin@umich.edu\nYang Li, yangly@umich.edu\nLicense\nMIT\nAcknowledgments\n'], 'url_profile': 'https://github.com/fontaine618', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'MIT license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'Bellevue, WA', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Linear Regression S & P 500: Project Overview\n\nCreated a model (RMSE ~ 16) to estimate the closing price for the S & P 500 to show ability in linear regression\nUsed existing data that contained the open, high, low, and closing price as well as the daily volume\nCreated four different features for our model involving closing price and volume information\nUsed a linear regression model to predict closing prices on our test set and evaluated model performance in different ways\n\nCode and Reference Used\nPython Version: 3.7\nPackages: pandas, numpy, matplotlib, sklearn\nResources\nData\nhttps://raw.githubusercontent.com/NickyThreeNames/DataquestGuidedProjects/master/Guided%20Project-%20Predicting%20the%20stock%20market/sphist.csv\nStock Market Definitions\nhttps://en.wikipedia.org/wiki/S%26P_500_Index\nhttps://en.wikipedia.org/wiki/Volume_(finance\nhttps://www.investtech.com/main/market.php?CountryID=44&p=staticPage&fn=helpItem&tbReport=h_PVC\nData Cleaning\nThe data cleaning involved getting our dates into datetime objects and reversing the index of the dataframe so we it was listed from\nearliest to most recent price.\nFeature Engineering\nCreated the following four features for our model:\n\n5 day closing price average\n30 day closing price average\nratio of the standard deviation of the closing price for the previous week to the previous year\nratio of the standard deviation of volume for the previous week to the previous year\n\nModel Building\nI used a linear regression model, as it was the purpose of this exercise.\nModel Performance\nThe RMSE of our model on the test set was apporximately 16. I also tried to evaluate using this type of model in some form of a trading\nsystem even though it lags behind price during well defined trends either up or down. The model does fairly well establishing turns or\ntrend changes in the stock market. This can be seen graphically below.\n\n'], 'url_profile': 'https://github.com/kenp8842', 'info_list': ['Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 14, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Feb 6, 2020', 'Python', 'MIT license', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}"
"{'location': 'Nashik,India', 'stats_list': [], 'contributions': '178 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PranavPatil7', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'Lima, Perú', 'stats_list': [], 'contributions': '235 contributions\n        in the last year', 'description': ['House Prices competencia en Kaggle\nPor Sandro Marcelo Peirano Gozalvez\nsandromarcelo.peirano@gmail.com\nScripts para competencia ""House Prices"" de Kaggle\nFlujo de Trabajo\nEstá conformado por dos partes\n\nLa Imputación de Datos\nEl modelamiento\n\nSe ha dividido debido a la necesidad de preservar el resultado de partes intermedias debido a la cantidad de computación necesaria\npara completarlas. Específicamente, se va a usar imputación con el paquete MICE.\nTODO\nDefinir Feature Engineering\n'], 'url_profile': 'https://github.com/Ordnas1', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Gowtham-Sabareesan', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '637 contributions\n        in the last year', 'description': ['Author: Rahul Kejriwal - rxk88@psu.edu\nStat 380: Coin Flip Regression\nI divided the data into train and test datasets by a 80:20 ratio and I did not do any feature engineering at all since the features are coin flips which should be random.\nI used XGBoost to predict the dataset and used logistic regression as my objective hyperparamter.\nRanked above the full points benchmark. :)\n'], 'url_profile': 'https://github.com/rkej', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic_Regression-Heart-Data-Set\n'], 'url_profile': 'https://github.com/SauravSanjay', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['linear-regression-with-gradient-descent\n'], 'url_profile': 'https://github.com/annweshasarma', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chandansingh2693', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akhila66', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/915456', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'Hydrabad', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RushiB007', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 21, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}"
"{'location': 'CHENNAI', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vkaravindraman', 'info_list': ['Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'C#', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Heartattack_prediction-using-Logistic-Regression\n'], 'url_profile': 'https://github.com/prachai', 'info_list': ['Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'C#', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NFL--Predictor-Model---Linear-Regression\nThis is my first Python project used to teach me how to work with Regression, Python, and sklearn. It incorporates statistics from the last 20 years of NFL games, including team attributes which are used to forecast the total points in that game. Rather simple, this Linear Regression model utilizes simple Machine Learning strategies to produce output based on past statistics.\nData Collection\nThe data that was collected above can be seen in the two csv files ""historical_teams"" and ""game_data"". Historical data is read as a dictionary of teams, with each key being defined as the Concatenation of both the team name and year, example ""ARI2002"". Each key\'s definition resembles a list of its attributes, such as yards/passing attempt. One each of these keys are processed, we load in the game data. Each game calls one of these keys for both teams as well as passes in individual weather data for the game, such as wind speed. Each value of the key\'s definition is pssed in as a training input alongside the individualied wether data.\nRegression\nSince it was simply a starter regression model, I decided it was best to utilize the simplest method of regression: Linear Regression. I pass each value for each team as a training input and the total points scored by each team as the training output. I trained the model utilizing regression algorithms.\nConclusion\nAfter completing this project, I feel that I am beggining to develop a more complex understanding of Python and Sklearn. I lan to utilize this ideas in a more complex model for algorithmic swing trading through the zipline API.\n'], 'url_profile': 'https://github.com/ghartzell12', 'info_list': ['Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'C#', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Loan-Prediction-with-Linear-Regression\nThis project is made from scratch using only pandas and numpy\nAlgorithm has been written I have not used sklearn for this project.\nAccuracy:65.85%\n'], 'url_profile': 'https://github.com/tanmay4315', 'info_list': ['Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'C#', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'Hydrabad', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['House-Prices-Advanced-Regression-Techniques\nBuilding Machine Learning Pipelines: Data Analysis Phase\n'], 'url_profile': 'https://github.com/jaiswaldj', 'info_list': ['Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'C#', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '331 contributions\n        in the last year', 'description': ['Linear-REgression-AR-VR-view-\ncreated an AR/VR mobile application in Unity3D that can load a dataset derived from the numbers of patients\nwhich the hospital staff took care of during previous years, then make a prediction of how many patients they could expect\nto receive in future. During a meeting, this kind of application can be used in combination with image targets on paper\ndocuments, thus enhancing the interactivity of a report by making things easier to understand and visualize in people’s minds\nfor funding and planning purposes. This served as an intro to mobile AR and VR for me using arkit and google cardboard VR\nmain menu of application has functionality to input year to predict for,\nthan a normal in game view of the resulting graph VR view AR View  device need ios 11 or higher\n'], 'url_profile': 'https://github.com/shoshanimayan', 'info_list': ['Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'C#', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Data Science Project Template\nTemplate adapted from Cookiecutter Data Science\nConvention\nFollowing this directory structure\n|--project_name                           <- Project root level that is checked into github\n  |--project                              <- Project folder\n    |--README.md                          <- Top-level README for developers\n    |--run_project.r                      <- Script to run build_features.r and train_model.r\n    |--volume\n    |   |--data\n    |   |   |--external                   <- Data from third party sources\n    |   |   |--interim                    <- Intermediate data that has been transformed\n    |   |   |--processed                  <- The final model-ready data\n    |   |   |--raw                        <- The original data dump\n    |   |\n    |   |--models                         <- Trained model files that can be read into R or Python\n    |\n    |--required\n    |   |--requirements.txt               <- The required libraries for reproducing the Python environment\n    |   |--requirements.r                 <- The required libraries for reproducing the R environment\n    |\n    |\n    |--src\n    |   |\n    |   |--features                       <- Scripts for turning raw and external data into model-ready data\n    |   |   |--build_features.r\n    |   |\n    |   |--models                         <- Scripts for training and saving models\n    |   |   |--train_model.r\n    |   |\n    |\n    |\n    |\n    |--.getignore                         <- List of files not to sync with github\n\n'], 'url_profile': 'https://github.com/dicicch', 'info_list': ['Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'C#', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'Seattle, Washington', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KIRANCHITTA88', 'info_list': ['Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'C#', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['Decision-Tree-Regression-from-Scratch\nImplementation of Decision Tree regressor from scratch using only Numpy and pandas for prediction of Sale price while handling categorical variables and performing neccessary data imputation.\n'], 'url_profile': 'https://github.com/rishabkr', 'info_list': ['Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'C#', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}","{'location': 'Varanasi', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Image_Classification_NN_Logistic_Regression\n'], 'url_profile': 'https://github.com/ShivanshGupta55', 'info_list': ['Python', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'C#', 'Updated Feb 9, 2020', 'R', 'Updated Feb 3, 2020', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '331 contributions\n        in the last year', 'description': ['Linear-REgression-AR-VR-view-\ncreated an AR/VR mobile application in Unity3D that can load a dataset derived from the numbers of patients\nwhich the hospital staff took care of during previous years, then make a prediction of how many patients they could expect\nto receive in future. During a meeting, this kind of application can be used in combination with image targets on paper\ndocuments, thus enhancing the interactivity of a report by making things easier to understand and visualize in people’s minds\nfor funding and planning purposes. This served as an intro to mobile AR and VR for me using arkit and google cardboard VR\nmain menu of application has functionality to input year to predict for,\nthan a normal in game view of the resulting graph VR view AR View  device need ios 11 or higher\n'], 'url_profile': 'https://github.com/shoshanimayan', 'info_list': ['C#', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 29, 2020', '1', 'SAS', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 10, 2020', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['1 House Prices: Advanced Regression Techniques\n1.1 Description\nAsk a home buyer to describe their dream house, and they probably won’t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition’s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n1.2 Objective\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames and Iowa. the goal is to predict the sales price for each house. For each Id in the test set, the model will predict the value of the SalePrice variable.\n'], 'url_profile': 'https://github.com/CarlosSilva34', 'info_list': ['C#', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 29, 2020', '1', 'SAS', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 10, 2020', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['FAA---Regression-on-Flight-Data\n1)Predicted the landing distance of two commercial flights in the given data of 950 flights to estimate the risk of landing overrun using multiple linear regression and polynomial regression in SAS. 2)Performed detailed EDA on the predictor and response variables and validated the different models using residual analysis.\n'], 'url_profile': 'https://github.com/guptaanupreet', 'info_list': ['C#', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 29, 2020', '1', 'SAS', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 10, 2020', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'Updated Feb 4, 2020']}","{'location': 'Omaha, Nebraska, USA', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SanchitVerma', 'info_list': ['C#', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 29, 2020', '1', 'SAS', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 10, 2020', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'Updated Feb 4, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TanyaChaudhary', 'info_list': ['C#', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 29, 2020', '1', 'SAS', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 10, 2020', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'Updated Feb 4, 2020']}","{'location': 'Melbourne, Australia', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mattkay123', 'info_list': ['C#', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 29, 2020', '1', 'SAS', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 10, 2020', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-regression-by-neural-network\nPython feedforward neural network for fitting a polynomial regression. Written using Keras/Tensorflow. Runs into trouble for polynomial degree > 1.\n'], 'url_profile': 'https://github.com/atbruland', 'info_list': ['C#', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 29, 2020', '1', 'SAS', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 10, 2020', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shucool', 'info_list': ['C#', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 29, 2020', '1', 'SAS', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 10, 2020', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kolyadav', 'info_list': ['C#', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 29, 2020', '1', 'SAS', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 10, 2020', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Adil-EL', 'info_list': ['C#', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 29, 2020', '1', 'SAS', 'Updated Feb 7, 2020', '1', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Updated Feb 10, 2020', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'R', 'Updated Feb 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '400 contributions\n        in the last year', 'description': ['stock_prediction\nThe main objective of this project is to find the best model to predict the future stock market value (for next 20 days) from historical data for a particular company (ex: Google). This project will demonstrate how to “tune” the model will affect the results. The experimental results demonstrate that traditional ML algorithms may have a better performance.\n'], 'url_profile': 'https://github.com/nishnareddy1', 'info_list': ['Python', 'Updated Feb 3, 2020', '3', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'TeX', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'Austin, TX', 'stats_list': [], 'contributions': '1,016 contributions\n        in the last year', 'description': ['project_boston_houseprice_predictions\n'], 'url_profile': 'https://github.com/esharma3', 'info_list': ['Python', 'Updated Feb 3, 2020', '3', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'TeX', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'Vallejo, CA', 'stats_list': [], 'contributions': '218 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JeffTheAggie', 'info_list': ['Python', 'Updated Feb 3, 2020', '3', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'TeX', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'United States ', 'stats_list': [], 'contributions': '376 contributions\n        in the last year', 'description': ['Predicting-Concrete-Strength-using-Regression-with-Keras\nPredicting Concrete Strength using Regression with Keras--- In this notebook,I have used Keras library to build a regression model that predicts the concrete strength given its ingredients values like Cement, BlastFurnace, Flyash,Water Superplasticizer, Coarse aggregate, Fine aggregate and age. I have used regression model because strength is a continuous data in the dataset.\n'], 'url_profile': 'https://github.com/ARGULASAISURAJ', 'info_list': ['Python', 'Updated Feb 3, 2020', '3', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'TeX', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'Sri Lanka', 'stats_list': [], 'contributions': '254 contributions\n        in the last year', 'description': ['PR-A1-WBCD-Logistic_Regression\nAuthor: K.A.T.S. JAYATHILAKA | 209338R | jayathilakakats.20@uom.lk\nDate: 2020-02-09\nThis repository is used to develop a classification model using logistic regression on the Wisconsin Breast Cancer Data (WBCD) as an assignment for module; Pattern Recognition in M.Sc in Data Science, Analytics and Engineering, University of Moratuwa.\n'], 'url_profile': 'https://github.com/tsj1992', 'info_list': ['Python', 'Updated Feb 3, 2020', '3', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'TeX', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/supratim5', 'info_list': ['Python', 'Updated Feb 3, 2020', '3', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'TeX', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'Lonavala,Maharashta,India', 'stats_list': [], 'contributions': '186 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saurabh-maurya', 'info_list': ['Python', 'Updated Feb 3, 2020', '3', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'TeX', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'Beijing', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['This post contains R code for the following research.\nPost-Lasso Inference for High-Dimensional Regression\nX. Jessie Jeng, Huimin Peng, Wenbin Lu\nhttps://arxiv.org/abs/1806.06304\n'], 'url_profile': 'https://github.com/penniepeng321', 'info_list': ['Python', 'Updated Feb 3, 2020', '3', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'TeX', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'Indore, India', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Linear-regression-model-for-House_Price_Prediction\n'], 'url_profile': 'https://github.com/1bhavesh1', 'info_list': ['Python', 'Updated Feb 3, 2020', '3', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'TeX', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'Kerala, India', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Regression with BIWI head pose dataset\nThis is a more advanced example to show how to create custom datasets and do regression with images.Task is to find the center of the head in each image.\n'], 'url_profile': 'https://github.com/yedukrishnanr', 'info_list': ['Python', 'Updated Feb 3, 2020', '3', 'Jupyter Notebook', 'Updated Feb 13, 2020', '1', 'R', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 9, 2020', 'TeX', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}"
"{'location': 'Mysuru', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SridharN1', 'info_list': ['Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 6, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['############################\nEnvironment: python 3.7\n############################\nCommand\npython main_realworld.py/main_synthetic.py int(times)\na) python main_realworld.py 100\nInput: realworld.npy (realworld dataset)\nOutput: result1_realworld.csv emp1_realworld.csv\nb) python main_synthetic.py 100\nOutput: synthetic_gaussian.npy (synthetic dataset with Gaussian errors)\nresult1_synthetic_gaussian.csv (statistics)\nsynthetic_cauchy.npy (synthetic dataset with Cauchy errors)\nresult1_synthetic_cauchy.csv (statistics)\nemp1_synthetic.csv (all empirical errors)\n% Due to privacy issue, we do not provide the real-world dataset.\nYou can change the input dataset to be your own panel dataset ""XXX.npy""\nin which the ((iT-T+t)-th row is (x_{it}, y_{it}), i.e.,\nthe observation of i-th individual at t-th time period.\n############################\n'], 'url_profile': 'https://github.com/huanglx12', 'info_list': ['Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': [""Kaggle-House.Prices-Advanced.Regression.Techniques\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nFor Further details go check out: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview\n""], 'url_profile': 'https://github.com/hgoyal194', 'info_list': ['Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 6, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['House-Prices-Prediction-Advanced-Regression-Techniques\nThe dataset which is downloaded from the below link(kaggle) to predict house price using advanced regression techniques.\nSo tried with multiple steps like data preprocessing, EDA, Feature Engineering and model building with various Machine Learning algorithms but now the SVM has scored well by competing with Xgboost, Decision Tree and Linear Regression.\nkaggle: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/NAVEENRH', 'info_list': ['Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 6, 2020']}","{'location': 'Lonavala,Maharashta,India', 'stats_list': [], 'contributions': '186 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saurabh-maurya', 'info_list': ['Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bmagnificent86', 'info_list': ['Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 6, 2020']}","{'location': 'Spain', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nokutu', 'info_list': ['Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/khpraful', 'info_list': ['Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Step-by-step-Regression-Analysis-using-R\n'], 'url_profile': 'https://github.com/ShravyaUSF', 'info_list': ['Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Oct 22, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 26, 2020', 'HTML', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020']}","{'location': 'Montreal, Quebec', 'stats_list': [], 'contributions': '644 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AymenRumi', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 26, 2020', 'HTML', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '148 contributions\n        in the last year', 'description': ['Air-Quality-Prediction\nPredicted Pollution Levels by building linear regression model, designed a machine learning model which can predict the air quality index.\nTech Stack :- Python, ML (Linear Regression)\n'], 'url_profile': 'https://github.com/Swapnil-Powar', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 26, 2020', 'HTML', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Forecasting_sales_kaggle\nThis project is about Forecasting. I used data from kaggle in order to predict sale products based on many features.\nFeatures had to be engineered ! In order to achieve a good accuracy we need to add as many features as we can. Also, you have to dive into the word of data and understand it in order to make modelling possible.\nI used Random Forest regressor , XGboost regressor and deep learning layers in order to get the highest accuracy.\nI used MAE and MSE as metrics.\nIt appears that XGboost and Deep learning had the best accuracy which means the lowest MAE\n'], 'url_profile': 'https://github.com/yasminabelhadj', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 26, 2020', 'HTML', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['Data Analysis - R\nData analysis in R with data from Wave 6 of the World Values Survey. Official data can be found here. However, for this project a specific Data Processing has been done: here the processed data.\nTablet of Contents\n\nGeneral info\nCode\nTechnologies\nSetup\n\nGeneral info\nThis is a university project from the course of Statistics and Methodology from Tilburg University. The overall goal is to choose a set of variables (5) among hundreds of them in the World Values Survey and use these variables in a Multiple Linear Regression (MLR) method for both, inference and prediction. In the first case, the goal is to look for significance effect on heppiness while for prediction, the goal is to predict FinStat, that is, financial situation.\nThe full description of the project can be found here. In addition to that, a report has been done. In the report you can find all the solutions and main insights.\nCode\n\nData Cleaning\nExploratory Data Analysis\nMultiple Linear Regression\nPredictive Modelling\n\nIf you want to run everything, here you can find all the code.\nTechnologies\nAll the project was created with:\n\nR\n\nSetup\nFor running this code you need:\n\nMASS library\nMLmetrics library\n\nOn top of that, other custum functions were used.\n'], 'url_profile': 'https://github.com/F-Lauria', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 26, 2020', 'HTML', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020']}","{'location': 'New York, New York', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/omarp120', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 26, 2020', 'HTML', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/luisespriella9', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 26, 2020', 'HTML', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['ACLR\nAutocorrelation LD regression: a tool to efficiently estimate the autocorrelation of latent effects in large genetic data sets\nIntroduction\nACLR is a software tool to infer the autocorrelation of latent genetic effects on a human trait or disease. Specifically, it uses data from large cohorts that contain genome-wide genetic variant data as well as trait or disease values for each individual in the cohort. It then returns correlation estimates of the trait effects of these genetic variants depending on the distance from each other along the genome, i.e. the autocorrelation as a function of genomic distance. As is typical for such data sets, the method is primarily aimed at applications where the number of genetic effects (typically tens of millions) is much larger than the number of individuals (typically hundreds of thousands).\nMany previous genetic analysis tools have explicitly or implicitly assumed that effects of nearby genetic variants are uncorrelated. This tool was designed to test and potentially challenge this assumption, which has important implications for finding causal genes in human disease as well as improve disease prediction from genetic data.\nSummary of statistical method and implementation\nStatistically, the method corresponds to estimating the autocorrelation of latent effects in a linear or probit model with the number of features being much larger than the number of data points. The tool regresses functions of marginal effect estimates (estimates from single feature regressions) onto so-called distance-dependent LD scores. These distance-dependent LD scores are defined by the matrix RSR, where R = XTX with X the design matrix of the linear/probit model, S a matrix indicating which genetic feature pairs are within the distance of interest on the genome, and R the feature-by-feature sample covariance matrix (in genetics referred to as LD matrix).\nSince the number of features are typically in the tens of millions, calculating and using the full feature-by-feature sample covariance matrix R is not possible. However, since covariances of distant features are close to zero, the tool implements a tailored banded matrix-based approach. Since S matrices are usually sparse, computation time is further reduced by using sparse matrix calculations. Also, while the majority of the matrix operations are called using Python, reading in data from compressed genotype files and building initial data matrices is performed using a specifically developed C++ subroutine, that is part of this tool. This, together with the code being fully parallelizable, makes it possible to run the method on large genetic data sets.\nRequirements and installation\nThe Python 2.7 based code only uses common libraries such as Numpy, Pandas, scipy, and scikit-learn. C++ library requirements: Gnu Science Library, Intel Math Kernel Library, zlib, C standard library, POSIX threads, all of which are freely available. Before compiling the code, the locations of these libraries have to be updated in the makefile to the appropriate location. Then use ‘make’ do compile the executable ‘ld_mat_calc’.\nUsage\nFirst, use ""dist_ld_score.py"" to calculate distance dependent LD scores based on the provided genotype data and list of genetic variants to be used. Then use ""regression.py"" to combine these scores with marginal effect estimates of the target trait(s) to calculate effect variance and distance dependent covariance effects as well as block-Jackknife error estimates.\nCommand line arguments\nRequired arguments for ""dist_ld_score.py"":\n--bgen-file: path to compressed genotype data file in BGEN v1.2 format (see https://www.well.ox.ac.uk/~gav/bgen_format/spec/v1.2.html)\n--snp-file: table of genetic variants (SNPs) to be used; required columns: ""bgen_pos"", position in the BGEN file; ""bp_pos"", base pair position in the genome; ""chrom"", chromosome number; ""rsid"", SNP ID; ""freq"", the SNP population frequency; ""alleleA"" and ""alleleB"", major and minor allele of the SNP\n--indi-file: binary file containing a vector of positions (32 bit signed integer) in the BGEN file for each individual to be used in the analysis\n--sum-stat-file: marginal effects (summary statistics) file in LDSC format (https://github.com/bulik/ldsc/wiki)\nOptional arguments for ""dist_ld_score.py"":\n--out: set name of output csv files; default is ""ld_scores.csv""\n--batch-num and --batch-num-total: only process input data from a subset of SNPs for parallelization; e.g. ""--batch-num 2 --batch-num-total 6"" means that only the second of 6 equally sized subsets of all SNPs get processed\n--annot-file: zipped functional annotation data file to be used for advanced analyses (please contact author for specific instructions)\nRequired arguments for ""regression.py"":\n--ld-score-file: output file from ""dist_ld_score.py""; default ""ld_scores.csv""\n--sum-stat-file: marginal effect (summary statistics) file in LDSC format (https://github.com/bulik/ldsc/wiki)\n--prior-env-var: prior variance of environmental noise compared to the variance of a single SNP effect\n--trait-num: number of traits used\nOptional arguments for ""regression.py"":\n--ld-score-file: output file from ""dist_ld_score.py""; default ""ld_scores.csv""\n--out-file and --err-file: name of output files for covariance estimates and error estimates respectively; default is ""result.txt"" and ""error.txt""\n--jk-block-num: number of Jackknife blocks used for error estimation; default is 200\n--annot-num and --base-var-annot: number functional annotations and base variance annotations to be used for advanced analyses (please contact author for specific instructions)\nContact\nAuthor: Armin Schoech. Please email arminschoech@g.harvard.edu for comments and questions.\nLicense\nThe ACLR software tool is free under the GNU General Public License v3.0 (GPLv3).\n'], 'url_profile': 'https://github.com/arminschoech', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 26, 2020', 'HTML', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Testing the hypothesis that returns, on average, are higher during November-April months than in May-October in financial markets. Comparison of monthly MSCI World Index closure prices for 36 countries in the chosen sample.\nThe data indicates satistical significance (p => 10%) in 7 countries; Austria, Denmark, Ireland, Japan, Indonesia, Korea and Portugal.\nThe results produced are:\n\n\n36 regressions: for each country in the sample. Monthly returns are regressed against a dummy variable (1 if month is November-April and 0 if month is May-October).\n\n\nA table with relevant regression output parameters, displayed by country.\n\n\nA table with November-April changes in percentage (average per country for the period) against May-October returns (average per country for the period). Further highlights November-April as a more profitable.\n\n\n'], 'url_profile': 'https://github.com/veronikavaskova', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 26, 2020', 'HTML', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020']}","{'location': 'Jaipur', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Titanic-Dataset-Aalysis-and-Model-Deployment-\nPerformed exploratory data analysis on Titanic dataset from kaggle and and deployed a model based on linear regression algorithm.\n'], 'url_profile': 'https://github.com/raghavsh2901', 'info_list': ['Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2021', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 26, 2020', 'HTML', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'Updated Jul 17, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['BE-TU-Study-Design-Impacts\nContains data and analysis scripts for meta-regression analysis to explore study design impacts on BE-TU research\n'], 'url_profile': 'https://github.com/Laura-k-a', 'info_list': ['Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'JavaScript', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Colchester, Essex', 'stats_list': [], 'contributions': '165 contributions\n        in the last year', 'description': ['House Prediction\nHouse prediction tool website for CE101 Team A1\nVisit Website\nhttps://house-predictor.herokuapp.com/\n\nFeatures\nWebsite is created using Flask. The model is integrated as a module. This repository is linked to Heroku and offers Continuos Integration and Continuos Development features for full production requirements.\n'], 'url_profile': 'https://github.com/RanaSharjeel', 'info_list': ['Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'JavaScript', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saitejakomuravelly', 'info_list': ['Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'JavaScript', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['California housing price prediction\nMachine learning algorithms are very powerful to predict housing price based on historical data. The problem statement is to predict median house values in Californian districts, given a number of features from these districts. The California Housing Prices dataset from the StatLib repository. This dataset is based on data from the 1990 California census.\n'], 'url_profile': 'https://github.com/hirenhk15', 'info_list': ['Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'JavaScript', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Pune,Maharashtra', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Maching Learning Series\nHi Friends,\n\nI am creating machine learning series in which I will try my best to share applied machine learning Techniques used in ML project development.\nI will be using pythong language for development in will we are going to learn sklearn, tensorflow, keras for machine learning modeling. I am using seaborn for data visualisation.\nI am also going to create video series for these projects stay tuned :)\n\n\n\n\nTopic\nNote\nLink\n\n\n\n\nGetting into ML\nTherotical introduction of Machine learning\n\n\n\nData visualisation\nAll required plots for data visualisation using seaborn\nseaborn-data-visualisation link\n\n\nProject 1 : Classification with Titanic\nPractical Hands-on about applied ML techniques in solving Classification problem\nClassfication with Titanic link\n\n\nProject 2 : Regression with Housing Prices\nPractical Hands-on about applied ML techniques in solving Regression problem\nRegression with Housing Prices link\n\n\nProject 3:Outlier with Credit Card Fraud\nApplying sklearn package supported algorithms for finding outlier\nOutlier with Credit Fraud link\n\n\nProject 4:Anomaly with Time series\n\n\n\n\nProject 5:Clustering with NBA players\n\n\n\n\nProject 6: Its NLP\n\n\n\n\n\n'], 'url_profile': 'https://github.com/AshayKing', 'info_list': ['Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'JavaScript', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Decorah, Iowa', 'stats_list': [], 'contributions': '224 contributions\n        in the last year', 'description': [""Machine Learning Applied\nThis package contains various Machine Learning algorithms, simplified and applied. This package is inspired by 'Machine Learning' course of Andrew Ng at Coursera. My goal is to keep expanding it. Feel free to contribute!\nFeatures\n\nLinear Regression (with only one feature or input): useful for cases where you would like to predict an output based on the given inputs (only one type of feature/input) and their output\nLinear Regression (with multiple features or inputs): useful for cases where you would like to predict an output based on the given inputs (multiple features/inputs possible) and their output\nLogistic Regression: useful for cases where you would like to predict the possibility of an output based on the given two features/ inputs and their output\n\nHow to Install\nnpm install machine-learning-applied\nHow to Use\nvar {LinearRegressionUni} = require('machine-learning-applied');\nvar {LinearRegressionMulti} = require('machine-learning-applied');\nvar {LogisticPrediction} = require('machine-learning-applied');\n\nLinearRegressionUni takes an input array and an output array of numbers, and the target input for which prediction is to happen. Then, it outputs the predicted number.\nLinearRegressionMulti takes multiple input arrays, an output array and the target input for which prediction is to happen. Then, it outputs the predicted number.\nLogisticRegression takes two input array with their output array which contains 0s and 1s, and the two desired inputs for which prediction is to happen. Then, it outputs a probability in range of 0 and 1.\nNote:\nEach of the above functions are well-documented. So, while you use the function, you should see the descriptions. Also, learning rate and number of iterations are given at default. Change it only if you know what you are doing.\nComing soon\n\nImage Classifier\nNeural Network\nK-means Clustering\nand more...\n\nQuick Note\nI am actively seeking a full-time position. Please let me know if you know of any at: faqima01@luther.edu. Thanks a lot!\nLicense\nThe MIT License © 2020 Massi Faqiri. All rights reserved.\nEnjoy!\n""], 'url_profile': 'https://github.com/massifaqiri', 'info_list': ['Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'JavaScript', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'JavaScript', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Predicting-Survival-of-Titanic-Passengers-using-Logistic-Regression\nKaggle Competition | Titanic Machine Learning from Disaster\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nIn this contest, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\nThis Kaggle Getting Started Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning.""\n'], 'url_profile': 'https://github.com/sagar8086', 'info_list': ['Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'JavaScript', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rajeshasrinivas', 'info_list': ['Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'JavaScript', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Sep 5, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Feb 18, 2020', '1', 'JavaScript', 'MIT license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'HTML', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '1,218 contributions\n        in the last year', 'description': ['Parameter_Selection_for_Linear_Support_Vector_Regression_EXP_Code\nEXP repo location\ngithub\nAll data are in\nPlease choose data from data link and download it to data folder\nFolder description\nIn this experiments, we have three types of solver: grid, particle swarm optimization(pso), and annealing(anl).\n\ngrid approach is in grid-approach\npso and anl approaches are in pso-anl-approach\n\nRead README.md in each folder for instruction.\n'], 'url_profile': 'https://github.com/jyhsia5174', 'info_list': ['C++', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Predicting-risky-plane-landings-using-Linear-Regression-Modelling\nMotivation:\nTo reduce the risk of landing overrun.\nGoal:\nUse various linear regression model to study what factors and how they would impact the landing distance of a commercial flight.\nData:\nLanding data (landing distance and other parameters) from 950 commercial flights.\nVariable Dictionary:\nAircraft: The make of an aircraft (Boeing or Airbus).\nDuration (in minutes): Flight duration between taking off and landing. The duration of a normal flight should always be greater than 40min.\nNo_pasg: The number of passengers in a flight.\nSpeed_ground (in miles per hour): The ground speed of an aircraft when passing over the threshold of the runway. If its value is less than 30MPH or greater than 140MPH, then the landing would be considered as abnormal.\nSpeed_air (in miles per hour): The air speed of an aircraft when passing over the threshold of the runway. If its value is less than 30MPH or greater than 140MPH, then the landing would be considered as abnormal.\nHeight (in meters): The height of an aircraft when it is passing over the threshold of the runway. The landing aircraft is required to be at least 6 meters high at the threshold of the runway.\nPitch (in degrees): Pitch angle of an aircraft when it is passing over the threshold of the runway.\nDistance (in feet): The landing distance of an aircraft. More specifically, it refers to the distance between the threshold of the runway and the point where the aircraft can be fully stopped. The length of the airport runway is typically less than 6000 feet.\n'], 'url_profile': 'https://github.com/sagar8086', 'info_list': ['C++', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['C++', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['C++', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Assignment-1-Comparison-of-Regression-Models-on-Predicting-Medical-Costs\n'], 'url_profile': 'https://github.com/vivianchen98', 'info_list': ['C++', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rajeshasrinivas', 'info_list': ['C++', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'Ghaziabad', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prateekpr', 'info_list': ['C++', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['CNN-regression-model-for-NIR-using-Fastai-V1\nFrom presentation 2020-02-03 ""Run deep learning models yourself with your own data - from hardware to prediction"" with the Chemometrics section of the Swedish Chemical Society\n'], 'url_profile': 'https://github.com/MJosefson', 'info_list': ['C++', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['C++', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['C++', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 13, 2020', 'Updated Feb 6, 2020']}","{'location': 'Nagpur', 'stats_list': [], 'contributions': '274 contributions\n        in the last year', 'description': ['Predicting Crop Productivity Using Linear Regression\nABSTRACT\n\n\nAgriculture is believed to be as backbone of Indian economic system. For the past few\ndecades, agriculture field has seen lots of technological changes to improve better productivity.\n\n\nThe world population grows steadily but the resources for crop production continuously diminish.\n\n\nTherefore there have been considerable efforts to develop innovative approaches for\nsustainable crop production. Using prediction methods, farmers can enhance the productivity of crops.\n\n\nThese methods are used to find the required quantity of crops, seeds, humidity, water level and other\nsupplements.\n\n\nKEY WORDS :- Precision Agriculture, Weka, ZeroR Algorithm, Matplotlib, Predictive Analysis.\n\n\nProposed System\n\n\nData are collected for different weather conditions, soil, humidity, air quality, crop maturity,and statistics of previous few year data have taken under consideration and future will be predicted by using machine learning algorithm .\n\n\nThough previous monitoring techniques gathers the crop conditions properly, prediction results have not yet been optimized. First of all, researchers do not have clear idea about crop condition and crop monitoring methods .\n\n\nThey should know how to monitor crop condition on different circumstances. So crop characteristics should be well monitored by researchers to deliver good results in prediction methods.\n\n\nBasically problems in predictions are finding proper algorithm for prediction methods and assuming different location results for predictions.\n\n\n'], 'url_profile': 'https://github.com/NimishNagapure', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 13, 2020', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 13, 2020', 'Updated Feb 6, 2020']}","{'location': 'Minneapolis, Minnesota', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Understanding Fantasy Premier League Player Prices using Regression\nDetailed analysis of FPL player prices for 2017-18\nBusiness Problem\nFantasy Premier League(FPL) is a virtual game which is based on the most popular game in the world, soccer. It is based on the English 1st division soccer league. In a nutshell, the games rules are as follows: You have a budget of 100 million pounds, and you get to select 15 players for your team. Their actions in real life games such as goals scored, assists, clean sheets(stopping the opponent from scoring) are all actions that yield points in the game. The players of the game are allowed 1 free transfer every week and can make additional ones in exchange for points. The premier league itself is a billion-dollar industry and has over 600 million viewers every season, spanning 150 countries. FPL is one way they bring in new viewers and retain current viewers. Players of the game are now watching not only their favorite teams play, but also the lesser performing teams as well. This directly leads to more sponsorships for the premier league, and as a result more revenue. The number of people playing the game has grown hugely, starting in 2002 and going on to get 2 million players in 2010 and over 6 million in 2019. it is imperative to the league that the game players keep playing. To keep this profitable base of their viewers happy, they need to optimize the game as best they can.\nThe price of the soccer players in FPL is a highly debated topic. Some believe a player is underpriced and the others are convinced he is overpriced. To make things clearer, let’s look at an example. One of the best players in the league right now, Raheem Sterling, was priced this year at £12 million. It is key to note however that he is only selected by 30% of players of the game. If he was instead priced at £6m he would have been in more than 90% of teams. To make sure that the game is competitive and doesn’t get boring, FPL must come up with an algorithm that correctly sets the price for these players. There have been instances where people just stop playing the game if their favorite player is priced too high, or if a well performing player is priced low, and consequently ends up in every team. This is the question that we are looking to answer. What could be the various factors that go into the algorithm that the FPL uses to generate the player prices? How are each of these factors related to the price of a player? How far can we see into this black box of an algorithm?\nData Description\nTo answer these questions, we needed a comprehensive data set. We got our dataset from Kaggle. The data was for the 2017-18 season. The data we got was inherently messy and we had to clean it up before we could use it. We removed null values, impractical outliers and redundant or irrelevant columns. The final data set then contained the following information for each soccer player who played for the 2017-18 season: \n● Name : Name of the soccer player \n● FPL value : The value of the soccer player in the game (in millions of pounds)\n• Market Value : The value of the soccer player in real life (in millions of pounds) \n• FPL points : The points that the soccer player got in the game\n• Page views : Number of Wikipedia page views for the player \n• Club : The soccer club that the player plays for\nAge : We segregated the players into 6 age brackets as per the recommended business standards. The reason for this categorization is that a player aged 18 and a player aged 19 is treated and priced in a similar fashion provided all other metrics of measurements are same. \n○ Category 1: 17 - 21 \n○ Category 2: 22 - 24 \n○ Category 3: 25 - 27\n○ Category 4: 28 - 31 \n○ Category 5: 32 - 33 \n○ Category 6: 34 - 38\n● Position : The position where the player plays on the soccer pitch\n● Position Category : We categorized all the players into 4 distinct categories- \n○ Category 1: Attackers (goal scorers)\n○ Category 2: Midfielders (goal creators) \n○ Category 3: Defenders (goal protectors)\n○ Category 4: Goalkeepers (last line of defense) \n● Region : Nationality of the player\n● New signing : Whether the player was signed from another club this year\n● FPL selection : Percentage of FPL players who have selected that player in their team \n● Big Club : Does the player play for a top 6 club? Manchester United F.C, Chelsea F.C, Manchester City F.C, Arsenal F.C, Liverpool F.C and Tottenham Hotspur F.C. If a player plays for any of these clubs, then the big club value for that player is marked as Boolean 1.\n● Club : The soccer club that the player plays for\nIn the collected dataset, there are a total of 457 players and the average FPL value of these players is 5.447 million pounds. The maximum FPL value in the dataset is 12.5 million pounds and the minimum FPL value is 4 million pounds. The average market value of these players is 11.02 million pounds. The maximum market value of the players is 75 million pounds and the minimum market value is as low as 0.05 million pounds. The minimum age of player in the data set is 17 while the maximum age is 38. The median age of players is 27 years. Minimum FPL points for a player in the dataset are 0 while maximum are 264 points. The average FPL points scored by the players in dataset is 57.44 points. There is a total of 156 players from England, 28 from Spain, 25 from France, 20 from Netherlands, 18 from Belgium, 17 from Argentina and another 196 players from other countries. The maximum number of Wikipedia page views for a player is 7664 while the minimum number of page views for a player is 3. The average value of Wikipedia page views is 765.3.\nInterpretations and conclusions\nAfter performing multiple regression on this dataset, we recognized that only Big club, age, position, market value, FPL points and nationality of a player impacts his FPL price. Variables like FPL selection percentage, page views, new signing do not have such a significant impact on FPL price of a player. We hence did not consider these variables in our final model. Our final model equation is:\nY(FPL value) = 4.77 + 0.131(Market value) + 0.002(FPL points) + 0.108(Age Category 2) + 0.411(Age Category 3) + 0.191(Age Category 4) + 0.161(Age Category 5) – 0.053(Age Category 6) – 0.387(Position Category 2) – 0.547(Position Category 3) – 0.514(Position Category 4) – 0.263(No Big Club) + 0.326(Non British Player) - 0.0627(Market Value * Age Category 2) – 0.0722(Market Value * Age Category 3) – 0.068(Market Value * Age Category 4) – 0.057(Market Value * Age Category 5) – 0.0004(Market Value * Age Category 6) + 0.008(FPL Points * Age Category 2) + 0.005(FPL Points * Age Category 3) + 0.007(FPL Points * Age Category 4) + 0.008(FPL Points * Age Category 5) + 0.010(FPL Points * Age Category 6) - 0.049(Market Value * Position Category 2) – 0.044(Market Value * Position Category 3) - 0.047(Market Value * Position Category 4) – 0.0005(FPL Points * Position Category 2) – 0.001(FPL Points * Position Category 3) – 0.005(FPL Points * Position Category 4) + 0.020(Market Value * No Big Club) -0.003(Market Value * No Big Club) + 0.0328(Market Value * Non British Player)\nBased on the above equation, we can derive the following relationships between FPL value and all the predictor variables:\n● For a British center forward player, who is playing for a big club and is aged between 18-21, FPL value increases by 13.12% for every dollar increase in market value.\n● For a British center forward player, who is playing for a big club, is aged between 18-21 and has a market value of 0, FPL value increases by 0.2% for every unit increase in FPL points. Since it’s not possible for a player to have a market value of 0, this interpretation is meaningless.\n● For a British center forward player, having FPL points 0, who is playing for a big club and is aged between 22-24, the degree of change in FPL value is 6.27% less than that caused by a British center forward player, who is playing for a big club and is aged between 18-21 for every dollar increase in market value.\n● For a British center forward player, having FPL points 0, who is playing for a big club and is aged between 25-27, the degree of change in FPL value is 7.22% less than that caused by a British center forward player, who is playing for a big club and is aged between 18-21 for every dollar increase in market value.\n● For a British center forward player, having FPL points 0, who is playing for a big club and is aged between 28-31, the degree of change in FPL value is 6.88% less than that caused by a British center forward player, who is playing for a big club and is aged between 18-21 for every dollar increase in market value.\n● For a British center forward player, having FPL points 0, who is playing for a big club and is aged between 32-33, the degree of change in FPL value is 5.73% less than that caused by a British center forward player, who is playing for a big club and is aged between 18-21 for every dollar increase in market value.\n● For a British center forward player, having FPL points 0, who is playing for a big club and is aged 34 and above, the degree of change in FPL value is 0.04% less than that caused by a British center forward player, who is playing for a big club and is aged between 18-21 for every dollar increase in market value. \n● For a British mid-fielder, having FPL points 0, who is playing for a big club and is aged between 18-21, the degree of change in FPL value is 4.97% less than that caused by a British center forward player having similar characteristics for every dollar increase in market value.\n● For a British defender, having FPL points 0, who is playing for a big club and is aged between 18-21, the degree of change in FPL value is 4.42% less than that caused by a British center forward player having similar characteristics for every dollar increase in market value.\n● For a British goalkeeper, having FPL points 0, who is playing for a big club and is aged between 18-21, the degree of change in FPL value is 4.70% less than that caused by a British center forward player having similar characteristics for every dollar increase in market value.\n● For a British center forward player, having FPL points 0, who is not playing for a big club and is aged between 18-21, the degree of change in FPL value is 2.09% more than that caused by a British center forward player, who is playing for a big club and has similar characteristics for every dollar increase in market value.\n● For a British center forward player, having 0 FPL points, who is playing for a big club and is aged between 18-21, the degree of change in FPL value is 3.28% more than that caused by a center forward player, who is not British and has similar characteristics for every dollar increase in market value.\nConclusion\nFrom our analysis we can conclude that age of a player, his market value, FPL points scored in the season, nationality and club impact his FPL value. From these insights, we can gain a clearer understanding about how FPL is pricing players within the game. These insights could be used to further investigate the rising debate on whether a player is underpriced or overpriced within the game. Subsequently, whether the factors influencing the FPL price of a player are justified or need to be modified can also be established.\nLimitations\n● Data set is restricted to FPL season 2017-18. A wider data set with more variables and across more seasons would help us get an even deeper understanding of the variables which contribute to the FPL value. \n● In our model, we have restricted the nationality of a player as either British or non-British. Our interactions might get influenced by factors like if a player is French versus a player who is from Spain or Brazil since their skillset levels might be different.\n● With our model, we are only establishing a relationship between FPL value and dependent variables like age, position, nationality, club, market value and FPL points. Since we have got a high R2 from this model, we could probably further increase the scope in order to predict the FPL value of a player.\nContributors\nSampada Sathe, Rishab Prashanth, Shobhit Mishra, Yuxuan Wang\nData and References\nhttps://www.kaggle.com/karangadiya/fifa19\n'], 'url_profile': 'https://github.com/sampadasathe', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 13, 2020', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/IvankaCaron', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 13, 2020', 'Updated Feb 6, 2020']}","{'location': 'delhi', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['PROJECT ON LASSO\n'], 'url_profile': 'https://github.com/vksharma191171', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 13, 2020', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 13, 2020', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 13, 2020', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Lab 01: Linear Regression in R\nDataset: mtcars in base R\nDataset Description: The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\nCodebook:\nA data frame with 32 observations on 11 (numeric) variables.\n[, 1]\t mpg\t Miles/(US) gallon\n[, 2]\t cyl\t Number of cylinders\n[, 3]\t disp\t Displacement (cu.in.)\n[, 4]\t hp\t Gross horsepower\n[, 5]\t drat\t Rear axle ratio\n[, 6]\t wt\t Weight (1000 lbs)\n[, 7]\t qsec\t 1/4 mile time\n[, 8]\t vs\t Engine (0 = V-shaped, 1 = straight)\n[, 9]\t am\t Transmission (0 = automatic, 1 = manual)\n[,10]\t gear\t Number of forward gears\n[,11]\t carb\t Number of carburetors\n'], 'url_profile': 'https://github.com/KarenZhuqianZhou', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 13, 2020', 'Updated Feb 6, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '516 contributions\n        in the last year', 'description': ['Classification-Decision-Tree-Algorithm\nDecision tree algorithm falls under the category of supervised learning. They can be used to solve both regression and classification problem. The algorithm is implemented by computing the entropy and information gain.\nThis program aims at implementing the decision tree algorithm without the use of scikitlearn or other python build packages for decision tree algorithm\n'], 'url_profile': 'https://github.com/bsathyamur', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', '1', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'R', 'Updated Feb 13, 2020', 'Updated Feb 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Divyasubhedar', 'info_list': ['Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SyuyaAbe', 'info_list': ['Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shivinigam', 'info_list': ['Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['Probabilities-and-Statistics-Project\nExploring, creating visual representations and interpreting probability and statistic notions such as simple and multiple linear regression, mean, median, variance, quantiles and Laplace distribution, regarding a R data set.\n\nFeatures\n\ndescriptive statistic operations\nbuild of a few regression models, both simple and multiple\naddition of new data to the data set and to the regression models\ninterpretations of the results\n\nR Packages\n\ndata sets\nggplot2\nGGally\nscatterplot3d\n\nProject done in collaboration with:\n\nAna Puiu\nMadalina Cirstea\n\n'], 'url_profile': 'https://github.com/Natasa-C', 'info_list': ['Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aashish-khub', 'info_list': ['Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Boston', 'stats_list': [], 'contributions': '144 contributions\n        in the last year', 'description': ['Exploratory Data Analysis of Countries in World:-\nI have used EDA to find factors affecting a country\'s GDP per capita in order to make a predictive model of model countries GDP using linear regression. The hope is to find independent variables with a strong linear relation to the depenedent variable GDP ($ per capita).\nModelling of data\nThere are two types of supervised machine learning algorithms:  Regression and classification\nThe former predicts continuous value outputs while the latter predicts discrete outputs. For instance, predicting the price of a house in dollars is a regression problem whereas predicting whether a tumor is malignant or benign is a classification problem.\n\nLinear Regression Theory\nThe term “linearity” in algebra refers to a linear relationship between two or more variables. If we draw this relationship in a two-dimensional space (between two variables), we get a straight line.\nLinear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output). Hence, the name is Linear Regression. If we plot the independent variable (x) on the x-axis and dependent variable (y) on the y-axis, linear regression gives us a straight line that best fits the data points\nThe equation of the above line is :\nY= mx + b\n\nThere can be multiple straight lines depending upon the values of intercept and slope. Basically what the linear regression algorithm does is it fits multiple lines on the data points and returns the line that results in the least error.\n\n\nThis same concept can be extended to cases where there are more than two variables. This is called multiple linear regression. For instance, consider a scenario where you have to predict the price of the house based upon its area, number of bedrooms, the average income of the people in the area, the age of the house, and so on. In this case, the dependent variable(target variable) is dependent upon several independent variables. A regression model involving multiple variables can be represented as:\ny = b0 + m1b1 + m2b2 + m3b3 + … … mnbn\nThis is the equation of a hyperplane.\nRemember, a linear regression model in two dimensions is a straight line;\nin three dimensions it is a plane,\nand in more than three dimensions, a hyperplane.\n""Countries of World.csv"" :-\nThe Data\nGDP ($ per capita) (Target - the dependent variable)\nPossible independent variables (predictors)\nCountry\nRegion\nPopulation\nArea (sq. mi.)\nPop. Density (per sq. mi.)\nCoastline (coast/area ratio)\nNet migration\nInfant mortality (per 1000 births)\nLiteracy (%)\nPhones (per 1000)\nArable (%)\nCrops (%)\nOther (%)\nClimate\nBirthrate\nDeathrate\nAgriculture\nIndustry\nService\n'], 'url_profile': 'https://github.com/abhishekdabas31', 'info_list': ['Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/srisha03', 'info_list': ['Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '483 contributions\n        in the last year', 'description': ['wine-quality-dataset-implementation-in-TF-2.0\nBuilt different regression neural network models to analyze the impact of different additional layers on the result i.e the \'mse\' loss and \'mae\' error metrics.\nI have built different models to analyse the effects of different models or paramteres in the result (regression). Then i did some optimzation to analyze the results of different models and also plotted the graph of ""mse vs epochs"" and ""mae vs epochs""\nThen I have portrayed methods to reduce over-fitting of the model.\nBY regularzing the weights of the model.\nand By changing the optimizer and tuning it\'s parameters.\n'], 'url_profile': 'https://github.com/karanaryan07', 'info_list': ['Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RohanG12', 'info_list': ['Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '434 contributions\n        in the last year', 'description': ['Time-series-based-power-demand\nUsing Models of machine learning like Auto Regression(AR),Auto Regressive Integrated Moving Average(ARIMA),LSTM and finding the best fit model for it.\n'], 'url_profile': 'https://github.com/Ruthikreddy', 'info_list': ['Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Feb 4, 2020', 'R', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020']}"
"{'location': 'Patna India', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Arvindkumarsahu', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Wine Quality\nStep 1 : import all the following from the list\n\t- import pandas\n\t- import numpy\n\t- import matplotlib.pyplot as plt\n\t- from sklearn import linear_model\n\t- from sklearn.preprocessing import StandardScaler, LabelEncoder label_quality = LabelEncoder()\n\nStep 2 : Create a folder Wine, Save the Wine.ipynb in the same folder along with the following files\n\t- wine_test.csv\n\t- wine_train.csv\n\nStep 3 : Run the Jupyter Notebook Server by just typing “jupyter notebook” on terminal.\nStep 4 : Open the folder Wine and Run the Wine.ipynb\n'], 'url_profile': 'https://github.com/amananand97', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020']}","{'location': 'Varanasi', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Loan-Prediction\nPredicted whether the potential borrower should be granted a loan using logistic regression algorithm. Used machine learning algorithms and data visualization libraries to reach on the conclusion\n'], 'url_profile': 'https://github.com/saurav-91', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PRIYANKArythem3', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020']}","{'location': 'San Francisco, California, USA', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['neuralnetspytorch\nLearning Deep Neural Networks with PyTorch\n'], 'url_profile': 'https://github.com/msankar', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020']}","{'location': 'New jersey', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Dishtid', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/evanboddu', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rajeshasrinivas', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020']}","{'location': 'Beijing', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Selection and Inference for High-Dimensional Regression with Applications in Biomedical Research\nThis post contains R code for the research.\nhttps://repository.lib.ncsu.edu/handle/1840.20/34313\nTitle: Selection and Inference for High-Dimensional Regression with Applications in Biomedical Research\nAuthor: Peng, Huimin\nAdvisors: Wenbin Lu, Co-Chair\nXinge Jeng, Co-Chair\nJung-Ying Tzeng, Member\nHoward Bondell, Member\nAshley Brown, Graduate School Representative\nDate: 2017-05-05\nDegree: Doctor of Philosophy\nDiscipline: Statistics\nURI: http://www.lib.ncsu.edu/resolver/1840.20/34313\n'], 'url_profile': 'https://github.com/penniepeng321', 'info_list': ['Updated Feb 4, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 6, 2020', 'Updated Feb 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Python', 'Updated Feb 5, 2020', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 8, 2020', 'R', 'GPL-3.0 license', 'Updated Feb 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aniket-agra', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 22, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '177 contributions\n        in the last year', 'description': ['U.K. Economic Data Dash App\n\nDash is a Python framework for building analytical web applications. No JavaScript required.\nBuilt on top of Plotly.js, React and Flask, Dash ties modern UI elements like dropdowns, sliders, and graphs directly to your analytical Python code. Read our tutorial proudly crafted ❤️ by Dash itself.\n\n\nUser Guide\n\n\nOffline (PDF) Documentation\n\n\nDash Docs on Heroku (for corporate network that cannot access plotly.com)\n\n\nIntroduction\nThe economic development of Europe proceeded political consolidation and differentiation. As large sovereignties emerged and state regulation superseded guild and municipal regulation, economic development could extend freely over a wider territories. Between 1270 and 1870 England and Great Britain slowly progressed from the periphery of the European economy to centre-stage of an integrated global economy. In the process Britain escaped the Malthusian trap and by the eighteenth century had successfully reconciled a rising population with a rising standard of living.\nInstallation\nFrom source\n# make a local copy of directoy\ngit clone https://github.com/andronikmk/uk-data-dash-app.git\n\n# cd into correct directoy\ncd uk-data-dash-app\n\n# create enviornment\npipenv install\n\n# activate enviornment\npipenv shell\n\n# deploy locally\ngunicorn run:server\n[2020-07-22 16:34:33 -0400] [5442] [INFO] Starting gunicorn 20.0.4\n[2020-07-22 16:34:33 -0400] [5442] [INFO] Listening at: http://127.0.0.1:8000 (5442)\n[2020-07-22 16:34:33 -0400] [5442] [INFO] Using worker: sync\n[2020-07-22 16:34:33 -0400] [5444] [INFO] Booting worker with pid: 5444\nPlotly Dash App\n\n\n\n\n\n'], 'url_profile': 'https://github.com/andronikmk', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 22, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 8, 2020']}","{'location': 'NYC', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Classification-Projects\n'], 'url_profile': 'https://github.com/ardent17', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 22, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Zixuan0810', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 22, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '286 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bbfrederick', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 22, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': [""zombie_enterprises_classification\nusing neural_network and logistic_regression model to complete a binary classification\nraw data:\n\n企业基本信息\n年报数据(三年)\n融资数据（三年）\n知识产权数据\n\nprocessing data：(在Processing_all_data_X完成)\n首先对我们的原始数据进行检查，发现数据集如下特点:\n训练集:僵尸企业:丢失标签=5073:9977 ，接近1:2\n\n测试集:僵尸企业:正常企业:丢失标签=8928:21650:306,接近2:5\n\n首先，为了简化数据处理操作，将所有数据分为两组\n\n没有year信息的'企业基本信息'和'知识产权数据'合并(命名为A组)\n包括year信息的'年报数据'和'融资数据'合并（命名为B组）\n\n然后分别处理:\n对B组:\n\n对数据的year缺失值进行填充，包括2015，2016，2017这三年的数据。考虑到不同年份的各种影响因素，所以将三年数据分开进行填充，生成2015，2016，2017。其中，所有者权益合计=资产总额-负债总额，由于'所有者权益合计'缺失值较其他二者少一些，所以最后根据计算公式填充'所有者权益合计'。对于其他特征都采用本年相应特征数据的均值进行填充。\n完成缺失值的填充,(2015+2016+2017)/3，三表合一。\n再利用公式$$ /frac{x-mu}{std} $$对各维特征数据标准化。\n\n对A组:\n\n利用众数对各维数据缺失值填充\n对文本数据编码, '行业','企业类型','控制人类型','区域' 利用sklearn库的LabelEncoder().fit_transform()方法编码\n考虑到'注册时间'的数据较大，且和编码数据近似，故将2000年定为0年,依次类推。\n'注册资本'的特征利用公式$$ /frac{x-mu}{std} $$标准化\n\n合并AB\n\n对A组和B组企业ID取交集,然后将两组数据都有的ID部分合并，到此我们所有处理好的数据都合并为一个表，保存至encoded_all_data。\n\n分离flag为空的数据\n\n有标签数据占全部数据的77%,在精准模型诞生之前，如果采用某种手段将其余23%的flag补充上，势必会带来误差，即使误差很小，也改变了实际情况，以此为基础的模型学习的也不再是实际情况。77%（35648条）完整数据可用,基本满足后续模型需求。\n\nfeatures engineering\n\n现在我们的这个数据表格有23个特征列,数据维度比较高,对于neural_network模型来说是可接受的，但为了让我们的数据适应更多的模型,同时又具有很好的可解释性，需要对主要特征进行提取。以下统计图将各个特征与flag的情况可视化出来，橙色代表僵尸企业，蓝色代表正常企业。由图可见，纳税总额和净利润对分类作用最为明显，其次是主营业务收入，内部融资和贸易融资额度，股权融资成本，股权融资额度。而其他特征作用不明显，可以考虑舍弃，经过后面模型的验证，这样做确实是合理的。\n\n\n\n提取出这些特征，再次做统计图，可以更清晰看出各个特征对分类的贡献。\n\n\n\n提取出的这些特征\n\n\nclassification_model\n\nlogistic regression\n\nlogistic回归是一种广义线性回归（generalized linear model），与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有 wx + b，其中w和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将wx+b作为因变量，即y =wx+b,而logistic回归则通过函数L将wx+b对应一个隐状态p，p =L(wx+b),然后根据p 与1-p的大小决定因变量的值。如果L是logistic函数，就是logistic回归。简单来说，就是logistic回归会在线性回归后再加一层logistic函数的调用。logistic回归主要是进行二分类预测，Sigmod函数是最常见的logistic函数，因为Sigmod函数的输出的是是对于0~1之间的概率值，当概率大于0.5预测为1，小于0.5预测为0。\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,明天再写,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n模型训练结果:\n\n\n\nneural network\n\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,明天再写,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n模型训练结果:\n\n\n\nSVM\n\n,,,,,,\n\nK-means\n\n,,,,,,\nEnterprise image\n\nFind hyper-parameters about optimization\n,,,,,,,\n""], 'url_profile': 'https://github.com/HongdaChen', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 22, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 8, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '434 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SANJAY072000', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 22, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Quora-Question-Pair-Similarity\n\n1. Business Problem\n1.1 Description\n\n\nQuora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.\n\n\nOver 100 million people visit Quora every month, so it\'s no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n\n\n\nCredits: Kaggle\n\nProblem Statement\n\nIdentify which questions asked on Quora are duplicates of questions that have already been asked.\nThis could be useful to instantly provide answers to questions that have already been answered.\nWe are tasked with predicting whether a pair of questions are duplicates or not.\n\n1.2 Sources/Useful Links\n\nSource : https://www.kaggle.com/c/quora-question-pairs\n\nUseful Links\n\nDiscussions : https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb/comments\nKaggle Winning Solution and other approaches: https://www.dropbox.com/sh/93968nfnrzh8bp5/AACZdtsApc1QSTQc7X0H3QZ5a?dl=0\nBlog 1 : https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning\nBlog 2 : https://towardsdatascience.com/identifying-duplicate-questions-on-quora-top-12-on-kaggle-4c1cf93f1c30\n\n1.3 Real world/Business Objectives and Constraints\n\nThe cost of a mis-classification can be very high.\nYou would want a probability of a pair of questions to be duplicates so that you can choose any threshold of choice.\nNo strict latency concerns.\nInterpretability is partially important.\n\n2. Machine Learning Probelm\n2.1 Data\n2.1.1 Data Overview\n\nData will be in a file Train.csv\nTrain.csv contains 5 columns : qid1, qid2, question1, question2, is_duplicate\nSize of Train.csv - 60MB\nNumber of rows in Train.csv = 404,290\n\n2.1.2 Example Data point\n\n""id"",""qid1"",""qid2"",""question1"",""question2"",""is_duplicate""\n""0"",""1"",""2"",""What is the step by step guide to invest in share market in india?"",""What is the step by step guide to invest in share market?"",""0""\n""1"",""3"",""4"",""What is the story of Kohinoor (Koh-i-Noor) Diamond?"",""What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?"",""0""\n""7"",""15"",""16"",""How can I be a good geologist?"",""What should I do to be a great geologist?"",""1""\n""11"",""23"",""24"",""How do I read and find my YouTube comments?"",""How can I see all my Youtube comments?"",""1""\n\n2.2 Mapping the real world problem to an ML problem\n2.2.1 Type of Machine Leaning Problem\n\nIt is a binary classification problem, for a given pair of questions we need to predict if they are duplicate or not.\n\n2.2.2 Performance Metric\n\nSource: https://www.kaggle.com/c/quora-question-pairs#evaluation\n\nMetric(s):\n\nlog-loss : https://www.kaggle.com/wiki/LogarithmicLoss\nBinary Confusion Matrix\n\n2.3 Train and Test Construction\n\nWe build train and test by randomly splitting in the ratio of 70:30 or 80:20 whatever we choose as we have sufficient points to work with.\n\n'], 'url_profile': 'https://github.com/priyankamohan1001', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 22, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Personalized-Cancer-Diagnosis\n\n\n1. Business Problem\n\n\n\n\n\n\n1.1. Description\n\n\n\n\n\n\n\n Source: https://www.kaggle.com/c/msk-redefining-cancer-treatment/ \n Data: Memorial Sloan Kettering Cancer Center (MSKCC)\n Download training_variants.zip and training_text.zip from Kaggle. Context:\n Source: https://www.kaggle.com/c/msk-redefining-cancer-treatment/discussion/35336#198462 Problem statement : \n Classify the given genetic variations/mutations based on evidence from text-based clinical literature. \n\n\n\n\n\n\n\n1.2. Source/Useful Links\n\n\n\n\n\n\n\nSome articles and reference blogs about the problem statement\n\n\n\n\n\n\n\n\nhttps://www.forbes.com/sites/matthewherper/2017/06/03/a-new-cancer-drug-helped-almost-everyone-who-took-it-almost-heres-what-it-teaches-us/#2a44ee2f6b25\nhttps://www.youtube.com/watch?v=UwbuW7oK8rk \nhttps://www.youtube.com/watch?v=qxXRKVompI8\n\n\n\n\n\n\n\n\n1.3. Real-world/Business objectives and constraints.\n\n\n\n\n\n\n\n\nNo low-latency requirement.\nInterpretability is important.\nErrors can be very costly.\nProbability of a data-point belonging to each class is needed.\n\n\n\n\n\n\n\n\n2. Machine Learning Problem Formulation\n\n\n\n\n\n\n\n2.1. Data\n\n\n\n\n\n\n\n2.1.1. Data Overview\n\n\n\n\n\n\n\n\nSource: https://www.kaggle.com/c/msk-redefining-cancer-treatment/data\nWe have two data files: one conatins the information about the genetic mutations and the other contains the clinical evidence (text) that  human experts/pathologists use to classify the genetic mutations. \nBoth these data files are have a common column called ID\n \n  Data file's information:\n  \n\n      training_variants (ID , Gene, Variations, Class)\n      \n\n      training_text (ID, Text)\n      \n\n\n\n\n\n\n\n\n\n\n2.1.2. Example Data Point\n\n\n\n\n\n\n\ntraining_variants\n\nID,Gene,Variation,Class\n0,FAM58A,Truncating Mutations,1 \n1,CBL,W802*,2 \n2,CBL,Q249E,2 \n...\n training_text\n\nID,Text \n0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes (1). The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins (2). Although discovered almost 20 y ago (3, 4), CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells (5, 6) or as a tumor suppressor in others (7, 8). CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism (9). CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen (6). ...\n\n\n\n\n\n\n\n2.2. Mapping the real-world problem to an ML problem\n\n\n\n\n\n\n\n2.2.1. Type of Machine Learning Problem\n\n\n\n\n\n\n\n\n        There are nine different classes a genetic mutation can be classified into => Multi class classification problem\n\n\n\n\n\n\n\n\n\n2.2.2. Performance Metric\n\n\n\n\n\n\n\nSource: https://www.kaggle.com/c/msk-redefining-cancer-treatment#evaluation\nMetric(s):\n\nMulti class log-loss \nConfusion matrix \n\n\n\n\n\n\n\n\n2.2.3. Machine Learing Objectives and Constraints\n\n\n\n\n\n\n\n Objective: Predict the probability of each data-point belonging to each of the nine classes.\n\n Constraints:\n\n\nInterpretability\nClass probabilities are needed.\nPenalize the errors in class probabilites => Metric is Log-loss.\nNo Latency constraints.\n\n\n\n\n""], 'url_profile': 'https://github.com/priyankamohan1001', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 22, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RohanG12', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 22, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Feb 7, 2020', 'Python', 'Updated Feb 3, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 8, 2020']}"
"{'location': 'Pittsburgh , PA', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/avanish-fullstack', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/evanboddu', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '675 contributions\n        in the last year', 'description': ['iris-dataset-classification\nThis is the machine learning model which learns to classify three category of flowers, named Satosa, Versicolour and Virginica based on their sepal length , sepal width, petal length, petal width. Used Logistic regression as classification model\n'], 'url_profile': 'https://github.com/vishal-pandey', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kubilayerislik', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/priyankamohan1001', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Factors Governing Insurance Charges\n'], 'url_profile': 'https://github.com/shivinigam', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'Montreal, QC, Canada', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['mLabs\nImplemented various Machine Learning and Data Mining algorithms on 25 different datasets for classification and regression problems. Applied various data preprocessing, dimension reduction, hyperparameter search, validation and visualization techniques to gain valuable insights from the datasets.\n'], 'url_profile': 'https://github.com/yasheth', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Meachine-Learning\nApplied Principle Component Analysis and support vector machine (SVM) in a pipeline, used Python to predict heart diseaseUsed supervised learning methods: Logistic Regression adjust and compared it with pre-pruned Decision Tree and Random Forest\n'], 'url_profile': 'https://github.com/Yujia-Jacqueline-He', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Predicting_Attrition_with_Support_Vector_Machines\nThis simple project aims at predicting attrition with Support Vector Machine, using linear and Radial Basis kernels + a logistic regression. The projects is concluded by testing it (successfully) on a single employee.\nThis Project was done as a part of my time at LBS.\n'], 'url_profile': 'https://github.com/CnxLuc', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020']}","{'location': 'Irvine, California', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Hackathon_Portugese_Bank\n'], 'url_profile': 'https://github.com/anujapdixit', 'info_list': ['Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'R', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Feb 4, 2020', 'R', 'Updated Feb 8, 2020']}"
"{'location': 'United States / Cambridge MA / Lafayette LA', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BeauMeche', 'info_list': ['HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '2', 'Python', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'minneapolis', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Breast-Cancer-Prediction-using-classification-models\nClassifying Breast Cancer cells as Malignant or Benign using predictive modeling techniques like Decision Trees, K-Nearest-Neighbors and Logistic Regression, SVM and hyper-parameter tuning using GridSearch Cross Validation\n'], 'url_profile': 'https://github.com/vijay-kr', 'info_list': ['HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '2', 'Python', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/thejas-suvarna', 'info_list': ['HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '2', 'Python', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Brentwood, CA', 'stats_list': [], 'contributions': '331 contributions\n        in the last year', 'description': [""DESCRIPTION\nThere is record-breaking number of Americans with unsecured loans. In 2019, more than 19 million Americans had at least one unsecured personal loan. Personal lending is growing faster than credit card, auto, mortgage, and even student debt. With such incredible growth, FinTech firms are storming ahead of traditional loan processes. By using the latest machine learning techniques, these FinTech firms can continuously analyze large amounts of data and predict trends to optimize lending.\nSITUATION/TASK\nUsing Python to build and evaluate several machine learning models, predict credit risk using the machine learning algorithms to help banks and financial institutions predict anomalies, reduce risk cases, monitor portfolios, and provide recommendations on what to do in cases of fraud.\nAPPROACH\n\n\nMachine learning is the use of statistical algorithms to perform tasks such as learning from data patterns and making predictions.\n\n\nSupervised learning deals with labeled data, and there are two forms.\nIn supervised learning, the labels provide the correct answers.\n\n\nRegression is used to predict continuous variables.The regression model’s algorithms attempt to learn patterns that exist\namong factors given. If presented with new data, the model will make a prediction, based on previously learned patterns from\nthe dataset.\n\n\nClassification is used to predict discrete outcomes.The classification model’s algorithms attempts to learn patterns from the\ndata, and if the model is successful, gain the ability to make accurate predictions.\n\n\n\n\nUnsupervised learning algorithms work with datasets without labeled outcomes.\n\n\n\n\nTraining and testing groups from a given data set.\n\nTraining dataset to learn from it.\nTesting dataset to assess its performance.\n\n\n\nImplement the logistic regression, decision tree, random forest, and support vector machine algorithms.\n\nLogistic regression predicts binary outcomes, meaning that there are only two possible outcomes.\nDecision trees are used in decision analysis.\nRandom Forests does not have a complex tree like the ones created by decision trees, a random forest algorithm will sample the\ndata and build several smaller, simpler decision trees. Each tree is simpler because it is built from a random subset of features.\nSupport vector machine (SVM), like logistic regression, is a binary classifier. It can categorize samples into one of two\ncategories. There is a strict cutoff line that divides one classification from the other.\n\n\n\nInterpret the results of the logistic regression, decision tree, random forest, and support vector machine algorithms.\n\n\nCompare the advantages and disadvantages of each supervised learning algorithm.\n\n\nDetermine which supervised learning algorithm is best used for a given data set or scenario.\n\n\nUse ensemble and resampling techniques to improve model performance.\n\n\nRESULTS\n\n\nClassification Model\n\n\n\nDecision Tree\n\n\n\nLinear Regression\n\n\n\nTHINGS LEARNED\n\nExplain how a machine learning algorithm is used in data analytics.\nCreate training and test groups from a given data set.\nImplement the logistic regression, decision tree, random forest, and support vector machine algorithms.\nInterpret the results of the logistic regression, decision tree, random forest, and support vector machine algorithms.\nCompare the advantages and disadvantages of each supervised learning algorithm.\nDetermine which supervised learning algorithm is best used for a given data set or scenario.\nUse ensemble and resampling techniques to improve model performance.\n\nSOFTWARE/TOOLS\nPython and Scikit-learn\nSITUATION/TASK - CHALLENGE RESULTS\nBuilding and evaluating several machine learning models to assess credit risk, includes: Training a logistic regression classifier, calculating the balanced accuracy score, generating a confusion matrix and printing the classification report.\nSummary and analysis of the models’ performance:\nRandom OverSampling\n              pre       rec       spe        f1       geo       iba       sup\n\nhigh_risk       0.01      0.64      0.63      0.02      0.64      0.41        87\nlow_risk       1.00      0.63      0.64      0.77      0.64      0.40     17118\navg / total       0.99      0.63      0.64      0.77      0.64      0.40     17205\nIn random oversampling, instances of the minority class are randomly selected and added to the training set until the majority and minority classes are balanced. The accuracy score measures the performance of a classifier, so for a score of 64% in this model the performance of the classifier isn't that impressive. Though accuracy is not a good metric when the data is imbalanced. Precision and recall show low values as well, and a low precision means there are more false positives in the data. For the True Positive Rate(recall) it measures the classifier's completeness, and therefore a low value recall such as above, indicates a high number of false negatives\nSMOTE\n             pre       rec       spe        f1       geo       iba       sup\n\nhigh_risk       0.01      0.61      0.68      0.02      0.64      0.41        87\nlow_risk       1.00      0.68      0.61      0.81      0.64      0.41     17118\navg / total       0.99      0.68      0.61      0.80      0.64      0.41     17205\nWhile recall values in the SMOTE model, is a little better in the low-risk pool than in the random-oversampling, it is not by much. Like the Random Oversampling, precision is dismal, meaning in both risk pools meaning there are a lot of false positives (precision) and false negatives(recall) in this data as well. The balanced accuracy score is about 66%, not much greater than that in the previous model.\nIn both Oversampling examples above (SMOTE & Random Over Sampling) it is important to note that one model does not outperform the other. In fact, their metrics and performance are about the same. Plus, SMOTE is vulnerable to outliers.\nCluster Centroids Algorithm\n               pre       rec       spe        f1       geo       iba       sup\n\nhigh_risk       0.01      0.63      0.39      0.01      0.50      0.25        87\nlow_risk       1.00      0.39      0.63      0.56      0.50      0.24     17118\navg / total       0.99      0.39      0.63      0.56      0.50      0.24     17205\nSimilar to SMOTE is the Cluster Centroids Algorithm/Model. Centroids represents the clusters of generated data points of a majority class. With balanced accuracy metric approximating 66% the performance of the classifier is very similar to that of the SMOTE model.\nThe one conclusion is that resampling does not guarantee better results.\nSMOTEENN Algorithm\n               pre       rec       spe        f1       geo       iba       sup\n\nhigh_risk       0.01      0.72      0.59      0.02      0.65      0.43       101\nlow_risk       1.00      0.59      0.72      0.74      0.65      0.42     17104\navg / total       0.99      0.59      0.72      0.74      0.65      0.42     17205\nIn SMOTEENN we see that some of the metrics show an improvement. The recall for the high-risk pool example, is 72% that is the highest we have seen in all 4 sample models. The precision metrics is same as indicated in all 3 other models. The accuracy score is also approximately 66%.\nFinal Recommendation\nThe above metrics are so very similar throughout the different models, it is almost impossible to recommend one over the other.\nThat said, I would recommend the SMOTEENN algorithm since that combines aspects of both the oversampling and undersampling, which eliminates some of the challenges found in solely oversampling or undersampling alone.\n""], 'url_profile': 'https://github.com/aodoming', 'info_list': ['HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '2', 'Python', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Chandigarh,India', 'stats_list': [], 'contributions': '865 contributions\n        in the last year', 'description': ['ABOUT THE PROJECT\nThese are some eaxmples of applications of prediction models by application of deep learnining and machine learning applications.\nThe repo contains prediction examples based on job prediction.\nPrediction of jobs based on different parameters like : Age , Location ,resume ,experience , skills,top skills , Profile based classification .\nIt speaks about the non-numeric classification and prediction also .\nIt implements:\n1.) KNN\n2.) SVM\n3.)Collaborative filtering (hybrid model of deep+ML)\n2.)CNN model\n3.)Conditional assesment Python\nHow to use the script :\n-clone / git clone https://github.com/Anustup900/Machine-and-Deep-learning-Models.git\n-run age prediction .py a simple python script with input of int value of graduation year\n-get the out put of user age\n-run skillbasedpred.py predicts landing industry of user based on input string input of skill\n-run Recommender.py the recommendation model for similar user profile sharing based on input features by collaborative filtering\n-run feature processing and pred by parameters and encoder models for support scripting\n'], 'url_profile': 'https://github.com/Anustup900', 'info_list': ['HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '2', 'Python', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': [""A Machine Learning Regressor Model To Predict The Burned Area Of Forest Fires in the northeast region of Portugal by using meteorological data\nPoor forest management and firefighting techniques make Portugal especially\nvulnerable to wildfires as climate change makes hotter, longer summers more likely.\nA series of four initial deadly wildfires erupted across central Portugal in the afternoon of 17 June 2017 within minutes of each other,\nresulting in at least 66 deaths and 204 injured people - the largest loss of life due to wildfires in Portugal's history\nAn intense heat wave preceded the fires, with many areas of Portugal seeing temperatures in excess of 40 °C (104 °F)\nThe aim is to predict the burned area of forest fires, in the northeast region of Portugal,\nby using meteorological and other data (see details at: http://www.dsi.uminho.pt/~pcortez/forestfires)\nIn this ML kernel, I'll:\n\n\nPrepare the data for machine learning -\nThe target variable is very skewed towards 0.0 with no optimum value distribution, thus it will make sense to prepare the data for ML by modelling with the logarithm transform\n\n\nTrain a model using Random Forest and other Regressor models\n\n\nMeasure the accuracy of the model using RMSE and MSE\n\n\n""], 'url_profile': 'https://github.com/zuruoke', 'info_list': ['HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '2', 'Python', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['jupitercodes\nWith more than 3 years of experience as a data scientist, transcriptionist and translator.   Professional with high knowledge in biology, scientific analysis and management of statistical data. Focused on creating plots boxplot, dot plots, linear regression and anova.     Focused on learning programming languages like R, HTML, Python, Ruby, and C++.\n'], 'url_profile': 'https://github.com/Jupiter-Codes', 'info_list': ['HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '2', 'Python', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['ThinkStats2_MyAnswers\nMy answers to the python excercises in the book ThinkStats2. The book contains the following topics in statistics:\n1.Exploratory data analysis \n2.Distributions \n3.Probability mass functions \n4.Cumulative distribution functions \n5.Modeling distributions \n6.Probability density functions \n7.Relationships between variables \n8.Estimation \n9.Hypothesis testing \n10.Linear least squares \n11.Regression \n12.Time series analysis \n13.Survival analysis \n14.Analytic methods \n'], 'url_profile': 'https://github.com/sarthakbabbar3', 'info_list': ['HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '2', 'Python', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ronakkjain', 'info_list': ['HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '2', 'Python', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ravikanth99', 'info_list': ['HTML', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'HTML', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '2', 'Python', 'Updated Jun 7, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Binary-Classification-Python-vs-scikit-learn\nSeveral regression and binary classification algorithms are available in scikit-learn.\nIn this part I am first classifying the quality of wine using map function (threshold)\nof python then I used LabelBinarizer preprocessing function of scikit-learn method to\nclassify wine either 0 or 1.\n'], 'url_profile': 'https://github.com/sunpan76', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Updated Feb 4, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'R', 'Updated Jun 18, 2020', 'Updated Feb 4, 2020']}","{'location': 'Ghaziabad', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prateekpr', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Updated Feb 4, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'R', 'Updated Jun 18, 2020', 'Updated Feb 4, 2020']}","{'location': 'Texas', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/arpitapd', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Updated Feb 4, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'R', 'Updated Jun 18, 2020', 'Updated Feb 4, 2020']}","{'location': 'New jersey', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Support_Vector_Machine\nA support vector machine (SVM) is machine learning algorithm that analyzes data for classification and regression analysis. SVM is a supervised learning method that looks at data and sorts it into one of two categories. An SVM outputs a map of the sorted data with the margins between the two as far apart as possible.\n'], 'url_profile': 'https://github.com/Dishtid', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Updated Feb 4, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'R', 'Updated Jun 18, 2020', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Overview\nThe goal of this project is to use Pushshift\'s Reddit API to retrieve data from 2 different subreddits and build an optimized classification model to predict each post\'s origin subreddit\nSource of the data\n\n\nAskThe_Donald, a pro-Trump subreddit.\n\n\nEnough Trump Spam, an anti-Trump subreddit.\n\n\nData Gathering\nUsing the api, I gathered 5000 latest posts from each subreddit with purpose to create a balanced dataset, so the full dataset has  10,000 posts from both subreddits in total\nEDA\nData Cleaning\nSubreddit\nSince the purpose of the project is to classify subreddit, I changed column ""subreddit"" from subreddit name to 0 for anti-Trump, Enough Trump Spam and 1 for pro-Trump, AskThe_Donald.\nTitle and Self text\nThe column ""Title"" shows the title of the post, and ""self text"" shows the actual text in the post. There are many removed posts from reddit, which results in many null values in ""self text"" column. I filled null column with \'999\', and combined title and self text to 1 ""combined"" column\nStop words\nIn addition to import stopwords from sklearn, I manually created a stop word lists include words that are frequently shown in posts from both subreddits, pronouns, verbs, adjectives that have no important meanings.\nData Visualization\nThrough data visualization, I found some interesting relationships between users from both groups. For example, in Enough Trump Spam, there are significantly less text in posts, and the use of total vocabularies in the posts are also smaller than posts from AskThe_Donald. One cause for such imbalance might be due to the large amount memes, images, and other links that are anti-Trump, authors from Enough Trump Spam simply share these images and links instead of writing text. On the other side, there aren\'t as many outside resource for pro-Trump authors, so they need to use more words to explain their opinions.\nGraph\n\n\n\n\n\n\nModel\nFeature\nI used ""subreddit"" in 1 and 0 as y, and ""combination"", which combined title and self text as X. I splitted the data into 70% train data and 30% test.\nModel 1: Logistic Regression CountVectorizer\nUsing pipeline and GridsearchCV, I chose the beset parameter from the range and train a model based on the best parameter. In the end, I received 0.92 for train accuracy and 0.877 for test accuracy. The model predicted 79% of posts from Enough Trump Spam correctly, and 96% of posts from AskThe_Donald correctly.\n\nModel 2: Logistic Regression TFIDFVectorizer\nIn this model, I used the same parameter as CountVectorizer. By using its best parameter, I received 0.93 for train accuracy and 0.912 for test accuracy. The model predicted 87% of posts from Enough Trump Spam correctly, and 95% of posts from AskThe_Donald correctly.\n\nModel 3: Voting Classifier TFIDFVectorizer\nI used Adaboost, Gradient Boost, and Random Forest in the Voting Classifier to get the best model with TFIDFVectorizer. I received 0.93 for train acuracy and 0.91 for test accuracy. The model predict 87% of posts from Enough Trump Spam correctly, and 95% of posts from AskThe_Donald correctly.\n\nConclusion\n\nAll models perform relatively well to separate class.\nModel 2,3 achieve higher accuracy, so they fit well for pro-Trump authors.\nAll Models achieves high specificity, so they are optimal for anti-Trump authors.\nLow number of  text posts in anti-Trump subreddit might cause inaccuracy in prediction model. We need further research to identify if short on text is commonly seen or caused by small sample size\n\n'], 'url_profile': 'https://github.com/kevinlu1996', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Updated Feb 4, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'R', 'Updated Jun 18, 2020', 'Updated Feb 4, 2020']}","{'location': 'Ghaziabad', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prateekpr', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Updated Feb 4, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'R', 'Updated Jun 18, 2020', 'Updated Feb 4, 2020']}","{'location': 'Patna India', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Arvindkumarsahu', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Updated Feb 4, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'R', 'Updated Jun 18, 2020', 'Updated Feb 4, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['Filtering-sexist-text\nThis project is aimed at converting offensive, sexist text into pleasant or funny  sentences.\n'], 'url_profile': 'https://github.com/littlemisslilycane', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Updated Feb 4, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'R', 'Updated Jun 18, 2020', 'Updated Feb 4, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Project Topic : Multivariate-analysis-on-factors-influencing-life-expectancy\nData Set Description :\nThe dataset is taken from the Global Health Observatory (GHO) data repository under World Health Organization (WHO) keeps track of the health status as well as many other related factors for all countries. The data is from year 2000-2015 for 193 countries.\nProblem Statement :\nThe various factors affecting life expectancy like demographic variables, income composition, mortality rates, immunization, human development index, social and economic factors.\nQuestions to be addressed :\n1. How does immunization affect life expectancy rate?\n2. Which country should be given priority in order to improvise their life expectancy rate?\n3. Does life expectancy has any correlation with eating habits, lifestyle, exercise, smoking or drinking alcohol?\n4. What measures should a country take in order to increase its healthcare expenditure to improve its average lifespan?\nData set Dictionary :\n\n\n\nVariable Name\nDescription\nDatatype\nAccepts Null Values\n\n\n\n\nCountry\nCountry Name\nObject\nN\n\n\nYear\nYear\nObject\nN\n\n\nStatus\nDeveloped or Developing\nObject\nN\n\n\nLife Expectancy\nLife expectancy in age\nObject\nN\n\n\nAdult Mortality\nProbability of dying between 15 and 60 years per 1000 population\nObject\nN\n\n\ninfant deaths\nNumber of infant deaths per 1000 population\nObject\nN\n\n\nAlcohol\nrecorded per capita consumption(in litres)\nObject\nN\n\n\npercentage expenditure\nExpendidture on health as per GDP(%)\nObject\nN\n\n\nHepatitis B\nImmunization coverage among 1 year old(%)\nObject\nN\n\n\nMeasles\nNumber of reported cases per 1000 population\nObject\nN\n\n\nBMI\nAverage BMI of entire population\nObject\nN\n\n\nunder-five deaths\nNumber of under five deaths per 1000 population\nObject\nN\n\n\nPolio\nImmunization coverage amoung one year olds(%)\nObject\nN\n\n\nTotal Expenditure\nGovernment expenditure of health as a percentage of total govt. expenditure(%)\nObject\nN\n\n\nDiphtheria\nImmunization coverage amoung one year old(%)\nObject\nN\n\n\nHIV/AIDS\nDeaths per 1000 population\nObject\nN\n\n\nGDP\nper capita(USD)\nObject\nN\n\n\nPopulation\npopulation of the country\nObject\nN\n\n\nthinness 10-19 years\nThinness amomg children from age 10-19(%)\nObject\nN\n\n\nthinness 5-9 years\nThinness amomg children from age 5-9(%)\nObject\nN\n\n\nIncome composition of resources\nIndex ranging from 0-1\nObject\nN\n\n\nSchooling\nNumber of years of schooling\nObject\nN\n\n\n\n'], 'url_profile': 'https://github.com/poojadesai25', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Updated Feb 4, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'R', 'Updated Jun 18, 2020', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['datasciencehandson\n'], 'url_profile': 'https://github.com/HarshNagpal84', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Updated Feb 5, 2020', 'Python', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 9, 2020', '1', 'Updated Feb 4, 2020', '1', 'Python', 'Updated Feb 5, 2020', 'R', 'Updated Jun 18, 2020', 'Updated Feb 4, 2020']}"
"{'location': 'Greater Boston Area', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Estimating Median Home Values Based on Societal Variables\n\nCreated a machine learning model that could estimate a city\'s median home value based on factors within the city, such as average household income, school test scores and more.\nUtilized R programming language to webscrape information from over 1800 cities on the website ""www.AreaVibes.com""\nMultiple linear regression was determined to be the most effective machine learning technique to solve this challenge\nAnalyzed 10+ predictor variables to order to ascertain which were the most important in predicting the city\'s median home values\n\n\nBoxplots showing the distribution of median home values within the 5 states represented within the dataset\n'], 'url_profile': 'https://github.com/jgmonteirohub', 'info_list': ['HTML', 'Updated Sep 1, 2020', '1', 'Python', 'Updated Feb 12, 2021', '2', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 5, 2020']}","{'location': 'Washington D.C', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShinojJerald', 'info_list': ['HTML', 'Updated Sep 1, 2020', '1', 'Python', 'Updated Feb 12, 2021', '2', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 5, 2020']}","{'location': 'Ghaziabad', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prateekpr', 'info_list': ['HTML', 'Updated Sep 1, 2020', '1', 'Python', 'Updated Feb 12, 2021', '2', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/imran789924', 'info_list': ['HTML', 'Updated Sep 1, 2020', '1', 'Python', 'Updated Feb 12, 2021', '2', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SHRUTISINGHAL26', 'info_list': ['HTML', 'Updated Sep 1, 2020', '1', 'Python', 'Updated Feb 12, 2021', '2', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Red_wine_Portugal\nThis Project was made within the scope of the Big Data discipline.\nOne of the objectives of this project is to learn to work with the pyspark library, learning the differences between pyspark (parallel distribution) and pandas.\nThe dataset is a dataset referring to the red wine of Portugal, where it talks about the characteristics of the wine. An exploratory analysis is made to the dataset that includes the descriptions of variables and the creation of graphs, where we observe the correlation between variables.\nThen, models are created to predict the quality of red wine. Being the models, logistic regression and random forest.\n'], 'url_profile': 'https://github.com/rubenAlbuquerque', 'info_list': ['HTML', 'Updated Sep 1, 2020', '1', 'Python', 'Updated Feb 12, 2021', '2', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 5, 2020']}","{'location': 'Irvine, California', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Airline_Sentiment_Analysis\n'], 'url_profile': 'https://github.com/anujapdixit', 'info_list': ['HTML', 'Updated Sep 1, 2020', '1', 'Python', 'Updated Feb 12, 2021', '2', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Customer-lifetime-value-prediction\nFor an Auto Insurance company, we need to predict the conditions affecting customer lifetime value(CLV). CLV is the total revenue the client will derive from their entire relationship with a customer. Here we can see that as the name suggests we need to predict the customer lifetime value for each customer so as to make sure how much benefit each customer can repay to the company in exchange of the benefits he or she receives. Approach: We do a regression model to find out how and why the clv gets affected and how to tackle clv so that the company can benefit and also cluster the train dataset to understand behaviour of each segment. We start so by taking clv as the dependent variable and the whole model based on this variable.\n'], 'url_profile': 'https://github.com/Niks0107', 'info_list': ['HTML', 'Updated Sep 1, 2020', '1', 'Python', 'Updated Feb 12, 2021', '2', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 5, 2020']}","{'location': 'Ireland', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['A-MapReduce-Approach-to-Weather-and-Exchange-Rate-Analysis-\nData manipulation like attribute renaming, record filtering, missing record update and so on, were carried out on weather and exchange rate data sourced from Marine Institute and the Central Bank of Ireland respectively using Anaconda Navigator (v. 5.3, Jupyter Notebook, Python 3.6), R Studio Packages (v. 1.1.456) and Microsoft Office Excel (v.2016).\nUsing Java Programming Language, map and reduce functions were defined; the two datasets were mapped, Linear and Multiple Linear Regression analysis were carried out to establish the relationship between weather, some of its parameters and euro exchange rates for six countries.\nAll data were analyzed in parallel, using installed Hadoop on a Linux Ubuntu 16.4 instance running on OpenStack. From the analysis, it was discovered that wind direction and sea temperature have a positive correlation with Euro exchange rates for Australian Dollar, Great Britain Pounds, Hong Kong Dollars, Japanese Yen and US Dollars while Air Pressure has a negative correlation with the same. Also, air temperature has a negative correlation with Euro exchange rates for Great Britain Pounds and US Dollars but has a positive correlation with the Euro exchange rates for Hong Kong Dollars, Australian Dollars and Japanese Yen.\n'], 'url_profile': 'https://github.com/popeoba', 'info_list': ['HTML', 'Updated Sep 1, 2020', '1', 'Python', 'Updated Feb 12, 2021', '2', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 13, 2020', '1', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Updated Feb 5, 2020']}",
