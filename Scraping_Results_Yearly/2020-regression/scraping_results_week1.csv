"{'location': 'Clemson, SC, USA', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['ResNetRegression\nOptimal deep residual regression model is built for nonliear regression. Details could be found at\nChen, D.; Hu, F.; Nian, G.; Yang, T. Deep Residual Learning for Nonlinear Regression. Entropy 2020, 22, 193.\n'], 'url_profile': 'https://github.com/DowellChan', 'info_list': ['11', 'Python', 'Updated Feb 10, 2020', '11', 'MATLAB', 'Updated Nov 19, 2020', '9', 'Updated Jun 5, 2020', 'Python', 'Updated Oct 20, 2020', '3', 'Python', 'Updated Dec 6, 2020', '8', 'Python', 'Updated Jan 2, 2020', '3', 'Jupyter Notebook', 'Updated Jul 7, 2020', '2', 'Jupyter Notebook', 'Updated Jan 1, 2020', '2', 'Python', 'MIT license', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jul 26, 2020']}","{'location': 'Baltimore', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': [""OCT preprocessing:  Fully Convolutional Boundary Regression for Retina OCT Segmentation\nIntroduction\n\n\nThe code provides image reading from raw Spectralis(.vol) and Cirrus(.img) and Matlab files (.mat). It supports\nretina flatten, cropping, reconstruction and visualization e.t.c\nWe provide Matlab preprocessing steps and results in the papers:\nFully Convolutional Boundary Regression for Retina OCT Segmentation (MICCAI2019)\nDeep learning based topology guaranteed surface and MME segmentation of multiple sclerosis subjects from retinal OCT (BOE2019)\nThe paper's source code is patented and licensed thus will not be publicly available at current stage.\nGetting Started\nPrepare dataset\nDiabetic macular edema dataset (download)\nHealthy and Multiple Sclerosis dataset (download)\nFlattening retina and save images\nHealthy and MS subjects\n\n\nChange  ./hc/filename.txt  and ./hc/segname.txt to contain your downloaded data volumes and delineations\n\n\nRun ./Scripts/generate_hc_train('./hc/filename.txt','./hc/segname.txt')\n\n\nDME\n\nChange  filenames in ./Scripts/generate_dme_train.m to contain your downloaded data and run\n\nEach Bscans will be saved into 2D images (.png) and its manual labels are in Json format (.txt).\n# The python code for reading manual labels\n# self.labellist = sorted(list(Path('Your label path').glob('*.txt')))\nwith open(str(self.labellist[idx]),'r') as f:\n    dicts = json.loads(f.read())\nbds = np.array(dicts['bds'], dtype=np.float) - 1 \nmask = np.array(dicts['lesion'])\n\nMICCAI2019 results\n\n\nDownload the results and unzip it under the folder Evaluation\n\n\nHealthy and MS evaluation: Run hc_eval.m\nTraining: hc09-hc14 and ms13-ms21 (6 healthy and 9 MS, hc09, ms13, ms14 can be used for validation)\nTesting: hc01-hc08 and ms01-ms12 (8 healthy and 12 MS)\n\n\nDME evaluation: Run dme_eval.m\nStandard 50%-50% split are used as in the literature with a fixed training epoch.\n\n\nCitation\nIf you are using the healthy and MS dataset, please cite:\n@article{he2019retinal,\n  title={Retinal layer parcellation of optical coherence tomography images: Data resource for multiple sclerosis and healthy controls},\n  author={He, Yufan and Carass, Aaron and Solomon, Sharon D and Saidha, Shiv and Calabresi, Peter A and Prince, Jerry L},\n  journal={Data in brief},\n  volume={22},\n  pages={601--604},\n  year={2019},\n  publisher={Elsevier}\n}\n\n@inproceedings{he2019fully,\n  title={Fully Convolutional Boundary Regression for Retina OCT Segmentation},\n  author={He, Yufan and Carass, Aaron and Liu, Yihao and Jedynak, Bruno M and Solomon, Sharon D and Saidha, Shiv and Calabresi, Peter A and Prince, Jerry L},\n  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},\n  pages={120--128},\n  year={2019},\n  organization={Springer}\n}\n\n@article{he2020structured,\n  title={Structured layer surface segmentation for retina OCT using fully convolutional regression networks},\n  author={He, Yufan and Carass, Aaron and Liu, Yihao and Jedynak, Bruno M and Solomon, Sharon D and Saidha, Shiv and Calabresi, Peter A and Prince, Jerry L},\n  journal={Medical Image Analysis},\n  pages={101856},\n  year={2020},\n  publisher={Elsevier}\n}\n\n""], 'url_profile': 'https://github.com/heyufan1995', 'info_list': ['11', 'Python', 'Updated Feb 10, 2020', '11', 'MATLAB', 'Updated Nov 19, 2020', '9', 'Updated Jun 5, 2020', 'Python', 'Updated Oct 20, 2020', '3', 'Python', 'Updated Dec 6, 2020', '8', 'Python', 'Updated Jan 2, 2020', '3', 'Jupyter Notebook', 'Updated Jul 7, 2020', '2', 'Jupyter Notebook', 'Updated Jan 1, 2020', '2', 'Python', 'MIT license', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jul 26, 2020']}","{'location': 'China', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': [""PSO-IPSO-LSTM-regression\npython code，this project use pso and ipso to optim lstm's hyperparams, include learning rate,hidden-nodes and training epoch number. and finally use ipso-lstm for power load forcast.contact me 2919218574@qq.com for this code\npython tensorflow1.x 提出一种改进的粒子群算法ipso，用于lstm迭代次数，学习率，隐含层节点数的寻优，最后将ipso-lstm用于电力负荷的回归预测，以头一天平均温度、最高温度、最低温度、相对湿度、星期类型、24个时刻的负荷与预测日平均温度、最高温度、最低温度、相对湿度、星期类型，共34个特征作为输入，以预测日24个时刻的电力负荷作为输出，建立34输入24输出的预测模型。相关结果可以看我csdn博客：https://blog.csdn.net/qq_41043389/article/details/103765363\nmain1.py 为将pso与改进pso用于标准函数极值寻优\nmain2.py 为没有优化的两隐含层lstm\nmain3.py 为将pso用于优化两层lstm\nmain4.py 为将ipso用于优化两层lstm\nmain5.py 为将lstm,pso_lstm,ipso_lstm得到的结果画图\n需要代码的加我qq2919218574\n""], 'url_profile': 'https://github.com/fish-kong', 'info_list': ['11', 'Python', 'Updated Feb 10, 2020', '11', 'MATLAB', 'Updated Nov 19, 2020', '9', 'Updated Jun 5, 2020', 'Python', 'Updated Oct 20, 2020', '3', 'Python', 'Updated Dec 6, 2020', '8', 'Python', 'Updated Jan 2, 2020', '3', 'Jupyter Notebook', 'Updated Jul 7, 2020', '2', 'Jupyter Notebook', 'Updated Jan 1, 2020', '2', 'Python', 'MIT license', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jul 26, 2020']}","{'location': 'Gujarat, India', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['House-Price-Prediction\nProject implementing linear regression\nwhich predicts house price based on the information you provide\nOpen ""About the project"" for development details\n'], 'url_profile': 'https://github.com/pranavkutty', 'info_list': ['11', 'Python', 'Updated Feb 10, 2020', '11', 'MATLAB', 'Updated Nov 19, 2020', '9', 'Updated Jun 5, 2020', 'Python', 'Updated Oct 20, 2020', '3', 'Python', 'Updated Dec 6, 2020', '8', 'Python', 'Updated Jan 2, 2020', '3', 'Jupyter Notebook', 'Updated Jul 7, 2020', '2', 'Jupyter Notebook', 'Updated Jan 1, 2020', '2', 'Python', 'MIT license', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jul 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '328 contributions\n        in the last year', 'description': [""Random Forest BTC\nUses a random forest (RF) regression model to predict bitcoin prices 3 days ahead.\nTrained on data from 2010-2019.\nFeatures\n\nPrediction of:\n\nBTC price in USD 3 days ahead of input BTC data\n\n\nVisualization of:\n\nRF model fit to test and training data\n\n\n\n\n\n\nBasic Usage\nInstallation\nCreate venv and install requirements:\npython3 -m venv <venv-name>\nsource <venv-name>/bin/activate\ncd front/\nmake install\n\nNote: If mac users receive SSL: CERTIFCATE_VERIFY_FAILED URLError, please run the following commands\nin terminal to Install Certificates.command file:\nopen /Applications/Python\\ 3.7/Install\\ Certificates.command\nThis will allow the scraping to work for current day.\nCurrent predictions\nRun the front/Main.py file or equivalently:\ncd front/\nmake run\n\nload the resulting http://127.0.0.1:5000/ address in your browser.\nThe resulting prediction is based on the current day's BTC data.\nTo input custom data point, click on the manual input tab and fill in the appropriate fields.\nRetrain model\nTo train your own model, and view prediction results, edit this part of the script:\n    RFregressor = RandomForestRegressor(20)\n    RFregressor.build_forest(X_train, y_train)\n    y_pred = regressor.predict(X_test)\n    print(y_pred)\n\nwhere 20 in this case represents number of trees in the forest, X_train and X_test lists of dictionaries, each data point as a dictionary\nand y_pred a list of numbers, representing predictions of the tree.\nCommand line\nrun: python random_forest_regressor.py -{flags} (default behavior is to train a new model)\n\ntraining:\n\n--output file name where model will be saved, should end in .pkl (default: ra_model{utc_time}.pkl)\n--days number of days ahead to predict (default: 3)\n\n\nloading previous model:\n\n--model pkl file name of previous model to be loaded\n\n\n\nResults\nWithout retraining our model, to see the results based on 20 trees, bootstrap sample of original\ntraining size, and considering all features, simply run the random_forest_regressor.py file in the main folder.\nThese results should match those shown in the performance.txt file\nTests\nRun the test_forest_pytest.py file.\nFile explanations\nrandom_forest_regressor.py: the main backend source code.\nfront: contains main front-end source code.\nfinal_forest_drop0.pkl: our final saved model in pickle form\n(where we removed most of the the data points when bitcoin had 0 value).\nhyperparam_scripts: contains\nour scripts used to find ideal parameters.\nfeatures: combined to create raw_csvs files.\nother_models: other models used to compare with our own.\nperformance.txt: comparison of the errors for those models and our own.\nAcknowledgements\nWe used the following for our code:\nflask, sklearn, collections, matplotlib, pandas, numpy, and blockchain\nAuthors\nRyan Shuey, Evan Truong, & Elle McFarlane\n""], 'url_profile': 'https://github.com/ellemcfarlane', 'info_list': ['11', 'Python', 'Updated Feb 10, 2020', '11', 'MATLAB', 'Updated Nov 19, 2020', '9', 'Updated Jun 5, 2020', 'Python', 'Updated Oct 20, 2020', '3', 'Python', 'Updated Dec 6, 2020', '8', 'Python', 'Updated Jan 2, 2020', '3', 'Jupyter Notebook', 'Updated Jul 7, 2020', '2', 'Jupyter Notebook', 'Updated Jan 1, 2020', '2', 'Python', 'MIT license', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jul 26, 2020']}","{'location': 'China', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['-rnn-each-types-of-rnn-for-regression\n利用各种循环网络进行回归拟合，包括rnn，lstm，nlstm，bilstm\n'], 'url_profile': 'https://github.com/fish-kong', 'info_list': ['11', 'Python', 'Updated Feb 10, 2020', '11', 'MATLAB', 'Updated Nov 19, 2020', '9', 'Updated Jun 5, 2020', 'Python', 'Updated Oct 20, 2020', '3', 'Python', 'Updated Dec 6, 2020', '8', 'Python', 'Updated Jan 2, 2020', '3', 'Jupyter Notebook', 'Updated Jul 7, 2020', '2', 'Jupyter Notebook', 'Updated Jan 1, 2020', '2', 'Python', 'MIT license', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jul 26, 2020']}","{'location': 'MEERUT,UP', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['ML-Project-2\nMy second ML project from kaggle.(Based on Regression)\nproblem is based on Regression in which I applied the following algorithm:\nMultiple Linear Regression.\nANN.\nXGBRegressor\nRandom forest regressor\nI got best Result using XGBRegressor(xgboost).(score : 2.61187(evaluation using Root Mean Squared Error metric))\nproblem Description and datasets can be found here:\nhttps://www.kaggle.com/c/chh-ola\n'], 'url_profile': 'https://github.com/VASIL-ANSARI', 'info_list': ['11', 'Python', 'Updated Feb 10, 2020', '11', 'MATLAB', 'Updated Nov 19, 2020', '9', 'Updated Jun 5, 2020', 'Python', 'Updated Oct 20, 2020', '3', 'Python', 'Updated Dec 6, 2020', '8', 'Python', 'Updated Jan 2, 2020', '3', 'Jupyter Notebook', 'Updated Jul 7, 2020', '2', 'Jupyter Notebook', 'Updated Jan 1, 2020', '2', 'Python', 'MIT license', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jul 26, 2020']}","{'location': 'Hong Kong', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/clareyan', 'info_list': ['11', 'Python', 'Updated Feb 10, 2020', '11', 'MATLAB', 'Updated Nov 19, 2020', '9', 'Updated Jun 5, 2020', 'Python', 'Updated Oct 20, 2020', '3', 'Python', 'Updated Dec 6, 2020', '8', 'Python', 'Updated Jan 2, 2020', '3', 'Jupyter Notebook', 'Updated Jul 7, 2020', '2', 'Jupyter Notebook', 'Updated Jan 1, 2020', '2', 'Python', 'MIT license', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jul 26, 2020']}","{'location': 'Palo Alto', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': ['Generalized Linear Models in Python\nLast update: January 2020.\n\nA generalized linear model fits an exponential family distribution with a linear model. The resulting optimization problem is convex when the natural parameterization is used.\nExponential Family Distributions\nWe assume the response is generated from a distribution parameterized by ,\n\nHere  is the natural parameter,  is the sufficient statistic, and  is the log partition function. Using a linear model with a canonical link function (i.e. natural parameterization), we suppose\n\nNote that here .\nFisher Scoring\nThe model is fit using maximum likelihood. We take a natural gradient step using the Fisher information in ,\n\nNotice that by the chain rule, we have the following score and Hessian of the log-likelihood.\n\nThe Fisher information matrix with respect to the -th row of  is then the expected value of a constant (i.e. the Hessian has no dependence on observations ), so\n\nNotice this coincides with a Newton-Raphson step.\nReferences\n[1] Nelder, J.A., and Wedderburn, R.W.M. (1972). Generalized Linear Models. Journal of the Royal Statistical Society. Series A (General) 135, 370–384.\n\nAppendix\nHere we list useful results of exponential families.\nResult 1.  are sufficient statistics for the parameters .\n\nResult 2. The gradients of the log partition function yield moments of the sufficient statistics. First recall that\n\nNow observe that\n\nSimilarly it can be shown that \n\nResult 3. The negative log-likelihood of an exponential family distribution is always convex with respect to the natural parameters. This is because the Hessian is a constant positive semi-definite matrix in this case, coinciding with the variance of the sufficient statistics and with no dependence on observations.\n\n'], 'url_profile': 'https://github.com/tonyduan', 'info_list': ['11', 'Python', 'Updated Feb 10, 2020', '11', 'MATLAB', 'Updated Nov 19, 2020', '9', 'Updated Jun 5, 2020', 'Python', 'Updated Oct 20, 2020', '3', 'Python', 'Updated Dec 6, 2020', '8', 'Python', 'Updated Jan 2, 2020', '3', 'Jupyter Notebook', 'Updated Jul 7, 2020', '2', 'Jupyter Notebook', 'Updated Jan 1, 2020', '2', 'Python', 'MIT license', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jul 26, 2020']}","{'location': 'Germany', 'stats_list': [], 'contributions': '1,781 contributions\n        in the last year', 'description': ['Regression\nA collection of regressors\n'], 'url_profile': 'https://github.com/hanyas', 'info_list': ['11', 'Python', 'Updated Feb 10, 2020', '11', 'MATLAB', 'Updated Nov 19, 2020', '9', 'Updated Jun 5, 2020', 'Python', 'Updated Oct 20, 2020', '3', 'Python', 'Updated Dec 6, 2020', '8', 'Python', 'Updated Jan 2, 2020', '3', 'Jupyter Notebook', 'Updated Jul 7, 2020', '2', 'Jupyter Notebook', 'Updated Jan 1, 2020', '2', 'Python', 'MIT license', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jul 26, 2020']}"
"{'location': 'Thakurli', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/myselfamit', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'HTML', 'Updated Dec 31, 2019', 'JavaScript', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 6, 2020', 'M', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'egypt', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['regression\n'], 'url_profile': 'https://github.com/Amrsaadmahmoud', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'HTML', 'Updated Dec 31, 2019', 'JavaScript', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 6, 2020', 'M', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mohan0406', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'HTML', 'Updated Dec 31, 2019', 'JavaScript', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 6, 2020', 'M', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'New York, New York', 'stats_list': [], 'contributions': '174 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lmeninato', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'HTML', 'Updated Dec 31, 2019', 'JavaScript', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 6, 2020', 'M', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'Ireland', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/raoulbia', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'HTML', 'Updated Dec 31, 2019', 'JavaScript', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 6, 2020', 'M', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/bhandaryshefali', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'HTML', 'Updated Dec 31, 2019', 'JavaScript', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 6, 2020', 'M', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Applied-Regression-Analysis\n'], 'url_profile': 'https://github.com/SuyiWu', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'HTML', 'Updated Dec 31, 2019', 'JavaScript', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 6, 2020', 'M', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/maitrayanGhosh', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'HTML', 'Updated Dec 31, 2019', 'JavaScript', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 6, 2020', 'M', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'Tehran, Iran', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Regression\nIt is the project of Scientific Computation Course, May 2019.\nThis is a regression algorithm by U and V matrices obtained from SVD.\n'], 'url_profile': 'https://github.com/fatemehadadi', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'HTML', 'Updated Dec 31, 2019', 'JavaScript', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 6, 2020', 'M', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'BANGALORE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Paulamidas', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'HTML', 'Updated Dec 31, 2019', 'JavaScript', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 6, 2020', 'M', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}"
"{'location': 'Delhi', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Multiple_Linear_Regression\nThis is Supervised Maching Learning Model on Multiple Linear Regression on a small dataset of 50 Startups.\n'], 'url_profile': 'https://github.com/amitmittal1005', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Bharat-QBE', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Geely-Automation-Pricing-Model\nLinear Regression:-\nGeely Automotive Pricing Model to check which factors affect the price.\nBy Solon Kumar Das\nProblem Statement:\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\nWhich variables are significant in predicting the price of a car.\nHow well those variables describe the price of a car.\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the american market.\n\nBusiness Goal:\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\nData:\nCarPrice_Assignment.csv : Contains all the relevent specification about the cars in the US market and their pricing .\n'], 'url_profile': 'https://github.com/solondas007', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'Richardson Texas', 'stats_list': [], 'contributions': '266 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression\nThis  folder consists of two projects worked under Linear Regression by me.\nProject 1 :\nUSA_Housing:\n• Implemented Linear Regression of USA_Housing Dataset to find out the coefficients that determine the price of a square feet or a house in a particular locality.\n•\tTogether a boston housing dataset(real world dataset) has been analyzed and insights have been drawn out from that. The MAE,MSE,RMSE has been determined through this.\nProject 2:\nE-commerce insights:\n•\tImplemented Multivariate Linear Regression model to facilitate decision making on a Kaggle E-commerce dataset using Python and obtained an accuracy of 98%\n•\tProjected the insights and predictors correlation in Python 2D Visualization.\nProject 3:\nPerformance of Linear Regression on a simple Salary Dataset\n'], 'url_profile': 'https://github.com/SSR-ds', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': [""Lead-Scoring-Case-Study\nLogistic Regression\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses.\xa0\n\xa0\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%.\xa0\n\xa0\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example,\xa0if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as ‘Hot Leads’. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone.\nThere are a lot of leads generated in the initial stage but only a few of them come out as paying customers from the bottom. In the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.\n\xa0\nX Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be\xa0around 80%.\nThere are quite a few goals for this case study which are as follows:\nBuild a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.\nThere are some more problems presented by the company which your model should be able to adjust to if the company's requirement changes in the future so you will need to handle these as well. These problems are provided in a separate doc file. Please fill it based on the logistic regression model you got in the first step. Also, make sure you include this in your final PPT where you'll make recommendations.\nData:\nThe data provided (leads dataset) are from the past with around 9000 data points. This dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not. The target variable, in this case, is the column ‘Converted’ which tells whether a past lead was converted or not wherein 1 means it was converted and 0 means it wasn’t converted.\n""], 'url_profile': 'https://github.com/solondas007', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '240 contributions\n        in the last year', 'description': [""DC-Residence-Price-Prediction\nPracticing data preprocessing for ML, linear regression, polynomial regression\nAuthor: Nick Solovyev\nDataset: DC Residential Properties | Kaggle \nAttribution: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, by Aurelien Geron (O'Reilly). Copyright 2019 Kiwisoft S.A.S, 978-1-492-03264-9\n""], 'url_profile': 'https://github.com/SoloNick', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/samprithahm', 'info_list': ['3', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Predict Salary Based on Years of Experience\nObjective:\nUsing Linear Regression to Predict Salary Based on Years of Experience\nDataSet: 2 Columns, 30 Observations\nVariable 1: Years of Experience\nVariable 2: Salary\nAssumptions of Linear Regression:\n\nLinear Relationship between the features and target\nLittle or no Multicollinearity between the features\nHomoscedasticity Assumption\nNormal distribution of error terms\nLittle or No autocorrelation in the residuals\n\nGiven that all of these assumptions are met:\nTrain & Predict:\n\nSplit the dataset into a training and test set\nSplit the test by 1/3 and the rest of the data will return to the train\nRandom state: used in conjunction with the random number generator\n\nFitting Simple Linear Regression to the Training set\nFindings\nBased on this survey dataset, You can offer a canidate with 6 years of experience a salary of $212,929.30.\nAs an professional who wishes transfer into another  field, I was curious to know what would one year of experience look like.\nThus, a canidate with 0 years of experience can be offered a salary of $71,022.50.\n'], 'url_profile': 'https://github.com/drucilal', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jeromegonzaga', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KevinAQM', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019']}","{'location': 'kafrsaqr-zagazig-egypt', 'stats_list': [], 'contributions': '643 contributions\n        in the last year', 'description': ['regression-using-one-variable\nregression using one variables\ni am using data to make regression on it\nusing andrew course\nand google colab\n'], 'url_profile': 'https://github.com/ahmed-hassan97', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019']}","{'location': 'Pune', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': [""MLR-Car-Prices-Prediction - Multiple Linear Regression\nProblem Statement\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\nBusiness Goal\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\nData Preparation\nThere is a variable named CarName which is comprised of two parts - the first word is the name of 'car company' and the second is the 'car model'. For example, chevrolet impala has 'chevrolet' as the car company name and 'impala' as the car model name. You need to consider only company name as the independent variable for model building.\nModel Evaluation:\nWhen you're done with model building and residual analysis, and have made predictions on the test set, just make sure you use the following two lines of code to calculate the R-squared score on the test set.\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_pred)\nwhere y_test is the test data set for the target variable, and y_pred is the variable containing the predicted values of the target variable on the test set.\n""], 'url_profile': 'https://github.com/abhinandanbaheti', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019']}","{'location': 'Pune', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Housing-Price-Prediction\nAdvance Linear Regression\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.\nThe company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\nThe company wants to know:\nWhich variables are significant in predicting the price of a house, and\nHow well those variables describe the price of a house.\nAlso, determine the optimal value of lambda for ridge and lasso regression.\nBusiness Goal:\nYou are required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\nData Description:\nhttps://cdn.upgrad.com/UpGrad/temp/87f67e28-c47e-4725-ae3c-111142c7eaba/data_description.txt\n'], 'url_profile': 'https://github.com/solondas007', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jeromegonzaga', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Problem Statement :\nThe company is trying to decide whether to focus their efforts on their mobile or their website.\nConclusion :\nDevelop the Website to catch up to the performance of the mobile app, or develop the app more  that is what is working better.\nInterpreting the coefficients:\nA 1 unit increase in Avg. Session Length is associated with an increase of 25.98 total dollars spent.\nA 1 unit increase in Time on App is associated with an increase of 38.59 total dollars spent.\nA 1 unit increase in Time on Website is associated with an increase of 0.19 total dollars spent.\nA 1 unit increase in Length of Membership is associated with an increase of 61.27 total dollars spent.\n'], 'url_profile': 'https://github.com/AkashJadhav-git', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wardaharshad', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019']}","{'location': 'Canada', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['simple_linear_regression\nSimple Linear Regression project\n'], 'url_profile': 'https://github.com/abhijeetjainmca', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Apr 17, 2020', '1', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['STAT581A3\n'], 'url_profile': 'https://github.com/Samarmy', 'info_list': ['Updated Dec 31, 2019', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 5, 2020', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bhagyashree-rao', 'info_list': ['Updated Dec 31, 2019', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 5, 2020', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Logistic Regression\nImplementation of Logistic Regression\n'], 'url_profile': 'https://github.com/Ammar-Aslam', 'info_list': ['Updated Dec 31, 2019', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 5, 2020', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['django-project\nsales regression model\n'], 'url_profile': 'https://github.com/omisolaidowu', 'info_list': ['Updated Dec 31, 2019', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 5, 2020', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tejashwarskumar', 'info_list': ['Updated Dec 31, 2019', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 5, 2020', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tejashwarskumar', 'info_list': ['Updated Dec 31, 2019', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 5, 2020', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wardaharshad', 'info_list': ['Updated Dec 31, 2019', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 5, 2020', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['LR\nimplementation of logistic regression\n'], 'url_profile': 'https://github.com/xxhbdk', 'info_list': ['Updated Dec 31, 2019', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 5, 2020', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'Ahmedabad, Gujarat', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['Machine_learning\nSimple Linear Regression\n'], 'url_profile': 'https://github.com/pranjalg13', 'info_list': ['Updated Dec 31, 2019', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 5, 2020', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'Pune, India', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['LogisticRegression\nLogistic Regression in python\n'], 'url_profile': 'https://github.com/theSnehaThing', 'info_list': ['Updated Dec 31, 2019', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 5, 2020', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019']}"
"{'location': 'Santa Clara, CA', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['Understanding Logistic Regression from Scratch\nIntroduction\nIn statistics and data science, logistic regression is used to predict the probability of a certain class or event. Usually, the model is binomial, but can also extend to multinomial. It probably is one of the simplest yet extremely useful models for a lot of applications, with its fast implementation and ease of interpretation.\nThis tutorial will focus on the binomial logistic regression. I will discuss the basics of the logistic regression, how it is related to linear regression and how to construct the model in R using simply the matrix operation. Using only math and matrix operation (not the built-in model in R) will help us understand logistic regression under the hood.\nFinally, I will use the constructed model to classify some generated data and show the decision boundary.\nExample data to be classified\n\n\n\nLogistic regression decision boundary\n\n\n\nYou can also see the tutorial here.\nSimilar tutorial with Python can be viewed here.\n'], 'url_profile': 'https://github.com/JunWorks', 'info_list': ['3', 'R', 'Updated Jan 1, 2021', '3', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Updated Feb 1, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'Makefile', 'Updated Jan 28, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kpradyumna095', 'info_list': ['3', 'R', 'Updated Jan 1, 2021', '3', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Updated Feb 1, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'Makefile', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['詳しい使い方は以下のサイトを参照してください。\nhttps://poniti.hatenablog.com/?_ga=2.253328521.1817344917.1577867229-2140494847.1577867229\n'], 'url_profile': 'https://github.com/DKTPNIT', 'info_list': ['3', 'R', 'Updated Jan 1, 2021', '3', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Updated Feb 1, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'Makefile', 'Updated Jan 28, 2020']}","{'location': 'Tunisia', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Linear-Regression\nSome tests about linear regression with one feature, two features and hot encoding.\n1- Example and exercise about one and two features linear regression models.\n2- Example and exercise about one hot encoding using the word2number module to transform string data to numbers.\n3- Exercise about dummies and hot encoding.\n'], 'url_profile': 'https://github.com/Nalurilolz', 'info_list': ['3', 'R', 'Updated Jan 1, 2021', '3', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Updated Feb 1, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'Makefile', 'Updated Jan 28, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ravi230195', 'info_list': ['3', 'R', 'Updated Jan 1, 2021', '3', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Updated Feb 1, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'Makefile', 'Updated Jan 28, 2020']}","{'location': 'Evanston', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Intro-to-Artificial-Intelligence\nadversarial games, classification, and regression\nWPI CS:4341 (Introduction to Artificial Intelligence)\nGroup 28 (Nikolas Gamarra, Adam Moran, Ying Zhang)\nTerm Project: Bomberman AI\nProject Strategy\nOur overall strategy for this project was to begin with implementing A* to ensure\nwe had a simple basic algorithm that could tackle most of the challenges. Once A* was\nimplemented we set about improving our A* heuristic until we hit a limit of what we\nthought was achievable with A*. In order to try and solve some of the shortcomings of\nA*, we attempted to implement more advanced algorithms like expectimax and policy\niteration.\nA* Search Algorithm\nIn order to make our path we did an implementation of A*. There were no major\ndifferences between our implementation and traditional A* except that our priority\nqueue was structured as a tuple of a location tuple and a priority. We also created a\ncouple of simple helper functions to find the distance between two tuples, and check if\na monster is within a certain radius of a cell in order to inform our heuristic. We also\nneeded to implement a simple priority queue class. For our heuristic, normal cells were\ngiven a value of 1, walls were given a value proportional to the fuse time +20, cost of 60\nwas given to cells within a radius of 5 of a monster and a cost of 80 was given to cells\nwithin a radius of 2 of a monster. The A* algorithm returned a dictionary of the came\nfrom path. This dictionary was iterated backward into a list and then the next move was\nextracted. In the event that A* could not find a path (which happened when we were\nstuck in a corner between an explosion) a try-catch statement would prevent the\nprogram from dying and move the character to a random safe cell. Because walls had a\nvery high cost, our agent generally avoids them but will path to them if there is no other\noption or if the other option gets too close to monsters. In the general motion of the\ncharacter, a bomb is placed whenever a move is attempted into a wall.\nIn order to avoid pathing into cells that would be exploding in the next turn,\nseveral helper functions were added to keep track of where a bomb had been placed\nand how long till it exploded. If the character ever detects that it is attempting to move\ninto a cell that will explode it will instead make a random safe choice.\nA* 7 was made in an attempt to solve the issue of pathing into corners with\navoid_monster3. Unfortunately, this implementation did not produce any better results\nfor any variant than the other implementations we had.\nUtility Functions\nIn order to tackle the problem, we created several helper functions that are\nuniversally helpful.\nPlace smart bombs and several other helper functions keep a record of which\ncells will be exploding and when. Using this A* can have the ability to peer into the\nfuture without the help of looking into future states of the map or using expectimax.\nUsing this we could simply and effectively create an A* solution that would never path\ninto a cell that is about to explode.\nAnother important helper function we created for our A* was avoid_monster. This\nfunction was improved over three versions. In the event that a monster is found within a\ncertain radius of the character A* stops and instead the character tries to run away from\nthe monster. If the monster is in line with a potential bomb placement or within a certain\nradius a bomb will be placed. The avoid_monster2 behavior has a three part heuristic.\nFirst, the valid neighbors are considered. Of those, cells closer to the exit, further away\nfrom the monster, and bordering fewer walls score better. Avoid walls was added to\nattempt to avoid getting cornered. Ultimately We found that this was not enough to\nalways avoid getting trapped in corners.\nTo solve getting cornered we created avoid_monster3. This function does a\nsimple look into the future with sensed worlds and checks if it will survive running in for\na certain depth in each direction if the monster is aggressive and chases it. The set of\nresults where it survived is examined and the one furthest from the closest monster is\nchosen as the next move.\nExpectimax\nIn expectimax search, we estimated the probabilistic model of how the monster\nwill move in any state from the sensed world, we obtained utilities from the outcomes of\nthe current states of the world, and emerge the action. The implement of the\nprobability-based algorithm and reward system allows our agent to evaluate the risks\nand rewards of each move, which largely increased the pass rate.\nThe pseudocode of Expectimax we used is as follow:\ndef value(s):\nif s is a max node\nreturn maxValue(s)\nif s is an exp node\nreturn expValue(s)\nif s is a terminal node\nreturn evaluation(s)\ndef maxValue(s):\nvalues = [value(s’) for s’ in successors(s)]\nreturn max(values)\ndef expValue(s):\nvalues = [value(s’) for s’ in successors(s)]\nweights = [probability(s, s’) for s’ in successors(s)]\nreturn expectation(values, weights)\nFor future improvement, we would spend more time on developing the reward\nsystem. The current one is working but sometimes making unnecessary actions which\nmade the agent vulnerable from the placed bomb and monsters. More importantly, we\nwould increase the search level to go deeper for more accurate results.\nPolicy Iteration\nIn policy iteration, we basically wanted the agent to find the most beneficial\nactions in any given state. The idea was that we used a “SensedWorld” to modify the\nreturned world by performing actions or change the state of the environment without\naffecting other existing world instances. With the exception of the agent occupying the\nsame coordinates as the exit cell, all rewards would be negative to incentivize moving\ntowards the exit. It used the same code used to avoid corners, calculate proximity to a\nmonster, detect explosions before they happen, and calculate distances between two\ntuples to produce rewards. Unfortunately, how we would end up finding states would\nnot include all of the possible states (especially since we would have to consider each\ncombination of agent position, monster position, bomb position, explosion position, and\nwall position to have all the states and therefore the best action to do for each state)\nand would therefore create an incomplete policy, which defeats the purpose of doing\npolicy iteration.\nSolutions\nIn order to evaluate which test character we would use for each variant, we created a\nspreadsheet to track the performance of each one across multiple unseeded random trials. This\nway we could ensure we had the best performance possible for each variant and scenario.\nBelow this table is a simple explanation of why we chose that algorithm and why it performed\nwell. For an in-depth explanation of how the algorithm works look into the above sections on\neach algorithm.\nA4(retreat) A5(normal) A6(cheese) Expectimax 3 Expectimax 4 Policy Iteration A7(avoid3) BEST\nTotal: 72.36% 74.27% 33.00% 64.00% 77.73% 0.00% 67.00% 81.36%\nS1 ADV 66.0% 72.5% 66.0% 70.0% 76.0% 0.0% 74.0% 80.0%\nS1 V1 100.0% 100.0% 100.0% 100.0% 100.0% 0.0% 100.0% 100.0%\nS1 V2 90.0% 100.0% 100.0% 90.0% 90.0% 0.0% 100.0% 100.0%\nS1 V3 50.0% 72.7% 80.0% 50.0% 90.0% 0.0% 70.0% 90.0%\nS1 V4 40.0% 50.0% 50.0% 60.0% 60.0% 0.0% 50.0% 60.0%\nS1 V5 50.0% 40.0% 0.0% 50.0% 40.0% 0.0% 50.0% 50.0%\nS2 ADV 78.7% 76.0% 0.0% 58.0% 79.5% 0.0% 60.0% 82.7%\nS2 V1 100.0% 100.0% 0.0% 100.0% 100.0% 0.0% 100.0% 100.0%\nS2 V2 100.0% 100.0% 0.0% 90.0% 90.0% 0.0% 100.0% 100.0%\nS2 V3 75.0% 66.7% 0.0% 40.0% 73.3% 0.0% 70.0% 75.0%\nS2 V4 68.6% 66.7% 0.0% 50.0% 64.0% 0.0% 20.0% 68.6%\nS2 V5 50.0% 46.7% 0.0% 10.0% 70.0% 0.0% 10.0% 70.0%\nScenario 1:\nVariant 1:\nWe simply used our simplest version of A* (#5) for maps without monsters as it was reliable and\neffective. We chose to use the simplest versions for these as it would be less likely to run into\nerrors.\nVariant 2:\nFor scenario 1 we used our normal A* (#5) as it scored the best in our trials\nVariant 3:\nFor this variant we used our implementation of Expectimax (#4) as is scored best in our trials.\nVariant 4:\nFor this variant we used our implementation of Expectimax (#4) as is scored best in our trials.\nVariant 5:\nFor scenario 1 we used our retreating A* (#4) as it scored the best in our trials\nScenario 2:\nVariant 1:\nWe simply used our simplest version of A* (#5) for maps without monsters as it was reliable and\neffective. We chose to use the simplest versions for these as it would be less likely to run into\nerrors.\nVariant 2:\nWe again used A* (#5) for this variant as it consistently scored 100%\nVariant 3:\nFor scenario 2 we used a special version of A* (#4) that would retreat to the y location of the\nstart position. This behavior was implemented because we realized it was dangerous to enter\nthe tight spaces while a monster was present.\nVariant 4:\nFor scenario 2 we used a special version of A* (#4) that would retreat to the y location of the\nstart position. This behavior was implemented because we realized it was dangerous to enter\nthe tight spaces while a monster was present.\nVariant 5:\nFor this variant we used our implementation of Expectimax (#4) as is scored best in our trials.\nThis was because the different heuristic that was used for it performed better in variants with a\nlot of monsters as it moved more cautiously and placed more bombs.\nConclusion\nIn conclusion we were able to successfully exit the map over 80% of the time in our own testing\nwhen using the best algorithm for each scenario and variant. While we would have liked to\nimplement some more advanced AIs given more time, we successfully met our target of being\nable to get a good grade based on the rubric in our own tests.\n'], 'url_profile': 'https://github.com/PRINTF-yzhang', 'info_list': ['3', 'R', 'Updated Jan 1, 2021', '3', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Updated Feb 1, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'Makefile', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['Advanced-Regression\nRegression using the Regularization Technique\nBackground\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.\nThe company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\nThe company wants to know:\n\nWhich variables are significant in predicting the price of a house, and\nHow well those variables describe the price of a house.\n\nAlso, determine the optimal value of lambda for ridge and lasso regression.\nBusiness Goal\nModel the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/Jaidip1994', 'info_list': ['3', 'R', 'Updated Jan 1, 2021', '3', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Updated Feb 1, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'Makefile', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Midterm2\nClone this for Midterm 2 - Nov 11, 2019\nClone this repository and then create your own brach from the main to work on it.\nOne done- push it to your main branch and put in a pull request- indicating you have submitted your exam.\nThe exam are back due next Monday  November 18, 2019 at 9:00 AM.\nPARTs 1-2 and 3 are required, PART 4 is bonus.\nAlso - find attached extra credit questions from Brendan in the PDF file(exam2_extra.pdf).\nYou can submit answers to the questions in the PDF with a knitted markdown file, pushed into your branch with the rest of your exam.\nDo not push to the main!!\nThe tasks are listed in the R script in the project file- use it as a template for your own work!\nHere is a summary of the questions for a sneak peak.\nPART 1 ## Environmental Effects Evaluation\nSubmit a visual and output for each question below\n1.Is location effect significant?\na. What fraction of the variation observed in yield is attributable to\nLocation specific effects?\nb. Which location seems to be the highest yield location?\n\n\nIs Company effect significant?\na.Which company\'s varieties seem to perform the best across all regions?\nb.Which company\'s varieties seem to perform the worst across all regions?\n\n\nIs Region effect significant? How much variation in yield does region explain alone?\nHow about together with Company?\nc.Which company\'s varieties seem to perform the best within each region?\nd.Which company\'s varieties seem to perform the worst within  each region?\n\n\nIs location effect significant together with Company and Region?\ne.Which company\'s varieties seem to perform the best for each location?\nf.Which company\'s varieties seem to perform the worst for each location?\n\n\nDoes the seed treatments have a significant effect on the yield?\na.Which treatment seems to have the largest positive effect? Is it significant?\nb.What fraction of the variation observed in yield is attributable to seed treatments?\n\n\nWhat is the best model for explaining the variation in the yield data,WITHOUT GENOS!\nWhich location should I choose to use with which company\'s product\nto get the maximum yield? Should I apply Seed Treatment or not?\n\n\nGive your best prediction of maximum yield under these best case scenario conditions.\nWhich location should I choose to use with which company\'s product\nto get the maximum yield? Should I apply Seed Treatment or not?\nGive your best prediction of maximum yield under these best case scenario conditions.\n\n\nPART 2 ## Genetic Effects Evaluation\nSubmit a visual and output for each question below\nThe variables qmx1..qmx369  each represent a ""genetic_marker"" that was scored on\nthe entries that were included in the trials to identify if there are any genetic\ndeterminants that can be identified for high-yield.\nThere are a total of 369 factor variables- with 2 states each {0,1} that are scored\nNOTE THAT YOU CAN NOT USE VARIETY AND GENOTYPE at the SAME TIME\nGENOTYPE marker data Defines VARIETY!!\nUsing the example code for LASSO regression provided - fit a LASSO model\nto the data and investigate these marker variables for potential significant positive effects.\nGive your best prediction of maximum yield for a hypothetical variety, under the best\npossible environmental conditions we can offer.\n\nDoes the model fit improve with marker data?\nGenerate a list of markers that seem to have significant positive effects on yield,\nand provide their coefficients.\n\nWhich location should I choose to use with which company\'s product\nto get the maximum yield? Should I apply Seed Treatment or not?\nGive your best prediction of maximum yield under these best case scenario conditions.\nPART 3 ## Visualization\nSubmit a visual and output for each question below\n\nYield Distributions by location\na. Density Plot\nb. box whisker plot by location\nc. density plot by Company\nd. box whisker plot by Company ordered by median yield for company\ne. box whisker plot by Company ordered by median yield for company, for each location\n\nPART 4 # BONUS # LOGISTIC  LASSO  REGRESSION\nCreate a new binary variable based on the ""Company"" variable,\nwhere Companies whose products score at the top 5 performers across regions are coded as 1\nand the rest of the companies are coded as 0.\nEvaluate this variable versus the marker data [qmx1..qmx369] with ridge regression.\nReturn the list of markers and their LASSO predicted coefficients from the model.\n'], 'url_profile': 'https://github.com/Yuting918', 'info_list': ['3', 'R', 'Updated Jan 1, 2021', '3', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Updated Feb 1, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'Makefile', 'Updated Jan 28, 2020']}","{'location': 'Pune, India', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['RaqndomForestRegression\nRandom Forest Regression in Python\n'], 'url_profile': 'https://github.com/theSnehaThing', 'info_list': ['3', 'R', 'Updated Jan 1, 2021', '3', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Updated Feb 1, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'Makefile', 'Updated Jan 28, 2020']}","{'location': 'Germany', 'stats_list': [], 'contributions': '906 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bluhm', 'info_list': ['3', 'R', 'Updated Jan 1, 2021', '3', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Oct 28, 2020', 'Updated Feb 1, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'Makefile', 'Updated Jan 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['LinearRegression\nsimple Linear Regression with python\n'], 'url_profile': 'https://github.com/abutair', 'info_list': ['Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020', 'Shell', 'Updated Jan 3, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SukyoungCho', 'info_list': ['Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020', 'Shell', 'Updated Jan 3, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Regularized Logistic Regression\nImplementation of Regularized Logistic Regression\n'], 'url_profile': 'https://github.com/Ammar-Aslam', 'info_list': ['Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020', 'Shell', 'Updated Jan 3, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['RegressionTest\nSample repository for regression test\n'], 'url_profile': 'https://github.com/mukeshbharath', 'info_list': ['Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020', 'Shell', 'Updated Jan 3, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'Linz, AT', 'stats_list': [], 'contributions': '543 contributions\n        in the last year', 'description': ['DEAP Symreg\n'], 'url_profile': 'https://github.com/foolnotion', 'info_list': ['Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020', 'Shell', 'Updated Jan 3, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/planatscher', 'info_list': ['Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020', 'Shell', 'Updated Jan 3, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['linear-regression\n'], 'url_profile': 'https://github.com/yash-pythonman', 'info_list': ['Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020', 'Shell', 'Updated Jan 3, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'Addis Ababa, Ethiopia', 'stats_list': [], 'contributions': '268 contributions\n        in the last year', 'description': ['Simple-Linear-Regression-Without-Framework\nSimple linear regression without framework\nWhat is Simple Linear Regression?\nSimple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables:\n\nOne variable, denoted x, is regarded as the predictor, explanatory, or independent variable.\nThe other variable, denoted y, is regarded as the response, outcome, or dependent variable. \nBecause the other terms are used less frequently today, we\'ll use the ""predictor"" and ""response"" terms to refer to the variables encountered in this course. The other terms are mentioned only to make you aware of them should you encounter them. Simple linear regression gets its adjective ""simple,"" because it concerns the study of only one predictor variable. In contrast, multiple linear regression, which we study later in this course, gets its adjective ""multiple,"" because it concerns the study of two or more predictor variables.\n\n'], 'url_profile': 'https://github.com/Dawit-1621', 'info_list': ['Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020', 'Shell', 'Updated Jan 3, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'Pune, India', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['DecisionTreeRegression\nDecision Tree Regression in Python\n'], 'url_profile': 'https://github.com/theSnehaThing', 'info_list': ['Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020', 'Shell', 'Updated Jan 3, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 31, 2019']}","{'location': 'Pune, India', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['SupportVectorRegression\nSupport Vector Regression in Python\n'], 'url_profile': 'https://github.com/theSnehaThing', 'info_list': ['Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 5, 2020', 'Shell', 'Updated Jan 3, 2020', 'Python', 'Updated Apr 17, 2020', 'Updated Dec 30, 2019', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Aug 21, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 31, 2019']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['system-regression-cases\nsystem regression test cases set\n'], 'url_profile': 'https://github.com/Adriannne', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'HTML', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Julia', 'Updated Jan 30, 2020', 'R', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bparekh2', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'HTML', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Julia', 'Updated Jan 30, 2020', 'R', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['STAT581A3TP\n'], 'url_profile': 'https://github.com/Samarmy', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'HTML', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Julia', 'Updated Jan 30, 2020', 'R', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mohan0406', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'HTML', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Julia', 'Updated Jan 30, 2020', 'R', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['RegressionModels\nCourse Project for JHU Regression Models\nInstructions\nYou work for Motor Trend, a magazine about the automobile industry. Looking at a data set of a collection of cars, they are interested in exploring the relationship between a set of variables and miles per gallon (MPG) (outcome). They are particularly interested in the following two questions:\n“Is an automatic or manual transmission better for MPG”\n""Quantify the MPG difference between automatic and manual transmissions""\nReview criteria\nPeer Grading\nThe criteria that your classmates will use to evaluate and grade your work are shown below.\nEach criteria is binary: (1 point = criteria met acceptably; 0 points = criteria not met acceptably)\nCriteria\nDid the student interpret the coefficients correctly?\nDid the student do some exploratory data analyses?\nDid the student fit multiple models and detail their strategy for model selection?\nDid the student answer the questions of interest or detail why the question(s) is (are) not answerable?\nDid the student do a residual plot and some diagnostics?\nDid the student quantify the uncertainty in their conclusions and/or perform an inference correctly?\nWas the report brief (about 2 pages long) for the main body of the report and no longer than 5 with supporting appendix of figures?\nDid the report include an executive summary?\nWas the report done in Rmd (knitr)?\nMy Submission\nQuestion\nTake the mtcars data set and write up an analysis to answer their question using regression models and exploratory data analyses.\nYour report must be:\nWritten as a PDF printout of a compiled (using knitr) R markdown document.\nBrief. Roughly the equivalent of 2 pages or less for the main text. Supporting figures in an appendix can be included up to 5 total pages including the 2 for the main report. The appendix can only include figures.\nInclude a first paragraph executive summary.\nUpload your PDF by clicking the Upload button below the text box.\nPeer Grading\nThe criteria that your classmates will use to evaluate and grade your work are shown below.\nEach criteria is binary: (1 point = criteria met acceptably; 0 points = criteria not met acceptably)\nYour Course Project score will be the sum of the points and will count as 40% of your final grade in the course.\n'], 'url_profile': 'https://github.com/rimdzius', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'HTML', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Julia', 'Updated Jan 30, 2020', 'R', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'Tampa, FL', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['High Salary Expectations\nHSE Salary Prediction Model\nThe Problem:\nThe cost of HSE job offer rejections\nOne of the most neglected costs in Human Resources is the 10% of job offers rejected by High Salary Expectation (""HSE"") candidates.\nWhile a few job sites have taken a shot at salary prediction, most fail to recognize better salary estimates can reduce the cost of high salary expection (""HSE"") job offer rejections for employers. The hiring process holds complexity and subtle nuances which is an opportunity for continuous data science innovation. There will always be some irreducible error with human behavior, but in the same way Zillow revolutionized a baseline range for home values, the goal of this work is to predict a baseline salary range for public job postings.\nThis salary prediction model aims to mitigate the waste of employers interviewing candidates who expect higher salaries than offered by the position. According to Glassdoor, the average U.S. company spends about 4,000 USD to hire a new employee, and takes on average 42 days to fill an open position.\nToday, most applicant screening softwares efficiently filter out applicants who do not qualify for positions, but most algorithms fall short of filtering out applicants who expect higher salaries. According to Jobvite 2017 Recruiting Funnel Benchmark Report, 17% of candidates will reject a job offer, up 6% since 2015. According to the 2017 Recruiter Sentiment Study by the MRI Network, 57% of candidates reject job offers due to compensation.\nPublicly displaying salary predictions should act as a filter that discourages HSE candidates from entering into a company\'s hiring pipeline. An accurate salary prediction is good for both job seekers and companies because it mitigates the chances of everything falling apart at the last minute due to misaligned salary expectations.\nNote, the recruitment process of high-level employees can cost multiples of the average...\nCost of HSE Candidates include:\nCost of advertising the job posting (paid clicks from applicants expecting a higher salary)\nCost of time spent reviewing applications and interviewing ""higher salary expectation"" candidates.\nLost productivity (job seekers/employers time wasted if salary expectations don\'t match)\nCost of HSE Candidates = (Advertising + Application Review + Interviews + Lost Productivity) x (# Job Offers x HSE Rejection Rate)\nCost per HSE Candidate = 600 + 600 + 2,000 + 800 = 4,000 USD\nNumber of HSE Candidates per 1k Job Offers = 1,000 x (0.17 * 0.57) = 97 HSE Candidates\nCost of HSE Candidates per 1k Job Offers = 4,000 x 97 = 388,000 USD\nSalary Prediction Model Objectives:\nReduce the cost of HSE candidate job offer rejections.\nTo understand which job posting features contribute most to salary.\nTo tune a machine learning model that predicts the salary range of public job postings within 15%.\nTo implement a salary estimate that discourages HSE candidates from sending applications to lower salary positions.\n'], 'url_profile': 'https://github.com/dhershberger', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'HTML', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Julia', 'Updated Jan 30, 2020', 'R', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'Evanston', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Machine-Learning\ngreedy approximations, step-wise regression, matrix calculus, gradient descent, softmax regression...\n'], 'url_profile': 'https://github.com/PRINTF-yzhang', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'HTML', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Julia', 'Updated Jan 30, 2020', 'R', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '262 contributions\n        in the last year', 'description': ['Stable-SVM-and-Logistic-Regression\nExtension of Stable Regression (Bertsimas and al., 2019) to Support Vector Machines and Logistic Regression\n'], 'url_profile': 'https://github.com/hamzatazib', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'HTML', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Julia', 'Updated Jan 30, 2020', 'R', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': [""Segmented Regression\nControlling for salary, does the linear trend predicting number of home runs hit every year change through time? Additionally, does batting hand significantly moderate the effect of year on home runs and/or does league moderate the effect of year on home runs?\nTo answer this question, I used segmented regression (piecewise regression) with interactions. I used data from a baseball database compiled by author and journalist, Sean Lahman. This database contains baseball statistics from 1871 - 2018. I used bootstrap technique to determine the best beak for the model. The smallest RMSE from different models with different breakpoints was 0.947.\nModel assumptions were checked and outliers were removed by computing Cook's distances.\nThe model with the smallest RMSE had a breakpoint of 2009, so the year 2009 was used to separate the dataset. The overall model significantly fits the data and 1.014% of the variation in home runs is explained by the model, F(16, 12707) = 8.139, p < 0.05. The intercept of the model was -42.23, meaning that a batter hypothetically playing in year 0 would bat -42.23 home runs, t(12707) = -2.604, p < 0.05. For every one unit increase in year before 2009, home runs significantly increased by 0.0213, t(12707) = 2.616, p < 0.05. There is a non-significant main effect of the year 2009 increasing the number of home runs per year; after 2009, the number of home runs hit decreases by 0.1866, t(12707) = -0.830, p = 0.406. The increase in home runs each year between segments is non-significant, t(12707) = -0.225, p = 0.822. For every one unit increase in year after 2009, home runs non-significantly increase by 0.0111, t(12707) = 0.247, p = 0.805. The control variable, salary (in thousands), had a significant effect on home runs, b = 1.147 x 10-5, t(12707) = 4.345, p < 0.05.\nOf the interactions in this model, the slope of home runs was only moderated by the player’s batting hand before 2009. When going from a batter who bats with both hands to a batter that only uses their left hand, the slope for home runs before 2009 decreases by 0.0228, t(12707) = -2.707, p < 0.05. Similarly, when going from a batter who bats with both hands to a batter that only uses their right hand, the slope for home runs before 2009 decreases by 0.0248, t(12707) = -3.042, p < 0.05.\nUpon further investigation of the simple slopes, there was a significant impact of year on home runs for those who switched batting hands (b = 0.019, t(12707) = 2.429, p < 0.05). There was a non-significant impact of year on home runs for those who used their left hand (b = -0.003, t(12707) = -1.265, p = 0.2058). There was a significant impact of year on home runs for those who used their right hand (b = -0.005, t(12707) = -3.185, p < 0.05).\n""], 'url_profile': 'https://github.com/alyssapacleb', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'HTML', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Julia', 'Updated Jan 30, 2020', 'R', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Decision-Tree-Regression\nImplementing Decision Tree Regression in Python\nDecision tree algorithm creates a tree like conditional control statements to create its model hence it is named as decision tree.\nDecision tree machine learning algorithm can be used to solve both regression and classification problem.\nIn this post we will be implementing a simple decision tree regression model using python and sklearn.\nhttps://botbark.com/2020/01/01/decision-tree-regression-in-python-in-10-lines/\n'], 'url_profile': 'https://github.com/botbark', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'HTML', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Julia', 'Updated Jan 30, 2020', 'R', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Random-Forest-Regression\nImplementing Random Forest Regression in Python\nRandom Forest algorithm is like Decision Tree, which comprises more than one decision tree to create a model. Random Forest algorithm is an ensemble method. It creates more than one tree like conditional control statements to create its model hence it is named as Random Forest.\nhttps://botbark.com/2020/01/05/random-forest-regression-in-python-in-10-lines/\n'], 'url_profile': 'https://github.com/botbark', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Shahra-e-Faisal', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['LinearRegression\nPakistan Stock price prediction with linear regression.\n'], 'url_profile': 'https://github.com/ahsan-khan1999', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Linear-Regression\nImplementing both Simple and Multiple Linear Regression from scratch\n Read this blog written by me, to get a theoritical understanding of Linear Regression and its type.\n'], 'url_profile': 'https://github.com/NightmareNight-em', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Uberlândia - Minas Gerais - Brasil', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['Logistic Regression\nThe goal of this project is try to predict who will survive in titanic dataset. It was used Logistic Regression model.\nLanguage\nThe language is Python. Librarys: sklearn, pandas and numpy\n'], 'url_profile': 'https://github.com/joaoflauzino', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Madrid', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['\n\n\ntitle\nauthor\ndate\noutput\n\n\n\n\nMemoria Fundamentos de Analisis de Datos.\nCarlos Grande Nuñez, Veronica Gomez Gomez y Pablo Olmos Martinez\n11/29/2019\n\n\n\nhtml_document\n\n\n\n\n\n\n\nkeep_md\n\n\n\n\ntrue\n\n\n\n\n\n\n\n\n\n\n\n\n00 Introducción a la práctica y librerías\nPara la práctica hemos seleccionado una base de datos obtenida de Kaggle con los precios de viviendas del barrio King County en el estado de Washington (EEUU). Esta base de datos es de dominio público y consta de 21 variables con 21.613 observaciones.\nLa base de datos puede descargarse en el siguiente enlace: https://www.kaggle.com/swathiachath/kc-housesales-data\nLas librerias usadas para esta práctica son las siguientes:\n\nAmelia\nbrew\nbsplus\nDMwR2\ncar\ncarData\ncaret\ncluster\ndplyr\negg\nexpss\nfaraway\ngclus\nGGally\nggplot2\ngridExtra\nHmisc\nhtmltools\nISLR\nkableExtra\nknitr\nlattice\nmagrittr\nmice\nmlbench\nRColorBrewer\nreadr\nsos\ntidyr\nVIM\n\n\n01 Definición de objetivos\nDado que la base de datos elegida ha sido elaborada por una inmobiliaria, consideramos la variable ""price"", la cual representa el valor de venta de la vivienda, como la variable objetivo de la práctica.\n\nObjetivos generales\n\nAnalizar las variables de la base de datos seleccionada para su comprensión y posterior estudio.\nAplicar el módelo de regresión lineal múltiple para inferir la variable ""price"" seleccionada, que corresponde al precio de la vivienda.\n\n\nObjetivos específicos\n\n\nSeparar de los datos en 2 grupos de datos: trainning, control y testing.\n\nEl grupo training + control contiene el 90% de los datos, con el cual entrenaremos el modelo.\nEl grupo test contiene el 10% de los datos y se dejará como conjunto aislado hasta el final de la práctica como simulación de datos reales.\n\n\n\nRealizar un análisis exploratorio inicial de cada una de las variables de la base de datos.\n\nSe llevará a cabo separando las variables categóricas de las cualitativas para su posterior estudio.\n\n\n\nImputar las varaibles faltantes de la base de datos previo estudio\n\n\nAplicar las transformaciones necesarias a cada una de las variables.\n\n\nEntrenar el modelo matemático de regresión múltiple para la predicción de la varaible precio.\n\n\n\n02 Carga y aislamiento de datos TEST\nCarga de los datos (para la lectura correcta asegurarse de tener el archivo ""kc_house_data.csv"")\nrelPath <- getwd()\nsetwd(relPath)\nprice_tplusc <- read.csv(file=""./kc_house_data_missing.csv"", sep = "";"", header=TRUE, na = c("""", ""NA""), )\nhead(price_tplusc)\n##           id       date   price bedrooms bathrooms sqft_living sqft_lot floors\n## 1 7129300520 10/13/2014  221900        3      1.00        1180     5650      1\n## 2 6414100192 12/09/2014  538000        3      2.25        2570     7242      2\n## 3 5631500400  2/25/2015  180000        2      1.00         770    10000      1\n## 4 2487200875 12/09/2014  604000        4      3.00          NA     5000      1\n## 5 1954400510  2/18/2015  510000        3      2.00        1680     8080      1\n## 6 7237550310 05/12/2014 1230000        4      4.50        5420   101930      1\n##   waterfront view condition grade sqft_above sqft_basement yr_built\n## 1          0    0         3     7       1180             0     1955\n## 2          0    0         3     7       2170           400     1951\n## 3          0    0         3     6        770             0     1933\n## 4          0    0         5     7       1050           910     1965\n## 5         NA    0         3     8       1680             0     1987\n## 6          0    0         3    11       3890          1530     2001\n##   yr_renovated zipcode     lat     long sqft_living15 sqft_lot15\n## 1            0   98178 475.112 -122.257          1340       5650\n## 2         1991   98125  47.721 -122.319          1690       7639\n## 3            0   98028 477.379 -122.233          2720       8062\n## 4            0   98136 475.208 -122.393          1360       5000\n## 5            0   98074 476.168 -122.045          1800       7503\n## 6            0   98053 476.561 -122.005          4760     101930\n\nComo ya hemos mencionado anteriormente, en este apartado excluimos de la base de datos el grupo TESTING (10% de los datos) para seguir trabajando con los grupos TRAINING + CONTROL a lo largo de la práctica.\n\nTraining + control: ""price_tplusc"" (90% de observaciones)\nTesting: ""price_testing"" (10% de observaciones)\n\nset.seed(737)\ninTraining     <- createDataPartition(pull(price_tplusc), p = .9, list = FALSE, times = 1)\nprice_tplusc   <- slice(price_tplusc, inTraining)\nprice_testing  <- slice(price_tplusc, -inTraining)\nFinalmente obtenemos los siguientes grupos de observaciones\n\n\n\n Nombre del grupo \n observaciones \n\n\n\n\n Grupo TRAINING + CONTROL 90% \n 19439 \n\n\n Grupo TESTING 10% \n 1941 \n\n\n\n\n03 Análisis exploratorio inicial. EDA\nEn este apartado realizamos un primer análisis para comprender las variables y el estado original en el que se encuentra la base de datos.\n#Muestra de las primeras 5 filas de la base de datos\nkable(head(price_tplusc)) %>%\n  kable_styling() %>%\n  scroll_box(width = ""100%"", height = TRUE)\n\n\n\n id \n date \n price \n bedrooms \n bathrooms \n sqft_living \n sqft_lot \n floors \n waterfront \n view \n condition \n grade \n sqft_above \n sqft_basement \n yr_built \n yr_renovated \n zipcode \n lat \n long \n sqft_living15 \n sqft_lot15 \n\n\n\n\n 6414100192 \n 12/09/2014 \n 538000 \n 3 \n 2.25 \n 2570 \n 7242 \n 2 \n 0 \n 0 \n 3 \n 7 \n 2170 \n 400 \n 1951 \n 1991 \n 98125 \n 47.721 \n -122.319 \n 1690 \n 7639 \n\n\n 5631500400 \n 2/25/2015 \n 180000 \n 2 \n 1.00 \n 770 \n 10000 \n 1 \n 0 \n 0 \n 3 \n 6 \n 770 \n 0 \n 1933 \n 0 \n 98028 \n 477.379 \n -122.233 \n 2720 \n 8062 \n\n\n 2487200875 \n 12/09/2014 \n 604000 \n 4 \n 3.00 \n NA \n 5000 \n 1 \n 0 \n 0 \n 5 \n 7 \n 1050 \n 910 \n 1965 \n 0 \n 98136 \n 475.208 \n -122.393 \n 1360 \n 5000 \n\n\n 1954400510 \n 2/18/2015 \n 510000 \n 3 \n 2.00 \n 1680 \n 8080 \n 1 \n NA \n 0 \n 3 \n 8 \n 1680 \n 0 \n 1987 \n 0 \n 98074 \n 476.168 \n -122.045 \n 1800 \n 7503 \n\n\n 7237550310 \n 05/12/2014 \n 1230000 \n 4 \n 4.50 \n 5420 \n 101930 \n 1 \n 0 \n 0 \n 3 \n 11 \n 3890 \n 1530 \n 2001 \n 0 \n 98053 \n 476.561 \n -122.005 \n 4760 \n 101930 \n\n\n 1321400060 \n 6/27/2014 \n 257500 \n 3 \n 2.25 \n 1715 \n 6819 \n 2 \n 0 \n 0 \n 3 \n 7 \n 1715 \n 0 \n 1995 \n 0 \n 98003 \n 473.097 \n -122.327 \n 2238 \n 6819 \n\n\n\n#Muestra de variables de la base de datos seleccionada\nshow_df = data.frame(variable = names(price_tplusc),\n           classe = sapply(price_tplusc, typeof),\n           first_values = sapply(price_tplusc, function(x) paste0(head(x),  collapse = "", "")),\n           row.names = NULL)\nkable(show_df) %>%\n  kable_styling()\n\n\n\n variable \n classe \n first_values \n\n\n\n\n id \n double \n 6414100192, 5631500400, 2487200875, 1954400510, 7237550310, 1321400060 \n\n\n date \n integer \n 12/09/2014, 2/25/2015, 12/09/2014, 2/18/2015, 05/12/2014, 6/27/2014 \n\n\n price \n double \n 538000, 180000, 604000, 510000, 1230000, 257500 \n\n\n bedrooms \n integer \n 3, 2, 4, 3, 4, 3 \n\n\n bathrooms \n double \n 2.25, 1, 3, 2, 4.5, 2.25 \n\n\n sqft_living \n integer \n 2570, 770, NA, 1680, 5420, 1715 \n\n\n sqft_lot \n integer \n 7242, 10000, 5000, 8080, 101930, 6819 \n\n\n floors \n double \n 2, 1, 1, 1, 1, 2 \n\n\n waterfront \n integer \n 0, 0, 0, NA, 0, 0 \n\n\n view \n integer \n 0, 0, 0, 0, 0, 0 \n\n\n condition \n integer \n 3, 3, 5, 3, 3, 3 \n\n\n grade \n integer \n 7, 6, 7, 8, 11, 7 \n\n\n sqft_above \n integer \n 2170, 770, 1050, 1680, 3890, 1715 \n\n\n sqft_basement \n integer \n 400, 0, 910, 0, 1530, 0 \n\n\n yr_built \n integer \n 1951, 1933, 1965, 1987, 2001, 1995 \n\n\n yr_renovated \n integer \n 1991, 0, 0, 0, 0, 0 \n\n\n zipcode \n integer \n 98125, 98028, 98136, 98074, 98053, 98003 \n\n\n lat \n double \n 47.721, 477.379, 475.208, 476.168, 476.561, 473.097 \n\n\n long \n double \n -122.319, -122.233, -122.393, -122.045, -122.005, -122.327 \n\n\n sqft_living15 \n integer \n 1690, 2720, 1360, 1800, 4760, 2238 \n\n\n sqft_lot15 \n integer \n 7639, 8062, 5000, 7503, 101930, 6819 \n\n\n\n#Tabla resumen con los principales estadísticos\nkable(summary(price_tplusc)) %>%\n  kable_styling() %>%\n  scroll_box(width = ""100%"", height = TRUE)\n\n\n\n \n       id \n         date \n     price \n    bedrooms \n   bathrooms \n  sqft_living \n    sqft_lot \n     floors \n   waterfront \n      view \n   condition \n     grade \n   sqft_above \n sqft_basement \n    yr_built \n  yr_renovated \n    zipcode \n      lat \n      long \n sqft_living15 \n   sqft_lot15 \n\n\n\n\n \n Min.   :1.000e+06 \n 6/23/2014 :  127 \n Min.   :  78000 \n Min.   : 1.000 \n Min.   :0.500 \n Min.   :  370 \n Min.   :    520 \n Min.   :1.000 \n Min.   :0.000000 \n Min.   :0.0000 \n Min.   :1.000 \n 7      :8081 \n Min.   : 370 \n Min.   :   0.0 \n Min.   :1900 \n Min.   :   0.00 \n Min.   :98001 \n Min.   : 47.18 \n Min.   :-122.5 \n Min.   : 399 \n Min.   :   651 \n\n\n \n 1st Qu.:2.125e+09 \n 6/25/2014 :  116 \n 1st Qu.: 322000 \n 1st Qu.: 3.000 \n 1st Qu.:1.750 \n 1st Qu.: 1420 \n 1st Qu.:   5040 \n 1st Qu.:1.000 \n 1st Qu.:0.000000 \n 1st Qu.:0.0000 \n 1st Qu.:3.000 \n 8      :5446 \n 1st Qu.:1200 \n 1st Qu.:   0.0 \n 1st Qu.:1951 \n 1st Qu.:   0.00 \n 1st Qu.:98033 \n 1st Qu.:473.85 \n 1st Qu.:-122.3 \n 1st Qu.:1490 \n 1st Qu.:  5100 \n\n\n \n Median :3.905e+09 \n 6/26/2014 :  115 \n Median : 450000 \n Median : 3.000 \n Median :2.250 \n Median : 1910 \n Median :   7618 \n Median :1.500 \n Median :0.000000 \n Median :0.0000 \n Median :3.000 \n 9      :2333 \n Median :1560 \n Median :   0.0 \n Median :1975 \n Median :   0.00 \n Median :98065 \n Median :475.52 \n Median :-122.2 \n Median :1840 \n Median :  7620 \n\n\n \n Mean   :4.590e+09 \n 4/27/2015 :  114 \n Mean   : 540976 \n Mean   : 3.371 \n Mean   :2.116 \n Mean   : 2079 \n Mean   :  15084 \n Mean   :1.494 \n Mean   :0.007674 \n Mean   :0.2359 \n Mean   :3.411 \n 6      :1839 \n Mean   :1788 \n Mean   : 290.4 \n Mean   :1971 \n Mean   :  85.37 \n Mean   :98078 \n Mean   :431.00 \n Mean   :-122.2 \n Mean   :1986 \n Mean   : 12800 \n\n\n \n 3rd Qu.:7.335e+09 \n 3/25/2015 :  113 \n 3rd Qu.: 645000 \n 3rd Qu.: 4.000 \n 3rd Qu.:2.500 \n 3rd Qu.: 2550 \n 3rd Qu.:  10640 \n 3rd Qu.:2.000 \n 3rd Qu.:0.000000 \n 3rd Qu.:0.0000 \n 3rd Qu.:4.000 \n 10     :1018 \n 3rd Qu.:2210 \n 3rd Qu.: 560.0 \n 3rd Qu.:1997 \n 3rd Qu.:   0.00 \n 3rd Qu.:98118 \n 3rd Qu.:476.69 \n 3rd Qu.:-122.1 \n 3rd Qu.:2360 \n 3rd Qu.: 10082 \n\n\n \n Max.   :9.900e+09 \n 07/08/2014:  110 \n Max.   :7700000 \n Max.   :33.000 \n Max.   :8.000 \n Max.   :13540 \n Max.   :1651359 \n Max.   :3.500 \n Max.   :1.000000 \n Max.   :4.0000 \n Max.   :5.000 \n (Other): 704 \n Max.   :9410 \n Max.   :4820.0 \n Max.   :2015 \n Max.   :2015.00 \n Max.   :98199 \n Max.   :477.78 \n Max.   :-121.3 \n Max.   :6210 \n Max.   :871200 \n\n\n \n NA \n (Other)   :18744 \n NA \n NA\'s   :51 \n NA\'s   :35 \n NA\'s   :36 \n NA\'s   :53 \n NA\'s   :23 \n NA\'s   :23 \n NA\'s   :18 \n NA\'s   :27 \n NA\'s   :  18 \n NA\'s   :36 \n NA\'s   :36 \n NA\'s   :14 \n NA\'s   :10 \n NA \n NA\'s   :1 \n NA \n NA \n NA \n\n\n\n\nDescripción de variables\n\n\nid: valor único (Primary key).\n\n\ndate: fecha de venta de la vivienda.\n\n\nprice: precio de venta. Variable seleccionada para la aplicación del modelo y su posterior predicción.\n\n\nbedrooms: número de habitaciones por vivienda.\n\n\nbathrooms: número de baños por vivienda.\n\n\nsqft_living: superficie de la vivienda en pies cuadrados (superficie escriturada).\n\n\nsqft_lot: superficie de la parcela de la vivienda en pies cuadrados (superficie parcelaria).\n\n\nfloors: número de plantas por vivienda.\n\n\nwaterfront: si la vivienda tiene vistas al mar.\n\n\nview: el número de veces que se ha visitado la vivienda desde su puesta en venta.\n\n\ncondition*: el estado de la vivienda establecido mediante una variable numérica del 1 al 5.\n\n\ngrade*: nota general de la vivienda propuesta por el sistema de puntuación de la zona del 1 al 13.\n\n\nsqft_above: superficie de la huella perimetral de la vivienda sobre rasante en pies cuadrados.\n\n\nsqft_basement: superficie de la vivienda bajo rasante en piés cuadrados\n\n\nyr_built: año de construcción de la vivienda\n\n\nyr_renovated: año de la renovación de la vivienda. En caso de no haber sido renovada este parámetro se ha igualado a 0.\n\n\nzipcode: codigo postal de la vivienda.\n\n\nlat: latitud de la coordenada de la vivienda medida en pies.\n\n\nlong: longitud de la coordenada de la vivienda medida en pies.\n\n\nsqft_living15: superficie de la vivienda en el año 2015 (admite renovaciones).\n\n\nsqft_lot15: superficie de la parcela en el año 2015 (admite modificaciones)\n* http://info.kingcounty.gov/assessor/esales/Glossary.aspx?type=r#g\n\n\n\nA. Análisis univariante cuantitativo\n#Obtención de variables cuantitativas\ndf_cuantitativas = price_tplusc %>% select(3, 6, 7, 13:16, 18:dim(price_tplusc)[2])\n\ndata.frame(variable = names(df_cuantitativas),\n           classe = sapply(df_cuantitativas, typeof),\n           first_values = sapply(df_cuantitativas, function(x) paste0(head(x),  collapse = "", "")),\n           row.names = NULL) %>% kable() %>% kable_styling()\n\n\n\n variable \n classe \n first_values \n\n\n\n\n price \n double \n 538000, 180000, 604000, 510000, 1230000, 257500 \n\n\n sqft_living \n integer \n 2570, 770, NA, 1680, 5420, 1715 \n\n\n sqft_lot \n integer \n 7242, 10000, 5000, 8080, 101930, 6819 \n\n\n sqft_above \n integer \n 2170, 770, 1050, 1680, 3890, 1715 \n\n\n sqft_basement \n integer \n 400, 0, 910, 0, 1530, 0 \n\n\n yr_built \n integer \n 1951, 1933, 1965, 1987, 2001, 1995 \n\n\n yr_renovated \n integer \n 1991, 0, 0, 0, 0, 0 \n\n\n lat \n double \n 47.721, 477.379, 475.208, 476.168, 476.561, 473.097 \n\n\n long \n double \n -122.319, -122.233, -122.393, -122.045, -122.005, -122.327 \n\n\n sqft_living15 \n integer \n 1690, 2720, 1360, 1800, 4760, 2238 \n\n\n sqft_lot15 \n integer \n 7639, 8062, 5000, 7503, 101930, 6819 \n\n\n\n\nEstudio de la variable ""price"" (precio de venta).\nvar_price = df_cuantitativas$price\nname = ""price""\n\n# Descripción de la variable\ndescribe(var_price)\n## var_price \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##    19439        0     3410        1   540976   331230   210000   245000 \n##      .25      .50      .75      .90      .95 \n##   322000   450000   645000   889000  1170000 \n## \n## lowest :   78000   80000   81000   83000   84000\n## highest: 5350000 5570000 6890000 7060000 7700000\n\n# Visualización de la variable\np1 <- phist(df_cuantitativas, ., name)\np2 <- pbox(var_price, name)\ngrid.arrange(p1, p2, nrow=1)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nTras la visualización de la variable ""price"" mediente el histograma de frecuencias y el diagrama de caja, se puede observar que la variable no sigue una distribución normal y contiene múltiples outliars.\nLas posibles soluciones a plantearnos para su transformación son:\n\nTransformacion logaritmico 10\nRaiz cuadrada\nInversa 1/x\n\n\nEstudio de las variables ""sqft_living""  y ""sqft_living15"" (Superficie de la vivienda).\nvariable ""sqft_living"": superficie de la vivienda en pies cuadrados (superficie escriturada).\nvar_sqft = df_cuantitativas$sqft_living\nname = ""sqft_living""\n\n# Descripción de la variable\nde <- unlist(describe(df_cuantitativas$sqft_living))\nde[1:5]\n##                       descript                       counts.n \n## ""df_cuantitativas$sqft_living""                        ""19403"" \n##                 counts.missing                counts.distinct \n##                           ""36""                          ""975"" \n##                    counts.Info \n##                            ""1""\n\nsummary(var_sqft)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA\'s \n##     370    1420    1910    2079    2550   13540      36\n\n# Visualización de la variable\np1 <- phist(df_cuantitativas, ., name)\np2 <- pbox(var_sqft, name)\ngrid.arrange(p1, p2, nrow=1)\n\nvariable ""sqft_living15"": superficie de la vivienda en pies cuadrados (superficie escriturada).\nvar_sqft15 = df_cuantitativas$sqft_living15\nname = ""sqft_living15""\n\n# Descripción de la variable\nde <- unlist(describe(var_sqft15))\nde[1:5]\n##        descript        counts.n  counts.missing counts.distinct     counts.Info \n##    ""var_sqft15""         ""19439""             ""0""           ""745""             ""1""\n\nsummary(var_sqft15)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##     399    1490    1840    1986    2360    6210\n\n# Visualización de la variable\np1 <- phist(df_cuantitativas, 50, name)\np2 <- pbox(var_sqft15, name)\ngrid.arrange(p1, p2, nrow=1)\n\nTras la visualización de la variable ""sqft_living"" mediente el histograma de frecuencias y el diagrama de caja, se puede observar que la variable no sigue una distribución normal y contiene múltiples outliars.\nLas posibles soluciones a plantearnos para su transformación son:\n\nTransformacion logaritmico 10\nRaiz cuadrada\nInversa 1/x\n\nEstudio de las variables ""sqft_lot"" y ""sqft_lot15 (superficie de la parcela de la vivienda).\nVariables ""sqft_lot"": superficie de la parcela de la vivienda en pies cuadrados (superficie parcelaria).\nknitr::opts_chunk$set(message = FALSE)\nvar_lot = df_cuantitativas$sqft_lot\nname = ""sqft_lot""\n\n# Descripción de la variable\ndescribe(var_lot)\n## var_lot \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##    19386       53     9066        1    15084    17823     1821     3324 \n##      .25      .50      .75      .90      .95 \n##     5040     7618    10640    21342    43124 \n## \n## lowest :     520     572     600     609     635\n## highest:  982998 1024068 1074218 1164794 1651359\n\nsummary(var_lot)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA\'s \n##     520    5040    7618   15084   10640 1651359      53\n\n# Visualización de la variable\np1 <- phist(df_cuantitativas, ., name)\np2 <- pbox(var_lot, name)\ngrid.arrange(p1, p2, nrow=1)\n\nVariable ""sqft_lot15"":  superficie de la parcela en el año 2015 (admite modificaciones)\nvar_lot15 = df_cuantitativas$sqft_lot15\nname = ""sqft_lot15""\n\n# Descripción de la variable\ndescribe(var_lot15)\n## var_lot15 \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##    19439        0     8131        1    12800    13474     1991     3668 \n##      .25      .50      .75      .90      .95 \n##     5100     7620    10082    17811    37035 \n## \n## lowest :    651    659    660    748    750, highest: 434728 438213 560617 858132 871200\n\nsummary(var_lot15)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##     651    5100    7620   12800   10082  871200\n\n# Visualización de la variable\np1 <- phist(df_cuantitativas, ., name)\np2 <- pbox(var_lot15, name)\ngrid.arrange(p1, p2, nrow=1)\n\nAunque a primera vista, se aprecian muchos valores cercanos al cero. Aunque si observamos el resultado de la función summary el valor mínimo es igual a una parcela de 651 pies cuadrados mientras el máximo es de 871.200 sqft. A esto hay que sumarle que el 75% de los valores se encuentran por debajo de los 10.083 sqft, por lo que sería conveniente categorizar esta variable por rangos.\nEn los apartados de transformación nos plantearemos las posibles soluciones a aplicar.\nEstudio de la variable ""sqft_above"" (superficie de la huella de la vivienda).\nvar_above = df_cuantitativas$sqft_above\nname = ""sqft_above""\n\n# Descripción de la variable\ndescribe(var_above)\n## var_above \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##    19403       36      890        1     1788      876      850      960 \n##      .25      .50      .75      .90      .95 \n##     1200     1560     2210     2940     3400 \n## \n## lowest :  370  380  390  410  420, highest: 7880 8020 8570 8860 9410\n\n# Visualización de la variable\np1 <- phist(df_cuantitativas, 50, name)\np2 <- pbox(var_above, name)\ngrid.arrange(p1, p2, nrow=1)\n\nAl tratarse de una variable de superficie de la huella, la variable coincidirá con la superficie de la vivienda en aquellos casos en los que la vivienda tenga una planta. De esta manera ""sqft_above"" al igual que ""sqft_living"" no puede aceptarse como una variable de distribución normal y probablemente se aplique sobre ambas la misma solución.\nEn los apartados de transformación nos plantearemos las posibles soluciones a aplicar.\nEstudio de la variable ""sqft_basement"" (superficie bajo rasante).\nvar_base = df_cuantitativas$sqft_basement\nname = ""sqft_basement""\n\n# Descripción de la variable\ndescribe(var_base)\n## var_base \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##    19403       36      297    0.776    290.4    420.9        0        0 \n##      .25      .50      .75      .90      .95 \n##        0        0      560      960     1180 \n## \n## lowest :    0   10   20   40   50, highest: 3260 3480 3500 4130 4820\n\nsummary(var_base)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA\'s \n##     0.0     0.0     0.0   290.4   560.0  4820.0      36\n\n# Visualización de la variable\np1 <- phist(df_cuantitativas, 50, name)\np2 <- pbox(var_base, name)\ngrid.arrange(p1, p2, nrow=1)\n\nEn esta variable se han igualado a 0 aquellos casos en los que la vivienda no tenga sótano. Por otro lado hay que contar con que en la mayoría de los casos la superficie de sótano no computa con la de vivienda, aunque si puede influir en el valor de la vivienda. Probablemente deba categorizarse ya que el 50% de los valores son iguales a 0.\nEn los apartados de transformación nos plantearemos las posibles soluciones a aplicar.\nEstudio de la variable ""yr_built"" (año de construcción de la vivienda).\nvar_year = df_cuantitativas$yr_built\nname = ""yr_built""\n\n# Descripción de la variable\ndescribe(var_year)\n## var_year \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##    19425       14      116        1     1971    33.42     1915     1926 \n##      .25      .50      .75      .90      .95 \n##     1951     1975     1997     2007     2011 \n## \n## lowest : 1900 1901 1902 1903 1904, highest: 2011 2012 2013 2014 2015\n\nsummary(var_year)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA\'s \n##    1900    1951    1975    1971    1997    2015      14\n\n# Visualización de la variable\np1 <- phist(df_cuantitativas, 50, ""yr_built"")\np2 <- pbox(var_year, ""yr_built"")\ngrid.arrange(p1, p2, nrow=1)\n\nSe puede observar que hay mayor cantidad de viviendas modernas que de antiguas en la base de datos sin seguir una distribución normal, registranto el máximo entre el año 2000 y 2010. Probablemente haya que categorizarla en grupos de antigüedad\nEn los apartados de transformación nos plantearemos las posibles soluciones a aplicar.\nEstudio de la variable ""yr_renovated"" (año de renovación de la vivienda).\nvar_renove = df_cuantitativas$yr_renovated\nname = ""yr_renovated""\n\n# Descripción de la variable\ndescribe(var_renove)\n## var_renove \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##    19429       10       70    0.123    85.37    163.5        0        0 \n##      .25      .50      .75      .90      .95 \n##        0        0        0        0        0 \n## \n## lowest :    0 1934 1940 1944 1945, highest: 2011 2012 2013 2014 2015\n##                                                                             \n## Value          0  1935  1940  1945  1950  1955  1960  1965  1970  1975  1980\n## Frequency  18598     1     2     5     4    13    10    14    21    22    40\n## Proportion 0.957 0.000 0.000 0.000 0.000 0.001 0.001 0.001 0.001 0.001 0.002\n##                                                     \n## Value       1985  1990  1995  2000  2005  2010  2015\n## Frequency     81    90    79   103   138    74   134\n## Proportion 0.004 0.005 0.004 0.005 0.007 0.004 0.007\n## \n## For the frequency table, variable is rounded to the nearest 5\n\n# Visualización de la variable\np1 <- phist(df_cuantitativas, 25, name)\np2 <- pbox(var_renove, name)\ngrid.arrange(p1, p2, nrow=1)\n\nEl problema que observamos en esta variable es que aquellas viviendas que no han sido renovadas se han igualado a 0, no permitiendo visualizar la variable correctamente. En el apartado de transformación de variables estudiaremos tratarla como una categórica.\nB. Analisis univariante cualitativo\n#Obtención de variables cuantitativas\ndf_cualitativas = price_tplusc %>% select(2, 4, 5, 8:12, 17)\n\ndata.frame(variable = names(df_cualitativas),\n           classe = sapply(df_cualitativas, typeof),\n           first_values = sapply(df_cualitativas, function(x) paste0(head(x),  collapse = "", "")),\n           row.names = NULL) %>% kable() %>% kable_styling()\n\n\n\n variable \n classe \n first_values \n\n\n\n\n date \n integer \n 12/09/2014, 2/25/2015, 12/09/2014, 2/18/2015, 05/12/2014, 6/27/2014 \n\n\n bedrooms \n integer \n 3, 2, 4, 3, 4, 3 \n\n\n bathrooms \n double \n 2.25, 1, 3, 2, 4.5, 2.25 \n\n\n floors \n double \n 2, 1, 1, 1, 1, 2 \n\n\n waterfront \n integer \n 0, 0, 0, NA, 0, 0 \n\n\n view \n integer \n 0, 0, 0, 0, 0, 0 \n\n\n condition \n integer \n 3, 3, 5, 3, 3, 3 \n\n\n grade \n integer \n 7, 6, 7, 8, 11, 7 \n\n\n zipcode \n integer \n 98125, 98028, 98136, 98074, 98053, 98003 \n\n\n\nEstudio de la variable ""date"" (fecha de venta de la vivienda):\nDado que el objetivo de la práctica es poder predecir el precio de la vivienda, sería interesante para nuestro modelo separar esta variable en mes y año ya que el precio de venta se puede ver influido por la estación y por el año en el que se realizó la venta.\ndates = data.frame(date = as.Date(df_cualitativas$date,""%m/%d/%Y""))\ndf_cualitativas$dates_m = format(dates,""%m"")\ndf_cualitativas$dates_y = format(dates,""%Y"")\n\n# Límites del campo date\ndates %>% summarise(min = min(date), max = max(date))\n##          min        max\n## 1 2014-05-02 2015-05-27\n\n# Tablas de frecuencias en función al mes y al año\ntable(df_cualitativas$dates_y) %>% kable(., col.names = c(\'Años\', \'Frecuencia\'))\n\n\n\n Años \n Frecuencia \n\n\n\n\n 2014 \n 13163 \n\n\n 2015 \n 6276 \n\n\n\ntable(df_cualitativas$dates_m) %>% kable(., col.names = c(\'Meses\', \'Frecuencia\'))\n\n\n\n Meses \n Frecuencia \n\n\n\n\n 01 \n 877 \n\n\n 02 \n 1118 \n\n\n 03 \n 1681 \n\n\n 04 \n 2006 \n\n\n 05 \n 2197 \n\n\n 06 \n 1951 \n\n\n 07 \n 1976 \n\n\n 08 \n 1743 \n\n\n 09 \n 1599 \n\n\n 10 \n 1712 \n\n\n 11 \n 1253 \n\n\n 12 \n 1326 \n\n\n\n# Diagrama de barras con los meses del año y el número de viviendas vendidas\np_barras2(df_cualitativas, df_cualitativas$dates_m, df_cualitativas$dates_y, xlab = \'Mes\')\n\nDe la variable fecha de venta de la vivienda, probablemente seleccionemos solo el año en el que se produjo la venta dado que los precios en el mercado suelen verse influidos por este parámetro.\nEstudio de la variable ""bedrooms"" (Número de habitaciones por vivienda):\nvar_rooms = df_cualitativas$bedrooms\nname = ""bedrooms""\n\n# Tablas de frecuencias en función al mes y al año\nsummary(var_rooms)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA\'s \n##   1.000   3.000   3.000   3.371   4.000  33.000      51\n\ntable(var_rooms) %>% kable(., col.names = c(name, \'Frecuencia\'))\n\n\n\n bedrooms \n Frecuencia \n\n\n\n\n 1 \n 172 \n\n\n 2 \n 2512 \n\n\n 3 \n 8805 \n\n\n 4 \n 6164 \n\n\n 5 \n 1435 \n\n\n 6 \n 239 \n\n\n 7 \n 38 \n\n\n 8 \n 12 \n\n\n 9 \n 6 \n\n\n 10 \n 3 \n\n\n 11 \n 1 \n\n\n 33 \n 1 \n\n\n\n# Diagrama de barras con los meses del año y el número de viviendas vendidas\np_barras(df_cualitativas, var_rooms, xlab = name)\n\nLa frecuencia máxima de habitaciones por vivienda se localiza en 3 habitaciones, probablemente deban categorizarse debido a que la distribución no es normal y contiene multiples outliers que la desequilibran.\nEstudio de la variable ""bathrooms"" (Número de baños/aseos por vivienda):\nvar_bathrooms = df_cualitativas$bathrooms\nname = ""bathrooms""\n\n# Tablas de frecuencias en función al mes y al año\ntable(var_bathrooms) %>% kable(., col.names = c(name, \'Frecuencia\'))\n\n\n\n bathrooms \n Frecuencia \n\n\n\n\n 0.5 \n 4 \n\n\n 0.75 \n 64 \n\n\n 1 \n 3474 \n\n\n 1.25 \n 7 \n\n\n 1.5 \n 1282 \n\n\n 1.75 \n 2754 \n\n\n 2 \n 1722 \n\n\n 2.25 \n 1843 \n\n\n 2.5 \n 4848 \n\n\n 2.75 \n 1051 \n\n\n 3 \n 652 \n\n\n 3.25 \n 535 \n\n\n 3.5 \n 661 \n\n\n 3.75 \n 140 \n\n\n 4 \n 121 \n\n\n 4.25 \n 72 \n\n\n 4.5 \n 89 \n\n\n 4.75 \n 23 \n\n\n 5 \n 21 \n\n\n 5.25 \n 11 \n\n\n 5.5 \n 10 \n\n\n 5.75 \n 4 \n\n\n 6 \n 6 \n\n\n 6.25 \n 2 \n\n\n 6.5 \n 2 \n\n\n 6.75 \n 2 \n\n\n 7.5 \n 1 \n\n\n 7.75 \n 1 \n\n\n 8 \n 2 \n\n\n\n# Diagrama de barras con los meses del año y el número de viviendas vendidas\np_barras(df_cualitativas, var_bathrooms, xlab = name)\n\nvar_bathrooms = df_cualitativas$bathrooms %>% ceiling()\nLa variable bathrooms es decimal, podríamos pensar que es un error, pero al ser decimales basados en múltiplos de 25 probablemente contabilizasen el aseo estandar como 0.25 y el baño estandar como 1 categorizando el resto en función a su superficie. Sería bueno redondear estos números a la alza ya que un aseo sigue siendo un estancia más.\nLa variable ""bathrooms"" sigue una distribución muy similar a la de la variable ""bedrooms"" por lo que aplicaremos las mismas soluciones.\nEstudio de la variable ""floors"" (Número de plantas por vivienda):\nComo en los casos anteriores la variable ""floors"" es decimal, pero a diferencia de los anteriores solo encontramos decimales con saltos de 0.5 por lo que redondearlos hacia arriba o hacia abajo todos podría perjudicarnos posteriormente en el modelo. De esta manera mantendremos los datos como en el origen para su correcta visualización.\nvar_floors = df_cualitativas$floors\nname = ""floors""\n\n# Tablas de frecuencias en función al mes y al año\ntable(var_floors) %>% kable(., col.names = c(name, \'Frecuencia\'))\n\n\n\n floors \n Frecuencia \n\n\n\n\n 1 \n 9587 \n\n\n 1.5 \n 1715 \n\n\n 2 \n 7430 \n\n\n 2.5 \n 147 \n\n\n 3 \n 531 \n\n\n 3.5 \n 6 \n\n\n\n# Diagrama de barras con los meses del año y el número de viviendas vendidas\np_barras(df_cualitativas, var_floors, xlab = name)\n\nSe puede observar que el número de valores decimales es reducido en comparación con el resto, pero creemos que deberían mantenerse sin redondear ya que una vivienda de dos plantas con la misma superficie y otra con la mitad de la superficie en su planta superior pueden discrepar mucho en el precio.\nEstudio de la variable ""waterfront"" (viviendas frente a grandes masas de agua):\nvar_waterfront = df_cualitativas$waterfront\nname = ""waterfront""\n\n# Tablas de frecuencias en función al mes y al año\ntable(var_waterfront) %>% kable(., col.names = c(name, \'Frecuencia\'))\n\n\n\n waterfront \n Frecuencia \n\n\n\n\n 0 \n 19267 \n\n\n 1 \n 149 \n\n\n\n# Diagrama de barras con los meses del año y el número de viviendas vendidas\np_barras(df_cualitativas, var_waterfront, xlab = name)\n\nEsta variable es de tipo dummy por lo que no necesitaría modificación alguna salvo por la imputación de datos faltantes. Se puede observar que tan solo el 1% de las viviendas de la base de datos están ubicadas frente a grandes masas de agua.\nEstudio de la variable ""view"" (número de visitas que ha recibido la vivienda):\nvar_view = df_cualitativas$view\nname = ""waterfront""\n\n# Tablas de frecuencias en función al mes y al año\ntable(var_view) %>% kable(., col.names = c(name, \'Frecuencia\'))\n\n\n\n waterfront \n Frecuencia \n\n\n\n\n 0 \n 17505 \n\n\n 1 \n 302 \n\n\n 2 \n 859 \n\n\n 3 \n 458 \n\n\n 4 \n 297 \n\n\n\n# Diagrama de barras con los meses del año y el número de viviendas vendidas\np_barras(df_cualitativas, var_view, xlab = name)\n\nDado que la mayoría de las viviendas no han recibido visitas, inicialmente no parece una variable que pueda afectar mucho al precio de la vivienda por lo que podría ser deshechada o convertida en dummy tras el análisis multivariante.\nEstudio de la variable ""condition"" (estado de la vivienda del 1 al 5):\nvar_condition = df_cualitativas$condition\nname = ""condition""\n\n# Tablas de frecuencias en función al mes y al año\ntable(var_condition) %>% kable(., col.names = c(name, \'Frecuencia\'))\n\n\n\n condition \n Frecuencia \n\n\n\n\n 1 \n 28 \n\n\n 2 \n 161 \n\n\n 3 \n 12564 \n\n\n 4 \n 5120 \n\n\n 5 \n 1539 \n\n\n\n# Diagrama de barras con los meses del año y el número de viviendas vendidas\np_barras(df_cualitativas, var_condition, xlab = name)\n\nLa variable ""condition"" determina el estado de la vivienda clasificándola en una puntuación del 1 al 5, dado que la mayor parte de las viviendas se distribuyen en los puntos 3 y 4 podríamos simplificarla categorizándola nuevamente.\nEstudio de la variable ""grade"" (nota general de la vivienda del 1 al 13):\nvar_grade = df_cualitativas$grade\nname = ""grade""\n\n# Tablas de frecuencias en función al mes y al año\ntable(var_grade) %>% kable(., col.names = c(name, \'Frecuencia\'))\n\n\n\n grade \n Frecuencia \n\n\n\n\n 10 \n 1018 \n\n\n 11 \n 368 \n\n\n 12 \n 79 \n\n\n 13 \n 12 \n\n\n 3 \n 1 \n\n\n 4 \n 26 \n\n\n 5 \n 217 \n\n\n 6 \n 1839 \n\n\n 7 \n 8081 \n\n\n 8 \n 5446 \n\n\n 9 \n 2333 \n\n\n s \n 1 \n\n\n\n# Diagrama de barras con los meses del año y el número de viviendas vendidas\np_barras(df_cualitativas, var_grade, xlab = name)\n\nAl igual que la anterior, la variable ""grade"" determina la nota general de la vivienda propuesta por la zona, dado que la mayor parte de las viviendas se distribuyen entre los valores del 6 al 10 podríamos simplificarla creando nuevas categorías.\nEstudio de la variable ""zipcode"" (código postal):\nvar_zipcode = df_cualitativas$view\nname = ""zipcode""\n\n# Tablas de frecuencias en función al mes y al año\ntable(var_zipcode) %>% kable(., col.names = c(name, \'Frecuencia\'))\n\n\n\n zipcode \n Frecuencia \n\n\n\n\n 0 \n 17505 \n\n\n 1 \n 302 \n\n\n 2 \n 859 \n\n\n 3 \n 458 \n\n\n 4 \n 297 \n\n\n\n# Diagrama de barras con los meses del año y el número de viviendas vendidas\np_barras(df_cualitativas, var_zipcode, xlab = name)\n\nLa variable ""zipcode"" podría ser de ayuda pero debido a que el 90% de las viviendas en la base de datos se ubican bajo el mismo código postal, ésta podría ser deshechada.\nc. Análisis multivariante cuantitativo\nComentar procedimiento\nnumeric_cols <- c(""sqft_living"", ""sqft_lot"", ""sqft_living15"", ""price"", ""sqft_basement"", ""sqft_above"")\nnumeric_cols2 <- c(""yr_built"", ""yr_renovated"", \'lat\', \'long\', ""price"")\n\ndf_cuantitativas %>% select(numeric_cols) %>%\n  na.omit() %>%\n  ggpairs(columns=1:6)\n\ndf_cuantitativas %>% select(numeric_cols2) %>%\n  na.omit() %>%\n  ggpairs(columns=1:5)\n\np1 <- df_cuantitativas %>% select(numeric_cols) %>%\n  na.omit() %>%\n  ggcorr()\n\np2 <- df_cuantitativas %>% select(numeric_cols2) %>%\n  na.omit() %>%\n  ggcorr()\n\ngrid.arrange(p1, p2, nrow=1)\n\nCorrelaciones superiores al 40%:\nall_cols <- c(""sqft_living"", ""sqft_lot"", ""sqft_living15"", ""price"", ""sqft_basement"", ""sqft_above"", ""yr_built"", ""yr_renovated"", \'lat\', \'long\', ""price"")\n\n# Correlaciones numéricas de las variables\nz <- df_cuantitativas %>% select(all_cols) %>%\n  na.omit() %>%\n  cor()\n\n# Tabla de correlaciones\nas.data.frame(as.table(z)) %>% subset(., abs(Freq) > 0.4) %>% subset(., abs(Freq) != 1) %>% .[order(.$Freq, decreasing = TRUE),] %>%\n  .[c(seq(1, nrow(.), by=2)),]\n##             Var1          Var2      Freq\n## 6     sqft_above   sqft_living 0.8778606\n## 3  sqft_living15   sqft_living 0.7549953\n## 26    sqft_above sqft_living15 0.7296436\n## 4          price   sqft_living 0.7039337\n## 36    sqft_above         price 0.6085944\n## 24         price sqft_living15 0.5838456\n## 5  sqft_basement   sqft_living 0.4346360\n## 57      yr_built    sqft_above 0.4219294\n## 70          long      yr_built 0.4110889\n\nTras este primer análisisi podemos observar que las variables que tienen mayor correlación son:\n\n""sqft_above"" y ""sqft_living"" (0.87): tiene sentido que la mayor relación se produzca entre la superficie de la vivienda y de la azotea o huella en el terreno.\n""sqft_living"" y ""sqft_living15"" (0.75): esta correlación es obvia dado que son las mismas variables de superficie de la vivienda actualizadas a 2015.\n""price"" y ""sqft_living"" (0.70): tiene también mucho sentido que el precio tenga una gran relación con la superficie de vivienda.\n""price"" y ""sqft_above"" (0.60): al igual que en la anterior el precio también tendrá una gran relación con la superficie de la huella.\n""sqft_living"" y ""sqft_basement"" (0.43): en los casos en los hay sótano parece que también hay mucha relación con la superficie de la vivienda por lo que sería interesante no convertir esta variable en variable dummy.\n""sqft_above"" y ""yr_built"" (0.42): curiosamente hay una correlación del 40% entre los metros cuadrados de huella y el año de construcción, probablemente por la normativa y por las promotoras que actuaron en esos momentos.\n""long"" y ""yr_built"" (0.41): además podemos ver una correlación entre el año de construcción y la longitud de la localización de la vivienda, ya que el crecimiento del núcleo de población se produciría en un eje.\n\nc. Análisis multivariante cualitativo\nComentar procedimiento\nEstudio de la variable ""bedrooms"" (número de habitaciones por vivienda):\nold = ""bedrooms""\nnew = ""cat_bedrooms""\n\n#categorizamos la variable y mostramos su tabla de frecuencias\nprice_tplusc[[new]] = cut2(price_tplusc[[old]], g=4)\ntable(price_tplusc[[new]])\n## \n## [1, 4)      4 [5,33] \n##  11489   6164   1735\n\n#visualización de los datos categorizados con respecto a la variable ""price""\np_densidad(price_tplusc, new)\n\nComentar resultados\nEstudio de la variable ""bathrooms"" (número de baños/aseos por vivienda):\nold = ""bathrooms""\nnew = ""cat_bathrooms""\n\n#categorizamos la variable y mostramos su tabla de frecuencias\nprice_tplusc[[new]] = cut2(price_tplusc[[old]], g=3)\ntable(price_tplusc[[new]])\n## \n## [0.50,2.00) [2.00,2.75) [2.75,8.00] \n##        7585        8413        3406\n\n#visualización de los datos categorizados con respecto a la variable ""price""\np_densidad(price_tplusc, new)\n\nComentar resultados\nEstudio de la variable ""floors"" (número de plantas por vivienda):\nold = ""floors""\nnew = ""cat_floors""\n\n#categorizamos la variable y mostramos su tabla de frecuencias\nprice_tplusc[[new]] = cut2(price_tplusc[[old]], g=3)\ntable(price_tplusc[[new]])\n## \n##       1.0 [1.5,2.5) [2.5,3.5] \n##      9587      9145       684\n\n#visualización de los datos categorizados con respecto a la variable ""price""\np_densidad(price_tplusc, new)\n\nComentar los resultados\nEstudio de la variable ""waterfront"" (número de plantas por vivienda):\nname = ""waterfront""\n\n#mostramos su tabla de frecuencias\ntable(price_tplusc[[name]])\n## \n##     0     1 \n## 19267   149\n\n#visualización de los datos categorizados con respecto a la variable ""price""\np_densidad(price_tplusc, name)\n\n#visualización de los datos categorizados con respecto a la variable ""long""\np1 = p_densidad2(price_tplusc, name, \'long\')\n#visualización de los datos categorizados con respecto a la variable ""lat""\np2 = p_densidad2(price_tplusc, name, \'lat\')\n\n# Creación de grid\ngrid.arrange(p1, p2, nrow=1)\n\nComentar los resultados\nEstudio de la variable ""view"" (número de plantas por vivienda):\nold = ""view""\nnew = ""cat_view""\n\n#categorizamos la variable y mostramos su tabla de frecuencias\nprice_tplusc[[new]] = cut2(price_tplusc[[old]], g=3)\ntable(price_tplusc[[new]])\n## \n##     0 [1,4] \n## 17505  1916\n\n#visualización de los datos categorizados con respecto a la variable ""price""\np_densidad(price_tplusc, new)\n\nComentar los resultados\nEstudio de la variable ""condition"" (número de plantas por vivienda):\nold = ""condition""\nnew = ""cat_condition""\n\n#categorizamos la variable y mostramos su tabla de frecuencias\nprice_tplusc[[new]] = cut2(price_tplusc[[old]], g=3)\ntable(price_tplusc[[new]])\n## \n## [1,4)     4     5 \n## 12753  5120  1539\n\n#visualización de los datos categorizados con respecto a la variable ""price""\np_densidad(price_tplusc, new)\n\nComentar los resultados\nEstudio de la variable ""grade"" (número de plantas por vivienda):\nold = ""grade""\nnew = ""cat_grade""\n\n#corrección reemplazando el string s por un ""NA""\ntable(price_tplusc[[old]])\n## \n##   10   11   12   13    3    4    5    6    7    8    9    s \n## 1018  368   79   12    1   26  217 1839 8081 5446 2333    1\n\nprice_tplusc[[old]][price_tplusc[[old]] == ""s""] <- NA\nprice_tplusc[[old]] = as.numeric(price_tplusc[[old]])\ntable(price_tplusc[[old]])\n## \n##    1    2    3    4    5    6    7    8    9   10   11 \n## 1018  368   79   12    1   26  217 1839 8081 5446 2333\n\n#categorizamos la variable y mostramos su tabla de frecuencias\nprice_tplusc[[new]] = cut2(price_tplusc[[old]], g=4)\ntable(price_tplusc[[new]])\n## \n## [ 1,10)      10      11 \n##   11641    5446    2333\n\n#visualización de los datos categorizados con respecto a la variable ""price""\np_densidad(price_tplusc, new)\n\nComentar los resultados\nEstudio de la variable ""zipcode"" (código postal):\nold = ""zipcode""\nnew = ""cat_zipcode""\n\n#categorizamos la variable y mostramos su tabla de frecuencias\nprice_tplusc[[new]] = cut2(price_tplusc[[old]], g=3)\ntable(price_tplusc[[new]])\n## \n## [98001,98042) [98042,98112) [98112,98199] \n##          6518          6463          6458\n\n#visualización de los datos categorizados con respecto a la variable ""price""\np_densidad(price_tplusc, new)\n\nComentar los resultados\n04 Detección, tratamiento e imputación de datos faltantes\nAnálisis previo\naggr_plot <- aggr(price_tplusc, col=c(\'#464159\',\'#c7f0db\'), numbers=TRUE, sortVars=TRUE,\n                  labels=names(price_tplusc), cex.axis=.7, gap=1, \n                  ylab=c(""Histogram of missing data"",""Pattern""))\n\n## \n##  Variables sorted by number of missings: \n##       Variable        Count\n##       sqft_lot 2.726478e-03\n##       bedrooms 2.623592e-03\n##   cat_bedrooms 2.623592e-03\n##    sqft_living 1.851947e-03\n##     sqft_above 1.851947e-03\n##  sqft_basement 1.851947e-03\n##      bathrooms 1.800504e-03\n##  cat_bathrooms 1.800504e-03\n##      condition 1.388960e-03\n##  cat_condition 1.388960e-03\n##         floors 1.183188e-03\n##     waterfront 1.183188e-03\n##     cat_floors 1.183188e-03\n##          grade 9.774165e-04\n##      cat_grade 9.774165e-04\n##           view 9.259736e-04\n##       cat_view 9.259736e-04\n##       yr_built 7.202017e-04\n##   yr_renovated 5.144298e-04\n##            lat 5.144298e-05\n##             id 0.000000e+00\n##           date 0.000000e+00\n##          price 0.000000e+00\n##        zipcode 0.000000e+00\n##           long 0.000000e+00\n##  sqft_living15 0.000000e+00\n##     sqft_lot15 0.000000e+00\n##    cat_zipcode 0.000000e+00\n\nImputación de las variables contínuas:\n#Muestra de variables de la base de datos seleccionada\nstructure(df_cuantitativas)\n\n\n\n variable \n classe \n first_values \n\n\n\n\n price \n double \n 538000, 180000, 604000, 510000, 1230000, 257500 \n\n\n sqft_living \n integer \n 2570, 770, NA, 1680, 5420, 1715 \n\n\n sqft_lot \n integer \n 7242, 10000, 5000, 8080, 101930, 6819 \n\n\n sqft_above \n integer \n 2170, 770, 1050, 1680, 3890, 1715 \n\n\n sqft_basement \n integer \n 400, 0, 910, 0, 1530, 0 \n\n\n yr_built \n integer \n 1951, 1933, 1965, 1987, 2001, 1995 \n\n\n yr_renovated \n integer \n 1991, 0, 0, 0, 0, 0 \n\n\n lat \n double \n 47.721, 477.379, 475.208, 476.168, 476.561, 473.097 \n\n\n long \n double \n -122.319, -122.233, -122.393, -122.045, -122.005, -122.327 \n\n\n sqft_living15 \n integer \n 1690, 2720, 1360, 1800, 4760, 2238 \n\n\n sqft_lot15 \n integer \n 7639, 8062, 5000, 7503, 101930, 6819 \n\n\n\n#Muestra de variables missing\nsapply(df_cuantitativas, function(x) sum(is.na(x))) %>% kable()\n\n\n\n \n x \n\n\n\n\n price \n 0 \n\n\n sqft_living \n 36 \n\n\n sqft_lot \n 53 \n\n\n sqft_above \n 36 \n\n\n sqft_basement \n 36 \n\n\n yr_built \n 14 \n\n\n yr_renovated \n 10 \n\n\n lat \n 1 \n\n\n long \n 0 \n\n\n sqft_living15 \n 0 \n\n\n sqft_lot15 \n 0 \n\n\n\n\n\nImputación de variables\nImputación de la variable ""sqft_living"" (superficie de la vivienda):\nLa variable ""sqft_living"" es una de las más importantes debído a su alta correlación con la variable ""price"". Para asegurarnos de que no haya sesgo vamos a usar el método knn sobre las variables más relaciondas. Una de las variables que más nos va a ayudar a la imputación será ""sqft_living15"" ya que esla superficie de la vivienda medida en 2015.\n#método de imputación knn\nimputed <- price_tplusc %>% select(price, sqft_living, sqft_living15, bedrooms, sqft_lot, sqft_above, sqft_living15) %>% VIM::kNN(variable=\'sqft_living\')\n\n#guardado de la variable\nprice_tplusc$sqft_living_im = imputed$sqft_living\n\n#Visualización y comprobación de la varaible\ncolumns <- c(""price"", ""sqft_living"", ""sqft_living15"")\nimputed %>% select(columns) %>%\n  na.omit() %>%\n  ggpairs(columns=1:length(columns))\n\n#Visualización y comprobación de los datos imputados\nimputed %>% select(sqft_living15, sqft_living, sqft_living_imp)  %>% marginplot(., delimiter = \'_imp\')\n\n\nTras imputar la variable podemos ver que la correlación se mantiene y que no se producen grandes cambios.\nImputación de la variable ""sqft_basement"" (superficie del sótano):\nLa variable ""sqft_basement"" es una variable dificil de imputar debido a que si la vivienda no tiene sótano su valor es cero. Por ello para su correcta imputación debe estudiarse el análisis multivariante.\nPodemos observar que las variables con mayor correlación con respecto a ""sqft_basement"" son:\n\nsqft_living con un 0.43\nbedrooms con un 0.30\nprice con un 0.32\nbathrooms con un 0.28\n\nPor ello usaremos estas variables para su imputación.\n#método de imputación knn\nimputed <- price_tplusc %>% select(sqft_basement, price, sqft_living, bedrooms, bathrooms) %>%\n  VIM::kNN(variable=\'sqft_basement\')\n\n#guardado de la variable\nprice_tplusc$sqft_basement_im = imputed$sqft_basement\nhead(price_tplusc)\n##           id       date   price bedrooms bathrooms sqft_living sqft_lot floors\n## 1 6414100192 12/09/2014  538000        3      2.25        2570     7242      2\n## 2 5631500400  2/25/2015  180000        2      1.00         770    10000      1\n## 3 2487200875 12/09/2014  604000        4      3.00          NA     5000      1\n## 4 1954400510  2/18/2015  510000        3      2.00        1680     8080      1\n## 5 7237550310 05/12/2014 1230000        4      4.50        5420   101930      1\n## 6 1321400060  6/27/2014  257500        3      2.25        1715     6819      2\n##   waterfront view condition grade sqft_above sqft_basement yr_built\n## 1          0    0         3     9       2170           400     1951\n## 2          0    0         3     8        770             0     1933\n## 3          0    0         5     9       1050           910     1965\n## 4         NA    0         3    10       1680             0     1987\n## 5          0    0         3     2       3890          1530     2001\n## 6          0    0         3     9       1715             0     1995\n##   yr_renovated zipcode     lat     long sqft_living15 sqft_lot15 cat_bedrooms\n## 1         1991   98125  47.721 -122.319          1690       7639       [1, 4)\n## 2            0   98028 477.379 -122.233          2720       8062       [1, 4)\n## 3            0   98136 475.208 -122.393          1360       5000            4\n## 4            0   98074 476.168 -122.045          1800       7503       [1, 4)\n## 5            0   98053 476.561 -122.005          4760     101930            4\n## 6            0   98003 473.097 -122.327          2238       6819       [1, 4)\n##   cat_bathrooms cat_floors cat_view cat_condition cat_grade   cat_zipcode\n## 1   [2.00,2.75)  [1.5,2.5)        0         [1,4)   [ 1,10) [98112,98199]\n## 2   [0.50,2.00)        1.0        0         [1,4)   [ 1,10) [98001,98042)\n## 3   [2.75,8.00]        1.0        0             5   [ 1,10) [98112,98199]\n## 4   [2.00,2.75)        1.0        0         [1,4)        10 [98042,98112)\n## 5   [2.75,8.00]        1.0        0         [1,4)   [ 1,10) [98042,98112)\n## 6   [2.00,2.75)  [1.5,2.5)        0         [1,4)   [ 1,10) [98001,98042)\n##   sqft_living_im sqft_basement_im\n## 1           2570              400\n## 2            770                0\n## 3           1820              910\n## 4           1680                0\n## 5           5420             1530\n## 6           1715                0\n\n#Visualización y comprobación de la varaible\ncolumns <- c(""price"", ""sqft_basement"", ""sqft_living"")\nimputed %>% select(columns) %>%\n  na.omit() %>%\n  ggpairs(columns=1:length(columns))\n\n#Visualización y comprobación de los datos imputados\nimputed %>% select(sqft_living, sqft_basement, sqft_basement_imp)  %>% marginplot(., delimiter = \'_imp\')\n\n\nImputación de la variable ""sqft_lot"" (superficie de la parcela):\nLa variable ""sqft_lot"" es una variable con baja correlación entre las demás salvo con ""sqft_lot15"" ya que es la misma variable actualizada en el año 2015, puediendo usarla directamente para imputarla.\n#método de imputación knn\nimputed <- price_tplusc %>% select(price, sqft_lot, sqft_lot15) %>%\n  VIM::kNN(variable=""sqft_lot"")\n\n#guardado de la variable\nprice_tplusc$sqft_lot_im = imputed$sqft_lot\n\n#Visualización y comprobación de la varaible\ncolumns <- c(""price"", ""sqft_lot"", ""sqft_lot15"")\nimputed %>% select(columns) %>%\n  na.omit() %>%\n  ggpairs(columns=1:length(columns))\n\n#Visualización y comprobación de los datos imputados\nimputed %>% select(sqft_lot15, sqft_lot, sqft_lot_imp)  %>% marginplot(., delimiter = \'_imp\')\n\n\nImputación de la variable ""sqft_above"" (superficie de la parcela):\nLa variable ""sqft_above"" es una variable....\n#método de imputación knn\nimputed <- price_tplusc %>% select(price, sqft_above, sqft_living, sqft_living15) %>%\n  VIM::kNN(variable=""sqft_above"")\n\n#guardado de la variable\nprice_tplusc$sqft_above_im = imputed$sqft_above\n\n#Visualización y comprobación de la varaible\ncolumns <- c(""price"", ""sqft_above"")\nimputed %>% select(columns) %>%\n  na.omit() %>%\n  ggpairs(columns=1:length(columns))\n\n#Visualización y comprobación de los datos imputados\nimputed %>% select(price, sqft_above, sqft_above_imp)  %>% marginplot(., delimiter = \'_imp\')\n\n\n**Imputación de la variable ""yr_built"" (superficie de la parcela):**\nLa variable ""yr_built"" es una variable con baja correlación entre las demás salvo con ""long"" probablemente por el crecimiento del barrio de manera direccional y con ""yr_renovated"" debido a que a más antiguo es el año de construcción más probable es su renovación.\n#método de imputación knn\nimputed <- price_tplusc %>% select(price, yr_built, long, yr_renovated) %>%\n  VIM::kNN(variable=""yr_built"")\n\n#guardado de la variable\nprice_tplusc$yr_built_im = imputed$yr_built\n\n#Visualización y comprobación de la varaible\ncolumns <- c(""price"", ""yr_built"")\nimputed %>% select(columns) %>%\n  na.omit() %>%\n  ggpairs(columns=1:length(columns))\n\n#Visualización y comprobación de los datos imputados\nimputed %>% select(long, yr_built, yr_built_imp)  %>% marginplot(., delimiter = \'_imp\')\n\n\nImputación de las variables discretas:\n#Muestra de variables de la base de datos seleccionada\nstructure(df_cualitativas)\n\n\n\n variable \n classe \n first_values \n\n\n\n\n date \n integer \n 12/09/2014, 2/25/2015, 12/09/2014, 2/18/2015, 05/12/2014, 6/27/2014 \n\n\n bedrooms \n integer \n 3, 2, 4, 3, 4, 3 \n\n\n bathrooms \n double \n 2.25, 1, 3, 2, 4.5, 2.25 \n\n\n floors \n double \n 2, 1, 1, 1, 1, 2 \n\n\n waterfront \n integer \n 0, 0, 0, NA, 0, 0 \n\n\n view \n integer \n 0, 0, 0, 0, 0, 0 \n\n\n condition \n integer \n 3, 3, 5, 3, 3, 3 \n\n\n grade \n integer \n 7, 6, 7, 8, 11, 7 \n\n\n zipcode \n integer \n 98125, 98028, 98136, 98074, 98053, 98003 \n\n\n dates_m \n list \n c(""12"", ""02"", ""12"", ""02"", ""05"", ""06"") \n\n\n dates_y \n list \n c(""2014"", ""2015"", ""2014"", ""2015"", ""2014"", ""2014"") \n\n\n\n#Muestra de variables missing\nsapply(df_cualitativas, function(x) sum(is.na(x))) %>% kable()\n\n\n\n \n x \n\n\n\n\n date \n 0 \n\n\n bedrooms \n 51 \n\n\n bathrooms \n 35 \n\n\n floors \n 23 \n\n\n waterfront \n 23 \n\n\n view \n 18 \n\n\n condition \n 27 \n\n\n grade \n 18 \n\n\n zipcode \n 0 \n\n\n dates_m \n 0 \n\n\n dates_y \n 0 \n\n\n\n#Correlaciones entre variables discretas numéricas\ndiscretas_col <- c(""bedrooms"", ""bathrooms"", \'price\', \'floors\')\n\nprice_tplusc %>% select(discretas_col) %>%\n  na.omit() %>%\n  ggpairs(columns=1:4)\n\n\n**Imputación de la variable ""bedrooms"" (habitaciones por vivienda):**\nLa variable ""berooms"" es una variable importante ya que está altamente relacionada con la superficie de la vivienda, con el número de baños y con el número de plantas.\n#método de imputación knn\nimputed <- price_tplusc %>% select(bedrooms, bathrooms, floors, sqft_living) %>%\n  VIM::kNN(variable=""bedrooms"")\n\n#guardado de la variable\nprice_tplusc$bedrooms_im = imputed$bedrooms\n\n#Visualización y comprobación de la varaible\ncolumns <- c(\'sqft_living\', \'bathrooms\')\nimputed %>% select(columns) %>%\n  na.omit() %>%\n  ggpairs(columns=1:length(columns))\n\n#Visualización y comprobación de los datos imputados\nimputed %>% select(sqft_living, bedrooms, bedrooms_imp)  %>% marginplot(., delimiter = \'_imp\')\n\n\n**Imputación de la variable ""bathrooms"" (baños/aseos por vivienda):**\nLa variable ""bathrooms"" es una variable importante ya que está altamente relacionada con la superficie de la vivienda y el número de habitaciones.\n#método de imputación knn\nimputed <- price_tplusc %>% select(bedrooms, bathrooms, sqft_living, floors) %>%\n  VIM::kNN(variable=""bathrooms"")\n\n#guardado de la variable\nprice_tplusc$bathrooms_im = imputed$bathrooms\n\n#Visualización y comprobación de la varaible\ncolumns <- c(""bedrooms"", ""bathrooms"")\nimputed %>% select(columns) %>%\n  na.omit() %>%\n  ggpairs(columns=1:length(columns))\n\n#Visualización y comprobación de los datos imputados\nimputed %>% select(bedrooms, bathrooms, bathrooms_imp)  %>% marginplot(., delimiter = \'_imp\')\n\n\n**Imputación de la variable ""floors"" (baños/aseos por vivienda):**\nLa variable ""floors"" es una variable importante ya que está altamente relacionada con la superficie de la vivienda, habitaciones y baños.\n#método de imputación knn\nimputed <- price_tplusc %>% select(bedrooms, bathrooms, floors, sqft_living) %>%\n  VIM::kNN(variable=""floors"")\n\n#guardado de la variable\nprice_tplusc$floors_im = imputed$floors\n\n#Visualización y comprobación de la varaible\ncolumns <- c(""sqft_living"", ""floors"")\nimputed %>% select(columns) %>%\n  na.omit() %>%\n  ggpairs(columns=1:length(columns))\n\n#Visualización y comprobación de los datos imputados\nimputed %>% select(sqft_living, floors, floors_imp)  %>% marginplot(., delimiter = \'_imp\')\n\n\n**Imputación de la variable ""waterfront"" (vivienda con vistas):**\nLa variable ""waterfront"" es una variable que influye de manera directa sobre el precio de la vivienda pero que no puede obtenerse de manera segura. Podríamos seleccionar las variables de ""long"" y de ""grade"" ya que las vistas al mar están relacionadas con la ubicación y con la nota dada por el vecindario.\n#método de imputación knn\nimputed <- price_tplusc %>% select(waterfront, long, grade) %>%\n  VIM::kNN(variable=""waterfront"")\n\n#guardado de la variable\nprice_tplusc$waterfront_im = imputed$waterfront\n\n#Visualización y comprobación de los datos imputados\nimputed %>% select(grade, waterfront, waterfront_imp)  %>% marginplot(., delimiter = \'_imp\')\n\n\n**Imputación de la variable ""view"" (vivienda con vistas):**\nLa variable ""view"" es una variable que influye de manera directa sobre el precio de la vivienda pero que no puede obtenerse de manera segur dado que es el número de veces que se ha visitado una vivienda en venta. Podríamos seleccionar las variables de ""condition"" y de ""grade"" ya que estarán muy relacionadas con las calidades de la vivienda, y las variables de superficie de vivienda y de huella dado que al estar muy relacionadas con el precio influiran en las visitas para su venta.\n#método de imputación knn\nimputed <- price_tplusc %>% select(view, sqft_above, sqft_living, grade, condition) %>%\n  VIM::kNN(variable=""view"")\n\n#guardado de la variable\nprice_tplusc$view_im = imputed$view\n\n#Visualización y comprobación de los datos imputados\nimputed %>% select(grade, view, view_imp)  %>% marginplot(., delimiter = \'_imp\')\n\n\n**Imputación de la variable ""condition"":**\nLa variable ""condition"" probablmente esté muy relacionada con la variable ""grade"" ya que es la calificación del vecindario, el año de construcción ""yr_built"" y la variable ""long"" que responde al año de construcción.\nnumeric_cols4 <- c(""condition"", ""grade"", ""yr_built"", \'yr_renovated\', \'long\')\n\nprice_tplusc %>% select(numeric_cols4) %>%\n  na.omit() %>%\n  ggpairs(columns=1:5)\n\ndescribe(price_tplusc$condition)\n## price_tplusc$condition \n##        n  missing distinct     Info     Mean      Gmd \n##    19412       27        5     0.71    3.411   0.6189 \n## \n## lowest : 1 2 3 4 5, highest: 1 2 3 4 5\n##                                         \n## Value          1     2     3     4     5\n## Frequency     28   161 12564  5120  1539\n## Proportion 0.001 0.008 0.647 0.264 0.079\n\n#método de imputación knn\nimputed <- price_tplusc %>% select(condition, grade, long, yr_built, bathrooms) %>%\n  VIM::kNN(variable=""condition"")\n\n#guardado de la variable\nprice_tplusc$condition_im = imputed$condition\n\n#Visualización y comprobación de los datos imputados\nimputed %>% select(grade, condition, condition_imp)  %>% marginplot(., delimiter = \'_imp\')\n\n\n**Imputación de la variable ""grade"":**\nnumeric_cols4 <- c(\'condition\', \'grade\', \'yr_built\', \'yr_renovated\', \'long\', \'bathrooms\', \'view\', \'waterfront\')\n\nprice_tplusc %>% select(numeric_cols4) %>%\n  na.omit() %>%\n  ggpairs(columns=1:5)\n\ndescribe(price_tplusc$grade)\n## price_tplusc$grade \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##    19420       19       11    0.903     8.82    1.948        1        8 \n##      .25      .50      .75      .90      .95 \n##        9        9       10       11       11 \n## \n## lowest :  1  2  3  4  5, highest:  7  8  9 10 11\n##                                                                             \n## Value          1     2     3     4     5     6     7     8     9    10    11\n## Frequency   1018   368    79    12     1    26   217  1839  8081  5446  2333\n## Proportion 0.052 0.019 0.004 0.001 0.000 0.001 0.011 0.095 0.416 0.280 0.120\n\nprice_tplusc %>% select(condition, grade)  %>% marginplot()\n\n#método de imputación knn\nimputed <- price_tplusc %>% select(numeric_cols4) %>%\n  VIM::kNN(variable=""grade"")\n\n#guardado de la variable\nprice_tplusc$grade_im = imputed$grade\n\n#Visualización y comprobación de los datos imputados\nimputed %>% select(condition, grade, grade_imp)  %>% marginplot(., delimiter = \'_imp\')\n\n\n\n05 Transformación de variables\n#Muestra de variables de la base de datos seleccionada\nshow_df = data.frame(variable = names(price_tplusc),\n           classe = sapply(price_tplusc, typeof),\n           first_values = sapply(price_tplusc, function(x) paste0(head(x),  collapse = "", "")),\n           row.names = NULL)\nkable(show_df) %>%\n  kable_styling()\n\n\n\n variable \n classe \n first_values \n\n\n\n\n id \n double \n 6414100192, 5631500400, 2487200875, 1954400510, 7237550310, 1321400060 \n\n\n date \n integer \n 12/09/2014, 2/25/2015, 12/09/2014, 2/18/2015, 05/12/2014, 6/27/2014 \n\n\n price \n double \n 538000, 180000, 604000, 510000, 1230000, 257500 \n\n\n bedrooms \n integer \n 3, 2, 4, 3, 4, 3 \n\n\n bathrooms \n double \n 2.25, 1, 3, 2, 4.5, 2.25 \n\n\n sqft_living \n integer \n 2570, 770, NA, 1680, 5420, 1715 \n\n\n sqft_lot \n integer \n 7242, 10000, 5000, 8080, 101930, 6819 \n\n\n floors \n double \n 2, 1, 1, 1, 1, 2 \n\n\n waterfront \n integer \n 0, 0, 0, NA, 0, 0 \n\n\n view \n integer \n 0, 0, 0, 0, 0, 0 \n\n\n condition \n integer \n 3, 3, 5, 3, 3, 3 \n\n\n grade \n double \n 9, 8, 9, 10, 2, 9 \n\n\n sqft_above \n integer \n 2170, 770, 1050, 1680, 3890, 1715 \n\n\n sqft_basement \n integer \n 400, 0, 910, 0, 1530, 0 \n\n\n yr_built \n integer \n 1951, 1933, 1965, 1987, 2001, 1995 \n\n\n yr_renovated \n integer \n 1991, 0, 0, 0, 0, 0 \n\n\n zipcode \n integer \n 98125, 98028, 98136, 98074, 98053, 98003 \n\n\n lat \n double \n 47.721, 477.379, 475.208, 476.168, 476.561, 473.097 \n\n\n long \n double \n -122.319, -122.233, -122.393, -122.045, -122.005, -122.327 \n\n\n sqft_living15 \n integer \n 1690, 2720, 1360, 1800, 4760, 2238 \n\n\n sqft_lot15 \n integer \n 7639, 8062, 5000, 7503, 101930, 6819 \n\n\n cat_bedrooms \n integer \n [1, 4), [1, 4), 4, [1, 4), 4, [1, 4) \n\n\n cat_bathrooms \n integer \n [2.00,2.75), [0.50,2.00), [2.75,8.00], [2.00,2.75), [2.75,8.00], [2.00,2.75) \n\n\n cat_floors \n integer \n [1.5,2.5), 1.0, 1.0, 1.0, 1.0, [1.5,2.5) \n\n\n cat_view \n integer \n 0, 0, 0, 0, 0, 0 \n\n\n cat_condition \n integer \n [1,4), [1,4), 5, [1,4), [1,4), [1,4) \n\n\n cat_grade \n integer \n [ 1,10), [ 1,10), [ 1,10), 10, [ 1,10), [ 1,10) \n\n\n cat_zipcode \n integer \n [98112,98199], [98001,98042), [98112,98199], [98042,98112), [98042,98112), [98001,98042) \n\n\n sqft_living_im \n integer \n 2570, 770, 1820, 1680, 5420, 1715 \n\n\n sqft_basement_im \n integer \n 400, 0, 910, 0, 1530, 0 \n\n\n sqft_lot_im \n integer \n 7242, 10000, 5000, 8080, 101930, 6819 \n\n\n sqft_above_im \n integer \n 2170, 770, 1050, 1680, 3890, 1715 \n\n\n yr_built_im \n integer \n 1951, 1933, 1965, 1987, 2001, 1995 \n\n\n bedrooms_im \n integer \n 3, 2, 4, 3, 4, 3 \n\n\n bathrooms_im \n double \n 2.25, 1, 3, 2, 4.5, 2.25 \n\n\n floors_im \n double \n 2, 1, 1, 1, 1, 2 \n\n\n waterfront_im \n integer \n 0, 0, 0, 0, 0, 0 \n\n\n view_im \n integer \n 0, 0, 0, 0, 0, 0 \n\n\n condition_im \n integer \n 3, 3, 5, 3, 3, 3 \n\n\n grade_im \n double \n 9, 8, 9, 10, 2, 9 \n\n\n\nTransformación de variables cuantitativas\n\n**Transformación de la variable ""price"" (valor de venta de la vivienda):**\n# Posibles soluciones\n# 1.-Transformacion logaritmico 10\n# 2.-Raiz cuadrada\n# 3.-Inversa 1/x\n\n# Prueba 1 . Logaritmo\nprice_log10 <- log10(price_tplusc$price)\n#Prueba 2 . Raiz cuadrada\nprice_sqrt <- sqrt(price_tplusc$price)\n\n# Variable transformada\nprice_tplusc$price_trans <- price_log10\n\n# Histogramas\nh1 <- phist(price_tplusc, ., \'price\')\nh2 <- phist(price_tplusc, ., \'price_trans\')\ngrid.arrange(h1, h2, nrow=1)\n\n# Quantil-quantil plot\nq1 <- qqplot.data(price_tplusc$price)\nq2 <- qqplot.data(price_log10)\nq3 <- qqplot.data(price_sqrt)\ngrid.arrange(q1, q2, q3, nrow=1)\n\n# test de normalidad\nlillie.test(price_log10)\n## \n## \tLilliefors (Kolmogorov-Smirnov) normality test\n## \n## data:  price_log10\n## D = 0.026553, p-value < 2.2e-16\n\nAunque tras aplicar la transformación logarítmica y el test de Kolmogorov-Smirnov el resultado a una distribución normal sigue siendo negativo, se puede ver un gran avance en el gráfico quantil-quantil.\n\n**Transformación de la variable ""sqft_living"" (Superficie de la vivienda):**\n# posibles soluciones\n# 1.-Transformacion logaritmico 10\n# 2.-Raiz cuadrada\n# 3.-Inversa 1/x\n# 4.-Box-Cox\n\n#Prueba 1 . Logaritmo\nprice_tplusc$sqft_living_trans = log10(price_tplusc$sqft_living_im)\n\nh1 = phist(price_tplusc, ., \'sqft_living_im\')\nh2 = phist(price_tplusc, ., \'sqft_living_trans\')\ngrid.arrange(h1, h2, nrow=1)\n\nq1 = qqplot.data(price_tplusc$sqft_living_im)\nq2 = qqplot.data(price_tplusc$sqft_living_trans)\ngrid.arrange(q1, q2, nrow=1)\n\n# test de normalidad\nlillie.test(price_tplusc$price_trans)\n## \n## \tLilliefors (Kolmogorov-Smirnov) normality test\n## \n## data:  price_tplusc$price_trans\n## D = 0.026553, p-value < 2.2e-16\n\nAunque tras aplicar la transformación logarítmica y el test de Kolmogorov-Smirnov el resultado a una distribución normal sigue siendo negativo se puede ver un gran avance en el gráfico quantil-quantil. Se esta manera nos quedaremos con la variable transformada\n\n**Transformación de la variable ""sqft_basement"" (Superficie del sótano):**\n# posibles soluciones\n# 1.-Transformacion logaritmico 10\n# 2.-Raiz cuadrada\n# 3.-Inversa 1/x\n# 4.-Box-Cox\n\n#Prueba 2 . Raiz cuadrada\nprice_tplusc$sqft_basement_trans = sqrt(price_tplusc$sqft_basement_im)\n\nh1 = phist(price_tplusc, ., \'sqft_basement_im\')\nh2 = phist(price_tplusc, ., \'sqft_basement_trans\')\ngrid.arrange(h1, h2, nrow=1)\n\nq1 = qqplot.data(price_tplusc$sqft_basement_im)\nq2 = qqplot.data(price_tplusc$sqft_basement_trans)\ngrid.arrange(q1, q2, nrow=1)\n\nTransformación de variables cualitativas\n\n#dado que la variable contiene muchos ceros cuando la casa no tiene sótano esta variable va a ser categorizada.\nold = ""basement_im""\nnew = ""sqft_basement_trans""\n\n# Convertimos la variable en categórica\nprice_tplusc$sqft_basement_trans <- price_tplusc$sqft_basement_im\nprice_tplusc$sqft_basement_trans[price_tplusc$sqft_basement_im == 0] = ""no_base""\nprice_tplusc$sqft_basement_trans[price_tplusc$sqft_basement_im <= 1000 & price_tplusc$sqft_basement_im != 0] = ""base(0:1000]""\nprice_tplusc$sqft_basement_trans[price_tplusc$sqft_basement_im > 1000] = ""base(1000, inf)""\nprice_tplusc$sqft_basement_trans <- as.factor(price_tplusc$sqft_basement_trans)\n\n# Diagrama de densidad\np_densidad2(price_tplusc, new, \'price_trans\')\n\n\n**Transformación de la variable ""sqft_lot"" (Superficie de la vivienda):**\n# posibles soluciones\n# 1.-Transformacion logaritmico 10\n# 2.-Raiz cuadrada\n# 3.-Inversa 1/\n# 4.-Box-Cox\n\nsummary(price_tplusc$sqft_lot_im)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##     520    5040    7620   15109   10650 1651359\n\n#Prueba 1 . Logaritmo\nlot_log10 <- log10(price_tplusc$sqft_lot_im)\n#Prueba 2 . Raiz cuadrada\nlot_sqrt <- sqrt(price_tplusc$sqft_lot_im)\nprice_tplusc$sqft_lot_trans <- lot_log10\n\nh1 <- phist(price_tplusc, ., \'sqft_lot_im\')\nh2 <- phist(price_tplusc, ., \'sqft_lot_trans\')\ngrid.arrange(h1, h2, nrow=1)\n\nq1 <- qqplot.data(price_tplusc$sqft_lot_im)\nq2 <- qqplot.data(lot_sqrt)\nq3 <- qqplot.data(lot_log10)\ngrid.arrange(q1, q2, q3, nrow=1)\n\n# Dado que la varaible es muy desigual vamos a categorizarla\nold = ""sqft_lot_im""\nnew = ""sqft_lot_trans""\n\n#categorizamos la variable y mostramos su tabla de frecuencias\nprice_tplusc[[new]] = cut2(price_tplusc[[old]], g=4)\ntable(price_tplusc[[new]])\n## \n## [  520,   5042) [ 5042,   7621) [ 7621,  10652) [10652,1651359] \n##            4866            4875            4841            4857\n\n#visualización de los datos categorizados con respecto a la variable ""price""\np_densidad2(price_tplusc, new, \'price_trans\')\n\n\nTransformación de la variable ""sqft_above"" (Superficie de la vivienda):\n# posibles soluciones\n# 1.-Transformacion logaritmico 10\n# 2.-Raiz cuadrada\n# 3.-Inversa 1/x\n# 4.-Box-Cox\n\nsummary(price_tplusc$sqft_above_im)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##     370    1200    1560    1788    2210    9410\n\n#Prueba 2 . Raiz cuadrada\nprice_tplusc$sqft_above_trans = sqrt(price_tplusc$sqft_above_im)\n\nh1 = phist(price_tplusc, ., \'sqft_above_im\')\nh2 = phist(price_tplusc, ., \'sqft_above_trans\')\ngrid.arrange(h1, h2, nrow=1)\n\nq1 = qqplot.data(price_tplusc$sqft_above_im)\nq2 = qqplot.data(price_tplusc$sqft_above_trans)\ngrid.arrange(q1, q2, nrow=1)\n\n# Dado que la varaible es muy desigual vamos a categorizarla\nold = ""sqft_above_im""\nnew = ""sqft_above_trans""\n\n#categorizamos la variable y mostramos su tabla de frecuencias\nprice_tplusc[[new]] = cut2(price_tplusc[[old]], g=4)\ntable(price_tplusc[[new]])\n## \n## [ 370,1210) [1210,1564) [1564,2216) [2216,9410] \n##        5036        4710        4865        4828\n\n#visualización de los datos categorizados con respecto a la variable ""price""\np_densidad2(price_tplusc, new, \'price_trans\')\n\n\nTransformación de la variable ""yr_build"" (Superficie de la vivienda):\n# posibles soluciones\n# 1.-Transformacion logaritmico 10\n# 2.-Raiz cuadrada\n# 3.-Inversa 1/x\n# 4.-Box-Cox\n\nsummary(price_tplusc$yr_built_im)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    1900    1951    1975    1971    1997    2015\n\n#Prueba 1. Transformacion logaritmico 10.\nprice_tplusc$yr_built_trans0 = log10(price_tplusc$yr_built_im)\n\nh1 = phist(price_tplusc, ., \'yr_built_im\')\nh2 = phist(price_tplusc, ., \'yr_built_trans0\')\ngrid.arrange(h1, h2, nrow=1)\n\nq1 = qqplot.data(price_tplusc$yr_built_im)\nq2 = qqplot.data(price_tplusc$yr_built_trans)\ngrid.arrange(q1, q2, nrow=1)\n\n#Categorizamos la variable\n# Dado que la varaible es muy desigual vamos a categorizarla\nold = ""yr_built_im""\nnew = ""yr_built_trans""\n\n#categorizamos la variable y mostramos su tabla de frecuencias\nprice_tplusc[[new]] = cut2(price_tplusc[[old]], g=3)\ntable(price_tplusc[[new]])\n## \n## [1900,1960) [1960,1990) [1990,2015] \n##        6686        6500        6253\n\n#visualización de los datos categorizados con respecto a la variable ""price""\np_densidad2(price_tplusc, new, \'price_trans\')\n\n\n\nTrasformaciṕn de variables cualitativas\ncolors = c(\'#6c7b95\', \'#8bbabb\', \'#c7f0db\')\n\nTransformación de la variable ""bedrooms_im"":\nprice_tplusc <- price_tplusc %>% mutate(bedrooms_trans = cut2(bedrooms_im, g=3))\n\nprice_tplusc %>%\n  select(bedrooms_trans) %>% table()\n## .\n## [1, 4)      4 [5,33] \n##  11518   6185   1736\n\nq1 <- price_tplusc %>%\n          select(price_trans, bedrooms_trans) %>%\n          ggplot(aes(x=price_trans, colour=bedrooms_trans)) +\n          geom_density() + scale_color_manual(values=colors)\n\nq2 <- price_tplusc %>%\n          select(price_trans, bedrooms_trans) %>%\n          ggplot(aes(y=price_trans, fill=bedrooms_trans)) +\n          geom_boxplot() + scale_fill_manual(values=colors)\n\ngrid.arrange(q1, q2, nrow=1)\n\n\nTransformación de la variable ""bathrooms_im"":\nprice_tplusc$bathrooms_im <- ceiling(price_tplusc$bathrooms_im)\n\nprice_tplusc <- price_tplusc %>% mutate(bathrooms_trans = cut2(bathrooms_im, g=3))\n\nprice_tplusc %>%\n  select(bathrooms_trans) %>% table()\n## .\n## [1,3)     3 [4,8] \n##  9324  8410  1705\n\nq1 <- price_tplusc %>%\n          select(price_trans, bathrooms_trans) %>%\n          ggplot(aes(x=price_trans, colour=bathrooms_trans)) +\n          geom_density() + scale_color_manual(values=colors)\n\nq2 <- price_tplusc %>%\n          select(price_trans, bathrooms_trans) %>%\n          ggplot(aes(y=price_trans, fill=bathrooms_trans)) +\n          geom_boxplot() + scale_fill_manual(values=colors)\n\ngrid.arrange(q1, q2, nrow=1)\n\n\nTransformación de la variable ""floors_im"":\n# Categorizamos la variable de plantas de vivienda en 2 grupos\nprice_tplusc <- price_tplusc %>% mutate(floors_trans = cut2(floors_im, g=2))\n\nprice_tplusc %>%\n  select(floors_trans) %>% table()\n## .\n## [1,2.0) [2,3.5] \n##   11318    8121\n\nq1 <- price_tplusc %>%\n          select(price_trans, floors_trans) %>%\n          ggplot(aes(x=price_trans, colour=floors_trans)) +\n          geom_density() + scale_color_manual(values=colors)\n\nq2 <- price_tplusc %>%\n          select(price_trans, floors_trans) %>%\n          ggplot(aes(y=price_trans, fill=floors_trans)) +\n          geom_boxplot() + scale_fill_manual(values=colors)\n\ngrid.arrange(q1, q2, nrow=1)\n\n# Categorizamos la variable de plantas de vivienda en 3 grupos\nprice_tplusc <- price_tplusc %>% mutate(floors_trans = cut2(floors_im, g=3))\n\nprice_tplusc %>%\n  select(floors_trans) %>% table()\n## .\n##       1.0 [1.5,2.5) [2.5,3.5] \n##      9600      9154       685\n\nq1 <- price_tplusc %>%\n          select(price_trans, floors_trans) %>%\n          ggplot(aes(x=price_trans, colour=floors_trans)) +\n          geom_density() + scale_color_manual(values=colors)\n\nq2 <- price_tplusc %>%\n          select(price_trans, floors_trans) %>%\n          ggplot(aes(y=price_trans, fill=floors_trans)) +\n          geom_boxplot() + scale_fill_manual(values=colors)\n\ngrid.arrange(q1, q2, nrow=1)\n\n\nTransformación de la variable ""condition_im"":\n price_tplusc <- price_tplusc %>% mutate(condition_trans = cut2(condition_im, g=3))\n\n price_tplusc %>%\n   select(condition_trans) %>% table()\n## .\n## [1,4)     4     5 \n## 12771  5129  1539\n\n price_tplusc %>%\n           select(price_trans, condition_trans) %>%\n           ggplot(aes(x=price_trans, colour=condition_trans)) +\n           geom_density() + scale_color_manual(values=colors)\n\nprice_tplusc %>%\n          select(price_trans, condition_trans) %>%\n          ggplot(aes(y=price_trans, fill=condition_trans)) +\n          geom_boxplot() + scale_fill_manual(values=colors)\n\n\nTransformación de la variable ""grade_im"":\n price_tplusc <- price_tplusc %>% mutate(grade_trans = cut2(grade_im, g=3))\n\n price_tplusc %>%\n   select(grade_trans) %>% table()\n## .\n## [ 1,10)      10      11 \n##   11655    5449    2335\n\n price_tplusc %>%\n           select(price_trans, grade_trans) %>%\n           ggplot(aes(x=price_trans, colour=grade_trans)) +\n           geom_density() + scale_color_manual(values=colors)\n\nprice_tplusc %>%\n          select(price_trans, grade_trans) %>%\n          ggplot(aes(y=price_trans, fill=grade_trans)) +\n          geom_boxplot() + scale_fill_manual(values=colors)\n\n\nTransformación de la variable ""waterfront_im"":\n price_tplusc <- price_tplusc %>% mutate(waterfront_trans = cut2(waterfront_im, g=2))\n\n price_tplusc %>%\n   select(waterfront_trans) %>% table()\n## .\n##     0     1 \n## 19290   149\n\n price_tplusc %>%\n           select(price_trans, waterfront_trans) %>%\n           ggplot(aes(x=price_trans, colour=waterfront_trans)) +\n           geom_density() + scale_color_manual(values=colors)\n\nprice_tplusc %>%\n          select(price_trans, waterfront_trans) %>%\n          ggplot(aes(y=price_trans, fill=waterfront_trans)) +\n          geom_boxplot() + scale_fill_manual(values=colors)\n\n\nTransformación de la variable ""view_im"":\n price_tplusc <- price_tplusc %>% mutate(view_trans = cut2(view_im, g=2))\n\n price_tplusc %>%\n   select(view_trans) %>% table()\n## .\n##     0 [1,4] \n## 17522  1917\n\n price_tplusc %>%\n           select(price_trans, view_trans) %>%\n           ggplot(aes(x=price_trans, colour=view_trans)) +\n           geom_density() + scale_color_manual(values=colors)\n\nprice_tplusc %>%\n          select(price_trans, view_trans) %>%\n          ggplot(aes(y=price_trans, fill=view_trans)) +\n          geom_boxplot() + scale_fill_manual(values=colors)\n\n\nResumen final\nComentarios sobre las variables a estudiar tras el análisis y la transformación de cada una:\n\nid: no será usada ya que no se considera un parámetro de la vivienda.\ndate: la fecha de venta de la vivienda no será usada inicialmente.\nprice: se usará la variable sin convertir y la variable convertida mediante el logaritmo.\nbedrooms: se categoriza esta variable en 3 grupos.\nbathrooms: se redondea la variable a la alza y la categorizaremos en 3 grupos.\nsqft_living: se aplica la conversion de logaritmo.\nsqft_lot: debido al número tan alto de outliars se categoriza la variable en 3 grupos.\nfloors: se categoriza la variable en 3 grupos.\nwaterfront: variable dummy.\nview: se categoriza la variable en dummy.\ncondition: se categoriza la variable en 3 grupos.\ngrade: se categoriza la varaible en 3 grupos\nsqft_above: se categoriza la variable en 4 grupos.\nsqft_basement: se categoriza la variable en 3 grupos.\nyr_built: se categoriza la variable en 3 grupos.\nyr_renovated: se categoriza la variable en 2 grupos.\nzip_code: no será usada esta variable.\nlat: no será usada esta variable.\nlong: no será usada esta variable.\nsqft_living15: no será usada esta variable ya que es prácticamente la misma que sqft_living.\nsqft_lot15:no será usada esta variable ya que es prácticamente la misma que sqft_lot.\n\n\n06 Comprobación de los datos\ncolumns <- c(\'price\', \'price_trans\', \'sqft_living_trans\', \'sqft_basement_trans\', \'sqft_lot_trans\', \'sqft_above_trans\', \'yr_built_trans\', \'bedrooms_trans\', \'bathrooms_trans\', \'floors_trans\', \'condition_trans\', \'grade_trans\', \'waterfront_trans\', \'view_trans\')\n\n# Creación de la nueva tabla\ndf_transformadas <- select(price_tplusc, columns)\n\n# Estructura\ndf_transformadas  %>% structure()\n\n\n\n variable \n classe \n first_values \n\n\n\n\n price \n double \n 538000, 180000, 604000, 510000, 1230000, 257500 \n\n\n price_trans \n double \n 5.73078227566639, 5.25527250510331, 5.78103693862113, 5.70757017609794, 6.0899051114394, 5.41077723337721 \n\n\n sqft_living_trans \n double \n 3.40993312333129, 2.88649072517248, 3.26007138798507, 3.22530928172586, 3.73399928653839, 3.23426412437879 \n\n\n sqft_basement_trans \n integer \n base(0:1000], no_base, base(0:1000], no_base, base(1000, inf), no_base \n\n\n sqft_lot_trans \n integer \n [ 5042,   7621), [ 7621,  10652), [  520,   5042), [ 7621,  10652), [10652,1651359], [ 5042,   7621) \n\n\n sqft_above_trans \n integer \n [1564,2216), [ 370,1210), [ 370,1210), [1564,2216), [2216,9410], [1564,2216) \n\n\n yr_built_trans \n integer \n [1900,1960), [1900,1960), [1960,1990), [1960,1990), [1990,2015], [1990,2015] \n\n\n bedrooms_trans \n integer \n [1, 4), [1, 4), 4, [1, 4), 4, [1, 4) \n\n\n bathrooms_trans \n integer \n 3, [1,3), 3, [1,3), [4,8], 3 \n\n\n floors_trans \n integer \n [1.5,2.5), 1.0, 1.0, 1.0, 1.0, [1.5,2.5) \n\n\n condition_trans \n integer \n [1,4), [1,4), 5, [1,4), [1,4), [1,4) \n\n\n grade_trans \n integer \n [ 1,10), [ 1,10), [ 1,10), 10, [ 1,10), [ 1,10) \n\n\n waterfront_trans \n integer \n 0, 0, 0, 0, 0, 0 \n\n\n view_trans \n integer \n 0, 0, 0, 0, 0, 0 \n\n\n\nview_cols <- c(\'price_trans\', \'sqft_living_trans\', \'sqft_basement_trans\')\n\n#numeric\ndf_transformadas %>% select(view_cols) %>%\n  ggpairs(columns=1:3)\n\n\n07 Ajuste, interpretación y diagnosis del modelo de regresión lineal múltiple\nEn este apartado entrenaremos el modelo MLR mediante la variable ""price"" sin convertir, convertida mediante logaritmo, aplicación de Lasso y y la aplicación de Best Subset para búsqueda de variables automática.\n\nEntrenamiento del modelo ML con todas las variables\nEntrenamiento del logaritmo con variable ""price"" sin transformar\nlm_model_std = lm(price ~ sqft_living_trans + bedrooms_trans + bathrooms_trans + sqft_basement_trans + sqft_above_trans + grade_trans + condition_trans + view_trans + waterfront_trans + sqft_above_trans + yr_built_trans + sqft_lot_trans, data=df_transformadas)\n\nsummary(lm_model_std)\n## \n## Call:\n## lm(formula = price ~ sqft_living_trans + bedrooms_trans + bathrooms_trans + \n##     sqft_basement_trans + sqft_above_trans + grade_trans + condition_trans + \n##     view_trans + waterfront_trans + sqft_above_trans + yr_built_trans + \n##     sqft_lot_trans, data = df_transformadas)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1348541  -128892   -15169   103553  5719200 \n## \n## Coefficients:\n##                                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)                        -3227134      86410 -37.347  < 2e-16 ***\n## sqft_living_trans                   1185140      27710  42.769  < 2e-16 ***\n## bedrooms_trans4                      -47991       4690 -10.234  < 2e-16 ***\n## bedrooms_trans[5,33]                 -52898       7520  -7.034 2.07e-12 ***\n## bathrooms_trans3                      17186       5467   3.144 0.001670 ** \n## bathrooms_trans[4,8]                 289994       9198  31.526  < 2e-16 ***\n## sqft_basement_transbase(1000, inf)    32592       7781   4.189 2.82e-05 ***\n## sqft_basement_transno_base            44164       5779   7.642 2.23e-14 ***\n## sqft_above_trans[1210,1564)          -60925       6316  -9.646  < 2e-16 ***\n## sqft_above_trans[1564,2216)          -85081       8627  -9.862  < 2e-16 ***\n## sqft_above_trans[2216,9410]          -16487      12918  -1.276 0.201847    \n## grade_trans10                        -15629       4638  -3.370 0.000753 ***\n## grade_trans11                         35099       6548   5.360 8.41e-08 ***\n## condition_trans4                      22815       4568   4.994 5.95e-07 ***\n## condition_trans5                      59723       7171   8.329  < 2e-16 ***\n## view_trans[1,4]                      162160       6684  24.261  < 2e-16 ***\n## waterfront_trans1                    704083      21599  32.597  < 2e-16 ***\n## yr_built_trans[1960,1990)           -114509       4978 -23.002  < 2e-16 ***\n## yr_built_trans[1990,2015]           -151687       6182 -24.536  < 2e-16 ***\n## sqft_lot_trans[ 5042,   7621)        -94989       5364 -17.708  < 2e-16 ***\n## sqft_lot_trans[ 7621,  10652)        -98198       5703 -17.219  < 2e-16 ***\n## sqft_lot_trans[10652,1651359]        -66784       6035 -11.067  < 2e-16 ***\n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## Residual standard error: 252000 on 19417 degrees of freedom\n## Multiple R-squared:  0.539,\tAdjusted R-squared:  0.5385 \n## F-statistic:  1081 on 21 and 19417 DF,  p-value: < 2.2e-16\n\n# Residuals normality\nplot(lm_model_std, 1)\n\n# Residuals qq plot\nplot(lm_model_std, 2)\n\nSe puede observar que en este caso al aplicar el precio sin la conversión logarítimica el gráfico Q-Q sobre los residuos no se acerca a una distribución normal. Por otro lado, el error R2 es de 0.54.\nEntrenamiento del logaritmo con variable ""price"" transformada mediante logaritmo\nlm_model_log = lm(price_trans ~ sqft_living_trans + bedrooms_trans + bathrooms_trans + sqft_basement_trans + sqft_above_trans + grade_trans + condition_trans + view_trans + waterfront_trans + sqft_above_trans + yr_built_trans + sqft_lot_trans, data=df_transformadas)\n\n\nsummary(lm_model_log)\n## \n## Call:\n## lm(formula = price_trans ~ sqft_living_trans + bedrooms_trans + \n##     bathrooms_trans + sqft_basement_trans + sqft_above_trans + \n##     grade_trans + condition_trans + view_trans + waterfront_trans + \n##     sqft_above_trans + yr_built_trans + sqft_lot_trans, data = df_transformadas)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.64371 -0.10293  0.00503  0.10233  0.65169 \n## \n## Coefficients:\n##                                      Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)                         3.1834456  0.0508965  62.547  < 2e-16 ***\n## sqft_living_trans                   0.7829506  0.0163218  47.970  < 2e-16 ***\n## bedrooms_trans4                    -0.0240736  0.0027622  -8.715  < 2e-16 ***\n## bedrooms_trans[5,33]               -0.0400963  0.0044296  -9.052  < 2e-16 ***\n## bathrooms_trans3                    0.0149778  0.0032200   4.651 3.32e-06 ***\n## bathrooms_trans[4,8]                0.1198488  0.0054180  22.120  < 2e-16 ***\n## sqft_basement_transbase(1000, inf) -0.0114566  0.0045830  -2.500  0.01244 *  \n## sqft_basement_transno_base         -0.0001795  0.0034038  -0.053  0.95795    \n## sqft_above_trans[1210,1564)        -0.0122009  0.0037204  -3.279  0.00104 ** \n## sqft_above_trans[1564,2216)        -0.0224536  0.0050815  -4.419 9.98e-06 ***\n## sqft_above_trans[2216,9410]         0.0219629  0.0076087   2.887  0.00390 ** \n## grade_trans10                       0.0382870  0.0027317  14.016  < 2e-16 ***\n## grade_trans11                       0.0836074  0.0038570  21.677  < 2e-16 ***\n## condition_trans4                    0.0169452  0.0026907   6.298 3.08e-10 ***\n## condition_trans5                    0.0434966  0.0042237  10.298  < 2e-16 ***\n## view_trans[1,4]                     0.0873905  0.0039370  22.197  < 2e-16 ***\n## waterfront_trans1                   0.2096667  0.0127223  16.480  < 2e-16 ***\n## yr_built_trans[1960,1990)          -0.0840429  0.0029323 -28.661  < 2e-16 ***\n## yr_built_trans[1990,2015]          -0.1082314  0.0036415 -29.722  < 2e-16 ***\n## sqft_lot_trans[ 5042,   7621)      -0.0830848  0.0031596 -26.296  < 2e-16 ***\n## sqft_lot_trans[ 7621,  10652)      -0.0932985  0.0033590 -27.775  < 2e-16 ***\n## sqft_lot_trans[10652,1651359]      -0.0689835  0.0035545 -19.408  < 2e-16 ***\n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## Residual standard error: 0.1484 on 19417 degrees of freedom\n## Multiple R-squared:  0.5812,\tAdjusted R-squared:  0.5808 \n## F-statistic:  1283 on 21 and 19417 DF,  p-value: < 2.2e-16\n\n# Residuals normality\nplot(lm_model_log, 1)\n\n# Residuals qq plot\nplot(lm_model_log, 2)\n\n# Normality test\nlillie.test(lm_model_log$residuals)\n## \n## \tLilliefors (Kolmogorov-Smirnov) normality test\n## \n## data:  lm_model_log$residuals\n## D = 0.017163, p-value = 7.249e-14\n\nPodemos observar que tras entrenar el modelo LMR sobre la variable precio convertida mediante el logaritmo se obtiene una gráfica Q-Q con una distribución más ajustada a ser normal, a pesar de que tras el test de kolmogorov Smirnov no se acepta la normalidad. Además se puede observar que el error R2 aumenta a 0.58.\n\nSelección de varibles mediante Lasso\ndata_lasso <- df_transformadas[,-1] \nlambdas <- model.matrix(price_trans~., data = data_lasso)\ny <- df_transformadas$price_trans\n\n# Funciones de error por variable\nmodels_lasso <- glmnet(x = lambdas, y = y, alpha = 1)\nplot(models_lasso, xvar = ""lambda"", label = TRUE)\n\nset.seed(737)\n\n# Ajuste de la función de error\ncv_lasso <- cv.glmnet(x = lambdas, y = y, alpha = 1)\nplot(cv_lasso)\n\n# Registro resumen con variables seleccionadas por Lasso != 0\nout_eleven <- glmnet(lambdas,y,alpha=1,lambda = cv_lasso$lambda.1se)\nlasso_coef_eleven <- predict(out_eleven, type=""coefficients"")[1:21,]\nlasso_coef_eleven\n##                        (Intercept)                        (Intercept) \n##                       3.427094e+00                       0.000000e+00 \n##                  sqft_living_trans sqft_basement_transbase(1000, inf) \n##                       6.973286e-01                       0.000000e+00 \n##         sqft_basement_transno_base      sqft_lot_trans[ 5042,   7621) \n##                      -1.219528e-02                      -5.643702e-02 \n##      sqft_lot_trans[ 7621,  10652)      sqft_lot_trans[10652,1651359] \n##                      -6.543591e-02                      -3.562403e-02 \n##        sqft_above_trans[1210,1564)        sqft_above_trans[1564,2216) \n##                      -4.904879e-05                       0.000000e+00 \n##        sqft_above_trans[2216,9410]          yr_built_trans[1960,1990) \n##                       4.073876e-02                      -7.316130e-02 \n##          yr_built_trans[1990,2015]                    bedrooms_trans4 \n##                      -8.639597e-02                      -7.789614e-03 \n##               bedrooms_trans[5,33]                   bathrooms_trans3 \n##                      -1.530957e-02                       0.000000e+00 \n##               bathrooms_trans[4,8]              floors_trans[1.5,2.5) \n##                       9.399933e-02                       7.921263e-03 \n##              floors_trans[2.5,3.5]                   condition_trans4 \n##                       7.418065e-02                       9.470070e-03 \n##                   condition_trans5 \n##                       3.609647e-02\n\nPodemos observar que Lasso selecciona aquellas variables que permiten reducir la función de error llegando lo antes posible al mínimo sin penalizar en exceso a las predicciones del modelo.\n\n# Lambdas mínimos vs lambdas seleccionados\ncv_lasso\n## \n## Call:  cv.glmnet(x = lambdas, y = y, alpha = 1) \n## \n## Measure: Mean-Squared Error \n## \n##        Lambda Measure        SE Nonzero\n## min 0.0001739 0.02189 0.0002533      23\n## 1se 0.0023532 0.02212 0.0002605      20\n\nPodemos ver como para el error que considera Lasso como mínimo usa 23 variables, en cambio ha obtenido muy buenos resultados quitando 3 variables.\n\n# Test de normalidad de residuos\npreds_lasso <- predict(out_eleven,lambdas)\nresiduals_lasso <- y - preds_lasso\nlillie.test(residuals_lasso)\n## \n## \tLilliefors (Kolmogorov-Smirnov) normality test\n## \n## data:  residuals_lasso\n## D = 0.021656, p-value < 2.2e-16\n\n\n# Error de R2 tras la selección mediante Lasso\nrsq_lasso <- cor(y, preds_lasso)^2\nsprintf(""R2 = %f"", rsq_lasso)\n## [1] ""R2 = 0.580651""\n\nFinalmente podemos observar que el error R2 obtenido mediante Lasso es muy similar al obtenido manualmente en los apartados anteriores.\n\n\nSelección de variables automática mediante Best Subset\nMétodo 01: Best Subset.\nbest_subsets_models <- regsubsets(price_trans~., data = df_transformadas[,-1], nvmax = 22)\nsummary(best_subsets_models)\n## Subset selection object\n## Call: regsubsets.formula(price_trans ~ ., data = df_transformadas[, \n##     -1], nvmax = 22)\n## 23 Variables  (and intercept)\n##                                    Forced in Forced out\n## sqft_living_trans                      FALSE      FALSE\n## sqft_basement_transbase(1000, inf)     FALSE      FALSE\n## sqft_basement_transno_base             FALSE      FALSE\n## sqft_lot_trans[ 5042,   7621)          FALSE      FALSE\n## sqft_lot_trans[ 7621,  10652)          FALSE      FALSE\n## sqft_lot_trans[10652,1651359]          FALSE      FALSE\n## sqft_above_trans[1210,1564)            FALSE      FALSE\n## sqft_above_trans[1564,2216)            FALSE      FALSE\n## sqft_above_trans[2216,9410]            FALSE      FALSE\n## yr_built_trans[1960,1990)              FALSE      FALSE\n## yr_built_trans[1990,2015]              FALSE      FALSE\n## bedrooms_trans4                        FALSE      FALSE\n## bedrooms_trans[5,33]                   FALSE      FALSE\n## bathrooms_trans3                       FALSE      FALSE\n## bathrooms_trans[4,8]                   FALSE      FALSE\n## floors_trans[1.5,2.5)                  FALSE      FALSE\n## floors_trans[2.5,3.5]                  FALSE      FALSE\n## condition_trans4                       FALSE      FALSE\n## condition_trans5                       FALSE      FALSE\n## grade_trans10                          FALSE      FALSE\n## grade_trans11                          FALSE      FALSE\n## waterfront_trans1                      FALSE      FALSE\n## view_trans[1,4]                        FALSE      FALSE\n## 1 subsets of each size up to 22\n## Selection Algorithm: exhaustive\n##           sqft_living_trans sqft_basement_transbase(1000, inf)\n## 1  ( 1 )  ""*""               "" ""                               \n## 2  ( 1 )  ""*""               "" ""                               \n## 3  ( 1 )  ""*""               "" ""                               \n## 4  ( 1 )  ""*""               "" ""                               \n## 5  ( 1 )  ""*""               "" ""                               \n## 6  ( 1 )  ""*""               "" ""                               \n## 7  ( 1 )  ""*""               "" ""                               \n## 8  ( 1 )  ""*""               "" ""                               \n## 9  ( 1 )  ""*""               "" ""                               \n## 10  ( 1 ) ""*""               "" ""                               \n## 11  ( 1 ) ""*""               "" ""                               \n## 12  ( 1 ) ""*""               "" ""                               \n## 13  ( 1 ) ""*""               "" ""                               \n## 14  ( 1 ) ""*""               "" ""                               \n## 15  ( 1 ) ""*""               "" ""                               \n## 16  ( 1 ) ""*""               "" ""                               \n## 17  ( 1 ) ""*""               "" ""                               \n## 18  ( 1 ) ""*""               "" ""                               \n## 19  ( 1 ) ""*""               "" ""                               \n## 20  ( 1 ) ""*""               "" ""                               \n## 21  ( 1 ) ""*""               "" ""                               \n## 22  ( 1 ) ""*""               ""*""                               \n##           sqft_basement_transno_base sqft_lot_trans[ 5042,   7621)\n## 1  ( 1 )  "" ""                        "" ""                          \n## 2  ( 1 )  "" ""                        "" ""                          \n## 3  ( 1 )  "" ""                        "" ""                          \n## 4  ( 1 )  "" ""                        "" ""                          \n## 5  ( 1 )  "" ""                        "" ""                          \n## 6  ( 1 )  "" ""                        "" ""                          \n## 7  ( 1 )  "" ""                        "" ""                          \n## 8  ( 1 )  "" ""                        ""*""                          \n## 9  ( 1 )  "" ""                        ""*""                          \n## 10  ( 1 ) "" ""                        ""*""                          \n## 11  ( 1 ) "" ""                        ""*""                          \n## 12  ( 1 ) "" ""                        ""*""                          \n## 13  ( 1 ) "" ""                        ""*""                          \n## 14  ( 1 ) "" ""                        ""*""                          \n## 15  ( 1 ) "" ""                        ""*""                          \n## 16  ( 1 ) "" ""                        ""*""                          \n## 17  ( 1 ) "" ""                        ""*""                          \n## 18  ( 1 ) "" ""                        ""*""                          \n## 19  ( 1 ) "" ""                        ""*""                          \n## 20  ( 1 ) "" ""                        ""*""                          \n## 21  ( 1 ) "" ""                        ""*""                          \n## 22  ( 1 ) "" ""                        ""*""                          \n##           sqft_lot_trans[ 7621,  10652) sqft_lot_trans[10652,1651359]\n## 1  ( 1 )  "" ""                           "" ""                          \n## 2  ( 1 )  "" ""                           "" ""                          \n## 3  ( 1 )  "" ""                           "" ""                          \n## 4  ( 1 )  "" ""                           "" ""                          \n## 5  ( 1 )  "" ""                           "" ""                          \n## 6  ( 1 )  "" ""                           "" ""                          \n## 7  ( 1 )  "" ""                           "" ""                          \n## 8  ( 1 )  ""*""                           "" ""                          \n## 9  ( 1 )  ""*""                           ""*""                          \n## 10  ( 1 ) ""*""                           ""*""                          \n## 11  ( 1 ) ""*""                           ""*""                          \n## 12  ( 1 ) ""*""                           ""*""                          \n## 13  ( 1 ) ""*""                           ""*""                          \n## 14  ( 1 ) ""*""                           ""*""                          \n## 15  ( 1 ) ""*""                           ""*""                          \n## 16  ( 1 ) ""*""                           ""*""                          \n## 17  ( 1 ) ""*""                           ""*""                          \n## 18  ( 1 ) ""*""                           ""*""                          \n## 19  ( 1 ) ""*""                           ""*""                          \n## 20  ( 1 ) ""*""                           ""*""                          \n## 21  ( 1 ) ""*""                           ""*""                          \n## 22  ( 1 ) ""*""                           ""*""                          \n##           sqft_above_trans[1210,1564) sqft_above_trans[1564,2216)\n## 1  ( 1 )  "" ""                         "" ""                        \n## 2  ( 1 )  "" ""                         "" ""                        \n## 3  ( 1 )  "" ""                         "" ""                        \n## 4  ( 1 )  "" ""                         "" ""                        \n## 5  ( 1 )  "" ""                         "" ""                        \n## 6  ( 1 )  "" ""                         "" ""                        \n## 7  ( 1 )  "" ""                         "" ""                        \n## 8  ( 1 )  "" ""                         "" ""                        \n## 9  ( 1 )  "" ""                         "" ""                        \n## 10  ( 1 ) "" ""                         "" ""                        \n## 11  ( 1 ) "" ""                         "" ""                        \n## 12  ( 1 ) "" ""                         "" ""                        \n## 13  ( 1 ) "" ""                         "" ""                        \n## 14  ( 1 ) "" ""                         "" ""                        \n## 15  ( 1 ) "" ""                         "" ""                        \n## 16  ( 1 ) "" ""                         "" ""                        \n## 17  ( 1 ) "" ""                         "" ""                        \n## 18  ( 1 ) "" ""                         "" ""                        \n## 19  ( 1 ) ""*""                         ""*""                        \n## 20  ( 1 ) ""*""                         ""*""                        \n## 21  ( 1 ) ""*""                         ""*""                        \n## 22  ( 1 ) ""*""                         ""*""                        \n##           sqft_above_trans[2216,9410] yr_built_trans[1960,1990)\n## 1  ( 1 )  "" ""                         "" ""                      \n## 2  ( 1 )  "" ""                         "" ""                      \n## 3  ( 1 )  "" ""                         ""*""                      \n## 4  ( 1 )  "" ""                         ""*""                      \n## 5  ( 1 )  "" ""                         ""*""                      \n## 6  ( 1 )  "" ""                         ""*""                      \n## 7  ( 1 )  "" ""                         ""*""                      \n## 8  ( 1 )  "" ""                         ""*""                      \n## 9  ( 1 )  "" ""                         ""*""                      \n## 10  ( 1 ) "" ""                         ""*""                      \n## 11  ( 1 ) "" ""                         ""*""                      \n## 12  ( 1 ) ""*""                         ""*""                      \n## 13  ( 1 ) ""*""                         ""*""                      \n## 14  ( 1 ) ""*""                         ""*""                      \n## 15  ( 1 ) ""*""                         ""*""                      \n## 16  ( 1 ) ""*""                         ""*""                      \n## 17  ( 1 ) ""*""                         ""*""                      \n## 18  ( 1 ) ""*""                         ""*""                      \n## 19  ( 1 ) "" ""                         ""*""                      \n## 20  ( 1 ) "" ""                         ""*""                      \n## 21  ( 1 ) ""*""                         ""*""                      \n## 22  ( 1 ) ""*""                         ""*""                      \n##           yr_built_trans[1990,2015] bedrooms_trans4 bedrooms_trans[5,33]\n## 1  ( 1 )  "" ""                       "" ""             "" ""                 \n## 2  ( 1 )  "" ""                       "" ""             "" ""                 \n## 3  ( 1 )  "" ""                       "" ""             "" ""                 \n## 4  ( 1 )  ""*""                       "" ""             "" ""                 \n## 5  ( 1 )  ""*""                       "" ""             "" ""                 \n## 6  ( 1 )  ""*""                       "" ""             "" ""                 \n## 7  ( 1 )  ""*""                       "" ""             "" ""                 \n## 8  ( 1 )  ""*""                       "" ""             "" ""                 \n## 9  ( 1 )  ""*""                       "" ""             "" ""                 \n## 10  ( 1 ) ""*""                       "" ""             "" ""                 \n## 11  ( 1 ) ""*""                       "" ""             "" ""                 \n## 12  ( 1 ) ""*""                       "" ""             "" ""                 \n## 13  ( 1 ) ""*""                       "" ""             "" ""                 \n## 14  ( 1 ) ""*""                       "" ""             "" ""                 \n## 15  ( 1 ) ""*""                       "" ""             ""*""                 \n## 16  ( 1 ) ""*""                       ""*""             ""*""                 \n## 17  ( 1 ) ""*""                       ""*""             ""*""                 \n## 18  ( 1 ) ""*""                       ""*""             ""*""                 \n## 19  ( 1 ) ""*""                       ""*""             ""*""                 \n## 20  ( 1 ) ""*""                       ""*""             ""*""                 \n## 21  ( 1 ) ""*""                       ""*""             ""*""                 \n## 22  ( 1 ) ""*""                       ""*""             ""*""                 \n##           bathrooms_trans3 bathrooms_trans[4,8] floors_trans[1.5,2.5)\n## 1  ( 1 )  "" ""              "" ""                  "" ""                  \n## 2  ( 1 )  "" ""              "" ""                  "" ""                  \n## 3  ( 1 )  "" ""              "" ""                  "" ""                  \n## 4  ( 1 )  "" ""              "" ""                  "" ""                  \n## 5  ( 1 )  "" ""              ""*""                  "" ""                  \n## 6  ( 1 )  "" ""              ""*""                  "" ""                  \n## 7  ( 1 )  "" ""              ""*""                  "" ""                  \n## 8  ( 1 )  "" ""              ""*""                  "" ""                  \n## 9  ( 1 )  "" ""              ""*""                  "" ""                  \n## 10  ( 1 ) "" ""              ""*""                  "" ""                  \n## 11  ( 1 ) "" ""              ""*""                  "" ""                  \n## 12  ( 1 ) "" ""              ""*""                  "" ""                  \n## 13  ( 1 ) "" ""              ""*""                  "" ""                  \n## 14  ( 1 ) "" ""              ""*""                  "" ""                  \n## 15  ( 1 ) "" ""              ""*""                  "" ""                  \n## 16  ( 1 ) "" ""              ""*""                  "" ""                  \n## 17  ( 1 ) "" ""              ""*""                  "" ""                  \n## 18  ( 1 ) ""*""              ""*""                  "" ""                  \n## 19  ( 1 ) "" ""              ""*""                  ""*""                  \n## 20  ( 1 ) ""*""              ""*""                  ""*""                  \n## 21  ( 1 ) ""*""              ""*""                  ""*""                  \n## 22  ( 1 ) ""*""              ""*""                  ""*""                  \n##           floors_trans[2.5,3.5] condition_trans4 condition_trans5 grade_trans10\n## 1  ( 1 )  "" ""                   "" ""              "" ""              "" ""          \n## 2  ( 1 )  "" ""                   "" ""              "" ""              "" ""          \n## 3  ( 1 )  "" ""                   "" ""              "" ""              "" ""          \n## 4  ( 1 )  "" ""                   "" ""              "" ""              "" ""          \n## 5  ( 1 )  "" ""                   "" ""              "" ""              "" ""          \n## 6  ( 1 )  "" ""                   "" ""              "" ""              "" ""          \n## 7  ( 1 )  ""*""                   "" ""              "" ""              "" ""          \n## 8  ( 1 )  "" ""                   "" ""              "" ""              "" ""          \n## 9  ( 1 )  "" ""                   "" ""              "" ""              "" ""          \n## 10  ( 1 ) "" ""                   "" ""              "" ""              "" ""          \n## 11  ( 1 ) "" ""                   "" ""              "" ""              ""*""          \n## 12  ( 1 ) "" ""                   "" ""              "" ""              ""*""          \n## 13  ( 1 ) ""*""                   "" ""              "" ""              ""*""          \n## 14  ( 1 ) ""*""                   "" ""              ""*""              ""*""          \n## 15  ( 1 ) ""*""                   "" ""              ""*""              ""*""          \n## 16  ( 1 ) ""*""                   "" ""              ""*""              ""*""          \n## 17  ( 1 ) ""*""                   ""*""              ""*""              ""*""          \n## 18  ( 1 ) ""*""                   ""*""              ""*""              ""*""          \n## 19  ( 1 ) ""*""                   ""*""              ""*""              ""*""          \n## 20  ( 1 ) ""*""                   ""*""              ""*""              ""*""          \n## 21  ( 1 ) ""*""                   ""*""              ""*""              ""*""          \n## 22  ( 1 ) ""*""                   ""*""              ""*""              ""*""          \n##           grade_trans11 waterfront_trans1 view_trans[1,4]\n## 1  ( 1 )  "" ""           "" ""               "" ""            \n## 2  ( 1 )  "" ""           "" ""               ""*""            \n## 3  ( 1 )  "" ""           "" ""               ""*""            \n## 4  ( 1 )  "" ""           "" ""               ""*""            \n## 5  ( 1 )  "" ""           "" ""               ""*""            \n## 6  ( 1 )  ""*""           "" ""               ""*""            \n## 7  ( 1 )  ""*""           "" ""               ""*""            \n## 8  ( 1 )  ""*""           "" ""               ""*""            \n## 9  ( 1 )  ""*""           "" ""               ""*""            \n## 10  ( 1 ) ""*""           ""*""               ""*""            \n## 11  ( 1 ) ""*""           ""*""               ""*""            \n## 12  ( 1 ) ""*""           ""*""               ""*""            \n## 13  ( 1 ) ""*""           ""*""               ""*""            \n## 14  ( 1 ) ""*""           ""*""               ""*""            \n## 15  ( 1 ) ""*""           ""*""               ""*""            \n## 16  ( 1 ) ""*""           ""*""               ""*""            \n## 17  ( 1 ) ""*""           ""*""               ""*""            \n## 18  ( 1 ) ""*""           ""*""               ""*""            \n## 19  ( 1 ) ""*""           ""*""               ""*""            \n## 20  ( 1 ) ""*""           ""*""               ""*""            \n## 21  ( 1 ) ""*""           ""*""               ""*""            \n## 22  ( 1 ) ""*""           ""*""               ""*""\n\nfor (metric in c(""r2"", ""adjr2"", ""Cp"", ""bic"")){plot(best_subsets_models,scale=metric)}\n\nMétodo 02: forward.\nregfit_fwd <- leaps::regsubsets(price_trans~., df_transformadas[,-1], method=""forward"")\nfor (metric in c(""r2"", ""adjr2"", ""Cp"", ""bic"")){plot(regfit_fwd, scale=metric)}\n\nMétodo 03: backward.\nregfit_fwd <- leaps::regsubsets(price_trans~., df_transformadas, method=""backward"")\nfor (metric in c(""r2"", ""adjr2"", ""Cp"", ""bic"")){plot(regfit_fwd, scale=metric)}\n\nTras la aplicación de los siguientes métodos automáticos de selección de parámetros podemos observar que se acercan bastante a la selección de variable escogidas por el método Lasso.\n'], 'url_profile': 'https://github.com/charlstown', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '700 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\nMultiple Linear Regression using scikit-learn.\nData\nThe data was used from a kaggle Advertising data\n'], 'url_profile': 'https://github.com/pau-lo', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'San Diego ', 'stats_list': [], 'contributions': '187 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jonathanmatsen', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'kansas city', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['MachineLearning_Regression\nMachine Learning - University of Washington - Regression course\n'], 'url_profile': 'https://github.com/MRChaitanya', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Titanic_Dataset\nTitanic survival prediction using Logistic regression\nThe data has been split into two groups:\ntraining set (train.csv)\ntest set (test.csv)\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n'], 'url_profile': 'https://github.com/Surinder09', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['simple-linear\nA simple binary classifier using linear regression\n'], 'url_profile': 'https://github.com/hxnhatkhtn', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated May 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DavideItaly', 'info_list': ['Updated Dec 30, 2019', 'Python', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['student_data\nLinear regression practice on student data\n'], 'url_profile': 'https://github.com/nick-bhi', 'info_list': ['Updated Dec 30, 2019', 'Python', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['econometrics1\nEconometrics Regression of Number of Rooms\n'], 'url_profile': 'https://github.com/MaxVu12', 'info_list': ['Updated Dec 30, 2019', 'Python', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['ClassificationFunctions\nGaussian Generative Model and Logistic Regression Classifiers\n'], 'url_profile': 'https://github.com/moh-osman3', 'info_list': ['Updated Dec 30, 2019', 'Python', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019']}","{'location': 'Brooklyn,New York', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': [""MLProject\nOptimization of KNN and Linear regression algorithms\nKNN Introduction:\nKNN or K-nearest neighbours is a supervised algorithm used for classification and regression by considering k nearest training data to a test data. In this implementation, KNN is used for classification by counting the majority occurring classes in k nearest training examples.\nThe first dataset used is IRIS dataset with the objective to classify the dataset into 3 classes - 'Iris-setosa' 'Iris-versicolor', and 'Iris-virginica'. The second dataset used is Breast cancer dataset with 30 features.\nImprovement/Extension:\nIn some cases, the data are widely spread in feature space.After selecting the k-nearest neighbor, the basic KNN works by calculating the mode of classes in nearest neighbor data set for each test data. The limitation with this approach is when data points are widely spread in feature space, the neighbour selection is not efficient. There can be cases where test data is classified wrongly in a class that occurs more often in k nearest neighbor set while it lies closer to a less popular class in reality.\nTo solve this, we can classify using inverse distance weighted voting, where the training data closest to test data contributes more in classification.\nImplementation:\nIn this implementation, after selecting k nearest neighbors, following steps were taken:\n\nComputed the inverse of each distance\nFound the sum of the inverses,\nDivided each inverse by the sum\nVote is calculated by taking out sum of weights associated with each class. The predicted class for test data is the one with the highest vote.\n\nLinear Regression Introduction:\nLinear regression, a supervised machine learning algorithm,which assumes a linear relationship between the output and input variables. The output can be obtained from a linear combination of input variables and their weights.\nIn this implementation the sklearn-Boston housing database, and sklearn Diabetes Dataset have been used to study the performance of linear regression and its extension.\nImprovement:\nWhile the Linear Regression works well for small datasets but for large ones, not all input variables might be significant to predict the output. LASSO (Least Absolute Shrinkage and Selection Operator) is an extension of the Linear Regression algorithm which penalizes large weights by adding their absolute value of magnitude to the cost function. After certain iterations, the weights shrinks towards zero, thus discarding irrelevant features. This is useful in case of high dimensional dataset where some features do not contribute in classification. Weights are updated using coordinate descent algorithm where optimization is done for a single coordinate at a time.\nThe objective in Lasso regularization is to minimize the below cost function: E\u200b = E\u200b (w) + alpha ||w||\nlasso\u200b in\u200b\n""], 'url_profile': 'https://github.com/sunidhit', 'info_list': ['Updated Dec 30, 2019', 'Python', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019']}","{'location': 'Bucharest, Romania', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bogdanmacovei', 'info_list': ['Updated Dec 30, 2019', 'Python', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Linear Regression with Multiple Variables\nImplementatio of Linear Regression with Multiple Variables\n'], 'url_profile': 'https://github.com/Ammar-Aslam', 'info_list': ['Updated Dec 30, 2019', 'Python', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '229 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GitHubAcc12', 'info_list': ['Updated Dec 30, 2019', 'Python', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Car-Price-Pridiction-Problem\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\nBusiness Goal\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\nData Prepration:\n\nThe Dataset consist of 26 columns and 205 rows.\nIt is a supervised learning approach as ""Price"" is our target variable.\nCheck the info and null values counts in the dataset\nIn the \'CarName\' feature we are not interested in Name of the car models but only car companies. So we split the Manufacturers from there models and create a new column ""company"".\nSome of the manufacturers name in the new column are spelt wrong which is to be edited.\nExploratory Data analysis to find out the pattern in the data. Heatmap is created to check the correlation of the current numerical  variables with the target variable.\nOutliers analysis is done using boxplots for numerical features. As our we have very small data we cannot remove the outliers as it will result in loss of good chuck of data, instead we will cap the outliers to desired percentile.\n\nData Prepration For Modelling:\n\nTo reduce the final number of variables, we can segregate sub-categories which are in very less percentage in one single category, this  will reduce the number of features we get after we form dummy variables.\nLabel Encoding for features that have two sub-categories and for scaling we\'ve used MinMAx Scaler.\nIn our final dataset we end up with 42 features. Using this much variables with Linear Regression might not lead with desired results and removing a feature one by one will also take time to get good model. To save time we used Recursive Feature Elimination technique which give us some specific important variables for modelling.\n\nModelling:\n\nStatsmodels api is used for linear regression as it give us complete summary of all the statistical parameters.\nTo counter multicollinearity variance inflation factor of the features are calculated.\nFeatures with high P-values greater than 0.05 and VIF (variance inflation factor) greater than 3.0 are dropped\nAdjusted R2 score for training set 0.83 and for test score adjusted R2 score is 0.80\n\nConclusion:\nThrough linear regression we got around 10 important features which are affecting the price of the cars in United States vehicle market. The data follows the linear relation with the target variables so linear regression was the first choice for modelling. We could also use Lasso Regression but it will take high computational time.\n'], 'url_profile': 'https://github.com/shreyanshbehani', 'info_list': ['Updated Dec 30, 2019', 'Python', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Assignments 👨\u200d🏫📚🎓✅\nDear MLBC Student, After we see the regression section in the machine learning course, you're asked to do some exercises about regression on the different dataset to practice what you've learned, this assignments of will enhance your capacity skills to ensure the better understanding the regression concept.\nProblems\nYou will find each problem in a separate folder, you're not invited to solve all of them, one is enough but if you want to expand your knowledge and your skills go ahead and show what you can do. here are the problems names, please read the readme in each problem you want to solve.\n\nBoston Housing Problem.\nHeight Weight Problem.\nKC Housing.\nWeather Problem.\n\n💣 Please complete each sub-assignments it in its file and commit it.\nHelp\nThe repository of this course can be found at this Link, in which you can find in it some code example, lessons and so one to help you get started with your assignment. If you need extra help you can get it by making an issue in this repository, tag me (@Younes-Charfaoui) and then describe what do you need, We will review the solution and reply to all kind of help.\n""], 'url_profile': 'https://github.com/mlbc-101', 'info_list': ['Updated Dec 30, 2019', 'Python', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['Breast_Cancer_Classsification\nBreast Cancer Classification using Logistic Regression\nBreast cancer is cancer that develops from breast tissue. Signs of breast cancer may include a lump in the breast, a change in breast shape, dimpling of the skin, fluid coming from the nipple, a newly-inverted nipple, or a red or scaly patch of skin. In those with distant spread of the disease, there may be bone pain, swollen lymph nodes, shortness of breath, or yellow skin.\nLogistic regression is a classification algorithm used to assign observations to a discrete set of classes. Some of the examples of classification problems are Email spam or not spam, Online transactions Fraud or not Fraud, Tumor Malignant or Benign. Logistic regression transforms its output using the logistic sigmoid function to return a probability value.\nI gathered data from another github repository I have lost the link to, but as soon as I find it, I will link it bellow.\n'], 'url_profile': 'https://github.com/zahrael97', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'TypeScript', 'Updated Jan 2, 2020', '2', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['testPlanCreation\n\nTool that indicates which tests should not be included in the Regression Test Plans and the ones that should be\nNote: Only work with CIn/Motorola VPN.\n\nThe application was developed using:\n\nPython 3.3\nDjango 1.8\n\nSteps to use:\nStep 1:\n\nOpen a browser on this adress: 127.0.0.1:8000\nLog in with a CoreId\n\nStep 2:\n\nCreate a Test Plan manually at Dalek and input the key on the indicated field.\nInput the Regression Level (use a comma to separate).\nCopy a link of the Google spreadsheet that contain the Product Checklist.\nNote¹: Has to be a ""Google Spredsheet"". If the file has .xls extension, it will not work.\nNote²: It\'s necessary to share the spreadsheet with: ""testplancreation@testplancreate.iam.gserviceaccount.com""\n\nStep 3:\n\nFrom the list, choose a category (only one).\n\nStep 4:\n\nFrom the list, choose a subcategory (only one).\n\nStep 5:\n\nFrom the list choose a filter (only one)\nNote³: If you choose on step 4 ""DMT"" you can choose more than one filter.\nIf you choose on step 4 ""Experiences"" step 5 will not be necessary.\n\nStep 6:\n\nResults will be shown on the screen.\n\n'], 'url_profile': 'https://github.com/laissf', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'TypeScript', 'Updated Jan 2, 2020', '2', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['ANN Tester\nThis module runs several tests on a multilayer perceptron (MLP) regression fit.  MLP\'s have been theoretically proven able to reproduce continuous functions within an arbitrarily small epsilon.  These tests are designed to empirically study the amount of training data required to reproduce various functions and the interaction with the number of ""neurons"" and layers in the MLP.  Also of interest is the extensibility of ANN models for extrapolation or interpolation of ""voids"" in the training data.\nInstallation\nconda create -n <env> --file requirements.txt\nconda activate <env>\npython ann_tests/run_tests.py\n\nTest options\nThree tests are available.  \'basic\' is a test of three ANN architectures with different amounts of training data.  \'extrap\' is a test of input values outside of the training data range.  \'interp\' is a test of input values within a subset of the training data range which was excluded from training. \'dim\' is a test of increasing dimensionality of the input features.\nusage: run_tests.py [-h] [--test {interp,extrap,basic,dim,all}]\n\nRun ANN regression tests\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --test {interp,extrap,basic,dim,all}\n                        Test mode\n\n'], 'url_profile': 'https://github.com/matthew-baran', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'TypeScript', 'Updated Jan 2, 2020', '2', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020']}","{'location': 'Himanchal Pradesh', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['Board-Game-Predictor-Using-Machine-Learning\nLinear regression algorithm building a real project\nMachine Learning is slowly spreading its tentacles into all aspects of technology and even further. Better algorithms are helping devices become smarter and users to become more informed. Now, its time to add a little fun to Machine Learning and in this course, we have tried to do exactly that!\nBoard games have been a great way to pass the time, from simple ones like The Game of Life to more complicated ones like Dungeons & Dragons. But before you become vested in a Game, what if you could find out how popular it really is? What if it was rated to help you learn how fun it really is? Well, now using Machine Learning you can!\nHowever, for this project course we will focus on board games. The information that we will work on was scrubbed from a database of 80,000 board games and includes information such as minimum players, maximum players, minimum playtime, maximum playtime, etc. We will use the models to ensure that using all of this information, it gives an accurate prediction regarding the reviews.\nIn this course, we will walk you through all of the steps needed to generate this output including how to train the models, how to load and preprocess the dataset appropriately, and so much more. At the end of this course, you will not only have an accurate prediction of the board game reviews, but also hands-on experience to learn how you can actually train two significant Machine Learning algorithms to learn and sort data, as well as make accurate predictions using a data set.\n'], 'url_profile': 'https://github.com/ankitbaluni123', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'TypeScript', 'Updated Jan 2, 2020', '2', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['GPRegressionProblemUI\nThis project was generated with Angular CLI version 8.3.20.\nDevelopment server\nRun ng serve for a dev server. Navigate to http://localhost:4200/. The app will automatically reload if you change any of the source files.\nCode scaffolding\nRun ng generate component component-name to generate a new component. You can also use ng generate directive|pipe|service|class|guard|interface|enum|module.\nBuild\nRun ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build.\nRunning unit tests\nRun ng test to execute the unit tests via Karma.\nRunning end-to-end tests\nRun ng e2e to execute the end-to-end tests via Protractor.\nFurther help\nTo get more help on the Angular CLI use ng help or go check out the Angular CLI README.\n'], 'url_profile': 'https://github.com/david42dev', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'TypeScript', 'Updated Jan 2, 2020', '2', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020']}","{'location': 'China', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['GA-GPR-SVM-for-feature-select-regression\nuse GA-GPR-SVM for feature select\n'], 'url_profile': 'https://github.com/fish-kong', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'TypeScript', 'Updated Jan 2, 2020', '2', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020']}","{'location': 'Navi Mumbai', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nileshsinalkar', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'TypeScript', 'Updated Jan 2, 2020', '2', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['🏆 Linear Regression/Multiple Linear Regression in R using AutoMPG dataset Project 🏆\nAbout dataset\nThis dataset is a slightly modified version of the dataset provided in the StatLib library. In line with the use by Ross Quinlan (1993) in predicting the attribute ""mpg"", 8 of the original instances were removed because they had unknown values for the ""mpg"" attribute. The original dataset is available in the file ""auto-mpg.data-original"".\n""The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.""\nAttribute Information\n\nmpg: continuous\ncylinders: multi-valued discrete\ndisplacement: continuous\nhorsepower: continuous\nweight: continuous\nacceleration: continuous\nmodel year: multi-valued discrete\norigin: multi-valued discrete\ncar name: string (unique for each instance)\n\nMy Goal\nIn this project we will investigate the impact of a number of automobile engine factors on the vehicle’s mpg. The dataset auto-mpg.csv contains information for 398 different automobile models. Information regarding the number of cylinders, displacement, horsepower, weight, acceleration, model year, origin, and car name as well as mpg are contained in the file.\nUsing multiple linear regression to determine the relationship between mpg and different combinations of independent variables\n.\nConsider all possible combinations of independent variables. Here we will report all the appropriate information regarding regression. We have to investigate -\n\nMultiple R-squared\nAdjusted R-squared\nComplete Linear Regression equation\n\n'], 'url_profile': 'https://github.com/Sakshi09wal', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'TypeScript', 'Updated Jan 2, 2020', '2', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020']}","{'location': 'University of Texas,Dallas', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Naive-Bayes-and-Logistic-Regression-for-Text-Classification\nFOR NAIVE BAYES:\nCompilation: go the folder containing the files in cmd and execute\npython NB.py      <yes/no to remove stop-words>\nfor example : python NB.py train\\spam train\\ham test\\spam test\\ham stopWords.txt yes\nFOR LOGISTIC REGRESSION:\nCompilation: go the folder containing the files in cmd and execute\npython LR.py      <yes/no to remove stop-words>\nfor example : python LR.py train\\spam train\\ham test\\spam test\\ham stopWords.txt yes\n'], 'url_profile': 'https://github.com/allampally-bhargav', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'TypeScript', 'Updated Jan 2, 2020', '2', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020']}","{'location': 'Stockholm', 'stats_list': [], 'contributions': '163 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DipeshV', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'TypeScript', 'Updated Jan 2, 2020', '2', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '809 contributions\n        in the last year', 'description': ['WatchDoggo\nBinary/Deriv votality market trading bot using linear regression prediction\n'], 'url_profile': 'https://github.com/Kiryuumaru', 'info_list': ['C#', 'Updated Jul 12, 2020', 'HTML', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'Python', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ssak32', 'info_list': ['C#', 'Updated Jul 12, 2020', 'HTML', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'Python', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Shapley\nImplementation of Shapley Owen decomposition used in Multiple regression models.\nGetting started\nQuite often we would like to understand how the role of each independent variable in the overall model.A simpler way of doing this is by decomposing the R-Squared using the Shapley Owen decomposition\nInstallation\npip install shapley\nAttributes\ncoeff_ :dictionary  of shape (n_features,)\nEstimated coefficients for the Shapley Owen  problem.\nExample\nimport pandas as pd\nfrom Shapley import ShapleyValues\n\nworking_data = pd.read_csv('Shapely_input2.csv')\nX = working_data[working_data.columns.difference(['Y'])]\nY = working_data[['Y']]\nSp = ShapleyValues()\nShapley_Values =Sp.fit(X,Y)\nprint (Shapley_Values.coef_)\n\n{'X1': 0.0010701967805290535, 'X2': 0.010827740911131756, 'X3': 0.0024885120812901951, 'X4': 2.8260328860385538e-05}\n\n""], 'url_profile': 'https://github.com/MashNagesh', 'info_list': ['C#', 'Updated Jul 12, 2020', 'HTML', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'Python', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '175 contributions\n        in the last year', 'description': ['LinearRegression-Adipose-Tissue\nA case study has been coded using linear regression\n'], 'url_profile': 'https://github.com/Anand-s-cmd', 'info_list': ['C#', 'Updated Jul 12, 2020', 'HTML', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'Python', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'London Ontario', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Regression-analysys\nImplementing various regression algorithms to predict Boston house\xa0prices.\nPlease refer https://medium.com/@khushwant.rai78/machine-learning-implementing-various-regression-algorithms-to-predict-boston-house-prices-c87f961de981 for the detailed explanation.\n'], 'url_profile': 'https://github.com/khushwant18', 'info_list': ['C#', 'Updated Jul 12, 2020', 'HTML', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'Python', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SylwekSzewczyk', 'info_list': ['C#', 'Updated Jul 12, 2020', 'HTML', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'Python', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Project Overview\nLogistic Regression & Feature Selection\nLearnings from the project\n\nLogistic Regression techniques\nFeature Selection techniques\nPCA\nf-Score\nch2 Analysis\n\n'], 'url_profile': 'https://github.com/hn1201', 'info_list': ['C#', 'Updated Jul 12, 2020', 'HTML', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'Python', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kchamarty', 'info_list': ['C#', 'Updated Jul 12, 2020', 'HTML', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'Python', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'Champaign, IL USA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DennCardoso', 'info_list': ['C#', 'Updated Jul 12, 2020', 'HTML', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'Python', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'Jammu', 'stats_list': [], 'contributions': '281 contributions\n        in the last year', 'description': ['linear_regression-implementation\nimplementing linear regression to predict score of students.\n'], 'url_profile': 'https://github.com/Sachindrck', 'info_list': ['C#', 'Updated Jul 12, 2020', 'HTML', 'Updated Jan 1, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'Python', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}"
"{'location': 'Taguig, Philippines', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Ridge, Lasso, and Principal Components Regression (Water Data)\nWe created two models using a dataset collection of 300 sampled water districts in the Philippines.\nThe first model is a regression model with water prices as the output variable while\nthe second one is a categorical model where we created an output variable called wastage rating.\nThe Ridge, Lasso, and Principal Components Regression were employed in creating the models. It has been found that the best\nmodel for the regression model is the Lasso Regression Model, with an RMSE of 0.253.\nOn the other hand, the Principal Components Regression Model has been found to be the best model for classification\nwith an AUC of 0.509 and 54% Accuracy.\n'], 'url_profile': 'https://github.com/rlbartolome', 'info_list': ['Updated Jan 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 3, 2020', 'C', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 6, 2020', 'Updated Jan 1, 2020']}","{'location': 'Berlin', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/andreaspts', 'info_list': ['Updated Jan 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 3, 2020', 'C', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 6, 2020', 'Updated Jan 1, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dhakma', 'info_list': ['Updated Jan 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 3, 2020', 'C', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 6, 2020', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['IDM_XS\ncross section regression for IDM at the LHC\n'], 'url_profile': 'https://github.com/SydneyOtten', 'info_list': ['Updated Jan 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 3, 2020', 'C', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 6, 2020', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': [""Bayesian-Regression\nInsights into Car Features - Hierarchical Bayes Multinomial Logit Regression\nThe following study was conducted to draw out insights about the car feature preferences of the market. Individual-level parameters of the respondents were estimated through Hierarchical Bayes Logit. The results could be further utilized to predict any products' market share or for segmentation of the populace.\nCheers!!\n""], 'url_profile': 'https://github.com/ThePartyHub', 'info_list': ['Updated Jan 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 3, 2020', 'C', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 6, 2020', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/srijaytuladhar', 'info_list': ['Updated Jan 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 3, 2020', 'C', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 6, 2020', 'Updated Jan 1, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['multiple-linear-regression\n'], 'url_profile': 'https://github.com/yash-pythonman', 'info_list': ['Updated Jan 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 3, 2020', 'C', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 6, 2020', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['RegMods_AnalysisWk3\n(OPTIONAL) Data analysis practice from Coursera: Regression Models - Week 3\nYour assignment is to study how income varies across college major categories. Specifically answer: “Is there an association between college major category and income?”\nTo get started, start a new R/RStudio session with a clean workspace. To do this in R, you can use the q() function to quit, then reopen R. The easiest way to do this in RStudio is to quit RStudio entirely and reopen it. After you have started a new session, run the following commands. This will load a data.frame called college for you to work with.\ninstall.packages(""devtools"")\ndevtools::install_github(""jhudsl/collegeIncome"")\nlibrary(collegeIncome)\ndata(college)\n\nNext download and install the matahari R package with the following commands:\ndevtools::install_github(""jhudsl/matahari"")\nlibrary(matahari)\n\nThis package allows a record of your analysis (your R command history) to be documented. You will be uploading a file containing this record to GitHub and submitting the link as part of this quiz.\nBefore you start the analysis for this assignment, enter the following command to begin the documentation of your analysis:\ndance_start(value = FALSE, contents = FALSE)\n\nPlease upload this college_major_analysis.rds file to a public GitHub repository. In question 4 of this quiz, you will share the link to this file.\nA codebook for the dataset is given below:\n\nrank: Rank by median earnings\nmajor_code: Major code\nmajor: Major description\nmajor_category: Category of major\ntotal: Total number of people with major\nsample_size: Sample size of full-time, year-round individuals used for income/earnings estimates: p25th, median, p75th\np25th: 25th percentile of earnings\nmedian: Median earnings of full-time, year-round workers\np75th: 75th percentile of earnings\nperc_men: % men with major (out of total)\nperc_women: % women with major (out of total)\nperc_employed: % employed (out of total)\nperc_employed_fulltime: % employed 35 hours or more (out of employed)\nperc_employed_parttime: % employed less than 35 hours (out of employed)\nperc_employed_fulltime_yearround: % employed at least 50 weeks and at least 35 hours (out of employed and full-time)\nperc_unemployed: % unemployed (out of employed)\nperc_college_jobs: % with job requiring a college degree (out of employed)\nperc_non_college_jobs: % with job not requiring a college degree (out of employed)\nperc_low_wage_jobs: % in low-wage service jobs (out of total)\n\n**Question: Based on your analysis, would you conclude that there is a significant association between college major category and income?\n'], 'url_profile': 'https://github.com/thawatchai-p', 'info_list': ['Updated Jan 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 3, 2020', 'C', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 6, 2020', 'Updated Jan 1, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['heart-disease-prediction\nPredicting Heart Diseases - ML 2\n'], 'url_profile': 'https://github.com/shahshawaiz', 'info_list': ['Updated Jan 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 3, 2020', 'C', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 6, 2020', 'Updated Jan 1, 2020']}","{'location': 'Australia', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['gradientdescent_for_multivariate_linear_regression\nJupyter notebook with python code for multivariate linear regression.\n'], 'url_profile': 'https://github.com/LahiriBellarykar', 'info_list': ['Updated Jan 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 5, 2020', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 3, 2020', 'C', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 6, 2020', 'Updated Jan 1, 2020']}"
"{'location': 'New York, NY', 'stats_list': [], 'contributions': '262 contributions\n        in the last year', 'description': ['EVaRegression\nLinear Regression through Entropic Value at Risk minimization\n'], 'url_profile': 'https://github.com/hamzatazib', 'info_list': ['Julia', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Jan 5, 2020', 'Cuda', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Module 1 Final Project\nIntroduction\nIn this lesson, we\'ll review all of the guidelines and specifications for the final project for Module 1.\nObjectives\nYou will be able to:\n\nDescribe all required aspects of the final project for Module 1\nDescribe all required deliverables\nDescribe what constitutes a successful project\nDescribe what the experience of the project review should be like\n\nFinal Project Summary\nYou\'ve made it all the way through the first module of this course - take a minute to celebrate your awesomeness!\n\nAll that remains in Module 1 is to put our newfound data science skills to use with a final project! You should expect this project to take between 20 and 25 hours of solid, focused effort. If you\'re done way quicker, go back and dig in deeper or try some of the optional ""level up"" suggestions. If you\'re worried that you\'re going to get to 30 hrs and still not even have the data imported, reach out to an instructor in Slack ASAP to get some help!\nThe Dataset\nFor this project, you\'ll be working with the King County House Sales dataset. We\'ve modified the dataset to make it a bit more fun and challenging.  The dataset can be found in the file ""kc_house_data.csv"", in this repo.\nThe description of the column names can be found in the column_names.md file in this repository. As with most real world data sets, the column names are not perfectly described, so you\'ll have to do some research or use your best judgment if you have questions relating to what the data means.\nYou\'ll clean, explore, and model this dataset with a multivariate linear regression to predict the sale price of houses as accurately as possible.\nThe Deliverables\nFor online students, there will be five deliverables for this project (Note: On-campus students may have different requirements, please speak with your instructor):\n\nA well documented Jupyter Notebook containing any code you\'ve written for this project and comments explaining it. This work will need to be pushed to your GitHub repository in order to submit your project.\nAn organized README.md file in the GitHub repository that describes the contents of the repository. This file should be the source of information for navigating through the repository.\nA short Keynote/PowerPoint/Google Slides presentation (delivered as a PDF export) giving a high-level overview of your methodology and recommendations for non-technical stakeholders. Make sure to also add and commit this pdf of your non-technical presentation to your repository with a file name of presentation.pdf.\nA Blog Post\nA Video Walkthrough of your non-technical presentation. Some common video recording tools used are Zoom, Quicktime, and Nimbus. After you record your presentation, publish it on a service like YouTube or Google Drive, you will need a link to the video to submit your project.\n\nNote: On-campus students may have different requirements, please speak with your instructor.\nJupyter Notebook Must-Haves\nFor this project, your Jupyter Notebook should meet the following specifications:\nOrganization/Code Cleanliness\n\nThe notebook should be well organized, easy to follow,  and code should be commented where appropriate.\n\nLevel Up: The notebook contains well-formatted, professional looking markdown cells explaining any substantial code.  All functions have docstrings that act as professional-quality documentation\n\n\nThe notebook is written for technical audiences with a way to both understand your approach and reproduce your results. The target audience for this deliverable is other data scientists looking to validate your findings.\n\nVisualizations & EDA\n\nYour project contains at least 4 meaningful data visualizations, with corresponding interpretations. All visualizations are well labeled with axes labels, a title, and a legend (when appropriate)\nYou pose at least 3 meaningful questions and answer them through EDA.  These questions should be well labeled and easy to identify inside the notebook.\n\nLevel Up: Each question is clearly answered with a visualization that makes the answer easy to understand.\n\n\nYour notebook should contain 1 - 2 paragraphs briefly explaining your approach to this project.\n\nModel Quality/Approach\n\nYour model should not include any predictors with p-values greater than .05.\nYour notebook shows an iterative approach to modeling, and details the parameters and results of the model at each iteration.\n\nLevel Up: Whenever necessary, you briefly explain the changes made from one iteration to the next, and why you made these choices.\n\n\nYou provide at least 1 paragraph explaining your final model.\nYou pick at least 3 coefficients from your final model and explain their impact on the price of a house in this dataset.\n\nNon-Technical Presentation Must-Haves\nAnother deliverable should be a Keynote, PowerPoint or Google Slides presentation delivered as a pdf file in your fork of this repository with the file name of presentation.pdf detailing the results of your project.  Your target audience is non-technical people interested in using your findings to maximize their profit when selling their home.\nYour presentation should:\n\nContain between 5 - 10 professional-quality slides.\n\nLevel Up: The slides should use visualizations whenever possible, and avoid walls of text.\n\n\nTake no more than 5 minutes to present.\nAvoid technical jargon and explain the results in a clear, actionable way for non-technical audiences.\n\nBased on the results of your models, your presentation should discuss at least two concrete features that highly influence housing prices.\nBlog Post Must-Haves\nRefer back to the Blogging Guidelines for the technical requirements and blog ideas.\nThe Process\n(Note: On-campus students may have different processes, please speak with your instructor)\n1. Getting Started\nPlease start by reviewing this document. If you have any questions, please ask them in Slack ASAP so (a) we can answer the questions and (b) so we can update this repository to make it clearer.\nBe sure to let the instructor team know when you’ve started working on a project, either by reaching out over Slack or, if you are in a full-time or part-time cohort, by connecting with your Cohort Lead in your weekly 1:1. If you’re not sure who to reach out to, post in the #online-ds-sp-000 channel in Slack.\nOnce you\'re done with the first 8 sections, please start on the project. Do that by forking this repository, cloning it locally, and working in the student.ipynb file. Make sure to also add and commit a pdf of your presentation to the repository with a file name of presentation.pdf.\n2. The Project Review\nNote: On-campus students may have different review processes, please speak with your instructor.\n\nWhen you start on the project, please also reach out to an instructor immediately to schedule your project review (if you\'re not sure who to schedule with, please ask in Slack!)\n\nWhat to expect from the Project Review\nProject reviews are focused on preparing you for technical interviews. Treat project reviews as if they were technical interviews, in both attitude and technical presentation (sometimes technical interviews will feel arbitrary or unfair - if you want to get the job, commenting on that is seldom a good choice).\nThe project review is comprised of a 45 minute 1:1 session with one of the instructors. During your project review, be prepared to:\n1. Deliver your PDF presentation to a non-technical stakeholder.\nIn this phase of the review (~10 mins) your instructor will play the part of a non-technical stakeholder that you are presenting your findings to. The presentation  should not exceed 5 minutes, giving the ""stakeholder"" 5 minutes to ask questions.\nIn the first half of the presentation (2-3 mins), you should summarize your methodology in a way that will be comprehensible to someone with no background in data science and that will increase their confidence in you and your findings. In the second half (the remaining 2-3 mins) you should summarize your findings and be ready to answer a couple of non-technical questions from the audience. The questions might relate to technical topics (sampling bias, confidence, etc) but will be asked in a non-technical way and need to be answered in a way that does not assume a background in statistics or machine learning. You can assume a smart, business stakeholder, with a non-quantitative college degree.\n2. Go through the Jupyter Notebook, answering questions about how you made certain decisions. Be ready to explain things like:\n* ""How did you pick the question(s) that you did?""\n* ""Why are these questions important from a business perspective?""\n* ""How did you decide on the data cleaning options you performed?""\n* ""Why did you choose a given method or library?""\n* ""Why did you select those visualizations and what did you learn from each of them?""\n* ""Why did you pick those features as predictors?""\n* ""How would you interpret the results?""\n* ""How confident are you in the predictive quality of the results?""\n* ""What are some of the things that could cause the results to be wrong?""\n\nThink of the first phase of the review (~30 mins) as a technical boss reviewing your work and asking questions about it before green-lighting you to present to the business team. You should practice using the appropriate technical vocabulary to explain yourself. Don\'t be surprised if the instructor jumps around or sometimes cuts you off - there is a lot of ground to cover, so that may happen.\nIf any requirements are missing or if significant gaps in understanding are uncovered, be prepared to do one or all of the following:\n\nPerform additional data cleanup, visualization, feature selection, modeling and/or model validation\nSubmit an improved version\nMeet again for another Project Review\n\nWhat won\'t happen:\n\nYou won\'t be yelled at, belittled, or scolded\nYou won\'t be put on the spot without support\nThere\'s nothing you can do to instantly fail or blow it\n\nPlease note: We need to receive the URL of your repository at least 24 hours before and please have the project finished at least 3 hours before your review so we can look at your materials in advance.\nSubmitting your Project\nYou’re almost done! In order to submit your project for review, include the following links to your work in the corresponding fields on the right-hand side of Learn.\n\nGitHub Repo: Now that you’ve completed your project in Jupyter Notebooks, push your work to GitHub and paste that link to the right. (If you need help doing so, review the resources here.)\nReminder: Make sure to also add and commit a pdf of your non-technical presentation to the repository with a file name of presentation.pdf.\nBlog Post: Include a link to your blog post.\nRecord Walkthrough: Include a link to your video walkthrough.\n\nHit ""I\'m done"" to wrap it up. You will receive an email in order to schedule your review with your instructor.\nGrading Rubric\nOnline students can find a PDF of the grading rubric for the project here. On-campus students may have different review processes, please speak with your instructor.\nSummary\nThe end of module projects and project reviews are a critical part of the program. They give you a chance to both bring together all the skills you\'ve learned into realistic projects and to practice key ""business judgement"" and communication skills that you otherwise might not get as much practice with.\nThe projects are serious and important. They are not graded, but they can be passed and they can be failed. Take the project seriously, put the time in, ask for help from your peers or instructors early and often if you need it, and treat the review as a job interview and you\'ll do great. We\'re rooting for you to succeed and we\'re only going to ask you to take a review again if we believe that you need to. We\'ll also provide open and honest feedback so you can improve as quickly and efficiently as possible.\nFinally, this is your first project. We don\'t expect you to remember all of the terms or to get all of the answers right. If in doubt, be honest. If you don\'t know something, say so. If you can\'t remember it, just say so. It\'s very unusual for someone to complete a project review without being asked a question they\'re unsure of, we know you might be nervous which may affect your performance. Just be as honest, precise and focused as you can be, and you\'ll do great!\n'], 'url_profile': 'https://github.com/akshay-ghalsasi', 'info_list': ['Julia', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Jan 5, 2020', 'Cuda', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Introduction\nFrom the conception of simple games to arcade gaming and currently, living in the era of virtual gaming world\nfull of characters, the gaming industry has seen a meteoric rise. Game developers are working extensively to bring\nforward the new ideas, genres and styles. Games have become more complex, supports more players and equipped\nwith richer graphics and mechanics. Player Unknown’s Battlegrounds (PUBG) is a complex and competitive\nbattle game, with global tournaments rewarding cash prizes driving a lot of players to take part. Each match of\nPUBG pits about 100 players against each other some of which are in squads of two or four while others are alone,\nand the last person or squad surviving is deemed the winner, rewarded with chicken dinner. Therefore, the chance\nof winning is slim. As such, the problem many players face is that their chance of winning is very low, especially\nif they’re a new player. The winning chance increases as one plays the game and becomes more proficient and\nexperienced. Better accuracy, better reaction times and more can contribute to a winning combination for players.\nThere are two simple ways to play the game: kill or stay hidden. There is no particular way in which the game\nshould be played, it all depends on one’s gaming style.\nIt is interesting to explore player behaviour in PUBG as all players start as equal, with no predefined classes,\nattributes or weapons. The motivation for the project comes from the idea to explore and identify the key\nstrategies adopted by various players in the game. In this project, we set out to analyse data to predict the rank of a\nplayer (solo or in a squad) will end up achieving based on the different gaming predictors offered like kills or\ndamage done, weapons acquired as at the beginning of the game every player is without any weapon. Also, there\nis certain health-boosting equipment like health kits, energy drink which are acquired by players while scavenging\nfor weapons and these can be used in between the game to increase the player’s health or for recovery and hence,\nincrease one’s lifespan in the game.\nData Description\nThe dataset used provides a large number of anonymized PUBG game stats, and each row contains a player’s\npost-game stats. The data comes from matches of all types: solos, duos, squads, and custom; there is no guarantee\nof there being 100 players per match, nor at most 4 players per group. Below are the data fields:\n\nDBNOs - Number of enemy players knocked.\nAssists - Number of enemy players this player damaged that were killed by teammates.\nBoosts - Number of boost items used.\nDamageDealt - Total damage dealt. Note: Self-inflicted damage is subtracted.\nHeadshotKills - Number of enemy players killed with headshots.\nHeals - Number of healing items used.\nId - Player’s Id\nKillPlace - Ranking in match of number of enemy players killed.\nKillPoints - Kills-based external ranking of player. (Think of this as an Elo ranking where only kills\nmatter.) If there is a value other than -1 in rankPoints, then any 0 in killPoints should be treated as a\n“None”.\nKillStreaks - Max number of enemy players killed in a short amount of time.\nKills - Number of enemy players killed.\nLongestKill - Longest distance between player and player killed at time of death. This may be misleading,\nas downing a player and driving away may lead to a large longestKill stat.\nMatchDuration - Duration of match in seconds.\nMatchId - ID to identify match. There are no matches that are in both the training and testing set.\nMatchType - String identifying the game mode that the data comes from. The standard modes are “solo”,\n“duo”, “squad”, “solo-fpp”, “duo-fpp”, and “squad-fpp”; other modes are from events or custom matches.\nRankPoints - Elo-like ranking of player. This ranking is inconsistent and is being deprecated in the API’s\nnext version, so use with caution. Value of -1 takes place of “None”.\nRevives - Number of times this player revived teammates.\nRideDistance - Total distance traveled in vehicles measured in meters.\nRoadKills - Number of kills while in a vehicle.\nSwimDistance - Total distance traveled by swimming measured in meters.\nTeamKills - Number of times this player killed a teammate.\nVehicleDestroys - Number of vehicles destroyed.\nWalkDistance - Total distance traveled on foot measured in meters.\nWeaponsAcquired - Number of weapons picked up.\nWinPoints - Win-based external ranking of player. (Think of this as an Elo ranking where only winning\nmatters.) If there is a value other than -1 in rankPoints, then any 0 in winPoints should be treated as a\n“None”.\nGroupId - ID to identify a group within a match. If the same group of players plays in different matches,\nthey will have a different groupId each time.\nNumGroups - Number of groups we have data for in the match.\nMaxPlace - Worst placement we have data for in the match. This may not match with numGroups, as\nsometimes the data skips over placements.\nWinPlacePerc - The target of prediction. This is a percentile winning placement, where 1 corresponds to\n1st place, and 0 corresponds to last place in the match.\n\nThe data has only one categorical value called the “match type” with standard game modes: solo, duo,\nsquad, solo-fpp, duo-fpp, and squad-fpp\nData Pre-processing\nThe dataset is sourced from one of the Kaggle data analysis competition for prediction of PUBG final winning\nposition depending on various factors in the game [2]. Initial exploration of data didn’t highlight any anomalies\nand didn’t have any missing values. Although the actual data from the competition already was split into training\nand testing set of 4.45 million records and 1.93 million records respectively. However, due to hardware limitation,\nthe entire analysis was carried out using a scaled-down version of training data with 49144 records. The predictors\nin the dataset were standardized to have all the values on the same scale for analysis.\nThe margin of error is considered for the analysis is +/- 0.25 units. The size of the testing set (14743 records) was\nevaluated considering the margin of error. The training data set is worth 30% and the value is used to evaluate the\ntraining set size of 34401 records which is worth 70%.\nMethods\nThe objective variable (winPlacePerc) for the dataset was continuous with 26 predictors. The dataset is unique in a\nmanner that the objective variable is continuous, but it is bounded between 0 and 1. Considering the above factors\napart from the fact that the response variable is bounded, we were motivated to implement the following\nregression methodologies for analyzing the data :\n\nMultiple Linear Regression\nRandom Forest Regression\n\nResults And Discussion\nThe initial baseline accuracy was established to compare the results from models on testing and training set. For the\ntest set, the baseline accuracy was calculated as the mean of losses between the test response variable and the mean of\nthe test response variable. The mean of test loss evaluated was 0.267 units. For evaluation of model performance on\nthe training set, 5-fold cross-validation was used to calculate the mean squared error.\nResults from the regression methodologies were evaluated using the following metrics:\n\nMean Squared Error (MSE)\n\nResults from the above regression techniques were very close with each other the polynomial model performing\nthe best on the MSE values. Although, the plots of multiple linear regression models highlighted the limitation of\nlinear regression techniques to deal with datasets having response variable within a range. All the 3 multiple\nregression models predicted values that are less than 0 and more than 1, which are invalid predictions considering\nthe use-case.\nIn contrast, although the random forest regression technique didn’t perform as well as the polynomial model if\ncompared on MSE values, it had other significant advantages over the multiple linear regression techniques.\nRandom forest model resulted in all valid predictions for the test set within the range 0 and 1 overcoming the\nlimitations of all the 3 multiple linear regression model. Random forest resulted in a much\nsimpler model as compared to the polynomial model with some trade-off on performance. Overall with descent\nperformance and valid predictions, the random forest model was the best model for the analysis done on PUBG\ndataset.\n'], 'url_profile': 'https://github.com/YMOS', 'info_list': ['Julia', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Jan 5, 2020', 'Cuda', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Linear Regression With One Variable\nImplementation of linear regression with one variable (python)\n'], 'url_profile': 'https://github.com/Ammar-Aslam', 'info_list': ['Julia', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Jan 5, 2020', 'Cuda', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/srijaytuladhar', 'info_list': ['Julia', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Jan 5, 2020', 'Cuda', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/svkrisshna', 'info_list': ['Julia', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Jan 5, 2020', 'Cuda', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'Nairobi, Kenya', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['House-Price-Prediction\nThis project is about predicting the price of a house using 79 variables. We use different models and then take the average of their predictions.The data is from Kaggle and its called Ames dataset. There is a data_description.txt file that explains the data more\n'], 'url_profile': 'https://github.com/AlexSananka', 'info_list': ['Julia', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Jan 5, 2020', 'Cuda', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Linear_regression_Duke_university\nLinear_regression_Duke_university\n'], 'url_profile': 'https://github.com/tanviredusecond', 'info_list': ['Julia', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Jan 5, 2020', 'Cuda', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jezuina', 'info_list': ['Julia', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Jan 5, 2020', 'Cuda', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/proffdeep', 'info_list': ['Julia', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Jan 5, 2020', 'Cuda', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020']}"
"{'location': 'Noida,UP', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogitic Regression Examples\n'], 'url_profile': 'https://github.com/karan650g', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Swift', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 2, 2020', '1', 'Updated Jan 1, 2020', '1', 'MATLAB', 'Updated Dec 31, 2019', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/madhusekhar111-peetla', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Swift', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 2, 2020', '1', 'Updated Jan 1, 2020', '1', 'MATLAB', 'Updated Dec 31, 2019', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['regression_pitfalls\nA demonstration / discussion of the example given in Peters, Janzing and Scholkopf (2017) on estimating regression coefficients. Full youtube video and explanation here: https://youtu.be/WcUeKIh6gVw\n'], 'url_profile': 'https://github.com/matthewvowels1', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Swift', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 2, 2020', '1', 'Updated Jan 1, 2020', '1', 'MATLAB', 'Updated Dec 31, 2019', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/raj963', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Swift', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 2, 2020', '1', 'Updated Jan 1, 2020', '1', 'MATLAB', 'Updated Dec 31, 2019', 'Updated Dec 30, 2019']}","{'location': 'Pune, India', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['PolynomialRegression\n'], 'url_profile': 'https://github.com/theSnehaThing', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Swift', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 2, 2020', '1', 'Updated Jan 1, 2020', '1', 'MATLAB', 'Updated Dec 31, 2019', 'Updated Dec 30, 2019']}","{'location': 'United States of America', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['This project is the implementation of polynimoal linear regression in python using the scikit learn library.\nThe following are the goals of the code:\n1. To read a CSV file with the data inputs.\n2. Store the data into 2 variables based on dependent and independent vars (Here, it is assumed that the data has independent vars in all but the last column, with the data itself having 3 columns).\n3. Split the data into train and test data sets (80 percent data in train and 20 in test).\n4. Create the polynomial features.\n5. Create the polynimial regressor.\n6. Fit the train set to the regressor, and predict the outputs on the test set.\n7. Plot the outputs.\n'], 'url_profile': 'https://github.com/ukalla1', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Swift', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 2, 2020', '1', 'Updated Jan 1, 2020', '1', 'MATLAB', 'Updated Dec 31, 2019', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['linear-regression-\ni created basic OLS class and two desendent classes for Gradient descent and coordinate descent\nthe implmentation include analytic solution for OLS(oridinary least squares)\ngradient descent,coordinate descent and regularization  with  ridge regression, i used sklearn boston data for this implementation\nalso -there is MSE comparison between test and train- im showing the data is different with a t-test\n'], 'url_profile': 'https://github.com/asaelbarilan', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Swift', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 2, 2020', '1', 'Updated Jan 1, 2020', '1', 'MATLAB', 'Updated Dec 31, 2019', 'Updated Dec 30, 2019']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '312 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/khancuh', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Swift', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 2, 2020', '1', 'Updated Jan 1, 2020', '1', 'MATLAB', 'Updated Dec 31, 2019', 'Updated Dec 30, 2019']}","{'location': 'Moscow', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['GAMP-regression\nGeneralized Approximate Message Passsing (GAMP) with known variance and prior hyperparameters  for regression\nSource code from Dr. Dimitris Korobilis site https://sites.google.com/site/dimitriskorobilis/matlab/gamp which accompany article Korobilis, D. (2019). High-dimensional macroeconomic forecasting using message passing algorithms*. Journal of Business & Economic Statistics, 1–30. doi:10.1080/07350015.2019.1677472\n'], 'url_profile': 'https://github.com/Lcrypto', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Swift', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 2, 2020', '1', 'Updated Jan 1, 2020', '1', 'MATLAB', 'Updated Dec 31, 2019', 'Updated Dec 30, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/joram88', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Swift', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Apr 17, 2020', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 2, 2020', '1', 'Updated Jan 1, 2020', '1', 'MATLAB', 'Updated Dec 31, 2019', 'Updated Dec 30, 2019']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/asaelbarilan', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'R', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'China Shanghai', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['LinearRegression\nsome example by using LinearRegression\n'], 'url_profile': 'https://github.com/StrangeData-v', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'R', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sasheshmadaan', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'R', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/qootocholly', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'R', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dhdzmota', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'R', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'PUNE', 'stats_list': [], 'contributions': '317 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sudhanshu1304', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'R', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/maniratnam555', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'R', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/umairrr1999', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'R', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JieGu-925', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'R', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'Egypt,Cairo', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AyaKhaledYousef', 'info_list': ['Python', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'R', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}"
"{'location': 'PUNE', 'stats_list': [], 'contributions': '317 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sudhanshu1304', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Faruk-Geles', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gbjung759', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['""# LogisticRegression""\n'], 'url_profile': 'https://github.com/codejawk', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yogesh-kakwani', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Champaign, IL', 'stats_list': [], 'contributions': '1,066 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/davidb2', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '1,871 contributions\n        in the last year', 'description': ['LinearRegression\n.\n'], 'url_profile': 'https://github.com/Aakashdeveloper', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rajkamalbgowda', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '312 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/khancuh', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yuragorlo', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}"
"{'location': 'Thakurli', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/myselfamit', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019', 'Python', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'MATLAB', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Banglore,India', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Logistic_regression\nThis file shows how to apply the logistic regression and finding accuracy of model\n'], 'url_profile': 'https://github.com/aloksenapati', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019', 'Python', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'MATLAB', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Linear-Regression-\n'], 'url_profile': 'https://github.com/AnuragRachcha', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019', 'Python', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'MATLAB', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Noida sec 135', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ASHTYAGI2911', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019', 'Python', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'MATLAB', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['linearRegression\n\nregX only contains 1 funtion, it is called calc()\ncalc() makes a linear regression of two values.\ncalc() will make a own dataframe with the means of every agenumber.\ncalc() needs a panda dataframe as input!!!!\nIs is developed for the investigation of age and charges: So your searched columns are ""Age"" and ""charges""\nIt will not work when your columns are called different!!!!\ncalc() will return a List of m an b. You can look at an exemple in : testing.py\nthe call is ==> regX.regX.calc(df)\n\n'], 'url_profile': 'https://github.com/riccardito', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019', 'Python', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'MATLAB', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Lagos, Nigeria.', 'stats_list': [], 'contributions': '189 contributions\n        in the last year', 'description': ['Logistic-Regression-Example\nIn this machine learning example. I would be using logistic regression to predicting if a person would buy life insurnace based on his age using logistic regression\nPredicting if a person would buy life insurnace based on his age using logistic regression\n'], 'url_profile': 'https://github.com/Way4ward17', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019', 'Python', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'MATLAB', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shrikant3542', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019', 'Python', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'MATLAB', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Lagos, Nigeria.', 'stats_list': [], 'contributions': '189 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Way4ward17', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019', 'Python', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'MATLAB', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Chongqing/Chengdu', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['MATLAB_stacking_regression\nThis is the project used to reproduce the report experiment\n'], 'url_profile': 'https://github.com/SuperrrWu', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019', 'Python', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'MATLAB', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['linear_regression_miniproject\n'], 'url_profile': 'https://github.com/minghui-zhang', 'info_list': ['R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 31, 2019', 'Python', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'MATLAB', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/BogadhiNaveen', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 11, 2020', '1', 'MATLAB', 'MIT license', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ciyiming', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 11, 2020', '1', 'MATLAB', 'MIT license', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nayeemuddin-mohd', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 11, 2020', '1', 'MATLAB', 'MIT license', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['logistic_regression_miniproject\n'], 'url_profile': 'https://github.com/minghui-zhang', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 11, 2020', '1', 'MATLAB', 'MIT license', 'Updated Dec 31, 2019']}","{'location': 'PARIS', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dataforcast', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 11, 2020', '1', 'MATLAB', 'MIT license', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mwazirali', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 11, 2020', '1', 'MATLAB', 'MIT license', 'Updated Dec 31, 2019']}","{'location': 'seoul', 'stats_list': [], 'contributions': '210 contributions\n        in the last year', 'description': ['estate_regression_DNN_practice\n\ninput file : Real estate valuation data set.xlsx\n\n6가지 att를 이용하여 Y값(house price of unit area)을 선형회귀 모형을 이용하여 예측한다.\n'], 'url_profile': 'https://github.com/DongHyunByun', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 11, 2020', '1', 'MATLAB', 'MIT license', 'Updated Dec 31, 2019']}","{'location': 'Rio de Janeiro', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Simple Logistic Regression API Using Flask and Heroku\n'], 'url_profile': 'https://github.com/BrunoKruz', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 11, 2020', '1', 'MATLAB', 'MIT license', 'Updated Dec 31, 2019']}","{'location': 'Connecticut, United States', 'stats_list': [], 'contributions': '251 contributions\n        in the last year', 'description': [""Abalone Classification with Statistical Analysis\nTask: Predict the age of abalone from physical measurements.\nLibraries used in this repo: numpy, sklearn, pandas, matplotlib\nThis is a classic machine learning problem using UC Irvine's abalone database. More information at https://archive.ics.uci.edu/ml/datasets/Abalone\n\nImage Credit: (https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Abalone_at_California_Academy_of_Sciences.JPG/1200px-Abalone_at_California_Academy_of_Sciences.JPG\n""], 'url_profile': 'https://github.com/therealcyberlord', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 11, 2020', '1', 'MATLAB', 'MIT license', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Conic Optimization for Robust Quadratic Regression: Deterministic Bounds and Statistical Analysis\nMain scripts:\n\nState_Estimation_sdp.m    :     multi-round SDP simulation (5 sims for each data points)\nState_Estimation_socp.m   :     multi-round SOCP simulation (5 sims for each data points)\n\nVariables:\n\nresult    :   the RMSE estimation error\nresres    :   collects all RMSE values whose average were plotted\n\nIf using the code for a publication, please cite the paper:\n@inproceedings{molybog2018conic,\n  Title = {Conic optimization for robust quadratic regression: Deterministic bounds and statistical analysis},\n  Author = {Molybog, Igor and Madani, Ramtin and Lavaei, Javad},\n  Booktitle  = {2018 IEEE Conference on Decision and Control (CDC)},\n  Pages = {841--848},\n  Year = {2018},\n  Organization = {IEEE}\n}\n\n'], 'url_profile': 'https://github.com/igormolybog', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020', 'R', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'MIT license', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 11, 2020', '1', 'MATLAB', 'MIT license', 'Updated Dec 31, 2019']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['#LINEAR REGRESSION\n'], 'url_profile': 'https://github.com/vishwaraj1', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019']}","{'location': 'Pune, India', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['MultipleLinearRegression\n'], 'url_profile': 'https://github.com/theSnehaThing', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019']}","{'location': 'Pune, India', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['SimpleLinearRegression\n'], 'url_profile': 'https://github.com/theSnehaThing', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Classification of basic shape based on pointNet\nWe design a experiment based on PointNet to measure/fit the basic shape object size.\nThe experimental process is as follows:\n\nGenerate synthetic 3d point data with size classification label\nTrain classification network based on pointNet\nExtract the feature map of pointnet\nDesign the tail of network to fit the size label\n\nTODO\nAdd the size regression for baisc shape\n'], 'url_profile': 'https://github.com/2JONAS', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Custom Linear Regression\n'], 'url_profile': 'https://github.com/anbinhpham1107', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019']}","{'location': 'Canada', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""hr_data_LogisticRegression\nHR Dataset used for predicting an employee's attrition using Logistic Regression\n""], 'url_profile': 'https://github.com/abhijeetjainmca', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019']}","{'location': 'Chongqing/Chengdu', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SuperrrWu', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019']}","{'location': 'India', 'stats_list': [], 'contributions': '220 contributions\n        in the last year', 'description': ['Linear-Regression-Implementation\n'], 'url_profile': 'https://github.com/GarvTambi', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019']}","{'location': 'United States of America', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['This project is the implementation of decision tree regression algorithm in python using the scikit learn library.\nThe following are the goals of the code:\n1. To read a CSV file with the data inputs.\n2. Store the data into 2 variables based on dependent and independent vars (Here, it is assumed that the data has independent vars in all but the last column, with the data itself having 5 columns).\n3. OneHotEncode the categorical data.\n4. Split the data into train and test data (80% for train).\n5. Create a decision tree object of the decision tree class.\n6. Fit the object with the training data.\n7. Predict the output for the test data and compare it  with the actual outputs.\n8. View the tree itself.\n'], 'url_profile': 'https://github.com/ukalla1', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019']}","{'location': 'London, Ontario Canada', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Multiple Linear Regression Analysis\nThe objective of the project was to perform a multiple linear regression analysis to predict the number of registered members that used this bike-sharing system, and identify behaviors depending on the environmental and seasonal settings. For instance, weather conditions, precipitation, and other factors could affect rental behavior. Performing this analysis would help to understand better the trends depending on the season and the relationship between the total number of registered users and the variables of interests.\nThe data of the project was obtained from kaggle. In addition, the brief report can be found here and the code of the analysis here\n'], 'url_profile': 'https://github.com/dborgesm', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Python', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019']}"
"{'location': 'France', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['Linear-Regression\n\nData source : https://raw.githubusercontent.com/tuyenhavan/Statistics/Dataset/50-Startups.csv\nWorking Environment : RStudio 1.2.5001\nThe dataset contains 50 observations of 5 variables : Spending on R&D, Administration, Marketing, the State (New York, California, Florida) in which the Startup is based and the Profit generated by the Startup.\n\nObjective : Attempt a prediction of a startup\'s profitability based on its spending and the State.\nExploratory Data Analysis\n#Importing the data\n>startup_data <- read.csv(""50-startups.csv"")\n\n#Visualizing the data\n> str(startup_data)\n\'data.frame\':\t50 obs. of  5 variables:\n $ R.D.Spend      : num  165349 162598 153442 144372 142107 ...\n $ Administration : num  136898 151378 101146 118672 91392 ...\n $ Marketing.Spend: num  471784 443899 407935 383200 366168 ...\n $ State          : Factor w/ 3 levels ""California"",""Florida"",..: 3 1 2 3 2 3 1 2 3 1 ...\n $ Profit         : num  192262 191792 191050 182902 166188 ...\n\n#Given the relative small size of the dataset, cross-validation will not be performed.\n\n#Evaluation of correlation matrix between numerical variables\n> startup_subset <- startup_data %>% select(R.D.Spend, Administration, Marketing.Spend, Profit)\n\n> cor(startup_subset)\n                R.D.Spend Administration Marketing.Spend    Profit\nR.D.Spend       1.0000000     0.24195525      0.72424813 0.9729005\nAdministration  0.2419552     1.00000000     -0.03215388 0.2007166\nMarketing.Spend 0.7242481    -0.03215388      1.00000000 0.7477657\nProfit          0.9729005     0.20071657      0.74776572 1.0000000\n\n#The above matrix suggests Profit is strongly and positively correlated with Marketing and R&D spending.\n#Administration spending however shows low correlation with other variables and as such is not likely to affect Profit. To be confirmed.\n\n#Analysis of variance between Profit(quantitative) and State(qualitative) (Single factor)\n>startup_factor <- startup_data %>% select(State, Profit)\n>test_aov <- aov(Profit~State, data=startup_factor)\n>summary(test_aov)\n            Df    Sum Sq   Mean Sq F value Pr(>F)\nState        2 1.901e+09 9.503e+08   0.575  0.567 \nResiduals   47 7.770e+10 1.653e+09 \n\n#The p-value (=0.567) of aov test reveals ""If there is no influence of State on Profit, this is likely to happen 56.7% of the time.\n\nModel Fitting\n#Model fitting with all explanatory variables\n>my_model = lm(Profit~., data=startup_data)\n>summary(my_model)\nCall:\nlm(formula = Profit ~ ., data = startup_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-33504  -4736     90   6672  17338 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      5.013e+04  6.885e+03   7.281 4.44e-09 ***\nR.D.Spend        8.060e-01  4.641e-02  17.369  < 2e-16 ***\nAdministration  -2.700e-02  5.223e-02  -0.517    0.608    \nMarketing.Spend  2.698e-02  1.714e-02   1.574    0.123    \nStateFlorida     1.988e+02  3.371e+03   0.059    0.953    \nStateNew York   -4.189e+01  3.256e+03  -0.013    0.990    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9439 on 44 degrees of freedom\nMultiple R-squared:  0.9508,\tAdjusted R-squared:  0.9452 \nF-statistic: 169.9 on 5 and 44 DF,  p-value: < 2.2e-16\n\n#The above summary shows :\n#1-Considering a multiple linear model is a good idea since the Adjusted R-squared and Multiple R-squared are close to 1.\n#2-The p-values associated with factors StateFlorida & StateNew York are high >95% (the reference for the T test is California). This is in accordance with the AOV test performed earlier.\n#2-Therefore we accept the null hypothesis : There is no difference between the influence of Florida compared to California,the influence of New York compared to California & furthermore no influence of the State in which the Startup is located on its profitability.\n#3-The low p-value (<2e-16) of the T test on R.D.Spend suggests this variable has an influence on the Profit (null hypothesis rejected).\n#4-Administration and Marketing.Spend have high p-values (12% & 60%) which suggest these variables have no influence on Profit. To be confirmed in variable selection procedure.\n\nVariable Selection\n#State has no influence on Profit -> Removed from model\n#Considering only numerical explanatory variables in the model\n>my_model = lm(Profit~., data=startup_numeric)\n>summary(my_model)\nCall:\nlm(formula = Profit ~ ., data = startup_numeric)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-33534  -4795     63   6606  17275 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      5.012e+04  6.572e+03   7.626 1.06e-09 ***\nR.D.Spend        8.057e-01  4.515e-02  17.846  < 2e-16 ***\nAdministration  -2.682e-02  5.103e-02  -0.526    0.602    \nMarketing.Spend  2.723e-02  1.645e-02   1.655    0.105    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9232 on 46 degrees of freedom\nMultiple R-squared:  0.9507,\tAdjusted R-squared:  0.9475 \nF-statistic:   296 on 3 and 46 DF,  p-value: < 2.2e-16\n\n#1-Proceeding with a backward variable selection procedure, the variable Administration is excluded (highest p-value) => It has no influence on Profit.\n#1-The value of the Adjusted R-squared is 0.9475.\n\n#Considering only Marketing.Spend & R.D.Spend in the model\n>my_model = lm(Profit~R.D.Spend+Marketing.Spend, data=startup_numeric)\n>summary(my_model)\nCall:\nlm(formula = Profit ~ R.D.Spend + Marketing.Spend, data = startup_numeric)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-33645  -4632   -414   6484  17097 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     4.698e+04  2.690e+03  17.464   <2e-16 ***\nR.D.Spend       7.966e-01  4.135e-02  19.266   <2e-16 ***\nMarketing.Spend 2.991e-02  1.552e-02   1.927     0.06 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9161 on 47 degrees of freedom\nMultiple R-squared:  0.9505,\tAdjusted R-squared:  0.9483 \nF-statistic: 450.8 on 2 and 47 DF,  p-value: < 2.2e-16\n\n#1-The value of the Adjusted R-squared improves to 0.9483.\n#2-The global Fisher test shows low (good) p-value <2.2e-16 => Good model.\n#2-The p-value associated with Marketing.Spend is 6%. This variable can be rejected or accepted depending on the acceptable p-value threshold.\n#3-If the threshold is 7%, the p-value = 6% is judged acceptable and the conclusion is : Profit depends on both R.D.Spend and Marketing.Spend.\n#4-Furthermore removing Marketing.Spend variable gives a smaller Adjusted R-squared value of 0.9454 (as compared to 0.9483)  i.e >my_model = lm(Profit~R.D.Spend, data=startup_numeric)\n\nVerifying Assumptions on Residuals/Noise\n>par(mfrow=c(2,2)) #2x2 image layout\n>plot(my_model)\n\n#The multi-linear regression model relies on some fundamental assumptions made on noise. These assumptions must be verified for the model to be valid.\n#1- Independence of noise : This depends on how the samples were collected. For instance the startups should have been selected randomly in each state and a startup\'s data shouldn\'t be collected more than once.\n#2- Normality/Gaussianity of noise : The Q-Q (Quantile-Quantile) plot below, which shows the standardized residuals\' quantile plot vs the standard Normal dist. quantiles, reveals the noise can be considered as Gaussian.\n#2- The plotted points closely match the theoretical straight line.\n#3- Homoscedasticity : This assumption is verified as the residuals are equally spreaded along the range of fitted/predicted values of Profit. In other words, the variance of the residuals is constant.\n#4- The residuals vs Leverage plot shows all points lie well within the Cook\'s distance lines, so there are no influential outliers and the model is good.\n#5- In the Residuals vs Fitted plot, residuals are equally spread around the horizontal line without a distinct pattern. This suggests the model is good as there is no non-linear relationship hidden in the residuals. \n\n\nPredicting Profit with the model\n>newdata <- data.frame(R.D.Spend=300000, Marketing.Spend=200000)  #300000 spent on R&D and 200000 spent on Marketing\n>result1 <- predict(my_model, newdata, interval=""prediction"",level=0.95)  #""prediction"" for the prediction interval\n> result1\n       fit      lwr      upr\n1 291932.7 265282.9 318582.4\n# According to the model, 95% of startups that spend 300000 on R&D and 200000 on Marketing, irrespective of their Administration spending and location (NY, Florida or California)\n# earn a profit between 265282.9 and 318582.4.\n\n>result2 <- predict(my_model, newdata, interval=""confidence"",level=0.95) #""confidence"" for the confidence interval\n> result2\n       fit      lwr      upr\n1 291932.7 272682.6 311182.7\n# According to the model, 95% of startups that spend 300000 on R&D and 200000 on Marketing, irrespective of their Administration spending and location (NY, Florida or California)\n# earn, ON AVERAGE, a profit between 272682.6 and 311182.7.\n\nConclusion\nA linear model could be obtained with Profit depending on R&D and Marketing spending.\nThe linear model coefficient for R&D (0.796) compared with that of Marketing (0.029) suggests that investing on R&D is more profitable than investing on Marketing.\nAdministration spending and location do not influence the startups\' profit.\n\n'], 'url_profile': 'https://github.com/femtonelson', 'info_list': ['R', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'JavaScript', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 4, 2020', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '242 contributions\n        in the last year', 'description': ['Tutorial-for-Polynomial-Regression\nThis tutorial explains what polynomial regression is and how to use it to solve ML problems.\n'], 'url_profile': 'https://github.com/tom1484', 'info_list': ['R', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'JavaScript', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 4, 2020', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/holovkoserhii', 'info_list': ['R', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'JavaScript', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 4, 2020', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anushka2515', 'info_list': ['R', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'JavaScript', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 4, 2020', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Sydney', 'stats_list': [], 'contributions': '887 contributions\n        in the last year', 'description': ['Fit linear regression using PYMC3\nFit simple model: 𝑌 = 𝑋𝛽+𝜖, where 𝑌 is the output we want to predict (or dependent variable), 𝑋 is our predictor (or independent variable), and 𝛽 are the coefficients (or parameters) of the model we want to estimate. 𝜖 is an error term which is assumed to be normally distributed.\nIn baysian lingo this is rewritten as: 𝑌 ∼ N(𝑋𝛽,𝜎^2^), where 𝑌 as a random variable (or random vector) of which each element (data point) is distributed according to a Normal distribution. The mean of this normal distribution is provided by our linear predictor with variance 𝜎^2^.\n'], 'url_profile': 'https://github.com/mdekauwe', 'info_list': ['R', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'JavaScript', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 4, 2020', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '187 contributions\n        in the last year', 'description': ['Prerequisites\n\nPython 3\npip\n\nSetup \n\nClone the repository\nOpen the project folder in Terminal\nRun pip install -r dependencies\nThis will install the following dependencies\nrequests, sklearn, numpy, pandas, matplotlib, datetime, configparser\n\nUsing the config\nYou can set multiple values in config.ini.\n\n\n\nKey\nValue\n\n\n\n\nforecast_range\nAn int value describing the forecast range in days\n\n\nplot_history_range\nAn int value describing the amount of weather history visible in the output plot (Capped to history_range)\n\n\nhistory_range\nAn int value describing the amount of days to retrieve from weather history\n\n\nattribute\nThe attribute you want to predict. See Attributes\n\n\nuse_live_api\nWether you want to use the most recent weather data or the dump provided in assets/sampleData\n\n\nlat\nThe latitude of the location you want to predict the weather at\n\n\nlon\nThe longitude of the location you want to predict the weather at\n\n\n\nHow to use \nMake sure you have installed the dependencies from the Setup instructions before continuing.\n\nOpen the project folder in Terminal\nNavigate to the software folder\nRun python3 app.py\n\nAttributes \nThis is a list of all weather attributes this program is able to predict.\nIf you edit the config variable attribute refer to the key column.\n\n\n\nAttribute\nKey\n\n\n\n\nTemperature\ntemperature\n\n\nMaximum Temperature\ntemperature_min\n\n\nMinimum Temperature\ntemperature_max\n\n\nPrecipitation\nprecipitation\n\n\nWindspeed\nwindspeed\n\n\nPeak Gust\npeakgust\n\n\nPressure\npressure\n\n\n\n'], 'url_profile': 'https://github.com/realdegrees', 'info_list': ['R', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'JavaScript', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 4, 2020', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Navi Mumbai', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nileshsinalkar', 'info_list': ['R', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'JavaScript', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 4, 2020', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '312 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/khancuh', 'info_list': ['R', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'JavaScript', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 4, 2020', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '312 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/khancuh', 'info_list': ['R', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'JavaScript', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 4, 2020', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Charlotte, North Carolina', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anushka04', 'info_list': ['R', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'JavaScript', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 4, 2020', 'Updated Dec 30, 2019', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Aswini-DS', 'info_list': ['Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'MIT license', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Toronto, Ontario', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/faizywaizy', 'info_list': ['Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'MIT license', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '172 contributions\n        in the last year', 'description': ['Regression Enrichment Surface\nUnder dev.\nHow to use\nSimple case:\nTo produced average plots over some stratification (useful for dose response like data to produce results over types of cells for instance)\ntrues, preds = get_predicition_data()\nrds_model = rds.RegressionEnrichmentSurface(percent_min=-3)\n    rds_model.compute(trues, preds, samples=30)\n    rds_model.plot(save_file=args.metric_plot_prefix + ""rds_on_cell.png"",\n                   title=\'Regression Enrichment Surface (Avg over Unique Cells)\')\nTo produced average plots over some stratification (useful for dose response like data to produce results over types of cells for instance)\ntrues, preds, labels = get_predicition_data()\nrds_model = rds.RegressionEnrichmentSurface(percent_min=-3)\n    rds_model.compute(trues, preds, stratify=labels, samples=30)\n    rds_model.plot(save_file=args.metric_plot_prefix + ""rds_on_cell.png"",\n                   title=\'Regression Enrichment Surface (Avg over Unique Cells)\')\n\n\n\n\n\nCode for producing regression enrichment analysis\n\nFree software: MIT license\nDocumentation: https://regression-enrichment-surface.readthedocs.io.\n\n\nFeatures\n\nTODO\n\n\nCredits\nThis package was created with Cookiecutter and the audreyr/cookiecutter-pypackage project template.\n'], 'url_profile': 'https://github.com/aclyde11', 'info_list': ['Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'MIT license', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['SortLinearRegression\n排序算法线性回归分析\n通过岭回归分析排序算法时间复杂度，图形界面设计采用EasyX外部库\n\n'], 'url_profile': 'https://github.com/wao3', 'info_list': ['Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'MIT license', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '180 contributions\n        in the last year', 'description': ['#Linear and Poly Regression \nI try to rebuild the Regression from stratch. This includes Linear Regression (only one variable) and general Polynominal Regression.\n'], 'url_profile': 'https://github.com/minhlong94', 'info_list': ['Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'MIT license', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wardaharshad', 'info_list': ['Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'MIT license', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sadikshya-dot', 'info_list': ['Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'MIT license', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['KNN, Logistic Regression, SVM Model comparisons\nKexin Wang\n10/24/2019\nR Cleanup\ndata <- read.csv(""./tele.csv"")\n\n#convert y to\ndata$duration <- NULL\ndata$y = ifelse(data$y == \'yes\', 1, 0)\n\n#create dummy variables for each categorical variable\ndata = as.data.frame(model.matrix(~.-1, data = data))\n\n#normalizes data and binds it with colum y\ndata = cbind(as.data.frame(lapply(data[1:53], scale)), data$y)\n\n#reassigns last colum nae to y\ncolnames(data)[colnames(data) == ""data$y""] <- ""y""\nstr(data)\n\n\n## \'data.frame\':    41188 obs. of  54 variables:\n##  $ X                           : num  -1.73 -1.73 -1.73 -1.73 -1.73 ...\n##  $ age                         : num  1.53302 1.62897 -0.29018 -0.00231 1.53302 ...\n##  $ jobadmin.                   : num  -0.582 -0.582 -0.582 1.718 -0.582 ...\n##  $ jobblue.collar              : num  -0.538 -0.538 -0.538 -0.538 -0.538 ...\n##  $ jobentrepreneur             : num  -0.191 -0.191 -0.191 -0.191 -0.191 ...\n##  $ jobhousemaid                : num  6.153 -0.163 -0.163 -0.163 -0.163 ...\n##  $ jobmanagement               : num  -0.276 -0.276 -0.276 -0.276 -0.276 ...\n##  $ jobretired                  : num  -0.209 -0.209 -0.209 -0.209 -0.209 ...\n##  $ jobself.employed            : num  -0.189 -0.189 -0.189 -0.189 -0.189 ...\n##  $ jobservices                 : num  -0.327 3.062 3.062 -0.327 3.062 ...\n##  $ jobstudent                  : num  -0.147 -0.147 -0.147 -0.147 -0.147 ...\n##  $ jobtechnician               : num  -0.442 -0.442 -0.442 -0.442 -0.442 ...\n##  $ jobunemployed               : num  -0.159 -0.159 -0.159 -0.159 -0.159 ...\n##  $ jobunknown                  : num  -0.0899 -0.0899 -0.0899 -0.0899 -0.0899 ...\n##  $ maritalmarried              : num  0.808 0.808 0.808 0.808 0.808 ...\n##  $ maritalsingle               : num  -0.625 -0.625 -0.625 -0.625 -0.625 ...\n##  $ maritalunknown              : num  -0.0441 -0.0441 -0.0441 -0.0441 -0.0441 ...\n##  $ educationbasic.6y           : num  -0.243 -0.243 -0.243 4.119 -0.243 ...\n##  $ educationbasic.9y           : num  -0.415 -0.415 -0.415 -0.415 -0.415 ...\n##  $ educationhigh.school        : num  -0.548 1.824 1.824 -0.548 1.824 ...\n##  $ educationilliterate         : num  -0.0209 -0.0209 -0.0209 -0.0209 -0.0209 ...\n##  $ educationprofessional.course: num  -0.382 -0.382 -0.382 -0.382 -0.382 ...\n##  $ educationuniversity.degree  : num  -0.648 -0.648 -0.648 -0.648 -0.648 ...\n##  $ educationunknown            : num  -0.209 -0.209 -0.209 -0.209 -0.209 ...\n##  $ defaultunknown              : num  -0.514 1.947 -0.514 -0.514 -0.514 ...\n##  $ defaultyes                  : num  -0.00853 -0.00853 -0.00853 -0.00853 -0.00853 ...\n##  $ housingunknown              : num  -0.157 -0.157 -0.157 -0.157 -0.157 ...\n##  $ housingyes                  : num  -1.049 -1.049 0.953 -1.049 -1.049 ...\n##  $ loanunknown                 : num  -0.157 -0.157 -0.157 -0.157 -0.157 ...\n##  $ loanyes                     : num  -0.423 -0.423 -0.423 -0.423 2.365 ...\n##  $ contacttelephone            : num  1.32 1.32 1.32 1.32 1.32 ...\n##  $ monthaug                    : num  -0.42 -0.42 -0.42 -0.42 -0.42 ...\n##  $ monthdec                    : num  -0.0666 -0.0666 -0.0666 -0.0666 -0.0666 ...\n##  $ monthjul                    : num  -0.459 -0.459 -0.459 -0.459 -0.459 ...\n##  $ monthjun                    : num  -0.385 -0.385 -0.385 -0.385 -0.385 ...\n##  $ monthmar                    : num  -0.116 -0.116 -0.116 -0.116 -0.116 ...\n##  $ monthmay                    : num  1.41 1.41 1.41 1.41 1.41 ...\n##  $ monthnov                    : num  -0.333 -0.333 -0.333 -0.333 -0.333 ...\n##  $ monthoct                    : num  -0.133 -0.133 -0.133 -0.133 -0.133 ...\n##  $ monthsep                    : num  -0.118 -0.118 -0.118 -0.118 -0.118 ...\n##  $ day_of_weekmon              : num  1.96 1.96 1.96 1.96 1.96 ...\n##  $ day_of_weekthu              : num  -0.515 -0.515 -0.515 -0.515 -0.515 ...\n##  $ day_of_weektue              : num  -0.494 -0.494 -0.494 -0.494 -0.494 ...\n##  $ day_of_weekwed              : num  -0.496 -0.496 -0.496 -0.496 -0.496 ...\n##  $ campaign                    : num  -0.566 -0.566 -0.566 -0.566 -0.566 ...\n##  $ pdays                       : num  0.195 0.195 0.195 0.195 0.195 ...\n##  $ previous                    : num  -0.349 -0.349 -0.349 -0.349 -0.349 ...\n##  $ poutcomenonexistent         : num  0.398 0.398 0.398 0.398 0.398 ...\n##  $ poutcomesuccess             : num  -0.186 -0.186 -0.186 -0.186 -0.186 ...\n##  $ emp.var.rate                : num  0.648 0.648 0.648 0.648 0.648 ...\n##  $ cons.price.idx              : num  0.723 0.723 0.723 0.723 0.723 ...\n##  $ cons.conf.idx               : num  0.886 0.886 0.886 0.886 0.886 ...\n##  $ euribor3m                   : num  0.712 0.712 0.712 0.712 0.712 ...\n##  $ y                           : num  0 0 0 0 0 0 0 0 0 0 ...\n\nLogistic Regression\n#randomize data, split 80% of it into training data and 20% of it into testing data \ndata_rand = data[sample(nrow(data)), ]\ntrain <- data_rand[1:ceiling(0.8*nrow(data)), ]\ntest <-  data_rand[(ceiling(0.8*nrow(data))+1) :nrow(data), ]\ndim(train)\n\n\n## [1] 32951    54\n\n\ndim(test)\n\n\n## [1] 8237   54\n\n\nlogistic <- glm(y ~. ,data = train, family = ""binomial"")\n# use step(logistic) to find the optimized logistic model\nlogistic_optimized = glm(y ~ X + jobretired + jobstudent + maritalsingle + \n    maritalunknown + educationbasic.6y + educationhigh.school + \n    educationilliterate + educationprofessional.course + educationuniversity.degree + \n    defaultunknown + contacttelephone + monthjul + monthjun + \n    monthmar + monthmay + monthnov + monthoct + monthsep + day_of_weekmon + \n    day_of_weekwed + campaign + pdays + poutcomenonexistent + \n    poutcomesuccess + emp.var.rate + cons.price.idx + cons.conf.idx + \n    euribor3m, data = train, family = ""binomial"")\nsummary(logistic_optimized)\n\n\n## \n## Call:\n## glm(formula = y ~ X + jobretired + jobstudent + maritalsingle + \n##     maritalunknown + educationbasic.6y + educationhigh.school + \n##     educationilliterate + educationprofessional.course + educationuniversity.degree + \n##     defaultunknown + contacttelephone + monthjul + monthjun + \n##     monthmar + monthmay + monthnov + monthoct + monthsep + day_of_weekmon + \n##     day_of_weekwed + campaign + pdays + poutcomenonexistent + \n##     poutcomesuccess + emp.var.rate + cons.price.idx + cons.conf.idx + \n##     euribor3m, family = ""binomial"", data = train)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.1376  -0.3928  -0.3177  -0.2530   2.9345  \n## \n## Coefficients:\n##                               Estimate Std. Error  z value Pr(>|z|)    \n## (Intercept)                  -2.511466   0.024204 -103.761  < 2e-16 ***\n## X                             0.545377   0.074647    7.306 2.75e-13 ***\n## jobretired                    0.076668   0.016033    4.782 1.74e-06 ***\n## jobstudent                    0.034106   0.014787    2.306 0.021086 *  \n## maritalsingle                 0.021911   0.020424    1.073 0.283362    \n## maritalunknown                0.008532   0.018730    0.456 0.648731    \n## educationbasic.6y             0.022274   0.023018    0.968 0.333187    \n## educationhigh.school          0.017216   0.024405    0.705 0.480534    \n## educationilliterate           0.024498   0.015820    1.549 0.121485    \n## educationprofessional.course  0.023287   0.022805    1.021 0.307191    \n## educationuniversity.degree    0.053931   0.024838    2.171 0.029911 *  \n## defaultunknown               -0.114170   0.026220   -4.354 1.34e-05 ***\n## contacttelephone             -0.266232   0.035053   -7.595 3.08e-14 ***\n## monthjul                     -0.085411   0.030106   -2.837 0.004554 ** \n## monthjun                     -0.296568   0.035783   -8.288  < 2e-16 ***\n## monthmar                      0.142886   0.012782   11.178  < 2e-16 ***\n## monthmay                     -0.253308   0.029524   -8.580  < 2e-16 ***\n## monthnov                     -0.350463   0.032848  -10.669  < 2e-16 ***\n## monthoct                     -0.073979   0.015721   -4.706 2.53e-06 ***\n## monthsep                     -0.040477   0.013468   -3.005 0.002653 ** \n## day_of_weekmon               -0.091826   0.020965   -4.380 1.19e-05 ***\n## day_of_weekwed                0.049795   0.020200    2.465 0.013695 *  \n## campaign                     -0.112616   0.028119   -4.005 6.20e-05 ***\n## pdays                        -0.184174   0.039541   -4.658 3.20e-06 ***\n## poutcomenonexistent           0.168890   0.021697    7.784 7.03e-15 ***\n## poutcomesuccess               0.138506   0.038194    3.626 0.000287 ***\n## emp.var.rate                 -2.393563   0.177365  -13.495  < 2e-16 ***\n## cons.price.idx                1.000785   0.061274   16.333  < 2e-16 ***\n## cons.conf.idx                 0.089738   0.021010    4.271 1.94e-05 ***\n## euribor3m                     1.461711   0.184601    7.918 2.41e-15 ***\n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 23265  on 32950  degrees of freedom\n## Residual deviance: 18158  on 32921  degrees of freedom\n## AIC: 18218\n## \n## Number of Fisher Scoring iterations: 6\n\n\n#summary(logistic)$coeff[-1] < 0.05\n#names(summary(logistic))[summary(logistic)$coeff[-1] < 0.05]\nlibrary(lattice)\nlibrary(ggplot2)\n\n\n## Registered S3 methods overwritten by \'ggplot2\':\n##   method         from \n##   [.quosures     rlang\n##   c.quosures     rlang\n##   print.quosures rlang\n\n\ntoselect.x <- summary(logistic)$coeff[-1,4] < 0.05\nrelevant.x <- names(toselect.x)[toselect.x == TRUE] \nsig.formula <- as.formula(paste(""y ~"",paste(relevant.x, collapse= ""+"")))\nsig.model <- glm(formula=sig.formula,data=train)\npredictions <- predict(sig.model, newdata = test, type = ""response"")\nlibrary(caret)\n#data$y = as.factor(data$y)\n#levels(data$y) <- as.factor(predictions)\ntest_pred_logistic <- ifelse(predictions<0.5,0,1)\nconfusionMatrix(as.factor(test$y), as.factor(test_pred_logistic))\n\n\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction    0    1\n##          0 7230   95\n##          1  704  208\n##                                           \n##                Accuracy : 0.903           \n##                  95% CI : (0.8964, 0.9093)\n##     No Information Rate : 0.9632          \n##     P-Value [Acc > NIR] : 1               \n##                                           \n##                   Kappa : 0.3039          \n##                                           \n##  Mcnemar\'s Test P-Value : <2e-16          \n##                                           \n##             Sensitivity : 0.9113          \n##             Specificity : 0.6865          \n##          Pos Pred Value : 0.9870          \n##          Neg Pred Value : 0.2281          \n##              Prevalence : 0.9632          \n##          Detection Rate : 0.8777          \n##    Detection Prevalence : 0.8893          \n##       Balanced Accuracy : 0.7989          \n##                                           \n##        \'Positive\' Class : 0               \n## \n\n\n(7229+218)/(7229+101+689+218)\n\n\n## [1] 0.9040913\n\n\n#Accuracy rate of logistic model:(7229+218)/(7229+101+689+218)=0.9041\n\nkNN\n#randomize data, split 80% of it into training data and 20% of it into testing data \n#data_rand <- data_rand[complete.cases(data_rand),]\nknn_train = as.data.frame(data_rand[1:ceiling(0.8*nrow(data_rand)),-54], drop = FALSE )\nknn_test = as.data.frame(data_rand[ceiling(0.8*nrow(data_rand)+1):nrow(data_rand),-54], drop = FALSE )\n\nknn_train_labels = (data_rand[1:ceiling(0.8*nrow(data_rand)),54])\nknn_test_labels = (data_rand[ceiling(0.8*nrow(data_rand)+1):nrow(data_rand),54])\nlibrary(gmodels)\n\n\n## Warning: package \'gmodels\' was built under R version 3.6.1\n\n\nlibrary(class)\n\n\n## Warning: package \'class\' was built under R version 3.6.1\n\n\ntest_pred_knn = knn(train = knn_train, test = knn_test, cl = knn_train_labels, k = 3)\nCrossTable(x = knn_test_labels, y= test_pred_knn, prop.chisq = FALSE)\n\n\n## \n##  \n##    Cell Contents\n## |-------------------------|\n## |                       N |\n## |           N / Row Total |\n## |           N / Col Total |\n## |         N / Table Total |\n## |-------------------------|\n## \n##  \n## Total Observations in Table:  8237 \n## \n##  \n##                 | test_pred_knn \n## knn_test_labels |         0 |         1 | Row Total | \n## ----------------|-----------|-----------|-----------|\n##               0 |      6999 |       326 |      7325 | \n##                 |     0.955 |     0.045 |     0.889 | \n##                 |     0.916 |     0.543 |           | \n##                 |     0.850 |     0.040 |           | \n## ----------------|-----------|-----------|-----------|\n##               1 |       638 |       274 |       912 | \n##                 |     0.700 |     0.300 |     0.111 | \n##                 |     0.084 |     0.457 |           | \n##                 |     0.077 |     0.033 |           | \n## ----------------|-----------|-----------|-----------|\n##    Column Total |      7637 |       600 |      8237 | \n##                 |     0.927 |     0.073 |           | \n## ----------------|-----------|-----------|-----------|\n## \n## \n\n\n(7025+283)/8237\n\n\n## [1] 0.8872162\n\n\n#Accuracy rate of kNN model: (7025+283)/8237=0.8872\n\nSVM\ndata <- read.csv(""./tele.csv"")\n\n#convert y to\ndata$duration <- NULL\ndata$y = ifelse(data$y == \'yes\', 1, 0)\n\n#create dummy variables for each categorical variable\ndata = as.data.frame(model.matrix(~.-1, data = data))\n\n#normalizes data and binds it with colum y\ndata = cbind(as.data.frame(lapply(data[1:53], scale)), data$y)\n\n#reassigns last colum nae to y\ncolnames(data)[colnames(data) == ""data$y""] <- ""y""\nstr(data)\n\n\n## \'data.frame\':    41188 obs. of  54 variables:\n##  $ X                           : num  -1.73 -1.73 -1.73 -1.73 -1.73 ...\n##  $ age                         : num  1.53302 1.62897 -0.29018 -0.00231 1.53302 ...\n##  $ jobadmin.                   : num  -0.582 -0.582 -0.582 1.718 -0.582 ...\n##  $ jobblue.collar              : num  -0.538 -0.538 -0.538 -0.538 -0.538 ...\n##  $ jobentrepreneur             : num  -0.191 -0.191 -0.191 -0.191 -0.191 ...\n##  $ jobhousemaid                : num  6.153 -0.163 -0.163 -0.163 -0.163 ...\n##  $ jobmanagement               : num  -0.276 -0.276 -0.276 -0.276 -0.276 ...\n##  $ jobretired                  : num  -0.209 -0.209 -0.209 -0.209 -0.209 ...\n##  $ jobself.employed            : num  -0.189 -0.189 -0.189 -0.189 -0.189 ...\n##  $ jobservices                 : num  -0.327 3.062 3.062 -0.327 3.062 ...\n##  $ jobstudent                  : num  -0.147 -0.147 -0.147 -0.147 -0.147 ...\n##  $ jobtechnician               : num  -0.442 -0.442 -0.442 -0.442 -0.442 ...\n##  $ jobunemployed               : num  -0.159 -0.159 -0.159 -0.159 -0.159 ...\n##  $ jobunknown                  : num  -0.0899 -0.0899 -0.0899 -0.0899 -0.0899 ...\n##  $ maritalmarried              : num  0.808 0.808 0.808 0.808 0.808 ...\n##  $ maritalsingle               : num  -0.625 -0.625 -0.625 -0.625 -0.625 ...\n##  $ maritalunknown              : num  -0.0441 -0.0441 -0.0441 -0.0441 -0.0441 ...\n##  $ educationbasic.6y           : num  -0.243 -0.243 -0.243 4.119 -0.243 ...\n##  $ educationbasic.9y           : num  -0.415 -0.415 -0.415 -0.415 -0.415 ...\n##  $ educationhigh.school        : num  -0.548 1.824 1.824 -0.548 1.824 ...\n##  $ educationilliterate         : num  -0.0209 -0.0209 -0.0209 -0.0209 -0.0209 ...\n##  $ educationprofessional.course: num  -0.382 -0.382 -0.382 -0.382 -0.382 ...\n##  $ educationuniversity.degree  : num  -0.648 -0.648 -0.648 -0.648 -0.648 ...\n##  $ educationunknown            : num  -0.209 -0.209 -0.209 -0.209 -0.209 ...\n##  $ defaultunknown              : num  -0.514 1.947 -0.514 -0.514 -0.514 ...\n##  $ defaultyes                  : num  -0.00853 -0.00853 -0.00853 -0.00853 -0.00853 ...\n##  $ housingunknown              : num  -0.157 -0.157 -0.157 -0.157 -0.157 ...\n##  $ housingyes                  : num  -1.049 -1.049 0.953 -1.049 -1.049 ...\n##  $ loanunknown                 : num  -0.157 -0.157 -0.157 -0.157 -0.157 ...\n##  $ loanyes                     : num  -0.423 -0.423 -0.423 -0.423 2.365 ...\n##  $ contacttelephone            : num  1.32 1.32 1.32 1.32 1.32 ...\n##  $ monthaug                    : num  -0.42 -0.42 -0.42 -0.42 -0.42 ...\n##  $ monthdec                    : num  -0.0666 -0.0666 -0.0666 -0.0666 -0.0666 ...\n##  $ monthjul                    : num  -0.459 -0.459 -0.459 -0.459 -0.459 ...\n##  $ monthjun                    : num  -0.385 -0.385 -0.385 -0.385 -0.385 ...\n##  $ monthmar                    : num  -0.116 -0.116 -0.116 -0.116 -0.116 ...\n##  $ monthmay                    : num  1.41 1.41 1.41 1.41 1.41 ...\n##  $ monthnov                    : num  -0.333 -0.333 -0.333 -0.333 -0.333 ...\n##  $ monthoct                    : num  -0.133 -0.133 -0.133 -0.133 -0.133 ...\n##  $ monthsep                    : num  -0.118 -0.118 -0.118 -0.118 -0.118 ...\n##  $ day_of_weekmon              : num  1.96 1.96 1.96 1.96 1.96 ...\n##  $ day_of_weekthu              : num  -0.515 -0.515 -0.515 -0.515 -0.515 ...\n##  $ day_of_weektue              : num  -0.494 -0.494 -0.494 -0.494 -0.494 ...\n##  $ day_of_weekwed              : num  -0.496 -0.496 -0.496 -0.496 -0.496 ...\n##  $ campaign                    : num  -0.566 -0.566 -0.566 -0.566 -0.566 ...\n##  $ pdays                       : num  0.195 0.195 0.195 0.195 0.195 ...\n##  $ previous                    : num  -0.349 -0.349 -0.349 -0.349 -0.349 ...\n##  $ poutcomenonexistent         : num  0.398 0.398 0.398 0.398 0.398 ...\n##  $ poutcomesuccess             : num  -0.186 -0.186 -0.186 -0.186 -0.186 ...\n##  $ emp.var.rate                : num  0.648 0.648 0.648 0.648 0.648 ...\n##  $ cons.price.idx              : num  0.723 0.723 0.723 0.723 0.723 ...\n##  $ cons.conf.idx               : num  0.886 0.886 0.886 0.886 0.886 ...\n##  $ euribor3m                   : num  0.712 0.712 0.712 0.712 0.712 ...\n##  $ y                           : num  0 0 0 0 0 0 0 0 0 0 ...\n\n\nknn_train = as.data.frame(data_rand[1:ceiling(0.8*nrow(data_rand)),-54], drop = FALSE )\nknn_test = as.data.frame(data_rand[ceiling(0.8*nrow(data_rand)+1):nrow(data_rand),-54], drop = FALSE )\n\nknn_train_labels = (data_rand[1:ceiling(0.8*nrow(data_rand)),54])\nknn_test_labels = (data_rand[ceiling(0.8*nrow(data_rand)+1):nrow(data_rand),54])\nlibrary(gmodels)\nlibrary(class)\nlibrary(e1071)\n\n\n## Warning: package \'e1071\' was built under R version 3.6.1\n\n\nlibrary(kernlab)\n\n\n## \n## Attaching package: \'kernlab\'\n\n\n## The following object is masked from \'package:ggplot2\':\n## \n##     alpha\n\n\n# svm_model = ksvm(y ~ X + jobretired + jobstudent + maritalsingle + \n#     maritalunknown + educationbasic.6y + educationhigh.school + \n#     educationilliterate + educationprofessional.course + educationuniversity.degree + \n#     defaultunknown + contacttelephone + monthjul + monthjun + \n#     monthmar + monthmay + monthnov + monthoct + monthsep + day_of_weekmon + \n#     day_of_weekwed + campaign + pdays + poutcomenonexistent + \n#     poutcomesuccess + emp.var.rate + cons.price.idx + cons.conf.idx + \n#     euribor3m, data = train, kernel = ""rbfdot"")\ntrain$y = as.factor(train$y)\nsvm_model2 = ksvm(y ~ X + jobretired + jobstudent + maritalsingle + \n    maritalunknown + educationbasic.6y + educationhigh.school + \n    educationilliterate + educationprofessional.course + educationuniversity.degree + \n    defaultunknown + contacttelephone + monthjul + monthjun + \n    monthmar + monthmay + monthnov + monthoct + monthsep + day_of_weekmon + \n    day_of_weekwed + campaign + pdays + poutcomenonexistent + \n    poutcomesuccess + emp.var.rate + cons.price.idx + cons.conf.idx + \n    euribor3m, data = train, kernel = ""rbfdot"")\n# summary(svm_model)\ntest_pred_svm <- predict(svm_model2, test)\n#predictions <- ifelse(predictions >=0.5, 1, 0)\npredictions = test_pred_svm\nhead(predictions)\n\n\n## [1] 0 0 0 0 0 0\n## Levels: 0 1\n\n\ntable(predictions, test$y)\n\n\n##            \n## predictions    0    1\n##           0 7224  689\n##           1  101  223\n\n\nresult <- predictions == test$y\ntable(result)\n\n\n## result\n## FALSE  TRUE \n##   790  7447\n\n\nprop.table(table(result))\n\n\n## result\n##     FALSE      TRUE \n## 0.0959087 0.9040913\n\n\n(7218+235)/(7218+672+112+235)\n\n\n## [1] 0.9048197\n\nCombine all three models together\ncombined_result = as.numeric(test_pred_knn)-1+as.numeric(test_pred_logistic)+as.numeric(test_pred_svm) -1\nfinal_pred = ifelse(combined_result>=2, 1, 0 )\npred_table = prop.table(table(final_pred, test$y))\npred_table\n\n\n##           \n## final_pred          0          1\n##          0 0.87762535 0.08498240\n##          1 0.01165473 0.02573753\n\n\nprop.table(table(test_pred_logistic, test$y))\n\n\n##                   \n## test_pred_logistic          0          1\n##                  0 0.87774675 0.08546801\n##                  1 0.01153333 0.02525191\n\n\ncat(""accuracy="", pred_table[1,1]+pred_table[2,2])\n\n\n## accuracy= 0.9033629\n\nConclusion\nBased on our previous results,\n\nthe SVM model has an accuracy of 90.48%,\nthe logistic regression model has an accuracy of 90.41%,\nthe KNN model has an accuracy of 88.72%\nthe synthetic model has an accuracy of 90.40%\n\nFrom the result, although synthetic model is not more accurate than the\nlogistic regression model, we can still conclude that the synthetic model is\nbetter because the negative true chance is smaller, which indicates that a\nhigher chance of the phone call to be success. In other words, using the\nsynthetic model will help making a better decision in terms of efforts because\nof its least negative true chance.\n'], 'url_profile': 'https://github.com/wakexin', 'info_list': ['Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'MIT license', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mananashah007', 'info_list': ['Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'MIT license', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Multiple regreesion analysis\nDataset:\nThe dataset has data collected from New York, California and Florida about 50 business Startups ""17 in each state"". The variables used in the dataset are Profit, R&D spending, Administration Spending, and Marketing Spending.\nChallenge\nKnowing the anual profit of a company and how much it spent in each of these factors:\n\nState in wich it is settled\nResearch and development\nAdministration\nMarketing\n\n\n\nCreate a model that tells wich types of company yelds best results of profit, based on the given factors. So, given the amount spent in each sector, we would be able to predict the profit.\nAfter creating the model we should be able to answer questions like:\n_The company performs better in New York or California?\n_It is better to invest in companies that spends more on Research and developent or more on marketing?\n\n\n\nConsiderations\nMultiple linear regression is an extension of a simple linear regression, where we have more independent variables contributing to the value of the dependent variable.(y= b0 + b1x + b2x2 + ... + bnxn)\n\nAdding more variables to a multiple regression does not mean it will offer better predictions. To avoid the overfitting problem, the ideia is to pick the best variables to the model.\nWhen variables are corelated to each other, we have a problem called multicollinearity, and to a better model it should be avoided. The ideal is for all independent variables to be correlated with the dependent variable, but not with each other.\n\n\nResults:\nAfter plotting the data and analysing the variables, it was possible to conclude that the best model to predict the profit of the startups should be research and development.\n*The states in wich state they are settled does not have significant influence.\n*Althought Research&Delevepment appeared correlated (as research spending increases, so does marketing spending), the most significant and influent variable was R&D, so is the best factor to consider when investing.\n*Administration spend didn\'t show to be correlated with profit.\n'], 'url_profile': 'https://github.com/stdevelopr', 'info_list': ['Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'MIT license', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['TradingBinary-LinearRegression\nAlgorithm to predict the best parameters in a linear regression to trade binary options.\nChallenge:\nTo predict if the next closing price will be higher or lower, based on a linear regression model.\nMethod\nUsing the linear regression model, assume that the next closing price will be the value predicted by the model.\n\nIf the actual value is above the predicted, the guess is that the next closing price will get lower.\nIf the actual value is below the predicted, the guess is that the next closing price will get higher.\n\nParameters\nThe following parameters will be under control:\n\nThe size of the window in wich the regression line will be calculated.\nThe dispersion around the line, through the coeficient of determination\nvarying from 0 ( total dispersion) to 1 (concentrated on the line)\n\n\nResults\nRunning the algorithm with a window size varying from 3 to 30 and the coeficient of determination varying from 0.3 to 0.9\nwith 0.2 step, the best result was:\n\nWin ratio: 54.92%\nWindow size: 27\nCoefficient of determination: 0.9\n\n\nPrediction Plot\nHere we see all the closing prices as the small blue dots. The red ones are the points that fitted the rule of the coefficient of determination. The green stars are ones where it was possible to make profit.\n\nIn the zoom we can see that the predictions and profits are mostly concentrated where the dispersion is small and the prices follow almost linearly.\n\nConclusion\nAfter running trough the whole dataset 108 times with 108 different parameters:\nThe avarage win ratio was 0.49\nStandard deviation: 0.016\nSo, running this strategy in one dataset without caring about the parameters will probably result in loss most of the time.\nAfter adjusting the parameters it was possible to find a combination that resulted in a positive win ratio of 4.9%.\nIt was noted that the entry points were few, however.\n'], 'url_profile': 'https://github.com/stdevelopr', 'info_list': ['1', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shyakazulu', 'info_list': ['1', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['regression-using-mlp-keras\nIn this notebook regression model is implemented using keras multi-level perceptron neural network to predict the strength of concrete using given parameters\n'], 'url_profile': 'https://github.com/rishabhrastogi31', 'info_list': ['1', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'United States of America', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ukalla1', 'info_list': ['1', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Logistic-Regression-Tutor.\nSome theory and implementation.\n'], 'url_profile': 'https://github.com/guney1', 'info_list': ['1', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Istanbul', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HasanSiwi', 'info_list': ['1', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Nairobi, Kenya', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/IyvonneBett', 'info_list': ['1', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['regression_problem_2\nRed Hat evaluation assignment\n'], 'url_profile': 'https://github.com/Rajiv2806', 'info_list': ['1', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DanGolawski', 'info_list': ['1', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['LinearRegression-Project\nLinearRegression model with single & multiple variables.\n'], 'url_profile': 'https://github.com/AbhishekSen05', 'info_list': ['1', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Python', 'Updated Jan 2, 2020', 'Python', 'Updated Dec 31, 2019', 'Updated Jan 3, 2020', 'Java', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PrakharSrivastava96', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fangyiyu', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': [""\n\n\n\n\n\n\n\n\nThis project will be your first steps into AI and Machine Learning. You're going to start with a simple, basic machine learning algorithm. You will have to create a program that predicts the price of a car by using a linear function train with a gradient descent algorithm.\nMandatory part\nImplement a linear regression algorithm on a single element, the mileage of a car. To do this you must implement 2 programs:\n\nThe first program will be used to predict the price of a car based on its mileage. When you launch the program, it will ask you for mileage and should give you an approximate price of the car.\nThe second program will be used to train your model. It will read the data set and make a linear regression on this data.\n\nBonuses\n\nView the data on a graph.\nDisplay the line resulting from your linear regression on this same graph and see if it works!\nDisplay the curve resulting from your cost history.\nA program that checks the accuracy of your algorithm.\n\nInstall\nThis project uses Homebrew and Python. Go check them out if you don't have them locally installed.\nUse the package manager pip3 to install all needed packages.\npip3 install numpy\npip3 install matplotlib\npip3 install sklearn\n\nUsage\npython3 ft_linear_regression.py [flags]\npython3 priceEstimation.py\n\nFlags\n-p, --prediction            - show the prediction curve\n-ch, --cost_history         - show the cost history curve\n-cd, --coef_determination   - show the coefficient of determination\n\nRate\n125/100\n""], 'url_profile': 'https://github.com/maxisimo', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '177 contributions\n        in the last year', 'description': ['types-of-regression\nIt covers all the types of Regression Analysis in Machine Learning using python that can be used to predict the time required to file any patent. Using Python Programming and all the required libraries for regression and visualization.\n'], 'url_profile': 'https://github.com/rv20197', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '312 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/khancuh', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SangeethaSA', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020']}","{'location': 'UK', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['MORTOGA\nA tool for Multi-Objective Regression-Test Optimization based on Genetic Algorithm (GA)\nFor details, see the following paper:\nVahid Garousi, Ramazan Özkan, Aysu Betin-Can, ""Multi-objective regression test selection in practice: an empirical study in the defense software industry"", Elsevier Journal of Information and Software Technology, volume 103, pp. 40-54, November 2018, DOI: 10.1016/j.infsof.2018.06.007\n'], 'url_profile': 'https://github.com/vgarousi', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020']}","{'location': 'Beijing, China', 'stats_list': [], 'contributions': '368 contributions\n        in the last year', 'description': [""stock-price-predict\nUsing linear regression/artificial neural network(ANN)/long short term memory(LSTM) to predict stcok price\n\n\n\n\n\nDue to the author's limited English level and usage habits, we only provide comments in Chinese version. English readers can use translation software or ignore the comments.\n""], 'url_profile': 'https://github.com/stxupengyu', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020']}","{'location': 'Syracuse, New York', 'stats_list': [], 'contributions': '224 contributions\n        in the last year', 'description': ['Data_Science-Bootcamp-ML\nML Glossary:\n\nhttps://developers.google.com/machine-learning/glossary/\nhttps://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/\n\nEssential Concepts:\n\nCorrelation VS Causation:\n\n\nhttps://towardsdatascience.com/correlation-vs-causation-a-real-world-example-9e939c85581e\nhttps://towardsdatascience.com/why-correlation-does-not-imply-causation-5b99790df07e\n\n\nBias-Variance Trade off:\n\n\nhttps://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\nhttps://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9\n\n\nFeature Engineering:\n\n\nhttps://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b\nhttps://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\nhttps://elitedatascience.com/feature-engineering\nhttps://www.analyticsvidhya.com/blog/tag/feature-engineering/\n\n\nEvaluation Metric for Classification:\n\n\nhttps://medium.com/thalus-ai/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b?\nhttps://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226\n\n\nEvaluation Metric for Regression:\n\n\nhttps://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/\nhttps://medium.com/@george.drakos62/how-to-select-the-right-evaluation-metric-for-machine-learning-models-part-1-regrression-metrics-3606e25beae0?\nhttps://medium.com/acing-ai/how-to-evaluate-regression-models-d183b4f5853d\n\n\nKaggle Kernels:\n\n\nSRK-Winning solution: https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions\nML-Shivam Bansal: https://www.kaggle.com/shivamb/data-science-glossary-on-kaggle/notebook\n\nCheatsheets: https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463\n'], 'url_profile': 'https://github.com/harshdarji23', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020']}","{'location': 'Helsinki, Finland', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': ['house-prices-prediction\nPredicting housing prices. Using data from Kaggle competition House Prices: Advanced Regression Techniques\n'], 'url_profile': 'https://github.com/jphietala', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 29, 2020']}"
"{'location': 'Almaty, Kazakhstan', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/taglitis', 'info_list': ['Python', 'Updated Jan 4, 2020', 'TeX', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 3, 2021', 'CSS', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['MATH531 Final Project\nHow Weather Can Affect a NFL Game\nBy Michael Bubniak, Tianheng Chen, Akira Taniguchi\nPlease read ProjectReport.pdf\nDataScrapping.ipynb was created to web scrap all the necessary NFL data used in this project.\nRegression models were tested using RStudio.\n'], 'url_profile': 'https://github.com/AkiraTaniguchi0', 'info_list': ['Python', 'Updated Jan 4, 2020', 'TeX', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 3, 2021', 'CSS', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': [""-House-Pricing-Prediction-Advanced-Regression-Techniques\nHouse Prices: Advanced Regression Techniques Predict sales prices and practice feature engineering, RFs, and gradient boosting\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nPractice Skills\nCreative feature engineering\nAdvanced regression techniques like random forest and gradient boosting\n""], 'url_profile': 'https://github.com/mahaksharma', 'info_list': ['Python', 'Updated Jan 4, 2020', 'TeX', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 3, 2021', 'CSS', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Breast-Cancer\nThe purpose of this notebook is to determine whether we can predict whether a breast cancer tumor is malignant or benign based on its size and shape. Logistic regression, KMeans, and PCA were performed on the dataset.\n'], 'url_profile': 'https://github.com/akonishi2', 'info_list': ['Python', 'Updated Jan 4, 2020', 'TeX', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 3, 2021', 'CSS', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Waterloo', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Credit-Card-Fraud-Detection\nThis R script is written using various algorithms like Decision Trees, Logistic Regression, Artificial Neural Networks\n'], 'url_profile': 'https://github.com/dhavalthakur', 'info_list': ['Python', 'Updated Jan 4, 2020', 'TeX', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 3, 2021', 'CSS', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Logistic Regression - March Madness Predictor\nIn this repository, I predicted the outcomes of certain matchups in March\nMadness based on data provided by Merkle.  Using Logistic Regression (as\nwell as experimenting with other machine learning algorithms), I achieved\nan accuracy of predicting 76% of games accurately.\nRun the python notebook March-Madness-03062018-Experimenting-76%.ipynb\nto see results.\n'], 'url_profile': 'https://github.com/tonywu1999', 'info_list': ['Python', 'Updated Jan 4, 2020', 'TeX', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 3, 2021', 'CSS', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'United Kingdom', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Decline curve analysis\nThis is a mini-project in some data analysis and curve fitting to some anonymous oil well output time series data. The script produces several plots. The first plot visualises the aggregate output quantities for some input wells on a stacked bar chart. The second visualises the monthly aggregate output quanitites and a daily time series output for the input wells. The third then fits curves to the historical data using an Arp\'s decline curve and a least squares minimisation routine. The fourth plot uses the forecast data to predict the ultimate recoverable volume for each input well and plots this against the aggregate output on the final day of production.\nRunning the script\nDownload the .xlsx and and .py files and change to this directory in the terminal/command line. To run the script in iPython, enter either ""ipython decline_curve.py"" or run in iPython using the ""%run"" magic command. To tweak the current input well options see the bottom\nof decline_curve.py. Wellnames vary between ""P001"" to ""P754"" for the historical data and ""P001"" to ""P020"" for the forecast data. For the best plot quality figures should be maximised. Due to the size of the dataset, the script may take a couple of minutes to run.\nPrerequisites\nTo run the script the latest installs of Python3, numpy, pandas, seaborn, scipy and matplotlib are recommended. These can be downloaded by first downloading pip and then running, for example,\npip install numpy\n\nin the terminal/command line. Some additional packages my also require installing for reading the excel spreadsheet with Python. These can also be downloaded using pip.\nAuthors\n\nDan Burrows - Decline curve analysis - (https://github.com/dwb26)\n\n'], 'url_profile': 'https://github.com/dwb26', 'info_list': ['Python', 'Updated Jan 4, 2020', 'TeX', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 3, 2021', 'CSS', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '126 contributions\n        in the last year', 'description': ['jamesHardenFTAPredictor\nLinear Regression Model that predicts the number of Free Throw Attempts James Harden Shoots per Game.\nInstructions\n\nClone/Download this repository\nInstall Flask, Sklearn, numpy\ncd into the folder and enter the command python app\nFollow the instructions on the command prompt to view the web app\n\nThe data used to train the model was obtain from www.basketball-reference.com\nParameters of the Linear Regression Model\n\nMinutes Played\nOutcome of the Game (Win/Loose)\nTwo Pointer Attempts\nField Goal Percentage\nFree Throws Made\nPoints\nSteals\n\nScreenshot: https://github.com/satavanan-s/jamesHardenFTAPredictor/blob/master/Screenshot.JPG?raw=true\n'], 'url_profile': 'https://github.com/satavanan-s', 'info_list': ['Python', 'Updated Jan 4, 2020', 'TeX', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 3, 2021', 'CSS', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Warsaw', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/stodolkiewicz', 'info_list': ['Python', 'Updated Jan 4, 2020', 'TeX', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 3, 2021', 'CSS', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'ABV-IIITM,Gwalior', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['Kaggle-Housing-prices\nHouse prices dataset solution on Kaggle using Random Forest Regression technique\nLink to dataset-https:www.kaggle.com/c/house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/prakhar33', 'info_list': ['Python', 'Updated Jan 4, 2020', 'TeX', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'R', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 3, 2021', 'CSS', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['TCP_Connection_Analysis\nAnalyzes TCP connections to find contributing factors to half-closed connections, which occur when a TCP reset is sent after a TCP FIN/ACK. This topic is important because TCP resets can be forged and used in network attacks.\nI worked on this project with two classmates, and my individual contribution was primarily the TCP_data_cleaning.R script and the poisson_models.R script. These two scripts assemble the relevant features from data collected using Wireshark software into tabular format and fits several GLMs to analyze relevant predictors. Logistic regression models are used to predict the presence (zero or at least one) of TCP resets after a FIN/ACK, and zero-inflated negative binomial models are used to model the actual count of the TCP resets post FIN/ACK. Assumptions from the poisson models were violated due to zero inflation and overdispersion in the response variable.\nFindings from this analysis are compared to feature importance plots generated from the regression trees and random forest classification models, which my partners wrote. Regression trees modelled the count and the random forest model predicted the presence of TCP resets resets post FIN/ACK. These models were much better in handling multi-collinearity in some of the features as well as modelling the non-linearities in some of the features, specifically their interactions. However the negative binomial models gave meaningful interpretations for the predictors where linearity was satisfied.\nThis project was done with two of my peers for our Computer Networks class at Skidmore.\n'], 'url_profile': 'https://github.com/mikemiller442', 'info_list': ['R', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'MIT license', 'Updated Dec 5, 2020', 'R', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Counterfeit-Medicines-Sales Prediction\nProblem Statement\nCounterfeit medicines are fake medicines which are either contaminated or contain wrong or no active\ningredient. They could have the right active ingredient but at the wrong dose. Counterfeit drugs are illegal and\nare harmful to health. 10% of the world's medicine is counterfeit and the problem is even worse in developing\ncountries. Up to 30% of medicines in developing countries are counterfeit.\nMillions of pills, bottles and sachets of counterfeit and illegal medicines are being traded across the world. The\nWorld Health Organization (WHO) is working with International Criminal Police Organization (Interpol) to dislodge\nthe criminal networks raking in billions of dollars from this cynical trade.\nDespite all these efforts, counterfeit medicine selling rackets don’t seem to stop popping here and there. It has\nbecome a challenge to deploy resources to counter these; without spreading them too thin and eventually\nrendering them ineffective. Government has decided that they should focus on illegal operations of high net\nworth first instead of trying to control all of them. In order to do that they have collected data which will help them\nto predict sales figures given an illegal operation's characteristics.\nSolution-\nWe have two data sets\n1)Counterfeit_train.csv\n2)Counterfeit_test.csv\nSteps\n1)First load both train and test data using python pandas library.\n2)Concatenate both train and test data for data preprocessing and data cleaning.\n3)Drop all ID columns from the data because including ID columns in modeling process doesn't make sense.\n4)Make (n-1) dummies of all the categorical columns.\n5)Check Null values and impute Null values by mean in train data only.\n6)Data preprocessing is complete,now split train and test data.\n7)Again split train data into 80% and 20%,in which 80% data will be used for training and 20% data was used for testing,check the performance of the model.\n8)Build linear regression model on train data and make  predictions on 20% test data.\n9)Calculate Mean Absolute Error which is nothing but the difference between y_predicted and y_actual.\n10)Fit linear regression model on whole training data and make predictions on whole test data.\n11)Linear regression is base model,so for getting better performance use another ML models like Random Forest algorithm.\n12)Use RandomForestRegressor(Regression problem) and RandomizedSearchCv of sklearn library with 10 fold cross validation.\n13)Find the best estimator of randomized_search and by this best_estimator  fit on 80% train data and make prediction on 20% test data.\n14)Calculate Mean Absolute Error(Y_predicted-Y_actual).\n15)Fit on whole training data and make prediction on whole test data.\n16)Save our predictions in an csv file.\n17)From both the algorithms,Random Forest give better performance and better predictions.\n""], 'url_profile': 'https://github.com/hariomjangid687', 'info_list': ['R', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'MIT license', 'Updated Dec 5, 2020', 'R', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'Beijing, China', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Logistic-Regression-for-a-Cat-Classifier\nThis is a cat classifier that recognizes cats with 70% accuracy by using logistic regression.\n'], 'url_profile': 'https://github.com/LiuShuwen', 'info_list': ['R', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'MIT license', 'Updated Dec 5, 2020', 'R', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Car-Price-Prediction\nPredicting the car price based on certain independent features using Supervised Regression methods\nThis Repo helps in answering below questions.\n\n\nLoad the file “imports-85.data” into a dataframe from https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data and column names of this data set can be found here --https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names\n\n\nExplain the problem statement. What are you predicting and what attributes you have to predict?\n\n\nBrowse a sample record from the dataframe. Are there any missing values?\n\n\nHow many records are available in the data set and how many attributes. Do you think the depth (number of records) is sufficient given the breadth? In other words, is the sample likely to be a good representative of the universe?\n\n\nAnalyse the data distribution for the various attributes and share your observations.\n\n\nAre there any independent attributes which have |R| close to 1?\n\n\nWhich attributes seem to have a stronger relationship with the dependent variable (Price of the car)?\n\n\nGiven the above analysis, which algorithm will be more suitable? Why?\n\n\n4 IPYNB files are present for 4 different scenario:\nCase 1: Applying get_dummies() on categorial variables and considering all the features.\nConclusion: model performance good on train set (overfitting) but fails on test set (curse of dimensionality)\nCase 2: Applying LabelEncoder() on categorial variables and considering all the features\nConclusion: model performance good on train set compared to test set\nCase 3: Applying LabelEncoder() on categorial variables and considering strong features whose R value close to 1\nConclusion: model performance good on both train as well as test set\nCase 4: Using Stats model\nConclusion: Helps in finding Actual predictors which have p<0.5 and model parameters with R square and Adjusted R square.\n'], 'url_profile': 'https://github.com/satish678', 'info_list': ['R', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'MIT license', 'Updated Dec 5, 2020', 'R', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'Portland, Oregon', 'stats_list': [], 'contributions': '291 contributions\n        in the last year', 'description': [""spambase-classifier\nSVM, Naive Gaussian Bayes, and Logistic Regression Classifiers for the spambase dataset \nThe svm_classifier.py uses Scikit's builtin SVC to do the heavy lifting of the classification. My work constists of preprocessing the spambase.csv data. \nSVM achieves an accuracy of 93% when all the features were used.\nBy selecting the most important features by weight, the classifier achieves an accuracy of 80% by only using 2 out of 57 features\nThe Naive Bayes Gaussian classifier achieves an accuracy of 81% \nScikit's Logistic Regression was used and achieved and accuracy of 93%\nHow to Run\nTo use SVM:\npython svm_classifier.py\nTo use Naive Bayes Gaussian and Logistic classifier:\npython nbg_lr_classifier.py\nSVM ROC Curve\n\nSVM Accuracy Based on the Number of Features\n\n""], 'url_profile': 'https://github.com/dmunozc', 'info_list': ['R', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'MIT license', 'Updated Dec 5, 2020', 'R', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'Indianapolis', 'stats_list': [], 'contributions': '382 contributions\n        in the last year', 'description': ['Robust Mixture Regression\n\n\n\n\n\n\n\nInstall from CRAN\ninstall.packages(""RobMixReg)\nlibrary(""RobMixReg"")\n\nInstall from github for most updated package.\nPlease report the bug as the description in the Question&Problem.\nlibrary(""devtools"")\ndevtools::install_github(""changwn/RobMixReg"")\n\nTutorial\nA comprehensive and complete tutorial is here.\nNews\nThe package version control is in News.md\nCitations\nIf you find the code helpful in your resarch or work, please cite us.\n@article{wennan2020cat,\n  title={A New Algorithm using Component-wise Adaptive Trimming For Robust Mixture Regression},\n  author={Chang, Wennan and Wan, Changlin and Zhou, Xinyu and Zhang, Chi and Cao, Sha},\n  journal={arXiv preprint arXiv:2005.11599},\n  year={2020}\n}\n\n@article{chang2020supervised,\n  title={Supervised clustering of high dimensional data using regularized mixture modeling},\n  author={Chang, Wennan and Wan, Changlin and Zang, Yong and Zhang, Chi and Cao, Sha},\n  journal={arXiv preprint arXiv:2007.09720},\n  year={2020}\n}\nQuestions & Problems\nIf you have any questions or problems, please feel free to open a new issue here. We will fix the new issue ASAP.  You can also email the maintainers and authors below.\n\nWennan Chang\n(wnchang@iu.edu)\n\nPhD candidate at Biomedical Data Research Lab (BDRL) , Indiana University School of Medicine\n'], 'url_profile': 'https://github.com/changwn', 'info_list': ['R', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'MIT license', 'Updated Dec 5, 2020', 'R', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/slick9', 'info_list': ['R', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'MIT license', 'Updated Dec 5, 2020', 'R', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'Atlanta', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['TwitterAnalysis-PySpark\nBuilt a Logistic Regression Model pipeline to make predictions on twitter streaming data using PySpark\n'], 'url_profile': 'https://github.com/sravanroy', 'info_list': ['R', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'MIT license', 'Updated Dec 5, 2020', 'R', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'Australia', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['advanced_Regression_Ridge_and_Lasso\nAdvanced Regression - Finding the highly optimum features impacting dependent variable, selling price\nWork Background:\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia.\nThe company is looking at prospective properties to buy to enter the market. Required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\nThe company wants to know:\n\nWhich variables are significant in predicting the price of a house, and\nHow well those variables describe the price of a house.\n\nAlso, determine the optimal value of lambda for ridge and lasso regression.\nBusiness Goal:\nBeing analyst, required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/ashiashok0406', 'info_list': ['R', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'MIT license', 'Updated Dec 5, 2020', 'R', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'R', 'Updated Feb 13, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Demos\n'], 'url_profile': 'https://github.com/Shawnfeng92', 'info_list': ['R', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'MIT license', 'Updated Dec 5, 2020', 'R', 'Updated Jan 20, 2021', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'R', 'Updated Feb 13, 2020']}"
"{'location': 'India, Earth', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['ML-Life-Epectancy-Prediction\nLife-Expectancy-Prediction Using Machine Learning multiple linear regression model to predict the life expectancy\nAccuracy\naccuracy(R^2): 83.03143918684309%\nmae: 2.9234067436739193\nrmse: 3.8169164332218446\n'], 'url_profile': 'https://github.com/SkyThonk', 'info_list': ['1', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Oct 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'SAS', 'MIT license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Jan 5, 2020', '1', 'HTML', 'Updated Jun 24, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['fico-estimator\nFico-estimator is a Python =library for getting an aproximate fico score using a prescribed interest rate.\nIt is trained using 8,000 loans from Lending Clubs Marketplace. The library also includes a scraper for\ncollecting new loan data for further training and experimenting by adding other variables to the regression.\nInstallation\nUse the package manager [pip] Only compatable with python 2.7\npip install fico-estimator\nDependencies\nimport sklearn\nimport BeautifulSoup\nimport numpy\nimport pandas\n\nUsage\nimport fico-estimator as fe\n\n#get a fico estimate\nfe.predict_fico(rate)\n\n#train using the new data. Returns regression intercept and coeficient (y = mx + b)\nfe.train(data)\n\n#get lending club loan data from marketplace\nfe.scrape(html)\nContributing\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\nPlease make sure to update tests as appropriate.\nLicense\nMIT\n'], 'url_profile': 'https://github.com/Tamupiwa', 'info_list': ['1', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Oct 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'SAS', 'MIT license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Jan 5, 2020', '1', 'HTML', 'Updated Jun 24, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'Vancouver, BC Canada', 'stats_list': [], 'contributions': '414 contributions\n        in the last year', 'description': ['Iris-Species-Machine-Learning\nTo classify iris plants into three species in a given dataset\nDatasets & References\n\nUCI Machine Learning Repository\nKaggles - Iris Species\n\nAttribute Information\n\nsepal length in cm\nsepal width in cm\npetal length in cm\npetal width in cm\nclass:\n\nIris Setosa\nIris Versicolour\nIris Virginica\n\n\n\nHow to Run This Program\n\nTo install Python3 virtual environment:\n\nMakde a directory and run the following lines inside the directory:\n\npython3 -m pip install --user -U pip\npython3 -m pip install --user -U virtualenv\nvirtualenv env\n\n\nTo invoke the environment, type:\n\nsource env/bin/activate\n\nto deactivate, run:\n\ndeactivate\n\n\n\n\n\n\nTo install required modules and their dependencies, type:\n\npython3 -m pip install -U jupyter matplotlib numpy pandas scipy scikit-learn seaborn\nTo check your installation, type:\n\npython3 - c ""import jupyter, matplotlib, numpy, pnadas, scipy, sklearn, seaborn""\n\nThere should be no output and no error\n\n\n\n\n\n\n\n\nTo fire up Jupyter, type:\n\njupyter notebook\nIf facing notebook error such as ""... no web browser found: could not  locate runnable browser."", type:\n\njupyter notebook --no-browser\n\n\n\n\n\nConclusion\n\nUsing Petals over Sepal for training the data gives a much better accuracy which is reflected on the heatmap of correlation of features\n\n'], 'url_profile': 'https://github.com/danlee0528', 'info_list': ['1', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Oct 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'SAS', 'MIT license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Jan 5, 2020', '1', 'HTML', 'Updated Jun 24, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': '5349 N Fort Yuma Trail Tucson AZ 85750', 'stats_list': [], 'contributions': '1,451 contributions\n        in the last year', 'description': ['utl-using-linear-regression-with-base-sas-and-r-to-interpolate-missimg-values\nUsing linear regression with base sas and r to interpolate missimg values\nUsing linear regression with base sas and r to interpolate missimg values\ngithub\nhttps://tinyurl.com/wkwbsmt\nhttps://github.com/rogerjdeangelis/utl-using-linear-regression-with-base-sas-and-r-to-interpolate-missimg-values\n\nInterpolate missing values using linear regression\n\n    Three Solutions\n          a. using single data step to calculate sums of squares\n          b. using proc corr sums of squares\n          c. R\n\nStackOverflow\nhttps://tinyurl.com/vlbds6w\nhttps://stackoverflow.com/questions/57242496/r-linear-extrapolate-missing-values\n\nR linear extrapolate missing values\n\n *_                   _\n(_)_ __  _ __  _   _| |_\n| | \'_ \\| \'_ \\| | | | __|\n| | | | | |_) | |_| | |_\n|_|_| |_| .__/ \\__,_|\\__|\n        |_|\n;\n\ndata have;\n   input grp$ x y;\ncards4;\nA 2001 1.5\nA 2002 2.6\nA 2003 2.8\nA 2004 2.9\nA 2005 .\nB 2001 0.1\nB 2002 0.6\nB 2003 0.7\nB 2004 1.4\nB 2005 .\nC 2001 4.7\nC 2002 4.6\nC 2003 4.8\nC 2004 5.0\nC 2005 .\n;;;;\nrun;quit;\n\nWORK.HAVE total obs=15\n\n                      |  RULES\n                      |\n GRP      X      Y    |   Y\n                      |\n  A     2001    1.5   |\n  A     2002    2.6   |\n  A     2003    2.8   |            SLOPE         INTERCEPT\n  A     2004    2.9   |\n  A     2005     .    |  2.890 =   0.220 * 2005 + -438.21\n  B     2001    0.1   |\n  B     2002    0.6   |\n  B     2003    0.7   |\n  B     2004    1.4   |\n  B     2005     .    |\n  C     2001    4.7   |\n  C     2002    4.6   |\n  C     2003    4.8   |\n  C     2004    5.0   |\n  C     2005     .    |\n\n*            _               _\n  ___  _   _| |_ _ __  _   _| |_\n / _ \\| | | | __| \'_ \\| | | | __|\n| (_) | |_| | |_| |_) | |_| | |_\n \\___/ \\__,_|\\__| .__/ \\__,_|\\__|\n                |_|\n;\n\n\nGRP      X       Y     SLOPE    INTERCEPT\n\n A     2001    1.50      .           .\n A     2002    2.60      .           .\n A     2003    2.80      .           .\n A     2004    2.90      .           .\n\n A     2005    3.55*    0.44     -878.65\n\n B     2001    0.10      .           .\n B     2002    0.60      .           .\n B     2003    0.70      .           .\n B     2004    1.40      .           .\n\n B     2005*   1.70     0.40     -800.30\n\n C     2001    4.70      .           .\n C     2002    4.60      .           .\n C     2003    4.80      .           .\n C     2004    5.00      .           .\n\n C     2005*   5.05     0.11     -215.50\n\n* Interpolated;\n\n*              _       _            _\n  __ _      __| | __ _| |_ __ _ ___| |_ ___ _ __\n / _` |    / _` |/ _` | __/ _` / __| __/ _ \\ \'_ \\\n| (_| |_  | (_| | (_| | || (_| \\__ \\ ||  __/ |_) |\n \\__,_(_)  \\__,_|\\__,_|\\__\\__,_|___/\\__\\___| .__/\n                                           |_|\n;\n\n\ndata want;\n   retain sumx sumy sumxx sumxy sumyy n;\n   set have;\n   by grp notsorted;\n   if first.grp then do;\n     sumx  = 0;\n     sumy  = 0;\n     sumxx = 0;\n     sumxy = 0;\n     sumyy = 0;\n     n     = 0;\n   end;\n   if Y ne . then do;\n      sumx  = sum(sumx, x);\n      sumy  = sum(sumy, y);\n      sumxx = sum(sumxx, x ** 2);\n      sumxy = sum(sumxy, x * y);\n      sumyy = sum(sumyy, y * y);\n      n     = sum(n, 1);\n   end;\n   if last.grp then do;\n      nmr = sumxy  - sumx * sumy /n;\n      den = sqrt ( sumxx -(sumx)**2/n ) * sqrt ( sumyy -(sumy)**2/n );\n      slope = (sumxy - sumx * sumy / n)/(sumxx - sumx ** 2 / n);\n      intercept = sumy / n - slope * sumx / n;\n      corr=nmr/den;\n      put slope = ;\n      put intercept = ;\n      put corr=;\n      y=slope*x + intercept;\n   end;\n   keep grp x y slope intercept;\nrun;\n\n*_\n| |__     _ __  _ __ ___   ___    ___ ___  _ __ _ __\n| \'_ \\   | \'_ \\| \'__/ _ \\ / __|  / __/ _ \\| \'__| \'__|\n| |_) |  | |_) | | | (_) | (__  | (_| (_) | |  | |\n|_.__(_) | .__/|_|  \\___/ \\___|  \\___\\___/|_|  |_|\n         |_|\n;\n\nproc datasets lib=work mt=data mt=view;\n  delete avg ssq havSsq want;\nrun;quit;\n\nods output simplestats=avg;\nods output csscp=ssq;\nproc corr data=have(where=(Y ne .)) sscp csscp;\nby grp;\nvar x y;\nrun;\n\ndata havSsq/view=havSsq;\nmerge\n   avg(where=(variable=""X"") keep=grp variable mean      rename=mean=x_avg)\n   avg(where=(variable=""Y"") keep=grp variable mean      rename=(mean=y_avg))\n   ssq(where=(variable=""X"") keep=grp variable x y nobs  rename=(x=x_ssq y=xy_ssq))\n   ssq(where=(variable=""X"") keep=grp variable y         rename=(y=y_ssq));\n   Slope=xy_ssq/x_ssq;\n   Intercept=(y_avg-Slope*x_avg);\n   keep grp slope intercept;\nrun;quit;\n\ndata want;\n merge have havSsq;\n by grp;\n if y=. then y=intercept + slope*x;\nrun;\n\n*         ____\n  ___    |  _ \\\n / __|   | |_) |\n| (__ _  |  _ <\n \\___(_) |_| \\_\\\n\n;\noptions validvarname=upcase;\nlibname sd1 ""d:/sd1"";\ndata sd1.have;\n   input grp$ x y;\ncards4;\nA 2001 1.5\nA 2002 2.6\nA 2003 2.8\nA 2004 2.9\nA 2005 .\nB 2001 0.1\nB 2002 0.6\nB 2003 0.7\nB 2004 1.4\nB 2005 .\nC 2001 4.7\nC 2002 4.6\nC 2003 4.8\nC 2004 5.0\nC 2005 .\n;;;;\nrun;quit;\n\n\n%utlfkil(d:/xpt/want.xpt);\n\nproc datasets lib=work;\n  delete want fixR;\nrun;quit;\n\n%utl_submit_r64(\'\nlibrary(dplyr);\nlibrary(broom);\nlibrary(data.table);\nlibrary(SASxport);\nlibrary(haven);\nhave<-read_sas(""d:/sd1/have.sas7bdat"");\nfitted_models = have %>% group_by(GRP) %>%\ndo(model = lm(Y ~ X, data = .));\nwant<-as.data.frame(ungroup(fitted_models %>% tidy(model)));\nstr(want);\nwrite.xport(want,file=""d:/xpt/want.xpt"");\n\');\n\nlibname xpt xport ""d:/xpt/want.xpt"";\ndata want;\n  set xpt.want;\nrun;quit;\nlibname xpt clear;\n\n\nWORK.WANT total obs=6\n\n  GRP    TERM           ESTIMATE    STD_ERRO    STATISTI    P_VALUE\n\n   A     (Intercept)     -878.65     336.277    -2.61288    0.12055\n   A     X                  0.44       0.168     2.62016    0.12000\n   B     (Intercept)     -800.30     155.113    -5.15946    0.03557\n   B     X                  0.40       0.077     5.16398    0.03551\n   C     (Intercept)     -215.50     104.053    -2.07106    0.17417\n   C     X                  0.11       0.052     2.11695    0.16848\n\ndata fixR;\n    merge\n      want(where=(TERM=""(Intercept)"") rename=estimate=intercept)\n      want(where=(TERM=""X""          ) rename=estimate=slope)\n      have;\n  by grp;\n  if y=. then y=intercept + slope*x;\n  keep grp x y slope intercept;\nrun;quit;\n\n'], 'url_profile': 'https://github.com/rogerjdeangelis', 'info_list': ['1', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Oct 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'SAS', 'MIT license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Jan 5, 2020', '1', 'HTML', 'Updated Jun 24, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': [""ML_RF_LogicReg_pyspark\nMachineLearning (Random Forest, XGboost, logistic regression, cross validation) wit Pyspark of Apache Spark.\nSummary\n\nThis project is doing explorative data analysis using the pyspark and sql application programming interfaces (API's) of Apache spark.\n\n\nApache Spark is an open-source engine developed specifically for handling large-scale data processing and analytics. Spark offers the ability to access data in a variety of sources. Apache Spark is designed to accelerate analytics on Hadoop with speed and efficiency.\n\nwebopedia\n\nThe airtraffic system data records consist of the tables flights, planes,\nand airports. The data sources used  are csv-files stored locally. \nThe functions and methods applied here include SQL-queries and\nSQL-calculations and pyspark implementations like select, filter,\ncollect, join, and aggregate. \n\n\nThose can of course applied to Big Data on remote machines. This is the whole point of the Apache Spark system architecture. It allows even analysing streaming data in real time. The Spark master distributes then the data as Resilient Distributed Datasets (RDD) or immutable distributed collections of objects on the remote machines (workers) using a process of mapping, sort and shuffle, and reducing. RDD's are not generated in this project, but are in others.\n\n\n\nThe main focus of this project is prepare the data for machine learning for example by:\n\n\njoining\ncreating labels\ncleaning missing values\ndoing a train-test-split\n\n    \nand then apply Pyspark ML library alogrithms like:\n\n\nRandom Forest\nRandom Forest in cross-validation\nLogistic Regression in cross-validation\nGradient-Boosted Tree Classifier\n\n""], 'url_profile': 'https://github.com/RolfChung', 'info_list': ['1', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Oct 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'SAS', 'MIT license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Jan 5, 2020', '1', 'HTML', 'Updated Jun 24, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': [""Building A Classification Model for Predicting Suspicious firms Using A Logistic Regression Model\nIndia is the ‘Most Corrupt Country in Asia-Pacific Region’, according to a recent study by Transparency International, a Berlin-based NGO working against corruption.\nStudy also shown that the prop for this corruption was mostly facilitated by some so-called firms.\nHence, the Measures to tackle corruption and curb the growing menace of suspicious firms has dominated the national discourse over the past years.\nThe Indian government endorsed for the Auditor Office to audit some relevant firms in different sectors.\nThe goal was to help the auditors by building a classification model that can predict the fraudulent firm on the basis the present and historical risk factors.\nThe dataset I'll be working is a Exhaustive one year non-confidential data in the year 2015 to 2016 of firms collected from the Auditor Office of India to be used in building a predictor for classifying suspicious firms.\nIn this project, I'll:\n\n\nPrepare the data for machine learning\n\n\nTrain a model that can predict the fraudulent firm Using a Logistic Regression model\n\n\nMeasure the accuracy of the model\n\n\n""], 'url_profile': 'https://github.com/zuruoke', 'info_list': ['1', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Oct 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'SAS', 'MIT license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Jan 5, 2020', '1', 'HTML', 'Updated Jun 24, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'Massachusetts, USA', 'stats_list': [], 'contributions': '504 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/billdenney', 'info_list': ['1', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Oct 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'SAS', 'MIT license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Jan 5, 2020', '1', 'HTML', 'Updated Jun 24, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'Vitry-sur-Seine, France', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Baivab13', 'info_list': ['1', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Oct 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'SAS', 'MIT license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Jan 5, 2020', '1', 'HTML', 'Updated Jun 24, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': [""Altitude-Prediction-using-Regression-Models\nAltitude of a place is predicted from its latitude and longitude using regression models with regularization.\nA dataset of more than 3 Lakh places were taken from which 70% of that was used to train the model and the rest 30% was used for testing.\nRegression Analysis is used by taking and training models with polynomials of various degrees with regularization to avoid overfitting of the training data.\nThe model is developed in Python language using numpy and pandas packages to simplify data visualization and training.\nPolynomial models of upto 6 degrees were trained and tested and the results were analyzed to come up with the best possible model.\nThe Dataset contains 4 columns , out of which the 1st columns 'id' is removed and the rest 3 columns were used for training.\nThe next 3 columns were latitude , longitude and altitude respectively.\n""], 'url_profile': 'https://github.com/vedant781999', 'info_list': ['1', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Oct 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'SAS', 'MIT license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Jan 5, 2020', '1', 'HTML', 'Updated Jun 24, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}","{'location': 'Uberlândia - Minas Gerais - Brasil', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['Classification_models\nThe goal of this project is to predict who will have a heart disease. Logistic Regression, Decision Tree and XGBoost models were used.\nLanguage\nThe language is python. Librarys: Sklearn, XGBoost, pandas and numpy\n'], 'url_profile': 'https://github.com/joaoflauzino', 'info_list': ['1', 'Python', 'Updated Jan 1, 2020', 'Python', 'Updated Oct 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'SAS', 'MIT license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'R', 'Updated Jan 5, 2020', '1', 'HTML', 'Updated Jun 24, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Python-ML\nMachine learning libraries which include logistic regression and SVM\n'], 'url_profile': 'https://github.com/OhChristopher', 'info_list': ['Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Java', 'Updated Feb 10, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Updated Jan 1, 2020']}","{'location': 'Hong Kong', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Robust Regression with Huber loss\nThis is a toy example of robust regreesion with huber loss, which is suitable\nwhen training samples have outliers. The ground truth is a simple line. The results\nbelow show that huber loss could get rid of the outliers.\n\n'], 'url_profile': 'https://github.com/wangxin0716', 'info_list': ['Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Java', 'Updated Feb 10, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Updated Jan 1, 2020']}","{'location': 'Dubai, UAE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Deep-Learning-Method-comparison-with-Logistic-Regression-\nComparison of indian_diabetes dataset using Keras Deep Learning Method with Logistic Regression\nI would like to thank Jason Brownlee as he was the inspiration for me where he is using indian_diabetes dataset and explaining to determine the accuracy using Keras with the help of Deep Learning.\nIf you want to just check the article of how he did it.The link is being given below:\nhttps://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\nI just wanted to know the accuracy of indian_diabetes dataset using Logistic Regression and how much change is happening in the accuracy so I am posting my github code along with his code.\n'], 'url_profile': 'https://github.com/nairshobhit94', 'info_list': ['Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Java', 'Updated Feb 10, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['house_price_predictor\nHouse Price Predictor using normalized equation of Linear Regression (Machine Learning)\n'], 'url_profile': 'https://github.com/arnavbanerji', 'info_list': ['Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Java', 'Updated Feb 10, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sourabhjain19', 'info_list': ['Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Java', 'Updated Feb 10, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Updated Jan 1, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Movie  domestic total gross prediction\nData collection and EDA:\n\n\nMovie data Title, Rating, Genre, Budget, Runtime, Distributor and Domestic Total Gross are selected as features.\n\n\nData is collected by web scraping using Beautiful soup from https://www.boxofficemojo.com/ and Budget is from https://www.the-numbers.com/.\n\n\nData is cleaned and performed EDA:\n. Top 10 distributors and Distributor vs DTG\n. Top 10 genre and Genre vs DTG\n. Effect of seasonal release of movie on DTG\n. Effect of Rating on DTG\n\n\nLinear Regression Analysis\n\nLinear regression analysis is performed using SKlearn\nDiognostic plots like  residual vs predict and probability plot suggested that there is no direct linear relationship between target and features.\nIntroduced polynomials using sklearn PolynomialFeatures followed by Lasso regularization.\nFound that Movie Budget has the direct effect on DTG compared to other features.\n\nWhat can be done to improve model\nAlthough introducing polynomials could improve the model, it needs to be further improved by incuding other features such as cast and crew, Director, writer, studios and movie rating etc.\n'], 'url_profile': 'https://github.com/swarnathota', 'info_list': ['Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Java', 'Updated Feb 10, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/leoquennesson', 'info_list': ['Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Java', 'Updated Feb 10, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Updated Jan 1, 2020']}","{'location': 'Tiaret ', 'stats_list': [], 'contributions': '123 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/OUARED-A', 'info_list': ['Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Java', 'Updated Feb 10, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Updated Jan 1, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': [""House-Prices-Advanced-Regression-Techniques\nPredict sales prices and practice feature engineering, RFs, and gradient boosting\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nPractice Skills\nCreative feature engineering \nAdvanced regression techniques like random forest and gradient boosting\n\nAcknowledgments\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.\n""], 'url_profile': 'https://github.com/yeonathan', 'info_list': ['Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Java', 'Updated Feb 10, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Updated Jan 1, 2020']}","{'location': 'Chicago. IL', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': [""Multiple-regression-techniques-in-R\nThis notebook demonstrates multiple regression techniques in R using the 'House Sales in King County, Washington' dataset.\n""], 'url_profile': 'https://github.com/jessefranks', 'info_list': ['Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Java', 'Updated Feb 10, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 4, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Updated Jan 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/UtsavVasudevan', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Chennai-House-Price-Prediction-Regression-\nBased on the train dataset, you will need to develop a model that accurately predicts the real estate price in Chennai.\n'], 'url_profile': 'https://github.com/anupojusatish', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020']}","{'location': 'Addis Ababa, Ethiopia', 'stats_list': [], 'contributions': '268 contributions\n        in the last year', 'description': ['Simple Linear Regression Using Sklearn\nSimple linear regression\nInstallation\nImporting Libraries\n\nnumpy\npandas\nmatplotlib\nSklearn\n\nPorject Descriptions\nSimple linear regression without framework\nWhat is Simple Linear Regression?\nSimple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables:\n\nOne variable, denoted x, is regarded as the predictor, explanatory, or independent variable.\nThe other variable, denoted y, is regarded as the response, outcome, or dependent variable. \nBecause the other terms are used less frequently today, we\'ll use the ""predictor"" and ""response"" terms to refer to the variables encountered in this course. The other terms are mentioned only to make you aware of them should you encounter them. Simple linear regression gets its adjective ""simple,"" because it concerns the study of only one predictor variable. In contrast, multiple linear regression, which we study later in this course, gets its adjective ""multiple,"" because it concerns the study of two or more predictor variables.\n\nLicensing, Authors, Acknowledgements\n'], 'url_profile': 'https://github.com/Dawit-1621', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020']}","{'location': 'Roorkee', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['House_Prices-Advanced_Regression_Techniques\nThe above folder consists of kaggle dataset to House Prices: Advanced Regression Techniques competition and the python code for the same.\nThe submission file in this folder consists of the price data predicted on the test data, scored a RMSE of 0.14653.\n'], 'url_profile': 'https://github.com/RSIITR', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nayeemuddin-mohd', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '175 contributions\n        in the last year', 'description': ['LinnearRegression-CarPrice-prediction\npreparing a prediction model for predicting Price\n'], 'url_profile': 'https://github.com/Anand-s-cmd', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SangeethaSA', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression-on-Boston-Housing\nThere are 506 samples and 13 feature variables in this dataset.\nThe objective is to predict the value of prices of the house using the given features.\n'], 'url_profile': 'https://github.com/xusonghui0383', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Logistic-Regression-Heart-Disease-classifier\nDataset: https://www.kaggle.com/dileep070/heart-disease-prediction-using-logistic-regression\nUsed two classifier methods:\n\nLogistic Regression produced 84.43% accuracy\nDecision Tree produced 76.23% accuracy\n\n'], 'url_profile': 'https://github.com/rumya0', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Namrata23github', 'info_list': ['Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['IMDB-Movie-Scores-Predictions\nMovie ratings prediction using regressions and trees\n'], 'url_profile': 'https://github.com/zachamar', 'info_list': ['R', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 5, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Toronto, ON', 'stats_list': [], 'contributions': '175 contributions\n        in the last year', 'description': ['Simple 2D Linear Regression:\n\nMake sure gnuplot is installed on your system.\n\'sudo apt-get update\'\n\'sudo apt-get install gnuplot\'\nC++ Interface for gunuplot from:\nhttps://code.google.com/archive/p/gnuplot-cpp/\n""gnuplot_i.hpp"" contains all gnuplot related function calls\nbuild\n\'make\'\nrun\n\'./plot\'\n'], 'url_profile': 'https://github.com/anantojha', 'info_list': ['R', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 5, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Shanghai,China', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['lineral_regression_for_one_dimension_for_demonstration\nlineral_regression_for_one_dimension_for_demonstration\njust for fun.\na very simple exercise on lineral regression\n'], 'url_profile': 'https://github.com/hucaigang', 'info_list': ['R', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 5, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 5, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['EDA-Logistics-Regression-on-Titanic-Data-Set-\nTried to do some of the EDA and apply some regressions on Titanic data set, and found the results on Gender bases\n'], 'url_profile': 'https://github.com/chetanniloor', 'info_list': ['R', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 5, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 5, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kpradyumna095', 'info_list': ['R', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 5, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kpradyumna095', 'info_list': ['R', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 5, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Calgary, Alberta', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/btilds', 'info_list': ['R', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 5, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/LearningPlus', 'info_list': ['R', 'Updated Jan 2, 2020', 'C++', 'Updated Jan 5, 2020', 'Python', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 1, 2020']}"
"{'location': 'Hyderabad', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kpradyumna095', 'info_list': ['1', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Kolkata India', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['It is an ongoing project where programs will be opened and closed on the basis of our eye contraction and expansion.\n'], 'url_profile': 'https://github.com/akshayvit', 'info_list': ['1', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Linear Regression - Alumni Donation Case Study\nProject Link: http://rpubs.com/surabhik16/alumni_donation\nAlumni donations are an important source of revenue for colleges and universities. If administrators could determine the factors that influence increases in the percentage of alumni who donate, they might be able to implement policies that could lead to increased revenues.\nA study shows that students who have more access to the faculty are more likely to be satisfied. As a result, one might suspect that smaller class sizes and lower student-faculty ratios might lead to a higher percentage of satisfied graduates, which in turn might lead to an increase in the percentage of alumni who donate.\nIn this project we will answer the following questions:\nIs there a relationship between the factors and the alumni giving rate?\nHow strong is the relationship between the factors and alumni giving rate?\nHow accurately can we estimate the effect of each factor on the alumni giving rate?\n'], 'url_profile': 'https://github.com/surabhik16', 'info_list': ['1', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Python', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Prediction-of-Labours-earnings-Multiple-linear-regression-\nData Contains Age, Race, Educational detail and Labour earning for 1974, 1975. We need to predict the labours earning for 1978\n'], 'url_profile': 'https://github.com/UmerBhikan', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Updated Dec 31, 2019', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Titanic-Survival-Model-Using-Logistics-Regression\nLogistics Regression Class Exercise\nKexin Wang\nOct 02, 2019\n\nWe will do small exercise in Logistic Regression today. This will also\nbecome your submission for the day. We will do this work today in groups of\ntwo. This will later also become your submission for the next homework. For\nthe class submission, only one submission per team is needed - mention in\ncomments (and in author field above) who all worked on it together.\n\nTitanic Survival Model\nWe want to find out what factors affected passengers’ survival when Titanic\nsank. As survival is a binary measure, this task is well suited for a Logistic\nRegression analysis.\nTask A: Load and Explore the Data\nTraining Data (data for building our model) is saved in the file\nTitanicTrain.csv\nYou should import TitanicTrain.csv data. Check the structure of the data using\nstr() function.\n\n\nPClass: Passenger Class, Sibsp: Number of Siblings/Spouse aboard\n\n\nParch: Number of Parents/Children aboard\n\n\nAre there variables that are not in the right class? If yes then convert those variables to the correct class\n#Enter your code for loading and exploring the data here.\nTitanic<-read.csv(""/Users/JakeBecker3/Desktop/TitanicTrain.csv"")\nstr(Titanic)\n\'data.frame\':    891 obs. of  12 variables:\n$ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...\n$ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...\n$ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...\n$ Name       : Factor w/ 891 levels ""Abbing, Mr. Anthony"",..: 109 191 358 277 16 559 520 629 417 581 ...\n$ Sex        : Factor w/ 2 levels ""female"",""male"": 2 1 1 1 2 2 2 2 1 1 ...\n$ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...\n$ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...\n$ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...\n$ Ticket     : Factor w/ 681 levels ""110152"",""110413"",..: 524 597 670 50 473 276 86 396 345 133 ...\n$ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...\n$ Cabin      : Factor w/ 148 levels """",""A10"",""A14"",..: 1 83 1 57 1 1 131 1 1 1 ...\n$ Embarked   : Factor w/ 4 levels """",""C"",""Q"",""S"": 4 2 4 4 4 3 4 4 4 2 ...\nTitanic$PassengerId<-NULL\nTitanic$name<-NULL\nTitanic$Ticket<-NULL\nTitanic$Cabin<-NULL\nTitanic$Survived<-as.factor(Titanic$Survived)\nTitanic$Pclass<-as.factor(Titanic$Pclass)\nTitanic$Sex<-as.factor(Titanic$Sex)\nTitanic$Embarked<-as.factor(Titanic$Embarked)\navgage<-tapply(Titanic$Age, Titanic$Sex, mean, na.rm=T)\navgage\nfemale     male\n27.91571 30.72664\nTitanic$Age<-ifelse (is.na(Titanic$Age)&Titanic$Sex==""male"",\navgage[2], ifelse(is.na(Titanic$Age)&Titanic$Sex==""female"",\navgage[1], Titanic$Age))\nsummary(Titanic$Age)\nMin. 1st Qu.  Median    Mean 3rd Qu.    Max.\n0.42   22.00   30.00   29.74   35.00   80.00\nTask B: Build Our Model\n#Build a logistic regression model with Survival as the response variable. In this section, let\'s first build a main effects model. What variables do you need to include as the predictor variables?\n#Enter your code for your logistic regression model here.\nlogit1 <- glm(Survived ~ Sex + Age + Pclass + Parch + Fare + SibSp + Embarked, data=Titanic, family=""binomial"")\nsummary(logit1)\n\nCall:\nglm(formula = Survived ~ Sex + Age + Pclass + Parch + Fare +\nSibSp + Embarked, family = ""binomial"", data = Titanic)\n\nDeviance Residuals:\nMin       1Q   Median       3Q      Max\n-2.6276  -0.6107  -0.4192   0.6151   2.4521\n\nCoefficients:\nEstimate Std. Error z value Pr(>|z|)\n(Intercept)  16.448912 610.213699   0.027  0.97849\nSexmale      -2.691038   0.201133 -13.379  < 2e-16 ***\nAge          -0.039927   0.007910  -5.048 4.47e-07 ***\nPclass2      -0.931593   0.297933  -3.127  0.00177 **\nPclass3      -2.160520   0.298287  -7.243 4.39e-13 ***\nParch        -0.093693   0.119118  -0.787  0.43154\nFare          0.002236   0.002460   0.909  0.36353\nSibSp        -0.326134   0.109547  -2.977  0.00291 **\nEmbarkedC   -12.331526 610.213584  -0.020  0.98388\nEmbarkedQ   -12.376249 610.213636  -0.020  0.98382\nEmbarkedS   -12.774300 610.213571  -0.021  0.98330\n---\nSignif. codes:  0 \'\' 0.001 \'\' 0.01 \'\' 0.05 \'.\' 0.1 \' \' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\nNull deviance: 1186.66  on 890  degrees of freedom\nResidual deviance:  783.08  on 880  degrees of freedom\nAIC: 805.08\n\nNumber of Fisher Scoring iterations: 13\nlogit2 <- glm(Survived ~ Sex + Age + Pclass + Parch + SibSp + Fare, data=Titanic, family=""binomial"")\nsummary(logit2)\n\nCall:\nglm(formula = Survived ~ Sex + Age + Pclass + Parch + SibSp +\nFare, family = ""binomial"", data = Titanic)\n\nDeviance Residuals:\nMin       1Q   Median       3Q      Max\n-2.7125  -0.6050  -0.4263   0.6159   2.4385\n\nCoefficients:\nEstimate Std. Error z value Pr(>|z|)\n(Intercept)  3.862013   0.446357   8.652  < 2e-16 ***\nSexmale     -2.741177   0.198900 -13.782  < 2e-16 ***\nAge         -0.040234   0.007873  -5.110 3.21e-07 ***\nPclass2     -1.031817   0.293924  -3.510 0.000447 ***\nPclass3     -2.166318   0.290618  -7.454 9.04e-14 ***\nParch       -0.111150   0.117721  -0.944 0.345074\nSibSp       -0.353821   0.109573  -3.229 0.001242 **\nFare         0.002955   0.002444   1.209 0.226724\n---\nSignif. codes:  0 \'\' 0.001 \'\' 0.01 \'\' 0.05 \'.\' 0.1 \' \' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\nNull deviance: 1186.66  on 890  degrees of freedom\nResidual deviance:  787.86  on 883  degrees of freedom\nAIC: 803.86\n\nNumber of Fisher Scoring iterations: 5\n\n\nBuilging a model is always only the first step. The more important task is\ninterpreting the model - what insights does your model give you?\n#$Enter your text interpretation of your model here.\n#Being a male in the third class has the lowest survival rate.\n\nTask C: Improve Our Model\n\nWe will likely not be able to do this during the class exercise. This task\nis for you to do as your homework.\n\nHow can we improve our model. There are several things you can try:\n\nDo we need any interaction effects?\nDo any of the variables have non-linear effects - should we include them in the model as a square term?\nCan we clean the data better? Can we infer the missing values rather than losing all that information?\n\nPay specific attention to how will you compare whether any particular model is\nbetter than other models. Potential choices are looking at the AIC value\nand ANOVA test for nested models.\n#Enter your code for improving your model here.\nlogit3 <- glm(Survived ~ Sex + Pclass + Age + Parch + SibSp + Age*Pclass, data=Titanic, family=""binomial"")\nsummary(logit3)\n\n\n## \n## Call:\n## glm(formula = Survived ~ Sex + Pclass + Age + Parch + SibSp + \n##     Age * Pclass, family = ""binomial"", data = Titanic)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.4713  -0.5882  -0.4110   0.5857   2.4678  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  3.57411    0.53314   6.704 2.03e-11 ***\n## Sexmale     -2.78007    0.20136 -13.806  < 2e-16 ***\n## Pclass2     -0.12409    0.68022  -0.182  0.85524    \n## Pclass3     -1.78323    0.61612  -2.894  0.00380 ** \n## Age         -0.02647    0.01228  -2.156  0.03108 *  \n## Parch       -0.08320    0.11558  -0.720  0.47157    \n## SibSp       -0.34939    0.11257  -3.104  0.00191 ** \n## Pclass2:Age -0.03266    0.01930  -1.692  0.09058 .  \n## Pclass3:Age -0.01588    0.01812  -0.877  0.38062    \n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1186.66  on 890  degrees of freedom\n## Residual deviance:  786.57  on 882  degrees of freedom\n## AIC: 804.57\n## \n## Number of Fisher Scoring iterations: 5\n\n\nlogit4 <- glm(Survived ~ Sex + Pclass + Age + Parch + SibSp + Sex*Pclass, data=Titanic, family=""binomial"")\nsummary(logit4)\n\n\n## \n## Call:\n## glm(formula = Survived ~ Sex + Pclass + Age + Parch + SibSp + \n##     Sex * Pclass, family = ""binomial"", data = Titanic)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.1281  -0.6343  -0.4687   0.3781   2.5218  \n## \n## Coefficients:\n##                  Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)      5.392928   0.703077   7.670 1.71e-14 ***\n## Sexmale         -4.038748   0.626021  -6.451 1.11e-10 ***\n## Pclass2         -1.267825   0.735019  -1.725  0.08455 .  \n## Pclass3         -3.981520   0.634764  -6.272 3.55e-10 ***\n## Age             -0.046569   0.008529  -5.460 4.77e-08 ***\n## Parch           -0.036384   0.121030  -0.301  0.76371    \n## SibSp           -0.342002   0.110905  -3.084  0.00204 ** \n## Sexmale:Pclass2 -0.336316   0.805675  -0.417  0.67636    \n## Sexmale:Pclass3  2.150641   0.669401   3.213  0.00131 ** \n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1186.66  on 890  degrees of freedom\n## Residual deviance:  758.77  on 882  degrees of freedom\n## AIC: 776.77\n## \n## Number of Fisher Scoring iterations: 6\n\n\nTitanic$AgeCat<-as.factor(ifelse(Titanic$Age<16,""child"",""adult""))\nlogit5 <- glm(Survived ~ Sex + Pclass + Age + Parch + SibSp + AgeCat, data=Titanic, family=""binomial"")\nsummary(logit5)  \n\n\n## \n## Call:\n## glm(formula = Survived ~ Sex + Pclass + Age + Parch + SibSp + \n##     AgeCat, family = ""binomial"", data = Titanic)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.8630  -0.5855  -0.4218   0.5785   2.5316  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  3.555561   0.430768   8.254  < 2e-16 ***\n## Sexmale     -2.794801   0.201509 -13.869  < 2e-16 ***\n## Pclass2     -1.154329   0.261244  -4.419 9.93e-06 ***\n## Pclass3     -2.287323   0.243096  -9.409  < 2e-16 ***\n## Age         -0.024944   0.009111  -2.738 0.006185 ** \n## Parch       -0.162016   0.120526  -1.344 0.178872    \n## SibSp       -0.435033   0.120278  -3.617 0.000298 ***\n## AgeCatchild  1.334911   0.427425   3.123 0.001789 ** \n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1186.66  on 890  degrees of freedom\n## Residual deviance:  779.24  on 883  degrees of freedom\n## AIC: 795.24\n## \n## Number of Fisher Scoring iterations: 5\n\n\nlogit6 <- glm(Survived ~ Sex + Pclass + Age + Parch + SibSp + Sex*SibSp, data=Titanic, family=""binomial"")\nsummary(logit6)\n\n\n## \n## Call:\n## glm(formula = Survived ~ Sex + Pclass + Age + Parch + SibSp + \n##     Sex * SibSp, family = ""binomial"", data = Titanic)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.6383  -0.6063  -0.4260   0.5983   2.4424  \n## \n## Coefficients:\n##                Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)    4.229222   0.414829  10.195  < 2e-16 ***\n## Sexmale       -2.979858   0.227366 -13.106  < 2e-16 ***\n## Pclass2       -1.212427   0.263046  -4.609 4.04e-06 ***\n## Pclass3       -2.362536   0.244829  -9.650  < 2e-16 ***\n## Age           -0.040390   0.007868  -5.133 2.84e-07 ***\n## Parch         -0.084396   0.114571  -0.737 0.461353    \n## SibSp         -0.530553   0.146230  -3.628 0.000285 ***\n## Sexmale:SibSp  0.440110   0.191048   2.304 0.021242 *  \n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1186.66  on 890  degrees of freedom\n## Residual deviance:  784.42  on 883  degrees of freedom\n## AIC: 800.42\n## \n## Number of Fisher Scoring iterations: 5\n\n\nlogit7 <- glm(Survived ~ Sex + Pclass + Age + Parch + SibSp + Sex*Parch, data=Titanic, family=""binomial"")\nsummary(logit7)\n\n\n## \n## Call:\n## glm(formula = Survived ~ Sex + Pclass + Age + Parch + SibSp + \n##     Sex * Parch, family = ""binomial"", data = Titanic)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.5062  -0.6051  -0.4187   0.5901   2.4956  \n## \n## Coefficients:\n##                Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)    4.123359   0.405483  10.169  < 2e-16 ***\n## Sexmale       -3.020015   0.220761 -13.680  < 2e-16 ***\n## Pclass2       -1.162646   0.264474  -4.396 1.10e-05 ***\n## Pclass3       -2.287530   0.245885  -9.303  < 2e-16 ***\n## Age           -0.037863   0.007836  -4.832 1.35e-06 ***\n## Parch         -0.295367   0.140689  -2.099 0.035779 *  \n## SibSp         -0.360501   0.108705  -3.316 0.000912 ***\n## Sexmale:Parch  0.699464   0.221275   3.161 0.001572 ** \n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1186.66  on 890  degrees of freedom\n## Residual deviance:  780.07  on 883  degrees of freedom\n## AIC: 796.07\n## \n## Number of Fisher Scoring iterations: 5\n\nWhat is your best model? Explain your best model - what are you seeing as\nsignificant? What is the interpretation? What does it mean?\nEnter your text interpretation here.\n#Logit4 is the best model becuase it has the lowest AIC. It tells us that being a male at class 3 has higher survival rate compared to the one in class 2.Logit5 shows that being a child has higher survival rate. Loigt6 shows that in siblings, brothers have higehr survival rate than sisters. According to logit7, a dad has higher survival rate than mom do when traveling with children on Titanic.\n\nTask D: Predict Outcomes in Testing Data\n\nWe will likely not be able to do this during the class exercise. This task\nis for you to do as your homework.\n\nWe have saved a small part of data to test our model. This is the Testing\nData. We will use this data to see how good of a prediction is made by the\nmodel we created in Task D above.\nYou should do the following:\n\n\nImport the testing data in TitanicTest.csv\n\n\nCheck that you have all the predictor variables. It will natually not have the response variable\n\n\nDo any variable need recoding to match the training dataset? If yes then do the necessary variable recoding\n\n\nPredict the Survival Probability for the Testing Dataset using the model developed on the training dataset in Step C above\n\n\nConvert probabilities to a a binary outcome (1 for Prob > 0.5, otherwise 0 - or choose your own threshold)\n#Enter your code for predicting outcomes in testing data here.\n#setwd(""C:/TO414 data"")\nTitanicTest<-read.csv(""/Users/JakeBecker3/Desktop/TitanicTest.csv"")\nsummary(TitanicTest)\nPassengerId         Pclass\nMin.   : 892.0   Min.   :1.000\n1st Qu.: 996.2   1st Qu.:1.000\nMedian :1100.5   Median :3.000\nMean   :1100.5   Mean   :2.266\n3rd Qu.:1204.8   3rd Qu.:3.000\nMax.   :1309.0   Max.   :3.000\n\nName         Sex\nAbbott, Master. Eugene Joseph            :  1   female:152\nAbelseth, Miss. Karen Marie              :  1   male  :266\nAbelseth, Mr. Olaus Jorgensen            :  1\nAbrahamsson, Mr. Abraham August Johannes :  1\nAbrahim, Mrs. Joseph (Sophie Halaut Easu):  1\nAks, Master. Philip Frank                :  1\n(Other)                                  :412\nAge            SibSp            Parch             Ticket\nMin.   : 0.17   Min.   :0.0000   Min.   :0.0000   PC 17608:  5\n1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.0000   113503  :  4\nMedian :27.00   Median :0.0000   Median :0.0000   CA. 2343:  4\nMean   :30.27   Mean   :0.4474   Mean   :0.3923   16966   :  3\n3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.0000   220845  :  3\nMax.   :76.00   Max.   :8.0000   Max.   :9.0000   347077  :  3\nNA\'s   :86                                        (Other) :396\nFare                     Cabin     Embarked\nMin.   :  0.000                  :327   C:102\n1st Qu.:  7.896   B57 B59 B63 B66:  3   Q: 46\nMedian : 14.454   A34            :  2   S:270\nMean   : 35.627   B45            :  2\n3rd Qu.: 31.500   C101           :  2\nMax.   :512.329   C116           :  2\nNA\'s   :1         (Other)        : 80\nTitanicTest$Pclass<-as.factor(TitanicTest$Pclass)\nTitanicTest$Sex<-as.factor(TitanicTest$Sex)\nTitanicTest$Embarked<-as.factor(TitanicTest$Embarked)\navgage<-tapply(TitanicTest$Age, TitanicTest$Sex, mean, na.rm=T)\navgage\nfemale     male\n30.27236 30.27273\nTitanicTest$Age<-ifelse (is.na(TitanicTest$Age)&TitanicTest$Sex==""male"",\navgage[2], ifelse(is.na(TitanicTest$Age)&TitanicTest$Sex==""female"",\navgage[1], TitanicTest$Age))\nsummary(TitanicTest$Age)\nMin. 1st Qu.  Median    Mean 3rd Qu.    Max.\n0.17   23.00   30.27   30.27   35.75   76.00\nsummary(TitanicTest)\nPassengerId     Pclass                                         Name\nMin.   : 892.0   1:107   Abbott, Master. Eugene Joseph            :  1\n1st Qu.: 996.2   2: 93   Abelseth, Miss. Karen Marie              :  1\nMedian :1100.5   3:218   Abelseth, Mr. Olaus Jorgensen            :  1\nMean   :1100.5           Abrahamsson, Mr. Abraham August Johannes :  1\n3rd Qu.:1204.8           Abrahim, Mrs. Joseph (Sophie Halaut Easu):  1\nMax.   :1309.0           Aks, Master. Philip Frank                :  1\n(Other)                                  :412\nSex           Age            SibSp            Parch\nfemale:152   Min.   : 0.17   Min.   :0.0000   Min.   :0.0000\nmale  :266   1st Qu.:23.00   1st Qu.:0.0000   1st Qu.:0.0000\nMedian :30.27   Median :0.0000   Median :0.0000\nMean   :30.27   Mean   :0.4474   Mean   :0.3923\n3rd Qu.:35.75   3rd Qu.:1.0000   3rd Qu.:0.0000\nMax.   :76.00   Max.   :8.0000   Max.   :9.0000\n\nTicket         Fare                     Cabin     Embarked\nPC 17608:  5   Min.   :  0.000                  :327   C:102\n113503  :  4   1st Qu.:  7.896   B57 B59 B63 B66:  3   Q: 46\nCA. 2343:  4   Median : 14.454   A34            :  2   S:270\n16966   :  3   Mean   : 35.627   B45            :  2\n220845  :  3   3rd Qu.: 31.500   C101           :  2\n347077  :  3   Max.   :512.329   C116           :  2\n(Other) :396   NA\'s   :1         (Other)        : 80\nTitanicTest$Survived <- ifelse(predict(logit4, TitanicTest, type = ""response"")>0.5,1,0)\nlibrary(tidyverse)\n── Attaching packages ─────────────────────────────────────────────────────── tidyverse 1.2.1 ──\n✔ ggplot2 3.1.0       ✔ purrr   0.3.2\n✔ tibble  2.1.1       ✔ dplyr   0.8.0.1\n✔ tidyr   0.8.3       ✔ stringr 1.4.0\n✔ readr   1.3.1       ✔ forcats 0.4.0\n── Conflicts ────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nanswer <- TitanicTest %>% select(PassengerId, Survived)\nanswer\nPassengerId Survived\n1           892        0\n2           893        0\n3           894        0\n4           895        0\n5           896        1\n6           897        0\n7           898        1\n8           899        0\n9           900        1\n10          901        0\n11          902        0\n12          903        0\n13          904        1\n14          905        0\n15          906        1\n16          907        1\n17          908        0\n18          909        0\n19          910        0\n20          911        0\n21          912        0\n22          913        0\n23          914        1\n24          915        1\n25          916        1\n26          917        0\n27          918        1\n28          919        0\n29          920        0\n30          921        0\n31          922        0\n32          923        0\n33          924        0\n34          925        0\n35          926        0\n36          927        0\n37          928        1\n38          929        1\n39          930        0\n40          931        0\n41          932        0\n42          933        0\n43          934        0\n44          935        1\n45          936        1\n46          937        0\n47          938        0\n48          939        0\n49          940        1\n50          941        0\n51          942        0\n52          943        0\n53          944        1\n54          945        1\n55          946        0\n56          947        0\n57          948        0\n58          949        0\n59          950        0\n60          951        1\n61          952        0\n62          953        0\n63          954        0\n64          955        1\n65          956        0\n66          957        1\n67          958        1\n68          959        0\n69          960        0\n70          961        1\n71          962        1\n72          963        0\n73          964        1\n74          965        1\n75          966        1\n76          967        0\n77          968        0\n78          969        1\n79          970        0\n80          971        1\n81          972        0\n82          973        0\n83          974        0\n84          975        0\n85          976        0\n86          977        0\n87          978        1\n88          979        1\n89          980        1\n90          981        0\n91          982        1\n92          983        0\n93          984        1\n94          985        0\n95          986        1\n96          987        0\n97          988        1\n98          989        0\n99          990        1\n100         991        0\n101         992        1\n102         993        0\n103         994        0\n104         995        0\n105         996        1\n106         997        0\n107         998        0\n108         999        0\n109        1000        0\n110        1001        0\n111        1002        0\n112        1003        1\n113        1004        1\n114        1005        1\n115        1006        1\n116        1007        0\n117        1008        0\n118        1009        1\n119        1010        0\n120        1011        1\n121        1012        1\n122        1013        0\n123        1014        1\n124        1015        0\n125        1016        0\n126        1017        1\n127        1018        0\n128        1019        0\n129        1020        0\n130        1021        0\n131        1022        0\n132        1023        0\n133        1024        0\n134        1025        0\n135        1026        0\n136        1027        0\n137        1028        0\n138        1029        0\n139        1030        1\n140        1031        0\n141        1032        0\n142        1033        1\n143        1034        0\n144        1035        0\n145        1036        0\n146        1037        0\n147        1038        0\n148        1039        0\n149        1040        0\n150        1041        0\n151        1042        1\n152        1043        0\n153        1044        0\n154        1045        0\n155        1046        0\n156        1047        0\n157        1048        1\n158        1049        1\n159        1050        0\n160        1051        1\n161        1052        1\n162        1053        0\n163        1054        1\n164        1055        0\n165        1056        0\n166        1057        0\n167        1058        0\n168        1059        0\n169        1060        1\n170        1061        1\n171        1062        0\n172        1063        0\n173        1064        0\n174        1065        0\n175        1066        0\n176        1067        1\n177        1068        1\n178        1069        0\n179        1070        1\n180        1071        1\n181        1072        0\n182        1073        0\n183        1074        1\n184        1075        0\n185        1076        1\n186        1077        0\n187        1078        1\n188        1079        0\n189        1080        0\n190        1081        0\n191        1082        0\n192        1083        0\n193        1084        0\n194        1085        0\n195        1086        0\n196        1087        0\n197        1088        1\n198        1089        1\n199        1090        0\n200        1091        1\n201        1092        1\n202        1093        0\n203        1094        0\n204        1095        1\n205        1096        0\n206        1097        0\n207        1098        0\n208        1099        0\n209        1100        1\n210        1101        0\n211        1102        0\n212        1103        0\n213        1104        0\n214        1105        1\n215        1106        0\n216        1107        0\n217        1108        1\n218        1109        0\n219        1110        1\n220        1111        0\n221        1112        1\n222        1113        0\n223        1114        1\n224        1115        0\n225        1116        1\n226        1117        0\n227        1118        0\n228        1119        1\n229        1120        0\n230        1121        0\n231        1122        0\n232        1123        1\n233        1124        0\n234        1125        0\n235        1126        0\n236        1127        0\n237        1128        0\n238        1129        0\n239        1130        1\n240        1131        1\n241        1132        1\n242        1133        1\n243        1134        0\n244        1135        0\n245        1136        0\n246        1137        0\n247        1138        1\n248        1139        0\n249        1140        1\n250        1141        0\n251        1142        1\n252        1143        0\n253        1144        0\n254        1145        0\n255        1146        0\n256        1147        0\n257        1148        0\n258        1149        0\n259        1150        1\n260        1151        0\n261        1152        0\n262        1153        0\n263        1154        1\n264        1155        1\n265        1156        0\n266        1157        0\n267        1158        0\n268        1159        0\n269        1160        1\n270        1161        0\n271        1162        0\n272        1163        0\n273        1164        1\n274        1165        0\n275        1166        0\n276        1167        1\n277        1168        0\n278        1169        0\n279        1170        0\n280        1171        0\n281        1172        1\n282        1173        0\n283        1174        1\n284        1175        1\n285        1176        1\n286        1177        0\n287        1178        0\n288        1179        0\n289        1180        0\n290        1181        0\n291        1182        0\n292        1183        1\n293        1184        0\n294        1185        0\n295        1186        0\n296        1187        0\n297        1188        1\n298        1189        0\n299        1190        0\n300        1191        0\n301        1192        0\n302        1193        0\n303        1194        0\n304        1195        0\n305        1196        1\n306        1197        1\n307        1198        0\n308        1199        0\n309        1200        0\n310        1201        0\n311        1202        0\n312        1203        0\n313        1204        0\n314        1205        0\n315        1206        1\n316        1207        1\n317        1208        0\n318        1209        0\n319        1210        0\n320        1211        0\n321        1212        0\n322        1213        0\n323        1214        0\n324        1215        0\n325        1216        1\n326        1217        0\n327        1218        1\n328        1219        0\n329        1220        0\n330        1221        0\n331        1222        1\n332        1223        0\n333        1224        0\n334        1225        1\n335        1226        0\n336        1227        0\n337        1228        0\n338        1229        0\n339        1230        0\n340        1231        0\n341        1232        0\n342        1233        0\n343        1234        0\n344        1235        1\n345        1236        0\n346        1237        1\n347        1238        0\n348        1239        0\n349        1240        0\n350        1241        1\n351        1242        1\n352        1243        0\n353        1244        0\n354        1245        0\n355        1246        1\n356        1247        0\n357        1248        1\n358        1249        0\n359        1250        0\n360        1251        0\n361        1252        0\n362        1253        1\n363        1254        1\n364        1255        0\n365        1256        1\n366        1257        0\n367        1258        0\n368        1259        1\n369        1260        1\n370        1261        0\n371        1262        0\n372        1263        1\n373        1264        0\n374        1265        0\n375        1266        1\n376        1267        1\n377        1268        0\n378        1269        0\n379        1270        0\n380        1271        0\n381        1272        0\n382        1273        0\n383        1274        1\n384        1275        1\n385        1276        0\n386        1277        1\n387        1278        0\n388        1279        0\n389        1280        0\n390        1281        0\n391        1282        1\n392        1283        1\n393        1284        0\n394        1285        0\n395        1286        0\n396        1287        1\n397        1288        0\n398        1289        1\n399        1290        0\n400        1291        0\n401        1292        1\n402        1293        0\n403        1294        1\n404        1295        1\n405        1296        0\n406        1297        0\n407        1298        0\n408        1299        0\n409        1300        1\n410        1301        1\n411        1302        1\n412        1303        1\n413        1304        1\n414        1305        0\n415        1306        1\n416        1307        0\n417        1308        0\n418        1309        0\n#write.csv(answer, file = ""wkx.csv"")\n\n\nTask E: Compete with Rest of the World\nWhat you just did happens to be one of the ongoing competitions at kaggle.com.\nhttps://www.kaggle.com/c/titanic has more details. Once you have predicted\noutcomes on the test data, you need to create a submission file that has two\ncolumns: “PasserngerID” and “Survived”. The second column should have 1 or 0\nfor survived or not. NA or missing values in this field is not permitted.\nCreate an account at kaggle and upload your solution at\nhttps://www.kaggle.com/c/titanic/submit\nKaggle will evaluate your submission and let you know\n\nWhat is your score (how good is your prediction)\nWhat is your rank among all submissions\n\nEnter here the following:\n1. Your Kaggle Username:wakexin\n2. Your Kaggle Rank:0.77511\n3. What did you learn in this exercise? Summarize your experience. What can you do better given more time?\nI learned how to build a model, interpret its outcome and use the model to predict outcomes from different data sets. I can explore the relationship between different variables and improve the accuracy of my model if given more time.\n\n\nWhen you are done, submit you files (RMD and HTML) to Canvas for our weekly\nhomework. Also submit you Kaggle rank and username. Lets see who in\nclass gets the best rank on Kaggle. There will be a surprise prize for the\nbest submission!\n\n'], 'url_profile': 'https://github.com/wakexin', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Updated Dec 31, 2019', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kpradyumna095', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Updated Dec 31, 2019', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vibhormalik97', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Updated Dec 31, 2019', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Updated Dec 31, 2019', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Linear Regression - Alumni Donation Case Study\nProject Link: http://rpubs.com/surabhik16/alumni_donation\nAlumni donations are an important source of revenue for colleges and universities. If administrators could determine the factors that influence increases in the percentage of alumni who donate, they might be able to implement policies that could lead to increased revenues.\nA study shows that students who have more access to the faculty are more likely to be satisfied. As a result, one might suspect that smaller class sizes and lower student-faculty ratios might lead to a higher percentage of satisfied graduates, which in turn might lead to an increase in the percentage of alumni who donate.\nIn this project we will answer the following questions:\nIs there a relationship between the factors and the alumni giving rate?\nHow strong is the relationship between the factors and alumni giving rate?\nHow accurately can we estimate the effect of each factor on the alumni giving rate?\n'], 'url_profile': 'https://github.com/surabhik16', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Updated Dec 31, 2019', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Updated Dec 31, 2019', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Updated Dec 31, 2019', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Updated Dec 31, 2019', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Updated Dec 31, 2019', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Kothrud, Pune', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nishadthelabourer', 'info_list': ['Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Raghu010', 'info_list': ['Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Lagos, Nigeria.', 'stats_list': [], 'contributions': '189 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Way4ward17', 'info_list': ['Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['H1b Visa prediction(accepence or denial)\nProject description\nThere are many people are willing to come USA and work. For them H1b visa or work visa is necessery to in USA. Current project is to find top companies that provide H1b visa with maximum acceptence.\nData collection and EDA\n. H1b data hasbeen taken from kaggle.\n. Since data is data base has been created on postgreSQL and desired data set is imported to jupyter note book for further analysis.\n. Data is cleaned and performed EDA Project3_dataCleaning.ipynb\nAnalysis\n. Data is unbalanced accepted to denial category are 97% vs 3% respectively. class denial- 1 and class accceptence - 0\n. Applied logistic regression and Random forest and calculated precision, recall and f1 scores. Recall and f1 scores are very low.\n. Applied  oversampling using SMOTE followed by Random forest it could slightly improve the F1 score.\n. Applied XGBoost which incresed f1 score slightly but not significant.\nConclusion\n. H1b visa denials are higher in small companies butnot in top companies.\n. Aim of the project is about to find top companies with highest visa approvals  precision score  can be considered as the desired metrics.\n. Extracted top companies my calculating ""feature importance"" by ""gain"".\n'], 'url_profile': 'https://github.com/swarnathota', 'info_list': ['Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kpradyumna095', 'info_list': ['Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vibhormalik97', 'info_list': ['Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'cham', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Linear-Rgression-and-Random-Forest-regression\nIn this repository using different solves data file according Linear Rgression and RandomForestRegression.\n'], 'url_profile': 'https://github.com/RonakGajera999', 'info_list': ['Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Predicting-purchase-of-Car-Insurance-Logistic-Regression-\nThe task is to predict for 1000 customers who were contacted during the current campaign, whether they will buy car insurance or not.\n'], 'url_profile': 'https://github.com/UmerBhikan', 'info_list': ['Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'HTML', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Dec 31, 2019', '1', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/slr248', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'KHARAGPUR, WEST BENGAL,INDIA.', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': [""SGD_linear_regression_boston_dataset\nNumber of Instances: 506\nNumber of Attributes: 13 numeric/categorical predictive\nMedian Value (attribute 14) is usually the target\nAttribute Information (in order):\n- CRIM     per capita crime rate by town\n- ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n- INDUS    proportion of non-retail business acres per town\n- CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n- NOX      nitric oxides concentration (parts per 10 million)\n- RM       average number of rooms per dwelling\n- AGE      proportion of owner-occupied units built prior to 1940\n- DIS      weighted distances to five Boston employment centres\n- RAD      index of accessibility to radial highways\n- TAX      full-value property-tax rate per $10,000\n- PTRATIO  pupil-teacher ratio by town\n- B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n- LSTAT    % lower status of the population\n- MEDV     Median value of owner-occupied homes in $1000's\nOBJECTIVE:\n1. Prediction of Boston-house Prices<br>\n2. Sklearn has Inbuilt SGD Linear regression, But How to Implement it manually.<br>\n3. Comparing SGD Linear and Manual linear Regression.\n\n""], 'url_profile': 'https://github.com/bhavesh0798', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '147 contributions\n        in the last year', 'description': ['Using Linear Regression to Predict Home Prices\nIn this project, I analyze the King County dataset and build a linear regression model to predict home prices. I answer three questions:\n\nWhat parameters influence house prices the most, and how are they ranked in order of significance?\nDuring what month is the supply of homes sold the greatest?\nHow can you use home grade to spot the best deals?\n\nBelow is a selection of my findings:\nPrice clusters are isolated according to geographic area\n\nMost transactions occur in May and fewest in January\n\nAmong homes of the same price, those with higher grade and sqfoot are the best deals\n\nPredictions close to actuals\n\nLinear regression can explain 61% of the variation in price\n\n'], 'url_profile': 'https://github.com/melisabardhi', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Heart-Attack-Predictor\nHeart attack prediction using, logistic regression and decision tree classification. The dataset and notebook files are attached.\n'], 'url_profile': 'https://github.com/pallavknayak', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}","{'location': 'Miami', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Classification_Algorithms-Healthcare_access_BRFSS-2018\nPredicting Healthcare Access using Classification Algorithms (BRFSS-2018)\n'], 'url_profile': 'https://github.com/prasadbhoite', 'info_list': ['Python', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Feb 19, 2021', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020']}"
"{'location': 'Iowa City, Iowa US', 'stats_list': [], 'contributions': '222 contributions\n        in the last year', 'description': ['logit.jl\nJulia implementation of logistic regression package loosely based upon SPost13 for Stata by J. Scott Long PhD at Indiana University\n'], 'url_profile': 'https://github.com/ldsands', 'info_list': ['GPL-3.0 license', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'R', 'MIT license', 'Updated Feb 25, 2021', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jan 5, 2020']}","{'location': 'Urbana, Illinois', 'stats_list': [], 'contributions': '190 contributions\n        in the last year', 'description': [""Life-Satisfaction-Index\nBuild a linear regression based on OECD's life satisfaction data and the IMF's GDP per capita data.\n""], 'url_profile': 'https://github.com/TheKivs', 'info_list': ['GPL-3.0 license', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'R', 'MIT license', 'Updated Feb 25, 2021', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jan 5, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': [""Problem Statement\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\nBusiness Goal\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\nData Preparation\nThere is a variable named CarName which is comprised of two parts - the first word is the name of 'car company' and the second is the 'car model'. For example, chevrolet impala has 'chevrolet' as the car company name and 'impala' as the car model name. we need to consider only company name as the independent variable for model building.\nCar+Price+Prediction+-+.ipynb contains all the analysis and model we have tried to solve the problem.\n""], 'url_profile': 'https://github.com/SamratSengupta', 'info_list': ['GPL-3.0 license', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'R', 'MIT license', 'Updated Feb 25, 2021', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/karthikharisamy', 'info_list': ['GPL-3.0 license', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'R', 'MIT license', 'Updated Feb 25, 2021', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jan 5, 2020']}","{'location': 'Stockholm, Sweden', 'stats_list': [], 'contributions': '346 contributions\n        in the last year', 'description': ['Statistics 101 using R\nRepository for the Statistical Data Analysis course of the EIT Digital data science master at UPM\n\n\n\n\n\nAssignments\n\nStatistical Analysis: aims to perform a in-depth stastical analysis over a dataset about the relations between socio-economic information and suicide rates by year and country.\nMultiple Linear Regression: the goal is to answer a set of questions regarding the creation of regression models for the prediction of diamonds price.\nTime Series: analyze a specific time series and fit an ARIMA model following the Box-Jenkins Methodology.\n\nAuthors\n\nMiguel Perez Mateo\nJunhui Liang\nAngel Igareta angel@igareta.com\n\n'], 'url_profile': 'https://github.com/angeligareta', 'info_list': ['GPL-3.0 license', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'R', 'MIT license', 'Updated Feb 25, 2021', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shojiro-tanaka', 'info_list': ['GPL-3.0 license', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'R', 'MIT license', 'Updated Feb 25, 2021', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jan 5, 2020']}","{'location': 'Bengaluru, Karnataka', 'stats_list': [], 'contributions': '174 contributions\n        in the last year', 'description': [""USA-Housing-Price-Prediction\nThis repository uses Linear Regression to predict the price of the houses of USA.\nThe data contains the following columns:\n\n'Avg. Area Income': Avg. Income of residents of the city house is located in.\n'Avg. Area House Age': Avg Age of Houses in same city\n'Avg. Area Number of Rooms': Avg Number of Rooms for Houses in same city\n'Avg. Area Number of Bedrooms': Avg Number of Bedrooms for Houses in same city\n'Area Population': Population of city house is located in\n'Price': Price that the house sold at\n'Address': Address for the house\n\n""], 'url_profile': 'https://github.com/Reactor11', 'info_list': ['GPL-3.0 license', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'R', 'MIT license', 'Updated Feb 25, 2021', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jan 5, 2020']}","{'location': 'Lekki Lagos Nigeria', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['Food-Delivery-Time\nA Regression problem to predict food delivery time from a location to another in india, [Hosted on MachineHack platform]\n'], 'url_profile': 'https://github.com/abofficial444', 'info_list': ['GPL-3.0 license', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'R', 'MIT license', 'Updated Feb 25, 2021', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['Drishti_mini_project\nIn this project, cosmic rays were classified into gamma and hadronic rays using logistic regression and basic neural networks\n'], 'url_profile': 'https://github.com/Mokshasood', 'info_list': ['GPL-3.0 license', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'R', 'MIT license', 'Updated Feb 25, 2021', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jan 5, 2020']}","{'location': 'Pune, India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['email_sendtime_prediction\nGiven features of new user X predict the sending time TS so that opening time TO will be as early as possible\n'], 'url_profile': 'https://github.com/singh-abhijeet', 'info_list': ['GPL-3.0 license', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Python', 'Updated Dec 30, 2019', 'R', 'MIT license', 'Updated Feb 25, 2021', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'AGPL-3.0 license', 'Updated Jan 5, 2020']}"
"{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '356 contributions\n        in the last year', 'description': [""Black Friday Shopping\nBackground\nBlack Friday is an informal name for the Friday following Thanksgiving Day in the United States, which is celebrated on the fourth Thursday of November. It is regarded as the beginning of America's Christmas shopping season.\nGoal\nUsing the data of a store, explore trends in the Black Friday shopping based on features like gender, occupation, age etc. and predict the amount of money that a person is likely to spend on Black Friday depending on the features.\nDataset\nKaggle Black Friday dataset (data from a single store): https://www.kaggle.com/karinne/black-friday\nSaved in: data/black_friday/\nDependencies\n\nPandas\nSeaborn\nNumpy\nMatplotlib\nScikit-learn\n\npip install -r requirements.txt\n\nData Analysis\nParameters\n\n12 parameters - 7 numerical and 12 object variables\n\nUser_ID - int64\nProduct_ID - object\nGender - object\nAge - object\nOccupation - int64\nCity_Category - object\nStay_In_Current_City_Years - object\nMarital_Status - int64\nProduct_Category_1 - int64\nProduct_Category_2 - float64\nProduct_Category_3 - float64\nPurchase - int64\n\n\n\nGender distribution\n\nAlmost 3 times more male customers than female customers.\nMaybe males are more likely to go out and buy something during the deals and rush.\n\nGender and Age distribution\n\nMost customers belong to the age group between 26 and 35, for both genders.\nYounger and older population do not shop much on Black Friday.\n\nCustomers and Products\n\nThere 5,891 different customers who have bought something from the store.\nThere 3631 different products sold in the store.\n\nMoney spent per Occupation\n\nPeople having occupations 0 and 4 spent the most money during Black Friday sales.\nPeople with occupations 8, 18, 19 spenbt the least amount of money.\n\nCity distribution\n\nThere are three different cities from which the customers belong.\n42% are from city A, 26.9% from B, andthe rest 31.1% from C.\n\n\nData Preprocessing\nCheck and handle missing values\n\nTotal records in raw train set: 550068\nTotal records in raw test set: 233599.\nThere are missing values in Product_Category_2:\n\nNo. of data in Product_Category_2 in train set = 376430; i.e. 31% missing.\nNo. of data in Product_Category_2 in test set = 161255; i.e. 31% missing.\n\n\nWe fill the missing values in Product_Category_2 with the mean value of the existing values in this column.\nThere are missing values in Product_Category_3:\n\nNo. of data in Product_Category_3 in train set = 166821; i.e. 70% missing.\nNo. of data in Product_Category_2 in test set = 71037; i.e. 70% missing.\n\n\nWe drop the Product_Category_3 column as there are a lot of missing values.\n\nRemove unrequired columns\n\nUser_ID is is the number assigned automatically to each customer and so is not useful for prediction. Therefore we remove the column from our dataset.\nProduct_ID is not a feature of the customer. Therefore we remove the same from our dataset.\n\nConvert Categorical features to Numeric\n\nConvert categorical values to one-hot encoded vectors.\n\nPrepare the Features and Labels\n\n\nThe features are all the numeric columns along wioth the vectorized columns:\n'Occupation', 'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'F', 'M', '0-17', '18-25', '26-35', '36-45', '46-50', '51-55', '55+', 'A', 'B', 'C', '0', '1', '2', '3', '4+'\n\n\nThe label is the 'Purchase' column\n\n\nSplit the data into Training and Test sets\n\nTest data: 40%\n\nCreate and Train Model\nLearned model: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\nIntercept parameter: 12027.61271472023\n                    Coefficient\nOccupation             5.030011\nMarital_Status       -60.424335\nProduct_Category_1  -408.954291\nProduct_Category_2   -72.068789\nF                   -265.881866\nM                    265.881866\n0-17                -564.213500\n18-25               -214.605258\n26-35                -14.122454\n36-45                 95.338560\n46-50                 89.687620\n51-55                381.530967\n55+                  226.384065\nA                   -283.925835\nB                   -125.364273\nC                    409.290108\n0                    -28.095073\n1                     -1.786906\n2                      3.219490\n3                      8.219723\n4+                    18.442765\n\nTest and Evaluate the Model\n\nMean Absolute Error (MAE): 3587.788450883482\nMean Squared Error (MSE): 22017757.08262227\n\nConclusion\n\nThe amount of money spent by a customer on Black Friday sales is predicted based on their gender, age, occupation etc.\n\nMore\n\nThe product that the customer is more likely to purchase on Black Friday sales is predicted based on their gender, age, occupation etc.\n\nIn this case the label is 'Product_Category_1'\n\n\n\n""], 'url_profile': 'https://github.com/likarajo', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', '3', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Karachi Pakistan', 'stats_list': [], 'contributions': '1,523 contributions\n        in the last year', 'description': ['ASSIGNMENT-FOR-BATCH-3\nASSIGNMENT FOR BATCH 3\nApply where it is necessary i:e data preprocessing,linear regression ,polynomial regression, and decision tree etc on the following data-set  and compare the accuracy   of all regression and take screenshots of output prediction and accuracy of all the regression which you are going to apply  and upload all the codes  on github with screenshot.Dont  try to copy others because we give you just half data-set  and when you show us output for future analysis we will compare your values with the remaining half of the data-set and then give you marks. Explanation of the data-set is as follows:\n\nTake 50 startups of any two countries and find out which country is going to provide best profit in future.\nAnnual temperature between two industries is given. Predict the  temperature in 2016 and 2017 using the past data of both country.\nData of global production of CO2 of a place is given between 1970s to 2010. Predict the CO2 production for the years 2011, 2012 and  2013 using the old data set.\nHousing price according to the ID is assigned to every-house. Perform future analysis where when ID is inserted the housing price is displayed.\nData of monthly experience and income distribution of different employs is given. Perform regression.\n\n'], 'url_profile': 'https://github.com/unaisshazan', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', '3', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Connalia', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', '3', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'Urbana, Illinois', 'stats_list': [], 'contributions': '190 contributions\n        in the last year', 'description': ['SVM-California-Housing\nSVM regressor on the California housing dataset (1990 statistics)\n'], 'url_profile': 'https://github.com/TheKivs', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', '3', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nCovariance and Correlation\nWe start the section by covering covariance and correlation, both of which relate to how likely two variables are to change together. For example, with houses, it wouldn\'t be too surprising if the number of rooms and the price of a house was correlated (in general, more rooms == more expensive).\nStatistical Learning Theory\nWe then explore statistical learning theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', '3', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nIntroduce Statsmodels for multiple regression\nPresent alternatives for running regression in Scikit Learn\n\nStatsmodels for multiple linear regression\nThis lecture will be more of a code-along, where we will walk through a multiple linear regression model using both Statsmodels and Scikit-Learn.\nRemember that we introduced single linear regression before, which is known as ordinary least squares. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(""auto-mpg.csv"") \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[""acceleration""]\nlogdisp = np.log(data[""displacement""])\nloghorse = np.log(data[""horsepower""])\nlogweight= np.log(data[""weight""])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[""acc""]= scaled_acc\ndata_fin[""disp""]= scaled_disp\ndata_fin[""horse""] = scaled_horse\ndata_fin[""weight""] = scaled_weight\ncyl_dummies = pd.get_dummies(data[""cylinders""], prefix=""cyl"")\nyr_dummies = pd.get_dummies(data[""model year""], prefix=""yr"")\norig_dummies = pd.get_dummies(data[""origin""], prefix=""orig"")\nmpg = data[""mpg""]\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 26 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_3     392 non-null uint8\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_70     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_1    392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(21)\nmemory usage: 23.4 KB\n\nThis was the data we had until now. As we want to focus on model interpretation and still don\'t want to have a massive model for now, let\'s only inlude ""acc"", ""horse"" and the three ""orig"" categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis= 1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_1\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n1\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n1\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n1\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n1\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n1\n0\n0\n\n\n\n\nA linear model using Statsmodels\nNow, let\'s use the statsmodels.api to run our ols on all our data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$, where, with $n$ predictors, X is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = ""mpg ~ acceleration+weight+orig_1+orig_2+orig_3""\nmodel = ols(formula= formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable ""mpg"" out of your data frame, and use the a ""+"".join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nOr even easier, simply use the .OLS-method from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors dataframe so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nInterpretation\nJust like for single multiple regression, the coefficients for our model should be interpreted as ""how does Y change for each additional unit X""? Do note that the fact that we transformed X, interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed X, the actual relationship is ""how does Y change for each additional unit X\'"", where X\' is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit learn\nYou can also repeat this process using Scikit-Learn. The code to do this can be found below. The Scikit-learn is generally known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit learn compared to Statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of Scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -0.71140721, -0.29903267,  1.01043987])\n\nThe intercept of the model is stored in the .intercept_-attribute.\n# intercept\nlinreg.intercept_\n21.472164286075383\n\nWhy are the coefficients different in scikit learn vs Statsmodels?\nYou might have noticed that running our regression in Scikit-learn and Statsmodels returned (partially) different parameter estimates. Let\'s put them side to side:\n\n\n\n\nStatsmodels\nScikit-learn\n\n\n\n\nintercept\n16.1041\n21.4722\n\n\nacceleration\n5.0494\n5.0494\n\n\nweight\n-5.8764\n-5.8764\n\n\norig_1\n4.6566\n-0.7114\n\n\norig_2\n5.0690\n-0.2990\n\n\norig_3\n6.3785\n1.0104\n\n\n\nThese models return equivalent results!\nWe\'ll use an example to illustrate this. Remember that minmax-scaling was used on acceleration, and standardization on log(weight).\nLet\'s assume a particular observation with a value of 0.5 for both acceleration and weight after transformation, and let\'s assume that the origin of the car = orig_3. The predicted value for mpg for this particular value will then be equal to:\n\n16.1041 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 6.3785 = 22.0691 according to the Statsmodels\n21.4722 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 1.0104 = 22.0691 according to the Scikit-learn model\n\nThe eventual result is the same. The extimates for the categorical variables are the same ""up to a constant"", the difference between the categorical variables, in this case 5.3681, is added in the intercept!\nYou can make sure to get the same result in both Statsmodels and Scikit-learn, by dropping out one of the orig_-levels. This way, you\'re essentially forcing the coefficient of this level to be equal to zero, and the intercepts and the other coefficients will be the same.\nThis is how you do it in Scikit-learn:\npredictors = predictors.drop(""orig_3"",axis=1)\nlinreg.fit(predictors, y)\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -1.72184708, -1.30947254])\n\nlinreg.intercept_\n22.482604160455665\n\nAnd Statsmodels:\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    22.4826     0.789    28.504  0.000    20.932    24.033\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1    -1.7218     0.653    -2.638  0.009    -3.005    -0.438\n\n\norig_2    -1.3095     0.688    -1.903  0.058    -2.662     0.043\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               9.59\n\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in both Scikit-Learn and Statsmodels. Before we discuss the model metrics in detail, let\'s go ahead and try out this model on the Boston Housing Data Set!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', '3', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', '3', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', '3', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets. Key takeaways include:\n\nThe Pearson Correlation (range: -1 -> 1) is a standard way to describe the correlation between two variables\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', '3', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nCovariance and Correlation\nWe start the section by covering covariance and correlation, both of which relate to how likely two variables are to change together. For example, with houses, it wouldn\'t be too surprising if the number of rooms and the price of a house was correlated (in general, more rooms == more expensive).\nStatistical Learning Theory\nWe then explore statistical learning theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 4, 2020', '3', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets. Key takeaways include:\n\nThe Pearson Correlation (range: -1 -> 1) is a standard way to describe the correlation between two variables\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Hyderabad , India', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['Bank-Marketing-Using-Random-Forest-and-Logistic-Regression\nAnalyzed the bank data , formed visualizations using seaborn , Built a model based on selected features which we got from OLS , RFE and VIF . And made predictions using Logistic and random forest algorithms.\n'], 'url_profile': 'https://github.com/DheerajPranav', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kpradyumna095', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['stock-prediction-using-svm-and-linear-regression-in-python\ndownload python shoftware\nhttp://python.org/download/\ngot the data from nsc(nationl stock exchnage) website using python code\nfrom datetime import date\nfrom nsepy import get_history\ninfy = get_history(symbol=\'ITNFY\',\nstart=date(1998,1,1),\nend=date(2019,1,1))\n#converting data into a csv file\ninfy.to_csv(\'infy.csv\', mode=\'a\', header=""FALSE"")\nusing this data creating svm model and linear regression\n'], 'url_profile': 'https://github.com/pratyusha0608', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nIntroduce Statsmodels for multiple regression\nPresent alternatives for running regression in Scikit Learn\n\nStatsmodels for multiple linear regression\nThis lecture will be more of a code-along, where we will walk through a multiple linear regression model using both Statsmodels and Scikit-Learn.\nRemember that we introduced single linear regression before, which is known as ordinary least squares. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(""auto-mpg.csv"") \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[""acceleration""]\nlogdisp = np.log(data[""displacement""])\nloghorse = np.log(data[""horsepower""])\nlogweight= np.log(data[""weight""])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[""acc""]= scaled_acc\ndata_fin[""disp""]= scaled_disp\ndata_fin[""horse""] = scaled_horse\ndata_fin[""weight""] = scaled_weight\ncyl_dummies = pd.get_dummies(data[""cylinders""], prefix=""cyl"")\nyr_dummies = pd.get_dummies(data[""model year""], prefix=""yr"")\norig_dummies = pd.get_dummies(data[""origin""], prefix=""orig"")\nmpg = data[""mpg""]\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 26 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_3     392 non-null uint8\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_70     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_1    392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(21)\nmemory usage: 23.4 KB\n\nThis was the data we had until now. As we want to focus on model interpretation and still don\'t want to have a massive model for now, let\'s only inlude ""acc"", ""horse"" and the three ""orig"" categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis= 1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_1\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n1\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n1\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n1\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n1\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n1\n0\n0\n\n\n\n\nA linear model using Statsmodels\nNow, let\'s use the statsmodels.api to run our ols on all our data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$, where, with $n$ predictors, X is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = ""mpg ~ acceleration+weight+orig_1+orig_2+orig_3""\nmodel = ols(formula= formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable ""mpg"" out of your data frame, and use the a ""+"".join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nOr even easier, simply use the .OLS-method from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors dataframe so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nInterpretation\nJust like for single multiple regression, the coefficients for our model should be interpreted as ""how does Y change for each additional unit X""? Do note that the fact that we transformed X, interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed X, the actual relationship is ""how does Y change for each additional unit X\'"", where X\' is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit learn\nYou can also repeat this process using Scikit-Learn. The code to do this can be found below. The Scikit-learn is generally known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit learn compared to Statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of Scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -0.71140721, -0.29903267,  1.01043987])\n\nThe intercept of the model is stored in the .intercept_-attribute.\n# intercept\nlinreg.intercept_\n21.472164286075383\n\nWhy are the coefficients different in scikit learn vs Statsmodels?\nYou might have noticed that running our regression in Scikit-learn and Statsmodels returned (partially) different parameter estimates. Let\'s put them side to side:\n\n\n\n\nStatsmodels\nScikit-learn\n\n\n\n\nintercept\n16.1041\n21.4722\n\n\nacceleration\n5.0494\n5.0494\n\n\nweight\n-5.8764\n-5.8764\n\n\norig_1\n4.6566\n-0.7114\n\n\norig_2\n5.0690\n-0.2990\n\n\norig_3\n6.3785\n1.0104\n\n\n\nThese models return equivalent results!\nWe\'ll use an example to illustrate this. Remember that minmax-scaling was used on acceleration, and standardization on log(weight).\nLet\'s assume a particular observation with a value of 0.5 for both acceleration and weight after transformation, and let\'s assume that the origin of the car = orig_3. The predicted value for mpg for this particular value will then be equal to:\n\n16.1041 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 6.3785 = 22.0691 according to the Statsmodels\n21.4722 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 1.0104 = 22.0691 according to the Scikit-learn model\n\nThe eventual result is the same. The extimates for the categorical variables are the same ""up to a constant"", the difference between the categorical variables, in this case 5.3681, is added in the intercept!\nYou can make sure to get the same result in both Statsmodels and Scikit-learn, by dropping out one of the orig_-levels. This way, you\'re essentially forcing the coefficient of this level to be equal to zero, and the intercepts and the other coefficients will be the same.\nThis is how you do it in Scikit-learn:\npredictors = predictors.drop(""orig_3"",axis=1)\nlinreg.fit(predictors, y)\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -1.72184708, -1.30947254])\n\nlinreg.intercept_\n22.482604160455665\n\nAnd Statsmodels:\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    22.4826     0.789    28.504  0.000    20.932    24.033\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1    -1.7218     0.653    -2.638  0.009    -3.005    -0.438\n\n\norig_2    -1.3095     0.688    -1.903  0.058    -2.662     0.043\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               9.59\n\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in both Scikit-Learn and Statsmodels. Before we discuss the model metrics in detail, let\'s go ahead and try out this model on the Boston Housing Data Set!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models. Moreover, you also got a brief introduction to data ethics. Remember that throughout your data work it is essential to consider personal privacy and the potential impacts of the data you have access to.\nRegression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preperation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\nEthics\nAside from regression, you also took a look at data privacy and ethics. You probably had already heard some of these ideas, but may have not been familiar with GDPR or privacy advocacy groups like the Electornic Frontier Foundation. The digital age has brought a slew of political and philosophical questions to the arena, and there are always fascinating (and disturbing) conversations to be had. Be sure to keep these and other issues at the forefront of your thought process, and not simply be dazzled by the power of machine learning algorithms. Ask yourself questions like, ""What is the algorithm being used for?"" or ""What are the ramifications or impact of this analysis/program/algorithm?"".\nWhen Einstein released his theory of relativity, its impact had tremendous benefit in advancing the field of physics yet the subsequent development of the Manhattan project was arguably a great detriment of humanity. To a similar vain, be thoughtful of which planes of thought you are operating on, and always be sure to include an ethical and philosophical perspective of the potential ramifications of your work.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models. Moreover, you also got a brief introduction to data ethics. Remember that throughout your data work it is essential to consider personal privacy and the potential impacts of the data you have access to.\nRegression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preperation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\nEthics\nAside from regression, you also took a look at data privacy and ethics. You probably had already heard some of these ideas, but may have not been familiar with GDPR or privacy advocacy groups like the Electornic Frontier Foundation. The digital age has brought a slew of political and philosophical questions to the arena, and there are always fascinating (and disturbing) conversations to be had. Be sure to keep these and other issues at the forefront of your thought process, and not simply be dazzled by the power of machine learning algorithms. Ask yourself questions like, ""What is the algorithm being used for?"" or ""What are the ramifications or impact of this analysis/program/algorithm?"".\nWhen Einstein released his theory of relativity, its impact had tremendous benefit in advancing the field of physics yet the subsequent development of the Manhattan project was arguably a great detriment of humanity. To a similar vain, be thoughtful of which planes of thought you are operating on, and always be sure to include an ethical and philosophical perspective of the potential ramifications of your work.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'Columbia, MD', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Google-App-Rate-Prediction-using-Random-Forest-Regression\nAbout this Dataset\nContext\nWhile many public datasets (on Kaggle and the like) provide Apple App Store data, there are not many counterpart datasets available for Google Play Store apps anywhere on the web. On digging deeper, I found out that iTunes App Store page deploys a nicely indexed appendix-like structure to allow for simple and easy web scraping. On the other hand, Google Play Store uses sophisticated modern-day techniques (like dynamic page load) using JQuery making scraping more challenging.\nContent\nEach app (row) has values for catergory, rating, size, and more.\nAcknowledgements\nThis information is scraped from the Google Play Store. This app information would not be available without it.\nInspiration\nThe Play Store apps data has enormous potential to drive app-making businesses to success. Actionable insights can be drawn for developers to work on and capture the Android market!\nNB: please check App Rate Pridiction. ipynp file for further undertand the work\n'], 'url_profile': 'https://github.com/gurmu', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Regression-and-time-series-analysis-using-house-sales-price\n•In this project I have Developed 8 Linear regression models with different combination of factors to find best fit among 8 models. We used train/test split method and 10-fold cross validation to validate the data.\n•Developed AR and ARMA time series model.\n•Performed exploratory analysis on the datasets to find the heat map, different trends/patterns in dataset.\n•Skills: Python Programming,Jupyter notebook,python libraries like sklearn, pandas, numpy, matplotlib, seaborn, statsmodels\nDataset links :\nRegression - https://www.kaggle.com/harlfoxem/housesalesprediction\nUsing the following features,model predict the best price in my analysis\n(price ~ sqft_living + grade + sqft_above + sqft_living15 +\nbathrooms + view + sqft_basement + lat + bedrooms + waterfront + floors + yr_renovated)\nTime series - https://www.census.gov/econ/currentdata/dbsearch?program=RESSALES&startYear=1963&endYear=2019&categories=FORSALE&dataType=TOTAL&geoLevel=US&notAdjusted=1&submit=GET+DATA&releaseScheduleId=\n'], 'url_profile': 'https://github.com/prashanthgardhas18', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Dec 31, 2019', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jun 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Short-Term-Load-Forecasting\n'], 'url_profile': 'https://github.com/TomerArzu', 'info_list': ['2', 'MATLAB', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'Stata', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['In this project, we will analyse customer-level data of a leading telecom firm for prepaid customers, build predictive models to identify customers at high risk of usage-based  churn and identify the main indicators of churn.\nIn the Indian and the southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.\nIn this project, you will define high-value customers based on a certain metric (mentioned later below) and predict churn only on high-value customers.\nUnderstanding the Business Objective and the Data\nThe dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively.\nThe goal is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.\nUnderstanding Customer Behaviour During Churn\nCustomers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are three phases of customer lifecycle :\nThe ‘good’ phase: In this phase, the customer is happy with the service and behaves as usual.\nThe ‘action’ phase: The customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‘good’ months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor’s offer/improving the service quality etc.)\nThe ‘churn’ phase: In this phase, the customer is said to have churned. You define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1/0 based on this phase, you discard all data corresponding to this phase.\nIn this case, since we are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month is the ‘churn’ phase.\nData Dictionary\nThe dataset can be download using this link. The data dictionary is provided for download below.\nData Dictionary - Telecom Churnfile_download\tDownload\nThe data dictionary contains meanings of abbreviations. Some frequent ones are loc (local), IC (incoming), OG (outgoing), T2T (telecom operator to telecom operator), T2O (telecom operator to another operator), RECH (recharge) etc.\nThe attributes containing 6, 7, 8, 9 as suffixes imply that those correspond to the months 6, 7, 8, 9 respectively.\nData Preparation\nThe following data preparation steps are crucial for this problem:\n\nDerive new features\n\nThis is one of the most important parts of data preparation since good features are often the differentiators between good and bad models.\n\nFilter high-value customers\n\nAs mentioned above, we  need to predict churn only for the high-value customers. Define high-value customers as follows: Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).\nAfter filtering the high-value customers, we would get about 29.9k rows.\n\nTag churners and remove attributes of the churn phase\n\nNow we have tagged the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes you need to use to tag churners are:\ntotal_ic_mou_9\ntotal_og_mou_9\nvol_2g_mb_9\nvol_3g_mb_9\nAfter tagging churners, we removed  all the attributes corresponding to the churn phase (all attributes having ‘ _9’, etc. in their names).\nModelling\nwe have Built models to predict churn. The predictive model that we have build would serve two purposes:\nIt will be used to predict whether a high-value customer will churn or not, in near future (i.e. churn phase). By knowing this, the company can take action steps such as providing special plans, discounts on recharge etc.\nIt will be used to identify important variables that are strong predictors of churn. These variables may also indicate why customers choose to switch to other networks.\nIn some cases, both of the above-stated goals can be achieved by a single machine learning model. But here, we have a large number of attributes, and thus we tried  using a dimensionality reduction technique such as PCA and then build a predictive model. After PCA, we have used ML classification models.\nAlso, since the rate of churn is typically low (about 5-10%, this is called class-imbalance) - we used techniques to handle class imbalance.\nFollowing are the steps we took :\nPreprocess data (convert columns to appropriate formats, handle missing values, etc.)\nConduct appropriate exploratory analysis to extract useful insights (whether directly useful for business or for eventual modelling/feature engineering).\nDerive new features.\nReduce the number of variables using PCA.\nTrain a variety of models, tune model hyperparameters, etc. (handle class imbalance using appropriate techniques).\nEvaluate the models using appropriate evaluation metrics. it is more important to identify churners than the non-churners accurately - choose an appropriate evaluation metric which reflects this business goal.\nFinally, choose a model based on some evaluation metric.\nThe above model will only be able to achieve one of the two goals - to predict customers who will churn. we can’t use the above model to identify the important features for churn. That’s because PCA usually creates components which are not easy to interpret.\nTherefore, build another model with the main objective of identifying important predictor attributes which help the business understand indicators of churn. A good choice to identify important variables is a logistic regression model or a model from the tree family. In case of logistic regression, make sure to handle multi-collinearity.\nAfter identifying important predictors, display them visually - you can use plots, summary tables etc. - whatever you think best conveys the importance of features.\nFinally, recommend strategies to manage customer churn based on your observations.\n'], 'url_profile': 'https://github.com/SamratSengupta', 'info_list': ['2', 'MATLAB', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'Stata', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'Nigeria', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Easy-to-Understand-Step-by-Step-Linear-Regression-Model-in-Python\nThis simplify building a linear regression model for the upcoming data scientists for understanding on how to build any model in Machine Learning.\nThis describes all the steps involve in building a linear regression model in python.\nData visualization is also examined in this work.\n'], 'url_profile': 'https://github.com/SPOGit', 'info_list': ['2', 'MATLAB', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'Stata', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DarshanaBhute', 'info_list': ['2', 'MATLAB', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'Stata', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': [""How do we know what determines property value?\n\nZillow, a real estate database company offers what they call 'Zestimate'. According to Zillow, Zestimate is an estimate on property value from their 'public and user submitted data taking into account home facts, location and market conditions'.\nHow can we make such predictions? What can we do to improve the predictions? This project explores dataset of features in properties sold in Ames, IA and studies how might we optimize the 'Zestimation' of property value.\nSource: https://www.zillow.com\n\nData Dictionary\nThis project used the data from the article, Ames Iowa: Alternative to the Boston Housing Data Set. This dataset was used in Ames, IA Assessor’s Office to assess properties sold in Ames, IA from 2006 to 2010. This dataset contains 81 features and 2930 observations.\nSource: http://jse.amstat.org/v19n3/decock/DataDocumentation.txt\n\nConclusion & Recommendation\n\n\nAdding more features does not always improve the prediction result\n\n\n\nWhy might we not use all of our features for the prediction? If we have more information to learn from, wouldn't that make the prediction perform better? To answer this question first, this approach is too costly most of the time.\nAs with most datasets, addition of engineered features and dummying the categorical features, which is a technique to turn categorial features into features that can be evaluated quantitatively were required for this project. As a result, I ended up with 1003 features. This means that I would need all these 1003 features to make predictions, which takes a lot of computational resources and meticulous data collecting.\nBesides the resource issue, using more features does not always improve the prediction. The plot above shows prediction score (R-square) for models that included features with certain threshold of correlation to the target (Correlation here is absolute value). This means at 0, features that had correlation to the target higher than 0 were included, which would mean all available features. The plot shows three types of scores represented in lines. It is shown that the scores have large difference when more features are added to the model. The large difference in the scores implies that the model is too complex and fails to make good predictions on new datasets.\n\n\n\nCoefficient weight from Linear Regression model or feature's correlation to the target cannot be used to decide if the feature is important for prediction.\n\n\n\nIf we can't use all our features, we have to select important features. First approach I thought of was coefficient weight of each feature to the target in a Linear Regression model and the feature's correlation to the target. Can we use any of these measures? My answer was yes and no. Although we can deduce the importance of the features from the two factors to certain degree, they do not provide perfectly accurate measure.\nThe plot above shows this trend. The first plot on the top shows coefficient weight of each features, which is presented in the x-axis with their index. The second plot on the bottom shows the correlation of the feature to the target. It is shown that the magnitude of the two measures does not correspond to each other.(I log-scaled coefficient weight and plotted absolute value of both coefficient weight and correlation show the trend more efficiently.) Accordingly, the feature's correlation to the target and coefficient weight from Linear Regression model are not credible source to judge how useful the features are in predicting our target.\n\n\n\nUse Lasso Regression model to select which features are important for prediction.\n\n\n\nHow can then we select features for the prediction? Lasso Regression can help with the process. Lasso Regression tells us which features are important by increasing the coefficient weight of important features and decreasing the ones that are not important for the prediction. For my prediction model, I dropped all features that had 0 coefficient weight from the Lasso Regression. As a result, the number of features to use went down to 49 features. What this implies is that the rest of 954 features could have been a noise to our prediction.\n\nWith Lasso Regression model and the 49 features selected, R-square value, one of my prediction scores was 0.93. This says that 93 percent of how the actual values changes is explained by my model. Another prediction score RMSE, which stands for root mean square error, was 20267$. This suggests that by average the predicted values was 20267$ different from the true values.\n\nNext Step\n\nWhat more can we do to improve our model? First, we can use Ridge Regression. This is another regularization model such as Lasso Regression. We can also try log-scaling our target or features to make their distribution more normally distributed. The impact of this can be seen in residual plots such as the one shown above. The more randomly scattered the data points in the plot, the more improvement in our model it implies.\n\nSource\n\nhttps://www.zillow.com\nhttp://jse.amstat.org/v19n3/decock/DataDocumentation.txt\n\n""], 'url_profile': 'https://github.com/dae-han', 'info_list': ['2', 'MATLAB', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'Stata', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vidisha2211', 'info_list': ['2', 'MATLAB', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'Stata', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'Cedar City', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Robbing-Peter-to-Pay-Paul-George\nAn approach to see where salary is distributed within the NBA when the salary cap is increased through interpretations of linear regression.\n'], 'url_profile': 'https://github.com/JamesCaron', 'info_list': ['2', 'MATLAB', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'Stata', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'Ilmenau, Germany', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['Machine Learning with Python\nThe project is about the creation of the best classifier using different classification techniquies of machine learning like KNN, Decision Tree, Support Vector Machine, and Logistic regression. This was the peer reviewed final project of the course Machine Learning with Python provided by IBM and coursera.\nHere is the course certificate https://www.coursera.org/account/accomplishments/verify/MJGRUERX7PSH\n'], 'url_profile': 'https://github.com/mshakeelt', 'info_list': ['2', 'MATLAB', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'Stata', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'Zurich, Switzerland', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Utkarsh-Bajpai', 'info_list': ['2', 'MATLAB', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'Stata', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['project-bike-sharing-demand-forecast\nThe project uses the Bike Sharing Demend Dataset on Kaggle. Regression models in this project are built on PySpark platform. Eventually, the prediction of test dataset is submitted to Kaggle for evaluation. With PySpark, additional works are also put in to develop an application that could run the model on HDFS, stream new data through flume, and make predictions in real-time.\nOverview\nI. Data Transformation and Feature Extraction\n\nAdd Weekday Column\nConvert Categorical Variables\nCheck Missing Values\nExtract Date and Time Features\n\nII. Model Development\n\nLinear Regression\nRandom Forest\nGradient-Boosted Tree Regression\nBest Model\n\nIII. Predict New Data\n'], 'url_profile': 'https://github.com/byrontang', 'info_list': ['2', 'MATLAB', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'Stata', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Oct 12, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Jupyter Notebook', 'Updated Jan 5, 2020']}"
"{'location': 'Japan', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Pytorch implementation of DNN-regressor using Entity Embedding[1]\nA Pytorch implementation of a DNN model based on the ""Entity Embedding"" technique.\nAnd its general application to a typical experimental flow for a real-world tabular data.\nSee the files model.py and demo.ipynb for more detail.\n\n[1] Guo, Cheng, and Felix Berkhahn. ""Entity embeddings of categorical variables."" arXiv preprint arXiv:1604.06737 (2016).\n\nmodel.py\nmodel.py shows an implementation of DNN for regression with input of factorized (*) categorical variables (first half) and numerical variables (second half).\nPreparing a unique Embedding Layer for each categorical variable and perform mapping to the corresponding dense vector space (Entity Embedding).\n\nExample:\n\nMapping a categorical variable with 7 categories which elements are Mon.(0), Tue.(1), Wed.(2), Thu.(3), Fri.(4), Sat.(5), Sun.(6) to 2D space by an Embedding Layer.\n\n\n\nRegression is performed by combining the embedded categorical features and numerical variables and inputting them to Fully Connected Layers.\n\n(*) Factorization: The process of converting each element of a categorical variable into a corresponding positive index.\n\ndemo.ipynb\ndemo.ipynb shows a experimental flow for solving a real-world problem using model.py.\nBy linking model.py with df2numpy.TransformDF2Numpy which is a tool for transforming pandas.DaraFrame to numpy.array,\nthe experimental flow realizes generality which is applicable to various tabler dataset.\n'], 'url_profile': 'https://github.com/kitayama1234', 'info_list': ['2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 16, 2021', 'R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Jan 5, 2020', '5', 'Python', 'MIT license', 'Updated May 17, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['opioid_logisticregressionmodel\nDescription:\nThe goal of this project was to build a logistic regression model in R to predict opioid usage based on demographic, personality, and drug usage data.\nAll data was taken from this site: https://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29\nBuilding the Model:\nStepwise selection was used for the building of the model, and the dataset was split into training and testing sets to  evaluate its success.\nVisualizing the Model:\nopioidmodel_finalcode.R contains code to visualize the model in several ways (with histograms, calibration-in-the-large plots, density plots, ROC curves).\n'], 'url_profile': 'https://github.com/yangeyange', 'info_list': ['2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 16, 2021', 'R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Jan 5, 2020', '5', 'Python', 'MIT license', 'Updated May 17, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Forensic-Handwriting-Recognition\nAchieved 98% accuracy on CEDAR dataset by developing a handwriting recognition tool using classiﬁcation techniques such as logistic regression, decision tree, neural networks, and support vector machine to compare handwriting of multiple writers ◦ Attained accuracy of 86% on CEDAR dataset by designing generative models and probabilistic graphical models to learn explainable features for handwritten image pair labeled from same or different writer\n'], 'url_profile': 'https://github.com/Yash-96', 'info_list': ['2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 16, 2021', 'R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Jan 5, 2020', '5', 'Python', 'MIT license', 'Updated May 17, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kunal-16', 'info_list': ['2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 16, 2021', 'R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Jan 5, 2020', '5', 'Python', 'MIT license', 'Updated May 17, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/umairahmad-ua', 'info_list': ['2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 16, 2021', 'R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Jan 5, 2020', '5', 'Python', 'MIT license', 'Updated May 17, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'Houston, Texas ', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Quantitative-Analytics-Projects\nResearch Projects involving the Calculation Examples utilizing The principles Chi Square, Anova, Linear Regressions\n'], 'url_profile': 'https://github.com/LGA112660', 'info_list': ['2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 16, 2021', 'R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Jan 5, 2020', '5', 'Python', 'MIT license', 'Updated May 17, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'Lahore, Pakistan', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Boston Housing Price Predictor\n\n\nTitle: Boston Housing Data\n\n\nSources:\n(a) Origin:  This dataset was taken from the StatLib library which is\nmaintained at Carnegie Mellon University.\n(b) Creator:  Harrison, D. and Rubinfeld, D.L. \'Hedonic prices and the\ndemand for clean air\', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.\n(c) Date: July 7, 1993\n\n\nPast Usage:\n\nUsed in Belsley, Kuh & Welsch, \'Regression diagnostics ...\', Wiley,\n1980.   N.B. Various transformations are used in the table on\npages 244-261.\nQuinlan,R. (1993). Combining Instance-Based and Model-Based Learning.\nIn Proceedings on the Tenth International Conference of Machine\nLearning, 236-243, University of Massachusetts, Amherst. Morgan\nKaufmann.\n\n\n\nRelevant Information:\nConcerns housing values in suburbs of Boston.\n\n\nNumber of Instances: 506\n\n\nNumber of Attributes: 13 continuous attributes (including ""class""\nattribute ""MEDV""), 1 binary-valued attribute.\n\n\nAttribute Information:\n\nCRIM      per capita crime rate by town\nZN        proportion of residential land zoned for lots over\n25,000 sq.ft.\nINDUS     proportion of non-retail business acres per town\nCHAS      Charles River dummy variable (= 1 if tract bounds\nriver; 0 otherwise)\nNOX       nitric oxides concentration (parts per 10 million)\nRM        average number of rooms per dwelling\nAGE       proportion of owner-occupied units built prior to 1940\nDIS       weighted distances to five Boston employment centres\nRAD       index of accessibility to radial highways\nTAX      full-value property-tax rate per $10,000\nPTRATIO  pupil-teacher ratio by town\nB        1000(Bk - 0.63)^2 where Bk is the proportion of blacks\nby town\nLSTAT    % lower status of the population\nMEDV     Median value of owner-occupied homes in $1000\'s\n\n\n\nMissing Attribute Values:  None.\n\n\n'], 'url_profile': 'https://github.com/profahad', 'info_list': ['2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 16, 2021', 'R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Jan 5, 2020', '5', 'Python', 'MIT license', 'Updated May 17, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'Jamshedpur, Jharkhand', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['Stock Prediction using news articles on a deep learning architecture\nFinal Year Capstone Project - VIT University 2020\nWelcome to my repository for my final year capstone project at VIT Chennai, India. This project aims to capture the essence of stock market prediction using news feed from top international websites.\nNeural Nets were trained on Google Colab using GPU, due to slow runtimes on my local machine.\n'], 'url_profile': 'https://github.com/Ronet05', 'info_list': ['2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 16, 2021', 'R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Jan 5, 2020', '5', 'Python', 'MIT license', 'Updated May 17, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'Gurugram, India', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Telecom Churn Predictor\nCustomer churn in a telecommunication company is a key issue for an organisation/provider. We have been provided with data of postpaid customers at a telecom company which has some structured pre-recorded key metrics such as usage behaviour, contract details and payment details. The data also provides us with information on users who cancelled their service (i.e Customer Churn)\nThis business report shall attempt to provide you an in-depth understanding of the features used for our analysis, followed by visualizations and the effect of these variables on each other. This will be followed by building predictive models using algorithms such as logistic regression, K-nearest neighbours and Naïve Bayes. We shall evaluate and compare these models to each other using various model performance evaluators and recommend an optimal approach to reduce or predict customer churn respectively\nThe aim of this exercise is to predict, if customers using services of a telecommunication company, are likely to cancel their service in the future. The management might want to understand the key factors which lead to customer cancelling their service to improve on those problem areas. The management would also might want to target these risk assets who are about to churn and pro-actively offer them lucrative offers to stay on with the company. Application and use case of such a predictive modelling activity are many fold.\nOur findings are based on data of postpaid customers at the telecommunication company where out of 3333 entries, about 483 people opted out of the service where as 2850 decided to stay on with the current provider.\nThe management may want to devise a marketing strategy for potential customers who may churn or improve their overall personal offerings and services by understanding issues that may lead to customer churn.\n'], 'url_profile': 'https://github.com/yashpixels', 'info_list': ['2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 16, 2021', 'R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Jan 5, 2020', '5', 'Python', 'MIT license', 'Updated May 17, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DarshanaBhute', 'info_list': ['2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 16, 2021', 'R', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'GPL-2.0 license', 'Updated Jan 5, 2020', '5', 'Python', 'MIT license', 'Updated May 17, 2020', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Jan 4, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Bike_Rental_Prediction\nThis project aims at predicting the number of bike rentals on given seasonal and environmental features. Achieved a 90% accuracy using Linear Regression with Temperature Type, Day type, Season being the important predicting parameters.\n1.1 Problem Statement: The objective is to forecast bike rental demand of Bike sharing program in Washington, D.C based on historical usage patterns in relation with weather, environment and other data. We would be interested in predicting the rentals on various factors including season, temperature, weather and building a model that can successfully predict the number of rentals on relevant factors.\n1.2 Data This dataset contains the seasonal and weekly count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding temperature and humidity information. Bike sharing systems are a new way of traditional bike rentals. The wohle process from memberhsip to rental and retrun back has become automatic. The data was generated by 500 bike-sharing programs and was collected by the Laboratory of Artificial Intelligence and Decision Support (LIAAD), University of Porto. Given below is the description of the data which is a (731, 16) shaped data, The variables are:\nweathersit: 1: Clear, Few clouds, Partly cloudy, 2: Mist and Cloudy, Mist and Broken clouds, Mist and Few clouds, Mist 3: Light Snow, Light Rain and Thunderstorm and Scattered clouds, Light Rain an Scattered clouds 4: Heavy Rain and Ice Pallets and Thunderstorm and Mist, Snow and Fog instant: record index dteday: date season: season (1:spring, 2:summer, 3:fall, 4:winter) yr: year (0: 2011, 1:2012) mnth: month ( 1 to 12) holiday: weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule) weekday: day of the week workingday: if day is neither weekend nor holiday is 1, otherwise is 0. temp: Normalized temperature in Celsius. The values are divided to 41 (max) atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max) hum: Normalized humidity. The values are divided to 100 (max) windspeed: Normalized wind speed. The values are divided to 67 (max) casual: count of casual users registered: count of registered users cnt: count of total rental bikes including both casual and registered\nI have used a combination of regression methods and several hypothesis testing to validate relevant features for the model. The model achieved 90% Accuracy in prediction using Linear Regression.\n'], 'url_profile': 'https://github.com/Kvenu012', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 7, 2020', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Updated Jan 7, 2020', 'R', 'Updated Feb 21, 2020']}","{'location': 'Bay Area, California ', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashudeo', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 7, 2020', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Updated Jan 7, 2020', 'R', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['House-price-prediction\nThis project is based on predicting house price of a given data set which i have downloaded from kaggle competition. (link : https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n'], 'url_profile': 'https://github.com/jd-215', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 7, 2020', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Updated Jan 7, 2020', 'R', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['mid_sized_car\n'], 'url_profile': 'https://github.com/salkenov1', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 7, 2020', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Updated Jan 7, 2020', 'R', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['DeftEval-2020-SemEval-2020---Task-6\n'], 'url_profile': 'https://github.com/MahmoudAlyy', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 7, 2020', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Updated Jan 7, 2020', 'R', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jdiego-miyashiro', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 7, 2020', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Updated Jan 7, 2020', 'R', 'Updated Feb 21, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Area Lookup\n| http://dq-27.herokuapp.com/us_places\n""How much impact would McDonalds, Starbucks have on real estate values?""\nIn addition to project for finding optimal area to live in terms of price and local amenties,\nI attempted to go in depth in relationship between the price of real estate and its nearby places.\n\n\nUtilizing database from: Zillow and Google GEO API,\nWebscraping crime data from : https://www.bestplaces.net/crime/\nand population density of zipcodes from : http://zipatlas.com/us/zip-code-comparison/population-density.1.htm\nThen displays the relationship between real estate values and local amenities.\nBelow is example of inputs: (For more Pre-gathered results of major cities (http://dq-27.herokuapp.com/us_places/demo))\n\n\nTools: Python, Flask, Pandas, statsmodel (linear regression)\n'], 'url_profile': 'https://github.com/yundk7', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 7, 2020', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Updated Jan 7, 2020', 'R', 'Updated Feb 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['These 3 examples are tutorials I wrote as part of my TA responsibilities for QBS 108: Applied Machine Learning, including logistic regression, regularization, and gradient descent.\nThese demos also demonstrate the utility of sklearn functions in performing simple ML tasks, such as the ones presented here.\n'], 'url_profile': 'https://github.com/bricard1', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 7, 2020', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Updated Jan 7, 2020', 'R', 'Updated Feb 21, 2020']}","{'location': 'Boston, NYC, Seattle, London', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Data-Science-Forecasting-Team-Project\nForecasting optimal stock inventory levels using regression and decision tree classifier techniques through sklearn, numpy, matplot, and pandas python libraries. Code snippets and an outline following the different steps involved when training, testing, and evaluating our models.\n\n'], 'url_profile': 'https://github.com/nomuka-luehr', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 7, 2020', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Updated Jan 7, 2020', 'R', 'Updated Feb 21, 2020']}","{'location': 'San Francisco ', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['AdInteraction\nUtilized K-NN, SVM, and Logistic Regression to predict the likeliness an individual would interact with an Ad and achieved up to 97.6% accuracy. Also used variable ranking to determine useful variables.\n'], 'url_profile': 'https://github.com/aicasas', 'info_list': ['Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 1, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 7, 2020', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 20, 2020', 'Updated Jan 7, 2020', 'R', 'Updated Feb 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['These 3 examples are tutorials I wrote as part of my TA responsibilities for QBS 108: Applied Machine Learning, including logistic regression, regularization, and gradient descent.\nThese demos also demonstrate the utility of sklearn functions in performing simple ML tasks, such as the ones presented here.\n'], 'url_profile': 'https://github.com/bricard1', 'info_list': ['Jupyter Notebook', 'Updated Jul 20, 2020', '1', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', '2', 'Jupyter Notebook', 'Updated Sep 10, 2020']}","{'location': 'Dallas, Texas', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['Airfare prediction with SouthWest Airlines entry in a new route.\nPredicting the airfares and changes in airfares with SouthWest Airlines entry in a new route. The key question what I have tried to answer\nis: Whether the presence or absence of Southwest Airlines (a low-cost entrant) would have any effect on fare?\nTable of Contents\n\nGeneral Info\nVariable Description\nScreenshots\nTechnologies and Methods\nStates\nContact\n\nGeneral Info\nThe data set contains the real data that were collected between Q3-1996 and Q2-1997. Data Cleanup was done initially to ensure that\nthere are no NULL records in the dataset. After ensuring proper data in the data set, Exploratory Data Analysis was done to understand\nthe data distribution and the correlation between variables. Linear Regression with subset selection process was used on the data set to\npredict the air fares.\nVariable Description\nThe data set contains 18 variables. The variable names and the description is mentioned below:\n\nS_CODE:   Starting airport’s code\nS_CITY:   Starting city\nE_CODE:   Ending airport’s code\nE_CITY:   Ending city\nCOUPON:   Average number of coupons for the route\nNEW:      Number of new carriers entering that route between Q3-96 and Q2-97\nVACATION: Whether (Yes) or not (No) a vacation route\nSW:       Whether (Yes) or not (No) Southwest Airlines serves that route\nHI:       Herfindahl index, a measure of market concentration (higher number means smaller number of available carriers on that route)\nS_INCOME: Starting city’s average personal income\nE_INCOME: Ending city’s average personal income\nS_POP:    Starting city’s population\nE_POP:    Ending city’s population\nSLOT:     Whether or not either endpoint airport is slot-controlled\nGATE:     Whether or not either endpoint airport has gate constraints\nDISTANCE: Distance between two endpoint airports in miles\nPAX:      Number of passengers on that route during period of data collection\nFARE:     Average fare on that route\n\nScreenshots\nThe below screenshot shows the correlation between variables. Few of the major observations are mentioned below:\n\n\n1 - It can be seen that FARE has highest positive correlation with DISTANCE.\nIt would mean that with increase in distance, the FARE is going to increase.\n2 - DISTANCE has a strong positive correlation between COUPON.\nIt means that, if distance between two points is more, then it is likely that there will be more coupons for that route.\n3 - DISTANCE has the high negative correlation with HI.\nIt can mean that, if distance between two points is less, then there would be lesser flights opertaing and so the HI index would be more.**\n4 - COUPON has the highest positive correlation with DISTANCE.\nIt would mean that if distance is more, then there is a possibility that there will be more coupons for that route.\n5 - COUPON has high negative correlation with HI.\nIt would mean that if a route has lesser flights, then the HI index would be more and coupons for that route will be less.\n\nTechnologies and Methods\n\nR-Studios\nMicrosoft Excel\nLinear Reression\nSubset Selection Process (Regsubsets and StepAIC)\n\nStatus\nProject is: finished\nContact\nIf you loved what you read here and feel like we can collaborate to produce some exciting stuff, or if you just want to shoot a question,\nplease feel free to connect with me on LinkedIn.\n'], 'url_profile': 'https://github.com/Sarthak-Mohapatra', 'info_list': ['Jupyter Notebook', 'Updated Jul 20, 2020', '1', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', '2', 'Jupyter Notebook', 'Updated Sep 10, 2020']}","{'location': 'Sun Prairie, WI', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Engy-22', 'info_list': ['Jupyter Notebook', 'Updated Jul 20, 2020', '1', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', '2', 'Jupyter Notebook', 'Updated Sep 10, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DarshanaBhute', 'info_list': ['Jupyter Notebook', 'Updated Jul 20, 2020', '1', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', '2', 'Jupyter Notebook', 'Updated Sep 10, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': [""Quora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.\nOver 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\nCredits: Kaggle\nProblem Statement\nIdentify which questions asked on Quora are duplicates of questions that have already been asked.\nThis could be useful to instantly provide answers to questions that have already been answered.\nWe are tasked with predicting whether a pair of questions are duplicates or not.\n1.2 Sources/Useful Links\nSource : https://www.kaggle.com/c/quora-question-pairs\nUseful Links\nDiscussions : https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb/comments\nKaggle Winning Solution and other approaches: https://www.dropbox.com/sh/93968nfnrzh8bp5/AACZdtsApc1QSTQc7X0H3QZ5a?dl=0\nBlog 1 : https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning\nBlog 2 : https://towardsdatascience.com/identifying-duplicate-questions-on-quora-top-12-on-kaggle-4c1cf93f1c30\nThe data along with models and matrices are to be found in this link\nhttps://drive.google.com/open?id=1OWZoiQDvAvgOa-IUnEQ-6QKSEp_pw1XO\n""], 'url_profile': 'https://github.com/SamratSengupta', 'info_list': ['Jupyter Notebook', 'Updated Jul 20, 2020', '1', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', '2', 'Jupyter Notebook', 'Updated Sep 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Credit-Default-Risk-Assessment\nPredicted credit worthiness using Decision trees, Random Forest, Logistic regression, SVM, K-NN, XGBoost. This is a Binary Classification Problem with Hyper parameter tuning. Target Variable Value is 0 for Non-Defaulter and 1 for Defaulter.Identified best model for Risk Assessment using ROC AUC.\n'], 'url_profile': 'https://github.com/deepa-code', 'info_list': ['Jupyter Notebook', 'Updated Jul 20, 2020', '1', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', '2', 'Jupyter Notebook', 'Updated Sep 10, 2020']}","{'location': 'Dublin, Ireland', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Stock-Market-Analysis-using-Python\nA python based terminal interface to analyze the real-time performance of 3000+ company securities listed on NASDAQ. An extensive set of modules based on Pandas, NumPy, Matplotlib and Seaborn libraries. Also deployed the prediction of stock prices using Linear Regression.\n'], 'url_profile': 'https://github.com/PoohTheFervidLearner', 'info_list': ['Jupyter Notebook', 'Updated Jul 20, 2020', '1', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', '2', 'Jupyter Notebook', 'Updated Sep 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Loan-status-prediction\nUpdated 2 minutes ago  Data preprocessing was done-Handled the missing values by replacing with mean and for char replaced with nearer one ,standardization and dummification was done. Data visualization was done. Logistic regression was applied. Accuracy was 83.7 predicted for test dataset and saved into a new csv file.\n'], 'url_profile': 'https://github.com/BOLLOJUAISHWARYA', 'info_list': ['Jupyter Notebook', 'Updated Jul 20, 2020', '1', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', '2', 'Jupyter Notebook', 'Updated Sep 10, 2020']}","{'location': 'Indore, Madhya Pradesh, India', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['car_price_prediction\nLinear regression algorithm was used in this project to predict the selling price of car. In this we, predict the price at which car will sell based on previous prices. The train.csv file provides the previous selling data for different brands and also determines the profit or loss if we sell the cars of a brand in future. The error rate is reduced to around 15% depending on the data for training the model.\n'], 'url_profile': 'https://github.com/piyushgangrade', 'info_list': ['Jupyter Notebook', 'Updated Jul 20, 2020', '1', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', '2', 'Jupyter Notebook', 'Updated Sep 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '678 contributions\n        in the last year', 'description': ['CMPE257FinalProject-NELA-GT-2018\nGoogle drive project colab location\nhttps://drive.google.com/drive/u/3/folders/1UI2BgfNbLYIYp1qwSAJ5aSNCCrvgrdAr\nDatasets\nNELA-GT-2018 data set - https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ULHLCB\nNews Coverage\nhttps://www.kaggle.com/aashita/nyt-comments\nhttps://www.kaggle.com/astoeckl/newsen\nhttps://www.kaggle.com/rmisra/news-category-dataset\nhttps://www.kaggle.com/dbs800/global-news-dataset\nSensational Feature\nhttps://www.thepersuasionrevolution.com/380-high-emotion-persuasive-words/\nNELA-GT-2018: A Large Multi-Labelled News Dataset for the Study of Misinformation in News Articles"". (2019-01-15)\nName: Sudha Amarnath\nStudent ID: 013709956\nBusiness Problem / Data narrative\nNews, Fake News, Misinformation Classification Selected on 194 sources in the NELA-GT-2018 dataset. A number of organizations and platforms have developed methods for assessing reliability and bias of news sources. These organizations come from both the research community and from practitioner communities. While each of these organizations and platforms provide useful assessments on their own, each uses different criteria and methods to make their assessments, and most of these assessments cover relatively few sources. Thus, in order to create a large, centralized set of veracity labels, the collected ground truth (GT) data from eight different sites, which all attempt to assess the reliability and/or the bias of news. These assessment sites are:\nNewsGuard\nPew Research Center\nWikipedia\nOpenSources\nMedia Bias/Fact Check (MBFC)\nAllSides\nBuzzFeed News\nPolitifact\nBased on the labels\\rating provided these provides on differnet sources, fakeness prediction can be made by on the given NELA-GT-2018 using NewsCoverage and Sensationalism features. Results can also be obtained in a modular way by creating import packages of the features classes and also by loading the automatically created PKL file while running the python scripts.\nData Collection\nNELA-GT-2018 data set - https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ULHLCB\nDataset Articles\nThe articles gathered in this dataset is found in an sqlite database. The database has one table name articles. This table has 4 textual columns:\ndate: Date of article in yyyy-mm-dd format.\nsource: Source of article.\nname: Title of article.\ncontent: Clean text content of article.\nThe rows of the article are sorted first with respect to date and then with respect to source.\nThe dataset\'s articles are also provided in plain-text files, with file-structure and file naming convension:\ndate/\nsource/\n----<title>.txt\nDataset Labels\nThe labels of sources are stored in a comma-seperated file labels.csv and in a human-readable format in labels.txt. Each row in the files contain information about a source. The column names use the naming convention <site_name>,<label_name>, where <site_name> is the name of the site providing the label and <label_name> is the name of the particular label. The following lists all columns in the labels files. The columns use different value, which is described below. Note that all columns can also have missing value (no data for that particular source).\nFeature 1 - News Coverage\nThe main idea is to find the integerity of the NELA-GT-2018 dataset topics against a source which could be the actual media like News Papers. There are high chances for the positive corelation when the comparision is done with the more reliable source like the News Channels. For this task, I am considering the News Coverage Datasets from Kaggle.\nhttps://www.kaggle.com/aashita/nyt-comments\nhttps://www.kaggle.com/astoeckl/newsen\nhttps://www.kaggle.com/rmisra/news-category-dataset\nhttps://www.kaggle.com/dbs800/global-news-dataset\nThe NELA-GT-2018 dataset topics span over a year , that are from 2018. Since the above data setup after preprocessing is similar for the coverage in year wise(2018), there could he high chances of co-relation. We then use this feature to perform fakeness classification for the NELA-GT-2018 Data set. Then I used TFIDF Vectorizer and Random Forest algorithm. The Accuracy for this model achieved 55%.\nDifferent approaches to classify text based on the news coverage information. The different approaches are as below\nCountVectorizer\nDoc2Vec Model\nTF-IDF Vectorizer\nThe Performance of these approaches are evaluated based on the accuracy score using the following algorithms.\nMultinomial Naive Bayes\nSVM\nSGD\nRandom Forest\nLogistic Regression\nData Preprocessing\nRemove non-letters/Special Characters and Punctuations\nConvert to lower case\nRemove punctuation\nTokenize\nRemove stop words\nLemmentize\nStemming\nRemove small words of length < 3\nWhat didn\'t work?\nSince the number of rows(700K) are higher in NELA-GT-2018, the time it takes to process the dataset cleaning and running algorithms is considerably in many hours. The distillation of this dataset took around 86 minutes. Most of The News Coverage dataset were not completely available for the year 2018 in a single dataset.\nWhat worked later?\nI am shuffling the original NELA-GT-2018 dataset with a random_state=1000. This would make sure everytime the notebook is run, same shuffeling is retained. Out of 700k+ rows, I\'ll be selecting around 15k rows for the project. The cleaning, doc2vec training of the of the dataset was around 45 minutes. Next I merged 4 datasets that had monthly news information from reputed news sources for the year 2018. This has nearly 40k rows.\nFeature 2 - Sensational Feature Prediction\nWith the close look of the words, and when some of them are combined selectively together, there are cues which would lead to emotions in the way the speaker has said in a certain context. Words when used correctly can transform an “eh whatever” into “wow that’s it!”. Words can make you go from literally ROFL to fuming with fury to an uncontrollable-urge-to-take-action-NOW-or-the-earth-may-stop-swinging -on-its-axis. Highly emotional words are capable capable of transforming an absolute no into almost yes and a “perhaps” into “for sure”! Words that are used:\nWhen you are trying to sell people a solution\nWhen you are trying to get them to take an action (like, share, subscribe, buy)\nWhen you are trying to get people to click and read your article\nWhen you are trying to get someone to agree with you\nI am using a dataset from high emotion persiasive words [ https://www.thepersuasionrevolution.com/380-high-emotion-persuasive-words/ ] where there are 1400+ words that are both positive and negative emotions that will help to predict the sensational score for an article. The data enrichment is done using SentiNet library which provides polarity associated with 50,000 natural language concepts. A polarity is a floating number between -1 and +1. Minus one is extreme negativity, and plus one is extreme positivity. The knowledge base is free. It can be downloaded as XML file. SenticNet 5 reaches 100,000 commonsense concepts by employing recurrent neural networks to infer primitives by lexical substitution.\nMethod used\nBy performing cosine similarity for each news in the NELA-GT-2018 Data set with the Sensational words results in a particular score for each topic. These topics are then given a sensational label based on the 50% sensataional score. For the score above 50% value, the sensational label is predicted as 1 otherwise its 0. Then I used TFIDF Vectorizer and Multinomial Naive Bayes algorithm. The Accuracy for this model achieved to 60%.\nFeature 3 - NelaEncodedLabels\nNewsGuard : Among the following, NewGuard overall_class is itself an encoded_label for the sources. A New column \'newsguard_label\' is amalgamated based on the 0/1 values of overall_class\nDoes not repeatedly publish false content\nGathers and presents information responsibly\nRegularly corrects or clarifies errors\nHandles the difference between news and opinion responsibly\nAvoids deceptive headlines\nWebsite discloses ownership and financing\nClearly labels advertising\nReveals who\'s in charge, including any possible conflicts of interest\nProvides information about content creators\nscore\noverall_class\nPew Research Center : Among the following, Pew Research Center inference columns known_by_40% has a binary value based on the poplarity of the source. A New column \'pewresearch_label\' is amalagamated based on the 0/1 values of known_by_40%\nknown_by_40%\ntotal\nconsistently_liberal\nmostly_liberal\', \'Pew Research Center, mixed\nmostly conservative\nconsistently conservative\',\nWikipedia : Label wikipedia_label is created for 0/1 value if fake its set to 0\nis_fake\nOpen Sources: Among the following, Open Sources inference columns bias has a 1, 2, 3 score based on the bias of the source. A New column \'opensourcebias_label\' is amalagamated based on the bias values 1-3\nreliable\nfake\nunreliable\nbias\nconspiracy\nhate\njunksci\nrumor\nblog\nclickbait\npolitical\nsatire\nstate\nMedia Bias: Media Bias inference columns label has a specific facts on the source. A New column \'mediabias_label\' is amalagamated based on the bias factors [ \'conspiracy_pseudoscience\', \'left_center_bias\', \'left_bias\', \'right_bias\', \'questionable_source\', \'right_center_bias\', \'least_biased\', \'satire\' ]\nlabel\nfactual_reporting\nextreme_left\nright\nextreme_right\npropaganda\nfake_news\nsome_fake_news\nfailed_fact_checks\nconspiracy\npseudoscience\nhate_group\nanti_islam\nnationalism\nAllsides: Among the following, Allsides inference columns community_label has a factors based on the public agreement for the source. A New column \'allsides_label\' is amalagamated based on the values [ \'somewhat agree\', \'somewhat disagree\', \'strongly agree\', \'agree\', \'strongly disagree\', \'absolutely agree\', \'disagree\' ]\nbias_rating\ncommunity_agree\ncommunity_disagree\ncommunity_label\nBuzzFeed: Only one column based on left/right leaning for the source and a new label buzzfeed_label is encoded with binary values\nleaning\nPolitiFact: A new label politificat_label is encoded based on the true/ false counts of these columns for a source.\nPants on Fire!\nFalse\nMostly False\nHalf-True\nMostly True\nTrue\nFor the rows having NaN values, it is retained as it is and not given any inference yet.\nModular Approach\nModular approach is being considered now for the team in a centralized directory. Sensational Feature is integrated in assignment 2 Separate functions have been included for the features NewsCoverage() Class is defined based on TFIDF Vectorizer and Multinomial Naive Bayes algorithm to easily predict the fakeness. SensationalPrediction() Class is defined using TFIDF Vectorizer and Multinomial Naive Bayes algorithm to easily predict the fakeness. NelaEncodeLabelPrediction() Class is defined using TFIDF Vectorizer and Multinomial Naive Bayes algorithm to easily predict the fakeness.\nRedefined the NewsCoverage() and SensationalPrediction() classes. Changed the algorithm for NewsCoverage Prediction to use the top document match from doc2vector output. For the NewsCoverage() Class Object pickle file is created at ../models/newscoverage_feature.pkl For the SensationalPrediction() Class Object pickle file is created at ../models/sensational_feature.pkl NelaEncodeLabelPrediction() Class Object pickle file is created at ../models/sensational_feature.pkl All the data sets and Models are located in AlternusVeraDataSets2019/FinalExam/Spartans/Sudha/input_data The Models are located in AlternusVeraDataSets2019/FinalExam/Spartans/Sudha/models Pickle load the NewsCoverage() Class Object and test the train_news for prediction Pickle load the SensationalPrediction() Class Object and test the train_news prediction Pickle load the NelaEncodedLabelPrediction() Class Object and test the train_news prediction\nNew python files are created in directory ./classes. init.py file defined for class imports NewsCoverage.py is defined for News Coverage Feature SensationalPrediction.py is defined for Sensational Prediction Feature Pickle models are saved when the script is run (guarded in main) Import class packages Instantiate class object Verify train set clean Prediction Probabilites are defined in the respective classes Checking Prediction score Calcualtion of polynomial eqation for the 3 features Performance Analysis.\nMachine Learning Life-cycle\n1. Configuration of the System : Iterative, Notebook, code structure, data, where will it reside, folders, cloud buckets etc.\n2. Data Collection : initial Data Set\n3. Set Data Narrative : Set Business Objectives, what use case are you solving for\n4. Exploratory Data Analysis and Visualization\n\nfeature analysis and engineering (for ML, for DL it\'s feature extraction)\nAnalyze data\nVisualize data\nRun Stats: mean, median, mode, correlation, variance\n.cor\npairplot()\ngini score\nfeature_importance with xgboost\n\n5. Data Prep: Curation\n\nFeature Selection and Extraction : what are the main features to use in this data set?\nData Verification: Do we have enough data?\nPossibility of Amalgamation1: Add Dataset 2\nData Cleansing\nData Regularization\nData Normalization\n\n6. Unsupervised Exploration : Find relevant Clusters in Your Data\n\nHow many clusters? Explore different k’s…\nSelect Clustering algorithms, run several and compare in a table\nWhat does each cluster mean? How do they contribute to your Data Narrative (Story)\nMeasure goodness of your clusters (e.g., BICs)\n\n7. Supervised Training Preparation: Data Curation : label your data set\n\nClassify Your Data Sets : Run different classification algorithms\nMeasure Classification Success\nWhat regression objectives should we have? Complete your , add to your Data Story\nRun Regressions using various algorithmsv5. Measure Success of Regressions and\nCompare Regressions in a table\n\n8. Metrics and Evaluation\n\nF1, R2, RMSE,\nPrecision, Recall, Accuracy\nConfusion Matrix\nOther metric as applicable to your project\n\n9. Distillation\n1.Entity Identification\n2.Customer Rank\n3.Sentiment\n4.Topic Modeling\nConclusion\nAmalgamted NELA dataset consists of Doc2Vec inferred vector values of the NewsCoverage dataset, SensationalScores and NelaEncodedLabels\nTFIDF Multinomial Naive Bayes Algorithm was selected for the 3 features-\nNelaEncodedLabel accuracy: 58%\nNewsCoverageFeature - accuracy: 48%\nSensational Feature - accuracy: 53%\nPerformance analysis of the valid news for Nela encoded label:\ntruePos= 1845\ntrueNeg= 640\nfalsePos= 68\nfalseNeg= 514\nignored= 917\naccuracy= 78%\nPolynomial score of the 3 fearures came up to 63%\nFor a modular approach 3 classes were created:\nNewsCoverage.py\nSensationalPrediction.py\nNelaEncodedLabelPrediction.py\nImport packages were created for all these 3 classes.\nGuard function was defined in the classes to create an instance to save the pkl files.\nImport of the classes was done to directly to retun accuracy and predicted probability and input name. Pkl files were created when the python scripts were run without the imports.\n'], 'url_profile': 'https://github.com/SudhaAmarnath', 'info_list': ['Jupyter Notebook', 'Updated Jul 20, 2020', '1', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Sep 10, 2019', 'Jupyter Notebook', 'Updated Jan 4, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', '2', 'Jupyter Notebook', 'Updated Sep 10, 2020']}"
"{'location': 'Hyderabad , India', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['Autombile-CAR-PRICE-PREDICTION-\n'], 'url_profile': 'https://github.com/DheerajPranav', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', '1', 'R', 'Updated Jan 2, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['German-Traffic-Sign-Classification\nThis project aims to bring out an in depth comparison among existing image classification models on German Traffic Sign benchmark (GTSRB) images. In this project famous machine learning algorithms like Logistic Regression, SVMs, MLPs etc. and deep learning models like ResNet and CNNs have been implemented. Apart from this we also employed ensemble learning algorithm like Random Forest to get clear comparsion based on evaluation metrics like ROC Curves, Precision, Accuracies etc. Transfer Learning has also been implemented by extracting features of CNN and using them in other models.\n\ufeffReadme file\n\nAll the preprocessed dataset is available Dataset and pickle files of trained models.\nData0.pickle contains the preprocessed  RGB data in which each image is of size 32x32x3, and shuffled.\nData5.pickle contains the preprocessed Grayscale image in which each image is of size 32x32x1, with Shuffling, Local Histogram Equalization.\nData3.pickle contains the preprocessed RGB images which are normalized.\nFile1.py uses Data5.pickle, which comprises of CNN model, SVM (rbf, linear, and poly with degree =3), Logistic, MultiLayer Perceptron, Random Forest with feature extracted from the CNN model last second Fc layer.\nFile1.py also contains the models like Multilayer Perceptron, Random Forest, SVM(all the three mentioned above) which trained on the image vector i.e. by directly flattening the image matrix(Grayscale).\nFile2.py uses Data5.pickle and in this we have performed the feature extraction with help HOG as a feature extractor, on  SVM (rbf, linear, and poly with degree =3), Logistic, MultiLayer Perceptron, Random Forest.\nFile3.py uses Data0.pickle and in this we have used pretrained model of RESNET50 import from torchvision and further did the transfer learning according to our model.\nFile4.py uses Data3.pickle and in this we have used pretrained model of RESNET50 on the normalized RGB dataset.\n\n'], 'url_profile': 'https://github.com/karamveer17156', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', '1', 'R', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vivianluckyzhou', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', '1', 'R', 'Updated Jan 2, 2020']}","{'location': 'Kolkata,India', 'stats_list': [], 'contributions': '612 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/khanfarhan10', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', '1', 'R', 'Updated Jan 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Analytics-Class-Projects\nIn the reposition please find relevant class projects where is use algorithms I have learnt on fake data sets.\nIncludes but not limited to: Neural Networks, Clutering (Kmeans, BIRCH), Trees (CART & C4.5), KNN, Regressions (linear, multilinear and logistic), Cost Benefit Analysis, Dimension Reduction, Time Series Analysis, Text Minning\n'], 'url_profile': 'https://github.com/zachamar', 'info_list': ['Jupyter Notebook', 'Updated Jan 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'MATLAB', 'Updated Jan 5, 2020', '1', 'R', 'Updated Jan 2, 2020']}",,,,,
