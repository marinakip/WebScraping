"{'location': 'New York', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['Stochastic Gradient Descent From Scratch\nThis notebook illustrates the nature of the Stochastic Gradient Descent (SGD) and walks through all the necessary steps to create SGD from scratch in Python. Gradient Descent is an essential part of many machine learning algorithms, including neural networks. To understand how it works you will need some basic math and logical thinking. Though a stronger math background would be preferable to understand derivatives, I will try to explain them as simple as possible.\nWe will work with the California housing dataset and perform a linear regression to predict apartment prices based on the median income in the block. We will start from the simple linear regression and gradually finish with Stochastic Gradient Descent. So let\'s get started.\nImporting Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import california_housing\nfrom sklearn.metrics import mean_squared_error\nCalifornia Housing Dataset\nScikit-learn comes with wide variety of datasets for regression, classification and other problems. Lets load our data into pandas dataframe and take a look.\nhousing_data = california_housing.fetch_california_housing()\nFeatures = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\nTarget = pd.DataFrame(housing_data.target, columns=[\'Target\'])\ndf = Features.join(Target)\nFeatures as MedInc and Target were scaled to some degree.\ndf.corr()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nTarget\n\n\n\n\nMedInc\n1.000000\n-0.119034\n0.326895\n-0.062040\n0.004834\n0.018766\n-0.079809\n-0.015176\n0.688075\n\n\nHouseAge\n-0.119034\n1.000000\n-0.153277\n-0.077747\n-0.296244\n0.013191\n0.011173\n-0.108197\n0.105623\n\n\nAveRooms\n0.326895\n-0.153277\n1.000000\n0.847621\n-0.072213\n-0.004852\n0.106389\n-0.027540\n0.151948\n\n\nAveBedrms\n-0.062040\n-0.077747\n0.847621\n1.000000\n-0.066197\n-0.006181\n0.069721\n0.013344\n-0.046701\n\n\nPopulation\n0.004834\n-0.296244\n-0.072213\n-0.066197\n1.000000\n0.069863\n-0.108785\n0.099773\n-0.024650\n\n\nAveOccup\n0.018766\n0.013191\n-0.004852\n-0.006181\n0.069863\n1.000000\n0.002366\n0.002476\n-0.023737\n\n\nLatitude\n-0.079809\n0.011173\n0.106389\n0.069721\n-0.108785\n0.002366\n1.000000\n-0.924664\n-0.144160\n\n\nLongitude\n-0.015176\n-0.108197\n-0.027540\n0.013344\n0.099773\n0.002476\n-0.924664\n1.000000\n-0.045967\n\n\nTarget\n0.688075\n0.105623\n0.151948\n-0.046701\n-0.024650\n-0.023737\n-0.144160\n-0.045967\n1.000000\n\n\n\n\nPreprocessing: Removing Outliers and Scaling\ndf[[\'MedInc\', \'Target\']].describe()[1:] #.style.highlight_max(axis=0)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nMedInc\nTarget\n\n\n\n\nmean\n3.482030\n1.722805\n\n\nstd\n1.364922\n0.749957\n\n\nmin\n0.499900\n0.149990\n\n\n25%\n2.452025\n1.119000\n\n\n50%\n3.303600\n1.635000\n\n\n75%\n4.346050\n2.256000\n\n\nmax\n7.988700\n3.499000\n\n\n\n\nIt seems that Target has some outliers (as well as MedInc), because 75% of the data has price less than 2.65, but maximum price go as high as 5. We\'re going to remove extremely expensive houses as they will add unnecessary noize to the data.\ndf = df[df.Target < 3.5]\ndf = df[df.MedInc < 8]\nRemoved Outliers\ndf[[\'MedInc\', \'Target\']].describe()[1:]\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nMedInc\nTarget\n\n\n\n\nmean\n3.482030\n1.722805\n\n\nstd\n1.364922\n0.749957\n\n\nmin\n0.499900\n0.149990\n\n\n25%\n2.452025\n1.119000\n\n\n50%\n3.303600\n1.635000\n\n\n75%\n4.346050\n2.256000\n\n\nmax\n7.988700\n3.499000\n\n\n\n\nWe will also scale MedInc and Target variables to [0-1].\ndef scale(x):\n    min = x.min()\n    max = x.max()\n    return pd.Series([(i - min)/(max - min) for i in x])\n\nX = scale(df.MedInc)\ny = scale(df.Target)\nX.max(), y.max() # features are scaled now\n(1.0, 1.0)\n\nCorrelation Between Price and Income\nVisually we can determine what kind of accuracy we can expect from the models.\nplt.figure(figsize=(16,6))\nplt.rcParams[\'figure.dpi\'] = 227\nplt.style.use(\'seaborn-whitegrid\')\nplt.scatter(X, y, label=\'Data\', c=\'#388fd8\', s=6)\nplt.title(\'Positive Correlation Between Income and House Price\', fontSize=15)\nplt.xlabel(\'Income\', fontSize=12)\nplt.ylabel(\'House Price\', fontSize=12)\nplt.legend(frameon=True, loc=1, fontsize=10, borderpad=.6)\nplt.tick_params(direction=\'out\', length=6, color=\'#a0a0a0\', width=1, grid_alpha=.6)\nplt.show()\n\nData is quite sparse, but we can still observe some linearity.\nSimple Linear Regression\nSimple linear regression can be described by only two parameters: slope m and intercept b, where x is our median income. Lets take a look at the formulas below:\n$$\\hat{y} = mx + b$$\n$$m = \\frac{\\overline{x}\\overline{y}-\\overline{xy}}{(\\overline{x})^2 - \\overline{x^2}} \\quad \\textrm{and} \\quad  b = y-mx$$\nIf we want to add some other features, like size of the apartment, our formula would look like this: $\\hat{y} = m_1x_1 + m_2x_2 + b$, where $m_1$ and $m_2$ are slopes for each feature $x_1$ and $x_2$. In this case we would call it multiple linear regression, but we could no longer use formulas above.\nclass SimpleLinearRegression:\n        \n    def fit(self, X, y):\n        self.X = X\n        self.y = y\n        self.m = ((np.mean(X) * np.mean(y) - np.mean(X*y)) / ((np.mean(X)**2) - np.mean(X**2)))\n        self.b = np.mean(y) - self.m * np.mean(X)\n    \n    def coeffs(self):\n        return self.m, self.b\n    \n    def predict(self):\n        self.y_pred = self.m * self.X + self.b\n        return self.y_pred\n    \n    def r_squared(self):\n        self.y_mean = np.full((len(self.y)), mean(self.y))\n        err_reg = sum((self.y - self.y_pred)**2)\n        err_y_mean = sum((self.y - self.y_mean)**2)\n        return (1 - (err_reg/err_y_mean))\ndef plot_regression(X, y, y_pred, log=None, title=""Linear Regression""):\n    \n    plt.figure(figsize=(16,6))\n    plt.rcParams[\'figure.dpi\'] = 227\n    plt.scatter(X, y, label=\'Data\', c=\'#388fd8\', s=6)\n    if log != None:\n        for i in range(len(log)):\n            plt.plot(X, log[i][0]*X + log[i][1], lw=1, c=\'#caa727\', alpha=0.35)\n    plt.plot(X, y_pred, c=\'#ff7702\', lw=3, label=\'Regression\')\n    plt.title(title, fontSize=14)\n    plt.xlabel(\'Income\', fontSize=11)\n    plt.ylabel(\'Price\', fontSize=11)\n    plt.legend(frameon=True, loc=1, fontsize=10, borderpad=.6)\n    plt.tick_params(direction=\'out\', length=6, color=\'#a0a0a0\', width=1, grid_alpha=.6)\n    plt.show()\nX = df.MedInc\ny = df.Target\nlr = SimpleLinearRegression()\nlr.fit(X, y)\ny_pred = lr.predict()\nprint(""MSE:"",mean_squared_error(y, y_pred))\nplot_regression(X, y, y_pred, title=""Linear Regression"")\nMSE: 0.34320521502255963\n\n\nResult of our model is the regression line. Just by looking at the graph we can tell that data points go well above and beyond our line, making predictions approximate.\nMultiple Linear Regression with Least Squares\nSimilar to from sklearn.linear_model import LinearRegression, we can calculate coefficients with Least Squares method. Numpy can calculate this formula almost instantly (of course depends on the amount of data) and precise.\n$$ m =(A^TA)^{-1} A^Ty $$\n$$m - parameters, : A - data, : y - target$$\nX = df.drop(\'Target\', axis=1) # matrix A, or all the features\ny = df.Target\nclass MultipleLinearRegression:\n    \'\'\'\n    Multiple Linear Regression with Least Squares    \n    \'\'\'    \n    def fit(self, X, y):\n        X = np.array(X)\n        y = np.array(y)\n        self.coeffs = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n        \n    def predict(self, X):\n        X = np.array(X)\n        result = np.zeros(len(X))\n        for i in range(X.shape[1]):\n            result += X[:, i] * self.coeffs[i]\n        return result\n    \n    def coeffs(self):\n        return self.coeffs\nmlp = MultipleLinearRegression()\nmlp.fit(X, y)\ny_pred = mlp.predict(X)\nmean_squared_error(y, y_pred)\n0.2912984534321039\n\nGradient Descent\nAbstract\nThe idea behind gradient descent is simple - by gradually tuning parameters, such as slope (m) and the intercept (b) in our regression function y = mx + b, we minimize cost.\nBy cost, we usually mean some kind of a function that tells us how far off our model predicted result. For regression problems we often use mean squared error (MSE) cost function. If we use gradient descent for the classification problem, we will have a different set of parameters to tune.\n$$ MSE = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 \\quad \\textrm{where} \\quad \\hat{y_i} = mx_i + b $$\nNow we have to figure out how to tweak parameters m and b to reduce MSE.\nPartial Derivatives\nWe use partial derivatives to find how each individual parameter affects MSE, so that\'s where word partial comes from. In simple words, we take the derivative with respect to m and b separately. Take a look at the formula below. It looks almost exactly the same as MSE, but this time we added f(m, b) to it. It essentially changes nothing, except now we can plug m and b numbers into it and calculate the result.\n$$ùëì(ùëö,ùëè)= \\frac{1}{n}\\sum_{i=1}^{n}(y_i - (mx_i+b))^2$$\nThis formula (or better say function) is better representation for further calculations of partial derivatives. We can ignore sum for now and what comes before that and focus only on $y - (mx + b)^2$.\nPartical Derivative With Respect to m\nWith respect to m means we derive parameter m and basically ignore what is going on with b, or we can say its 0. To derive with respect to m we will use chain rule.\n$$ [f(g(x))]\' = f\'(g(x)) * g(x)\' : - \\textrm{chain rule}$$\nChain rule applies when one function sits inside of another. If you\'re new to this, you\'d be surprised that $()^2$ is outside function, and $y-(\\boldsymbol{m}x+b)$ sits inside it. So, the chain rule says that we should take a derivative of outside function, keep inside function unchanged and then multiply by derivative of the inside function. Lets write these steps down:\n$$ (y - (mx + b))^2 $$\n\nDerivative of $()^2$ is $2()$, same as $x^2$ becomes $2x$\nWe do nothing with $y - (mx + b)$, so it stays the same\nDerivative of $y - (mx + b)$ with respect to m is $(0 - (x + 0))$ or $-x$, because y and b are constants, they become 0, and derivative of mx is x\n\nMultiply all parts we get following: $2 * (y - (mx+b)) * -x$.\nLooks nicer if we move -x to the left: $-2x *(y-(mx+b))$. There we have it. The final version of our derivative is the following:\n$$\\frac{\\partial f}{\\partial m} = \\frac{1}{n}\\sum_{i=1}^{n}-2x_i(y_i - (mx_i+b))$$\nHere, $\\frac{df}{dm}$ means we find partial derivative of function f (we mentioned it earlier) with respect to m. We plug our derivative to the summation and we\'re done.\nPartical Derivative With Respect to b\nSame rules apply to the derivative with respect to b.\n\n$()^2$ becomes $2()$, same as $x^2$ becomes $2x$\n$y - (mx + b)$ stays the same\n$y - (mx + b)$ becomes $(0 - (0 + 1))$ or $-1$, because y and mx are constants, they become 0, and derivative of b is 1\n\nMultiply all the parts together and we get $-2(y-(mx+b))$\n$$\\frac{\\partial f}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^{n}-2(y_i - (mx_i+b))$$\nFinal Function\nFew details we should discuss befor jumping into code:\n\nGradient descent is an iterative process and with each iteration (epoch) we slightly minimizing MSE, so each time we use our derived functions to update parameters m and b\nBecause its iterative, we should choose how many iterations we take, or make algorithm stop when we approach minima of MSE. In other words when algorithm is no longer improving MSE, we know it reached minimum.\nGradient descent has an additional parameter learning rate (lr), which helps control how fast or slow algorithm going towards minima of MSE\n\nThats about it. So you can already understand that Gradient Descent for the most part is just process of taking derivatives and using them over and over to minimize function.\ndef gradient_descent(X, y, lr=0.05, epoch=10):\n    \n    \'\'\'\n    Gradient Descent for a single feature\n    \'\'\'\n    \n    m, b = 0.2, 0.2 # parameters\n    log, mse = [], [] # lists to store learning process\n    N = len(X) # number of samples\n    \n    for _ in range(epoch):\n                \n        f = y - (m*X + b)\n    \n        # Updating m and b\n        m -= lr * (-2 * X.dot(f).sum() / N)\n        b -= lr * (-2 * f.sum() / N)\n        \n        log.append((m, b))\n        mse.append(mean_squared_error(y, (m*X + b)))        \n    \n    return m, b, log, mse\nPredicting House Price With Gradient Descent\nX = df.MedInc\ny = df.Target\n\nm, b, log, mse = gradient_descent(X, y, lr=0.01, epoch=100)\n\ny_pred = m*X + b\n\nprint(""MSE:"",mean_squared_error(y, y_pred))\nplot_regression(X, y, y_pred, log=log, title=""Linear Regression with Gradient Descent"")\n\nplt.figure(figsize=(16,3))\nplt.rcParams[\'figure.dpi\'] = 227\nplt.plot(range(len(mse)), mse)\nplt.title(\'Gradient Descent Optimization\', fontSize=14)\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'MSE\')\nplt.show()\nMSE: 0.3493097403876614\n\n\n\nStochastic Gradient Descent\nStochastic Gradient Descent works almost the same as Gradient Descent (also called Batch Gradient Descent), but instead of training on entire dataset, it picks only one sample to update m and b parameters, which makes it much faster. In the function below I made possible to change sample size (batch_size), because sometimes its better to use more than one sample at a time.\ndef SGD(X, y, lr=0.05, epoch=10, batch_size=1):\n        \n    \'\'\'\n    Stochastic Gradient Descent for a single feature\n    \'\'\'\n    \n    m, b = 0.5, 0.5 # initial parameters\n    log, mse = [], [] # lists to store learning process\n    \n    for _ in range(epoch):\n        \n        indexes = np.random.randint(0, len(X), batch_size) # random sample\n        \n        Xs = np.take(X, indexes)\n        ys = np.take(y, indexes)\n        N = len(Xs)\n        \n        f = ys - (m*Xs + b)\n        \n        # Updating parameters m and b\n        m -= lr * (-2 * Xs.dot(f).sum() / N)\n        b -= lr * (-2 * f.sum() / N)\n        \n        log.append((m, b))\n        mse.append(mean_squared_error(y, m*X+b))        \n    \n    return m, b, log, mse\nm, b, log, mse = SGD(X, y, lr=0.01, epoch=100, batch_size=2)\ny_pred = m*X + b\n\nprint(""MSE:"",mean_squared_error(y, y_pred))\nplot_regression(X, y, y_pred, log=log, title=""Linear Regression with SGD"")\n\nplt.figure(figsize=(16,3))\nplt.rcParams[\'figure.dpi\'] = 227\nplt.plot(range(len(mse)), mse)\nplt.title(\'SGD Optimization\', fontSize=14)\nplt.xlabel(\'Epochs\', fontSize=11)\nplt.ylabel(\'MSE\', fontSize=11)\nplt.show()\nMSE: 0.3462919845446769\n\n\n\nWe can observe how regression line went up and down to find right parameters and MSE not as smooth as regular gradient descent.\nSpeed Test for Gradient Descent vs SGD\nX = df.MedInc\ny = df.Target\nX = np.concatenate((X,X,X,X,X,X,X,X,X,X,X,X,X,X,X,X,X))\ny = np.concatenate((y,y,y,y,y,y,y,y,y,y,y,y,y,y,y,y,y))\nX.shape, y.shape\n((304946,), (304946,))\n\n%timeit SGD(X, y, lr=0.01, epoch=1000, batch_size=1)\n1.22 s ¬± 8.95 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n%timeit gradient_descent(X, y, lr=0.01, epoch=1000)\n2.02 s ¬± 79.4 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\nConclusion\n\nSGD is twice as fast as Gradient Descent (also called Batch Gradient Descent)\nOn sparse data, we can increase the batch size to speed up learning process. It\'s not a pure form of SGD, but we can call it a mini-batch SGD\nSmaller learning rate helps to prevent overfitting but can be adjusted accordingly\n\n'], 'url_profile': 'https://github.com/arseniyturin', 'info_list': ['11', 'Jupyter Notebook', 'Updated Nov 14, 2020', '10', 'MATLAB', 'MIT license', 'Updated Jun 30, 2020', '3', 'TypeScript', 'MIT license', 'Updated Feb 7, 2021', '26', 'R', 'Updated Feb 23, 2021', '11', 'Jupyter Notebook', 'Updated Jan 31, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '7', 'R', 'Updated Aug 19, 2020', '6', 'C++', 'GPL-3.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'Lund', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Deep Ordinal Regression with Label Diversity\n\n\n\n\nThis is the official codebase for the implementation of Deep Ordinal Regression with Label Diversity, accepted to ICPR2020.\nCode author: Axel Berg\nDependencies\n\nMatlab 2020a with the Deep Learning Toolbox\nCuda 10.1\n\nDataset Preparation\nUTKFace Dataset\nDownload the aligned and cropped images from https://susanqq.github.io/UTKFace/ and run age/data/createCroppedUtkDataset.m after setting the correct path to the dataset in the script.\nThis will create datastore objects for reading the images.\nThe train/test split is the same as the one used in ""Coral-CNN"" and can be found here: https://github.com/Raschka-research-group/coral-cnn\nBiwi Dataset\nWe use protocol 2 as described in the FSA-Net paper, which uses a 70-30 train/test split. You can prepare the dataset yourself using the original code from https://github.com/shamangary/FSA-Net and store the train/test splits as .mat-files.\nHistorical Images Dataset\nDownload the dataset from http://graphics.cs.cmu.edu/projects/historicalColor/ and add it set the datapath variable in date/dateParameters.m to point to it.\nTraining and Evaluation\nFor each dataset, run the iteration scripts to train the ResNet50 backbone for 10 iterations and output save the mean average error results on the test set in a text file. All the hyperparameters are the same for each method and are defined in a single function.\nThe following loss functions are supported:\n\nL2 - use the standard L2 (mean squares error) loss. This is the regression baseline\nCE -  use the cross entropy loss. This is the RvC baseline.\nEqual Bins - use the equal bins label diversity method\nRandom Bins - use the random bins label diversity methods.\nMultiple bins (for the HCI dataset) - uses the multiple bins labels diversity method specified in the paper\n\nCitation\nIf you find this work useful, please cite our paper:\n@misc{berg2020deep,\n    title={Deep Ordinal Regression with Label Diversity},\n    author={Axel Berg and Magnus Oskarsson and Mark O\'Connor},\n    year={2020},\n    eprint={2006.15864},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n\nReferences\nW. Cao, V. Mirjalili, and S. Raschka (2019). Consistent rank logits for ordinal regression with convolutional neural networks. arXiv preprint arXiv:1901.07884\nT.-Y. Yang, Y.-T. Chen, Y.-Y. Lin, and Y.-Y. Chuang, (2019) Fsa-net: Learning\nfine-grained structure aggregation for head pose estimation from a single\nimage\n'], 'url_profile': 'https://github.com/axeber01', 'info_list': ['11', 'Jupyter Notebook', 'Updated Nov 14, 2020', '10', 'MATLAB', 'MIT license', 'Updated Jun 30, 2020', '3', 'TypeScript', 'MIT license', 'Updated Feb 7, 2021', '26', 'R', 'Updated Feb 23, 2021', '11', 'Jupyter Notebook', 'Updated Jan 31, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '7', 'R', 'Updated Aug 19, 2020', '6', 'C++', 'GPL-3.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': [""wdio-visual-regression\nVisual regression tool for webdriver.io based on resemble.js. PR's are welcome!\n\n\nFeatures\n\n Default matchers for compare active viewport and elements\n Simple and flexible API for creating custom matcher\n Output report (Jasmine, Mocha, Cucumber) in JSON format (example)\n\nHow to use\n\nInstall the package\n\nnpm install --save-dev wdio-visual-regression\n\nImport VisualRegression class and add it to service option in your webdriverio config file\n\nconst { VisualRegression } = require('wdio-visual-regression');\n\nexports.config = {\n  // other configuration\n    services: [\n        [VisualRegression, {/* options */}]\n    ]\n}\nNote: You can find out all available options here\n\nUse available commands:\n\nbrowser.matchElement(name: string, element: WebdriverIO.Element): Promise<number>\nbrowser.matchViewport(name: string): Promise<number>\nOr create your own custom matcher for comparing anything that you need. See how to do it here\nAlso, you can take a look at example usage here\nGetting Started\nFollow to commands below for start dev environment\ngit clone git@github.com:ennjin/wdio-visual-regression.git\ncd wdio-visual-regression\nnpm ci\nFor start development\nnpm run start\nFor production build\nnpm run build\nRunning the tests\nFor running the tests type command\nnpm run e2e\nBuilt With\n\nnode-canvas - HTML5 canvas implementation for nodejs\nResemble.js - Image analysis and comparison\n\nLicense\nThis project is licensed under the MIT License - see the LICENSE.md file for details\n""], 'url_profile': 'https://github.com/ennjin', 'info_list': ['11', 'Jupyter Notebook', 'Updated Nov 14, 2020', '10', 'MATLAB', 'MIT license', 'Updated Jun 30, 2020', '3', 'TypeScript', 'MIT license', 'Updated Feb 7, 2021', '26', 'R', 'Updated Feb 23, 2021', '11', 'Jupyter Notebook', 'Updated Jan 31, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '7', 'R', 'Updated Aug 19, 2020', '6', 'C++', 'GPL-3.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'Heidelberg, Germany', 'stats_list': [], 'contributions': '733 contributions\n        in the last year', 'description': ['\nglmGamPoi \n\n\nFit Gamma-Poisson Generalized Linear Models Reliably.\n\nPronounciation: d íi …ôl …ôm …°am Ààpw…ë\nThe core design aims of glmGamPoi are:\n\nFit Gamma-Poisson models on arbitrarily large or small datasets\nBe faster than alternative methods, such as DESeq2 or edgeR\nCalculate exact or approximate results based on user preference\nSupport in-memory or on-disk data\nFollow established conventions around tools for RNA-seq analysis\nPresent a simple user-interface\nAvoid unnecessary dependencies\nMake integration into other tools easy\n\nInstallation\nYou can install the release version of\nglmGamPoi from\nBioConductor:\nif (!requireNamespace(""BiocManager"", quietly = TRUE))\n    install.packages(""BiocManager"")\n\nBiocManager::install(""glmGamPoi"")\nFor the latest developments, see the\nGitHub repo.\nIf you use this package in a scientific publication, please cite:\n\nglmGamPoi: Fitting Gamma-Poisson Generalized Linear Models on Single\nCell Count Data\nConstantin Ahlmann-Eltze, Wolfgang Huber\nBioinformatics; 2020-12-09; doi:\nhttps://doi.org/10.1093/bioinformatics/btaa1009\n\nExample\nLoad the glmGamPoi package\nlibrary(glmGamPoi)\nTo fit a single Gamma-Poisson GLM do:\n# overdispersion = 1/size\ncounts <- rnbinom(n = 10, mu = 5, size = 1/0.7)\n\n# design = ~ 1 means that an intercept-only model is fit\nfit <- glm_gp(counts, design = ~ 1)\nfit\n#> glmGamPoiFit object:\n#> The data had 1 rows and 10 columns.\n#> A model with 1 coefficient was fitted.\n\n# Internally fit is just a list:\nas.list(fit)[1:2]\n#> $Beta\n#>      Intercept\n#> [1,]  1.504077\n#> \n#> $overdispersions\n#> [1] 0.3792855\nThe glm_gp() function returns a list with the results of the fit. Most\nimportantly, it contains the estimates for the coefficients Œ≤ and the\noverdispersion.\nFitting repeated Gamma-Poisson GLMs for each gene of a single cell\ndataset is just as easy:\nI will first load an example dataset using the TENxPBMCData package.\nThe dataset has 33,000 genes and 4340 cells. It takes roughly 1.5\nminutes to fit the Gamma-Poisson model on the full dataset. For\ndemonstration purposes, I will subset the dataset to 300 genes, but keep\nthe 4340 cells:\nlibrary(SummarizedExperiment)\nlibrary(DelayedMatrixStats)\n# The full dataset with 33,000 genes and 4340 cells\n# The first time this is run, it will download the data\npbmcs <- TENxPBMCData::TENxPBMCData(""pbmc4k"")\n#> snapshotDate(): 2020-10-27\n#> see ?TENxPBMCData and browseVignettes(\'TENxPBMCData\') for documentation\n#> loading from cache\n\n# I want genes where at least some counts are non-zero\nnon_empty_rows <- which(rowSums2(assay(pbmcs)) > 0)\npbmcs_subset <- pbmcs[sample(non_empty_rows, 300), ]\npbmcs_subset\n#> class: SingleCellExperiment \n#> dim: 300 4340 \n#> metadata(0):\n#> assays(1): counts\n#> rownames(300): ENSG00000126457 ENSG00000109832 ... ENSG00000143819\n#>   ENSG00000188243\n#> rowData names(3): ENSEMBL_ID Symbol_TENx Symbol\n#> colnames: NULL\n#> colData names(11): Sample Barcode ... Individual Date_published\n#> reducedDimNames(0):\n#> altExpNames(0):\nI call glm_gp() to fit one GLM model for each gene and force the\ncalculation to happen in memory.\nfit <- glm_gp(pbmcs_subset, on_disk = FALSE)\nsummary(fit)\n#> glmGamPoiFit object:\n#> The data had 300 rows and 4340 columns.\n#> A model with 1 coefficient was fitted.\n#> The design formula is: Y~1\n#> \n#> Beta:\n#>             Min 1st Qu. Median 3rd Qu.   Max\n#> Intercept -8.51   -6.57  -3.91   -2.59 0.903\n#> \n#> deviance:\n#>  Min 1st Qu. Median 3rd Qu.  Max\n#>   14    86.8    657    1686 5507\n#> \n#> overdispersion:\n#>  Min  1st Qu. Median 3rd Qu.   Max\n#>    0 1.65e-13  0.288    1.84 24687\n#> \n#> Shrunken quasi-likelihood overdispersion:\n#>    Min 1st Qu. Median 3rd Qu.  Max\n#>  0.707   0.991      1    1.04 7.45\n#> \n#> size_factors:\n#>    Min 1st Qu. Median 3rd Qu.  Max\n#>  0.117   0.738   1.01    1.32 14.5\n#> \n#> Mu:\n#>       Min 1st Qu. Median 3rd Qu.  Max\n#>  2.34e-05 0.00142 0.0185  0.0779 35.8\nBenchmark\nI compare my method (in-memory and on-disk) with\nDESeq2 and\nedgeR. Both are\nclassical methods for analyzing RNA-Seq datasets and have been around\nfor almost 10 years. Note that both tools can do a lot more than just\nfitting the Gamma-Poisson model, so this benchmark only serves to give a\ngeneral impression of the performance.\n# Explicitly realize count matrix in memory so that it is a fair comparison\npbmcs_subset <- as.matrix(assay(pbmcs_subset))\nmodel_matrix <- matrix(1, nrow = ncol(pbmcs_subset))\n\n\nbench::mark(\n  glmGamPoi_in_memory = {\n    glm_gp(pbmcs_subset, design = model_matrix, on_disk = FALSE)\n  }, glmGamPoi_on_disk = {\n    glm_gp(pbmcs_subset, design = model_matrix, on_disk = TRUE)\n  }, DESeq2 = suppressMessages({\n    dds <- DESeq2::DESeqDataSetFromMatrix(pbmcs_subset,\n                        colData = data.frame(name = seq_len(4340)),\n                        design = ~ 1)\n    dds <- DESeq2::estimateSizeFactors(dds, ""poscounts"")\n    dds <- DESeq2::estimateDispersions(dds, quiet = TRUE)\n    dds <- DESeq2::nbinomWaldTest(dds, minmu = 1e-6)\n  }), edgeR = {\n    edgeR_data <- edgeR::DGEList(pbmcs_subset)\n    edgeR_data <- edgeR::calcNormFactors(edgeR_data)\n    edgeR_data <- edgeR::estimateDisp(edgeR_data, model_matrix)\n    edgeR_fit <- edgeR::glmFit(edgeR_data, design = model_matrix)\n  }, check = FALSE, min_iterations = 3\n)\n#> # A tibble: 4 x 6\n#>   expression               min   median `itr/sec` mem_alloc `gc/sec`\n#>   <bch:expr>          <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n#> 1 glmGamPoi_in_memory    1.26s     1.3s    0.773   534.11MB    3.35 \n#> 2 glmGamPoi_on_disk      4.65s     4.8s    0.200   852.91MB    1.40 \n#> 3 DESeq2                23.03s   23.87s    0.0420    1.08GB    0.350\n#> 4 edgeR                  5.92s    6.33s    0.154     1.18GB    1.08\nOn this dataset, glmGamPoi is more than 5 times faster than edgeR\nand more than 18 times faster than DESeq2. glmGamPoi does not\nuse approximations to achieve this performance increase. The performance\ncomes from an optimized algorithm for inferring the overdispersion for\neach gene. It is tuned for datasets typically encountered in single\nRNA-seq with many samples and many small counts, by avoiding duplicate\ncalculations.\nTo demonstrate that the method does not sacrifice accuracy, I compare\nthe parameters that each method estimates. The means and Œ≤ coefficients\nare identical, but that the overdispersion estimates from glmGamPoi\nare more reliable:\n# Results with my method\nfit <- glm_gp(pbmcs_subset, design = model_matrix, on_disk = FALSE)\n\n# DESeq2\ndds <- DESeq2::DESeqDataSetFromMatrix(pbmcs_subset, \n                        colData = data.frame(name = seq_len(4340)),\n                        design = ~ 1)\nsizeFactors(dds)  <- fit$size_factors\ndds <- DESeq2::estimateDispersions(dds, quiet = TRUE)\ndds <- DESeq2::nbinomWaldTest(dds, minmu = 1e-6)\n\n#edgeR\nedgeR_data <- edgeR::DGEList(pbmcs_subset, lib.size = fit$size_factors)\nedgeR_data <- edgeR::estimateDisp(edgeR_data, model_matrix)\nedgeR_fit <- edgeR::glmFit(edgeR_data, design = model_matrix)\n\nI am comparing the gene-wise estimates of the coefficients from all\nthree methods. Points on the diagonal line are identical. The inferred\nBeta coefficients and gene means agree well between the methods, however\nthe overdispersion differs quite a bit. DESeq2 has problems estimating\nmost of the overdispersions and sets them to 1e-8. edgeR only\napproximates the overdispersions which explains the variation around the\noverdispersions calculated with glmGamPoi.\nScalability\nThe method scales linearly, with the number of rows and columns in the\ndataset. For example: fitting the full pbmc4k dataset with subsampling\non a modern MacBook Pro in-memory takes ~1 minute and on-disk a little\nover 4 minutes. Fitting the pbmc68k (17x the size) takes ~73 minutes\n(17x the time) on-disk.\nDifferential expression analysis\nglmGamPoi provides an interface to do quasi-likelihood ratio testing\nto identify differentially expressed genes. To demonstrate this feature,\nwe will use the data from Kang et al.\n(2018) provided by the\nMuscData package. This is a single cell dataset of 8 Lupus patients\nfor which 10x droplet-based scRNA-seq was performed before and after\ntreatment with interferon beta. The SingleCellExperiment object\nconveniently provides the patient id (ind), treatment status (stim)\nand cell type (cell):\nsce <- muscData::Kang18_8vs8()\n#> snapshotDate(): 2020-10-27\n#> snapshotDate(): 2020-10-27\n#> see ?muscData and browseVignettes(\'muscData\') for documentation\n#> loading from cache\ncolData(sce)\n#> DataFrame with 29065 rows and 5 columns\n#>                        ind     stim   cluster            cell multiplets\n#>                  <integer> <factor> <integer>        <factor>   <factor>\n#> AAACATACAATGCC-1       107     ctrl         5 CD4 T cells        doublet\n#> AAACATACATTTCC-1      1016     ctrl         9 CD14+ Monocytes    singlet\n#> AAACATACCAGAAA-1      1256     ctrl         9 CD14+ Monocytes    singlet\n#> AAACATACCAGCTA-1      1256     ctrl         9 CD14+ Monocytes    doublet\n#> AAACATACCATGCA-1      1488     ctrl         3 CD4 T cells        singlet\n#> ...                    ...      ...       ...             ...        ...\n#> TTTGCATGCTAAGC-1       107     stim         6     CD4 T cells    singlet\n#> TTTGCATGGGACGA-1      1488     stim         6     CD4 T cells    singlet\n#> TTTGCATGGTGAGG-1      1488     stim         6     CD4 T cells    ambs   \n#> TTTGCATGGTTTGG-1      1244     stim         6     CD4 T cells    ambs   \n#> TTTGCATGTCTTAC-1      1016     stim         5     CD4 T cells    singlet\nFor demonstration purpose, I will work on a subset of the genes and\ncells:\nset.seed(1)\n# Take highly expressed genes and proper cells:\nsce_subset <- sce[rowSums(counts(sce)) > 100, \n                  sample(which(sce$multiplets == ""singlet"" & \n                              ! is.na(sce$cell) &\n                              sce$cell %in% c(""CD4 T cells"", ""B cells"", ""NK cells"")), \n                         1000)]\n# Convert counts to dense matrix\ncounts(sce_subset) <- as.matrix(counts(sce_subset))\n# Remove empty levels because glm_gp() will complain otherwise\nsce_subset$cell <- droplevels(sce_subset$cell)\nWe will identify which genes in CD4 positive T-cells are changed most by\nthe treatment. We will fit a full model including the interaction term\nstim:cell. The interaction term will help us identify cell type\nspecific responses to the treatment:\nfit <- glm_gp(sce_subset, design = ~ cell + stim +  stim:cell - 1,\n              reference_level = ""NK cells"")\nsummary(fit)\n#> glmGamPoiFit object:\n#> The data had 9727 rows and 1000 columns.\n#> A model with 6 coefficient was fitted.\n#> The design formula is: Y~cell + stim + stim:cell - 1\n#> \n#> Beta:\n#>                    Min   1st Qu. Median 3rd Qu.  Max\n#>    cellNK cells -1e+08 -1.00e+08  -3.74   -2.65 4.44\n#>     cellB cells -1e+08 -1.00e+08  -3.88   -2.94 4.47\n#> cellCD4 T cells -1e+08 -5.13e+00  -4.20   -3.05 4.50\n#> ...\n#> \n#> deviance:\n#>  Min 1st Qu. Median 3rd Qu.  Max\n#>    0    61.9    114     251 5706\n#> \n#> overdispersion:\n#>  Min 1st Qu. Median 3rd Qu.  Max\n#>    0       0  0.528    4.01 2762\n#> \n#> Shrunken quasi-likelihood overdispersion:\n#>    Min 1st Qu. Median 3rd Qu. Max\n#>  0.188   0.994      1    1.07 363\n#> \n#> size_factors:\n#>    Min 1st Qu. Median 3rd Qu.  Max\n#>  0.489   0.815   1.01     1.2 5.97\n#> \n#> Mu:\n#>  Min 1st Qu. Median 3rd Qu. Max\n#>    0 0.00364  0.016  0.0498 537\nTo see how the coefficient of our model are called, we look at the\ncolnames(fit$Beta):\ncolnames(fit$Beta)\n#> [1] ""cellNK cells""             ""cellB cells""             \n#> [3] ""cellCD4 T cells""          ""stimstim""                \n#> [5] ""cellB cells:stimstim""     ""cellCD4 T cells:stimstim""\nIn our example, we want to find the genes that change specifically in T\ncells. Finding cell type specific responses to a treatment is a big\nadvantage of single cell data over bulk data. To get a proper estimate\nof the uncertainty (cells from the same donor are not independent\nreplicates), we create a pseudobulk for each sample:\n# The contrast argument specifies what we want to compare\n# We test the expression difference of stimulated and control T-cells\n#\n# There is no sample label in the colData, so we create it on the fly\n# from `stim` and `ind` columns in colData(fit$data).\nde_res <- test_de(fit, contrast = `stimstim` + `cellCD4 T cells:stimstim`, \n                  pseudobulk_by = paste0(stim, ""-"", ind)) \n\n# The large `lfc` values come from groups were nearly all counts are 0\n# Setting them to Inf makes the plots look nicer\nde_res$lfc <- ifelse(abs(de_res$lfc) > 20, sign(de_res$lfc) * Inf, de_res$lfc)\n\n# Most different genes\nhead(de_res[order(de_res$pval), ])\n#>       name         pval     adj_pval f_statistic df1      df2        lfc\n#> 189   IFI6 1.212629e-07 0.0008316174    37.25346   1 53.33034   6.118008\n#> 6691 PSME2 1.709916e-07 0.0008316174    36.12175   1 53.33034   3.519394\n#> 5181 IFIT3 1.564660e-06 0.0050731499    29.18276   1 53.33034   7.872549\n#> 9689   MX1 5.336737e-06 0.0129776112    25.58876   1 53.33034   5.037912\n#> 5356  IRF7 1.086665e-05 0.0211399736    23.58467   1 53.33034   4.670868\n#> 2321   IGJ 1.348147e-05 0.0218557065    22.98818   1 53.33034 -12.445271\nThe test is successful and we identify interesting genes that are\ndifferentially expressed in interferon-stimulated T cells: IFI6,\nIFIT3, and IRF7 literally stand for Interferon Induced/Regulated\nProtein.\nTo get a more complete overview of the results, we can make a volcano\nplot that compares the log2-fold change (LFC) vs the logarithmized\np-values.\nlibrary(ggplot2)\nggplot(de_res, aes(x = lfc, y = -log10(pval))) +\n  geom_point(size = 0.6, aes(color = adj_pval < 0.1)) +\n  ggtitle(""Volcano Plot"", ""Genes that change most through interferon-beta treatment in T cells"")\n\nAnother important task in single cell data analysis is the\nidentification of marker genes for cell clusters. For this we can also\nuse our Gamma-Poisson fit.\nLet‚Äôs assume we want to find genes that differ between T cells and the B\ncells. We can directly compare the corresponding coefficients and find\ngenes that differ in the control condition:\nmarker_genes <- test_de(fit, `cellCD4 T cells` - `cellB cells`, sort_by = pval)\nhead(marker_genes)\n#>                          name          pval      adj_pval f_statistic df1\n#> 2873                     CD74 9.414538e-198 9.157522e-194   1411.8278   1\n#> 3150  HLA-DRA_ENSG00000204287 7.389637e-180 3.593950e-176   1228.0745   1\n#> 3152 HLA-DRB1_ENSG00000196126 1.921033e-121 6.228630e-118    717.8697   1\n#> 9116    CD79A_ENSG00000105369  2.307338e-74  5.610869e-71    390.5803   1\n#> 3166 HLA-DPA1_ENSG00000231389  3.226069e-70  6.275995e-67    364.8244   1\n#> 3167 HLA-DPB1_ENSG00000223865  2.257490e-64  3.659768e-61    329.2877   1\n#>           df2       lfc\n#> 2873 1070.895 -5.052300\n#> 3150 1070.895 -7.143245\n#> 3152 1070.895 -6.993047\n#> 9116 1070.895 -7.282279\n#> 3166 1070.895 -5.004210\n#> 3167 1070.895 -4.257008\nIf we want find genes that differ in the stimulated condition, we just\ninclude the additional coefficients in the contrast:\nmarker_genes2 <- test_de(fit, (`cellCD4 T cells` + `cellCD4 T cells:stimstim`) - \n                               (`cellB cells` + `cellB cells:stimstim`), \n                        sort_by = pval)\n\nhead(marker_genes2)\n#>                          name          pval      adj_pval f_statistic df1\n#> 2873                     CD74 8.764650e-187 8.525375e-183   1297.5198   1\n#> 3150  HLA-DRA_ENSG00000204287 5.304332e-175 2.579762e-171   1180.6034   1\n#> 3152 HLA-DRB1_ENSG00000196126 2.668295e-109 8.651501e-106    626.9933   1\n#> 3166 HLA-DPA1_ENSG00000231389  2.972347e-85  7.228005e-82    460.4820   1\n#> 3167 HLA-DPB1_ENSG00000223865  1.871362e-71  3.640548e-68    372.4584   1\n#> 9116    CD79A_ENSG00000105369  1.327524e-58  2.152138e-55    295.0837   1\n#>           df2           lfc\n#> 2873 1070.895 -4.753566e+00\n#> 3150 1070.895 -6.635859e+00\n#> 3152 1070.895 -5.969909e+00\n#> 3166 1070.895 -5.207105e+00\n#> 3167 1070.895 -5.086061e+00\n#> 9116 1070.895 -1.442695e+08\nWe identify many genes related to the human leukocyte antigen (HLA)\nsystem that is important for antigen presenting cells like B-cells, but\nare not expressed by T helper cells. The plot below shows the expression\ndifferences.\nA note of caution: applying test_de() to single cell data without the\npseudobulk gives overly optimistic p-values. This is due to the fact\nthat cells from the same sample are not independent replicates! It can\nstill be fine to use the method for identifying marker genes, as long as\none is aware of the difficulties interpreting the results.\n# Create a data.frame with the expression values, gene names, and cell types\ntmp <- data.frame(gene = rep(marker_genes$name[1:6], times = ncol(sce_subset)),\n                  expression = c(counts(sce_subset)[marker_genes$name[1:6], ]),\n                  celltype = rep(sce_subset$cell, each = 6))\n\nggplot(tmp, aes(x = celltype, y = expression)) +\n  geom_jitter(height = 0.1) +\n  stat_summary(geom = ""crossbar"", fun = ""mean"", color = ""red"") +\n  facet_wrap(~ gene, scales = ""free_y"") +\n  ggtitle(""Marker genes of B vs. T cells"")\n\nAcknowlegments\nThis work was supported by the EMBL International PhD Programme and the\nEuropean Research Council Synergy grant DECODE under grant agreement\nNo.\xa0810296.\nSession Info\nsessionInfo()\n#> R version 4.0.3 (2020-10-10)\n#> Platform: x86_64-apple-darwin17.0 (64-bit)\n#> Running under: macOS Mojave 10.14.6\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n#> \n#> locale:\n#> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n#> \n#> attached base packages:\n#> [1] parallel  stats4    stats     graphics  grDevices utils     datasets \n#> [8] methods   base     \n#> \n#> other attached packages:\n#>  [1] ggplot2_3.3.2               muscData_1.4.0             \n#>  [3] ExperimentHub_1.16.0        AnnotationHub_2.22.0       \n#>  [5] BiocFileCache_1.14.0        dbplyr_2.0.0               \n#>  [7] TENxPBMCData_1.8.0          HDF5Array_1.18.0           \n#>  [9] rhdf5_2.34.0                SingleCellExperiment_1.12.0\n#> [11] DelayedMatrixStats_1.12.1   DelayedArray_0.16.0        \n#> [13] Matrix_1.2-18               SummarizedExperiment_1.20.0\n#> [15] Biobase_2.50.0              GenomicRanges_1.42.0       \n#> [17] GenomeInfoDb_1.26.1         IRanges_2.24.0             \n#> [19] S4Vectors_0.28.0            BiocGenerics_0.36.0        \n#> [21] MatrixGenerics_1.3.1        matrixStats_0.57.0-9001    \n#> [23] glmGamPoi_1.3.6            \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] bitops_1.0-6                  bit64_4.0.5                  \n#>  [3] RColorBrewer_1.1-2            httr_1.4.2                   \n#>  [5] tools_4.0.3                   utf8_1.1.4                   \n#>  [7] R6_2.5.0                      colorspace_2.0-0             \n#>  [9] DBI_1.1.0                     rhdf5filters_1.2.0           \n#> [11] withr_2.3.0                   tidyselect_1.1.0             \n#> [13] DESeq2_1.30.0                 bit_4.0.4                    \n#> [15] curl_4.3                      compiler_4.0.3               \n#> [17] cli_2.2.0                     labeling_0.4.2               \n#> [19] scales_1.1.1                  bench_1.1.1                  \n#> [21] genefilter_1.72.0             rappdirs_0.3.1               \n#> [23] stringr_1.4.0                 digest_0.6.27                \n#> [25] rmarkdown_2.6                 XVector_0.30.0               \n#> [27] pkgconfig_2.0.3               htmltools_0.5.0              \n#> [29] sparseMatrixStats_1.3.2       limma_3.46.0                 \n#> [31] fastmap_1.0.1                 rlang_0.4.9                  \n#> [33] RSQLite_2.2.1                 shiny_1.5.0                  \n#> [35] farver_2.0.3                  generics_0.1.0               \n#> [37] BiocParallel_1.24.1           dplyr_1.0.2                  \n#> [39] RCurl_1.98-1.2                magrittr_2.0.1               \n#> [41] GenomeInfoDbData_1.2.4        fansi_0.4.1                  \n#> [43] Rcpp_1.0.5                    munsell_0.5.0                \n#> [45] Rhdf5lib_1.12.0               lifecycle_0.2.0              \n#> [47] edgeR_3.32.0                  stringi_1.5.3                \n#> [49] yaml_2.2.1                    zlibbioc_1.36.0              \n#> [51] grid_4.0.3                    blob_1.2.1                   \n#> [53] promises_1.1.1                crayon_1.3.4                 \n#> [55] lattice_0.20-41               profmem_0.5.0                \n#> [57] beachmat_2.6.2                splines_4.0.3                \n#> [59] annotate_1.68.0               locfit_1.5-9.4               \n#> [61] knitr_1.30                    pillar_1.4.7                 \n#> [63] geneplotter_1.68.0            XML_3.99-0.5                 \n#> [65] glue_1.4.2                    BiocVersion_3.12.0           \n#> [67] evaluate_0.14                 BiocManager_1.30.10          \n#> [69] vctrs_0.3.5                   httpuv_1.5.4                 \n#> [71] gtable_0.3.0                  purrr_0.3.4                  \n#> [73] assertthat_0.2.1              xfun_0.20                    \n#> [75] mime_0.9                      xtable_1.8-4                 \n#> [77] later_1.1.0.1                 survival_3.2-7               \n#> [79] tibble_3.0.4                  AnnotationDbi_1.52.0         \n#> [81] memoise_1.1.0                 ellipsis_0.3.1               \n#> [83] interactiveDisplayBase_1.28.0 BiocStyle_2.18.1\n'], 'url_profile': 'https://github.com/const-ae', 'info_list': ['11', 'Jupyter Notebook', 'Updated Nov 14, 2020', '10', 'MATLAB', 'MIT license', 'Updated Jun 30, 2020', '3', 'TypeScript', 'MIT license', 'Updated Feb 7, 2021', '26', 'R', 'Updated Feb 23, 2021', '11', 'Jupyter Notebook', 'Updated Jan 31, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '7', 'R', 'Updated Aug 19, 2020', '6', 'C++', 'GPL-3.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Pytorch Implementations on:\n\nLogistic Regression\nArtificial Neural Networks\nConvolutional Neural Networks\nRecurrent Neural Networks\n\nDataset used is MNIST dataset.\n'], 'url_profile': 'https://github.com/jelifysh', 'info_list': ['11', 'Jupyter Notebook', 'Updated Nov 14, 2020', '10', 'MATLAB', 'MIT license', 'Updated Jun 30, 2020', '3', 'TypeScript', 'MIT license', 'Updated Feb 7, 2021', '26', 'R', 'Updated Feb 23, 2021', '11', 'Jupyter Notebook', 'Updated Jan 31, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '7', 'R', 'Updated Aug 19, 2020', '6', 'C++', 'GPL-3.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Categorical-Feature-Encoding\n'], 'url_profile': 'https://github.com/jelifysh', 'info_list': ['11', 'Jupyter Notebook', 'Updated Nov 14, 2020', '10', 'MATLAB', 'MIT license', 'Updated Jun 30, 2020', '3', 'TypeScript', 'MIT license', 'Updated Feb 7, 2021', '26', 'R', 'Updated Feb 23, 2021', '11', 'Jupyter Notebook', 'Updated Jan 31, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '7', 'R', 'Updated Aug 19, 2020', '6', 'C++', 'GPL-3.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['11', 'Jupyter Notebook', 'Updated Nov 14, 2020', '10', 'MATLAB', 'MIT license', 'Updated Jun 30, 2020', '3', 'TypeScript', 'MIT license', 'Updated Feb 7, 2021', '26', 'R', 'Updated Feb 23, 2021', '11', 'Jupyter Notebook', 'Updated Jan 31, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '7', 'R', 'Updated Aug 19, 2020', '6', 'C++', 'GPL-3.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['PrivacyUnbiased\nThis software implements the methods described in Georgina Evans and Gary King,\n""Statistically Valid Inferences from Differentially Private Data Releases""; See\nhttp://garyking.org/dpd\nInstall the software by running the following code:\ndevtools::install_github(""georgieevans/PrivacyUnbiased"")\nlibrary(PrivacyUnbiased)\nRead an overview of the package:\nhttps://github.com/georgieevans/PrivacyUnbiased/blob/master/Example/Example.pdf\n'], 'url_profile': 'https://github.com/georgieevans', 'info_list': ['11', 'Jupyter Notebook', 'Updated Nov 14, 2020', '10', 'MATLAB', 'MIT license', 'Updated Jun 30, 2020', '3', 'TypeScript', 'MIT license', 'Updated Feb 7, 2021', '26', 'R', 'Updated Feb 23, 2021', '11', 'Jupyter Notebook', 'Updated Jan 31, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '7', 'R', 'Updated Aug 19, 2020', '6', 'C++', 'GPL-3.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'Paris, France', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['SEAL-FYP-Logistic-Regression\nMy final year project using Microsoft SEAL. The purpose of this project is to Create Functionalities for Matrices and Vectors and Build a Logistic Regression model using Fully Homomorphic Encryption.\nThe code also contains some benchmark tests for matrix and vector operations as well as some examples based on the original examples already provided in the SEAL library.\nNote: SEAL Version 3.4.5 is required. To view the graphs, you will need to have GNUPlot and Python 3 installed on your machine\nTable of Contents\n\nSetup for Linux\nSetup for Windows\nMatrix and Vector Operations\n\nLinear Transformation\nMatrix Multiplication\nMatrix Transpose\nMatrix Ops\nVector Ops\nBenchmark\n\n\nPolynomial Evaluation\n\nHorner\'s Method\nTree Method\n\n\nLogistic Regression\n\nNormal LR\nSEAL CKKS LR\n\n\nAbout the example files\n\nBFV\nEncoding\nLevels\nCKKS\nRotation\n\n\nBugs\n\nSetup for Linux\nFirst, make sure you have Microsoft SEAL installed. Follow the tutorial on https://github.com/Microsoft/SEAL.\nIf you have made any changes to the file name or added other files you will need to modify the CMakeLists.txt file accordingly.\nTo Build the project for the first time you need to run cmake . to generate the proper Makefile then you can build it with make.\nSetup for Windows\nRefer to the Windows installation of SEAL in https://github.com/Microsoft/SEAL.\nMatrix and Vector Operations\nLinear Transformation\nThe linear_transformation.cpp file contains an implementation of the linear transformation algorithm in the paper: https://eprint.iacr.org/2018/1041.pdf .\nThis implementation uses rotations with element-wise multiplication and addition to perform matrix vector multiplication without using dot products. With the SIMD capability of SEAL, this method performs computations pretty quickly.\nThe linear_transformation2.cpp file is a similar to linear_transformation.cpp but without debugging statements and 3 test cases. You can use it to perform stress tests on certain parameters.\nThe drawing below shows an example of linear transformation with a 4x4 matrix:\n\nIt is also possible to perform linear transformation on rectangular matrices using a hybrid approach proposed in in the paper ‚ÄúGAZELLE: A Low Latency Framework For Secure Neural Network Inference"". However I have implemented another way of doing it illustrated below:\n\nThe dot products are computed as follows:\n\nMatrix Multiplication\nThe matrix_multiplication.cpp file includes an implementation of the homomorphic matrix multiplication algorithm in the paper: https://eprint.iacr.org/2018/1041.pdf .\nThis implementation uses the linear transformation algorithm previously discusssed, Matrix Encoding and the 4 permutations of the input matrix: U_sigma, U_tau, V_k and W_k.\nIn order to compute those permutations, it is necessary to have the matrix encoded in row major order such as in this illustration:\n\nMatrix Transpose\nThe matrix_transpose.cpp file contains method for homomorphically transposing a matrix. Since the tranpose of a matrix is technically a permuation, we can simply encode the matrix into a ciphertext vector and perform linear transformation with a matrix U_transpose with corresponding 1s and 0s. The illustration below shows an example of this method with a 3x3 matrix:\n\nMatrix Ops\nThe matrix_ops.cpp file includes a naive method of performing matrix operations in CKKS. Here I am encoding every single element in the matrix and encrypting it instead of using entire rows from the matrix. A GNUPlot script and a data file are generated by running matrix_ops.\nVector Ops\nThe vector_ops.cpp file consists of a small performance test for BFV and CKKS with poly_modulus_degree = 8192.\nBenchmark\nThe benchmark.cpp file consists of performance tests for CKKS with 3 sets of input vectors of sizes 10, 100, 1000.\nRunning benchmark (after building the project) will generate a bench_<your poly modulus degree>.dat file and a corresponding script_<your poly modulus degree>.p file that can be used in GNUPlot. If you have gnuplot installed you can run the script file with gnuplot ""script_<your poly modulus degree>"". This will generate a canvas_""<your poly modulus degree>.html"" with a graph of the output.\nThe benchmark2.cpp is similar to the first benchmark file.\nPolynomial Evaluation\nThe file polynomial.cpp contains 2 methods to evaluate polynomials using SEAL based on the works of Hao Chen in  https://github.com/haochenuw/algorithms-in-SEAL/ :\nHorner\'s method\nHorner\'s method for evaluating polynomials uses D multiplications and additions where D is the degree of the polynomial. Therefore the Modulus chain used must be of length greater than D+1 because of the rescaling and modulus switching operations required after every multiplication.\nThe drawing below shows an example of Horner\'s method:\n\nTree Method\nThe tree method for evaluating polynomials uses log(D) multiplications and additions where D is the degree of the polynomial. This functions faster than Horner\'s method since it requires less operations which also allow us to have a lower poly_modulus_degree and smaller Modulus Chain.\nThe powers of x are pre-computed in a tree such as the example illustrated below:\n\nLogistic Regression\nThe goal of this project is eventually to implement a logistic regression model that could work over encrypted data. The dataset used is pulsar_stars.csv simply because it was easy to use: the features are integers and the labels are at the last column 0s and 1s. To use this dataset properly the features matrix has to be standardized, that is why I built a standard_scaler function that performs (value - mean )/ standard_deviation over the values of the features matrix. I have also provided helper functions for transforming the CSV file into a matrix of floats. The code for logistic regression is based on the code and explanation in https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html .\nNormal LR\nThis version of Logistic Regression has the functions:\n\nsigmoid: returns the sigmoid of a value\npredict: returns the sigmoid of the linear transformation between the features and the weights\ncost_function: returns the cost\nupdate_weights or Gradient Descent: returns the updated weights\ntrain: returns the new weights and the cost history after a certain number of iterations of training\n\nSEAL CKKS LR\nThis version of Logistic Regression works on encrypted data using the CKKS scheme. It has the same functions as the normal logistic regression code except they have been modified to work using the SEAL functions. Since there is no way to write the sigmoid function 1/(1 + e^-value) in SEAL because there are no division and exponential operation in HE, an approximation of it is required. The polynomial approximation used here is based on the finding in the paper https://eprint.iacr.org/2018/074.pdf and is of the form:\n\nf3(x) = 0.5 + 1.20096(x/8) - 0.81562(x/8)^3 with a polynomial of degree 3\nf5(x) = 0.5 + 1.53048(x/8) - 2.3533056(x/8)^3 + 1.3511295(x/8)^5 with a polynomial of degree 5\nf7(x) = 0.5 + 1.73496(x/8) - 4.19407(x/8)^3 + 5.43402(x/8)^5 - 2.50739(x/8)^7 with a polynomial of degree 7\n\nThe polynomial approximation of the sigmoid function can be evaluated with the polynomial evaluation methods: Horner\'s and Tree method.\nThe protocol of the LR-CKKS works as follows:\n\nIn theory, using higher degree polynomials for approximating the sigmoid function is better however this would require a lot of rescaling which would lead to losing a lot of precision bits. In order to get the best precision and performance, I used the degree 3 polynomial with Horner\'s method.\nAbout the example files\nAll the explanations below are based on the comments and code from the SEAL examples. If you need a more detailed explaination, please refer to the original SEAL examples.\n1 - BFV\nThe first file is the 1_bfv.cpp. It contains an example on how to use the bfv scheme in SEAL. The BFV encryption scheme is used mainly to encrypt integers. It requires three parameters:\n\nDegree of Polynomial Modulus: poly_modulus_degree\nCiphertext Coefficient Modulus: coeff_modulus\nPlaintext Coefficient Modulus: plain_modulus\n\nEach ciphertext has an invariant noise budget measured in bits that is consumed on every ciphertext operation. If the noise budget were to reach 0, the ciphertext would be too corrupted for decryption.\nThe noise budget is computed as follows: log2(coeff_modulus/plain_modulus). Choosing a larger coeff_modulus will give you a larger noise budget but will make computations a bit slower. The example provided uses a helper function from SEAL to create this parameter.\nThe size of a ciphertext in SEAL is the number of polynomials. A new ciphertext has a size of 2. Homomorphic Multiplication increases the size of the ciphertext: If two ciphertexts have sizes M and N then their multiplication will yield a size of M+N-1. The larger the ciphertext size the greater the consuption rate of the noise budget will be.\nIt is possible to reduce the size of ciphertexts from 3 to 2 by applying Relinearization to the ciphertexts. However this procedure comes at a certain computational cost.\n2 - Encoding\nThere are 3 types of encoding that can be used in SEAL: Integer Encoding , Batch Encoding and CKKS Encoding.\nThe reason you may want to encode your Plaintext before encrypting it is to avoid integer overflow. Integer overflow happens when the plaintext coefficients exceed plain_modulus.\n3 - Levels\nThe modulus switching chain is a chain of other encryption parameters derived from the original parameters. The parameters in the modulus switching chain are the same as the original parameters with the exception that size of the coefficient modulus is decreasing going down the chain. The example provided shows a coeff_modulus of 5 primes of sizes {50, 30, 30, 50, 50} bits. Thus, there are 5 levels in this chain:\n\n{50, 30, 30, 50, 50} -> Level 4 (Key level)\n{50, 30, 30, 50} -> Level 3 (Data level)\n{50, 30, 30}-> Level 2\n{50, 30} -> Level 1\n{50} -> Level 0 (Lowest level)\n\nModulus Switching is a technique of changing the ciphertext parameters down the chain. You may want to use this to gain computational performance from having smaller parameters. This method may reduce your ciphertext noise budget. If there is no need to perform further computations on a ciphertext, you can switch it down to the smallest (last) set of parameters in the chain before decrypting it.\n4 - CKKS\nThe CKKS encryption scheme focuses on performing operations on encrypted real and complex numbers. Homomorphic multiplication in CKKS causes the scales in ciphertexts to grow. The scale can be considered as the bit precision of the encoding. The scale must not get too close to the total size of coeff_modulus. You can rescale to reduce the scale and stabilize the scale expansion. Rescaling is a type of modulus switching, it removes the last of the primes from the coeff_modulus but it scales down the ciphertext by the removed prime.\nSuppose that the scale in a CKKS ciphertext is S and the last prime in the coeff_modulus is P. Rescaling to the next level changes the scale to S/P and removes the prime P from the coeff_modulus (just like in Modulus Switching). A good strategy is to set the initial scale S and primes P_i to be very close to each other. If ciphertexts have scale S before multiplication then they will have scale S^2 after multiplication and then S^2/P_i after rescaling thus S^2/P_i will be close to S again. Generally, for a circuit of depth D, we need to rescale D times, i.e., we need to be able to remove D primes from the coeff_modulus. Once we have only one prime left in the coeff_modulus, the remaining prime must be larger than S by a few bits to preserve the pre-decimal-point value of the plaintext.\nTherefore a generally good strategy is to choose the parameters for the CKKS scheme as follows:\n\nChoose a 60 bit prime as as the first prime in coeff_modulus giving us the highest precision when decrypting\nChoose another 60 bit prime as the last prime in coeff_modulus\nChoose the intermediate primes to be close to each other\n\nThe values I have used are {60, 40, 40, 60} with a poly_modulus_degree = 8192 which yields a coeff_modulus of 200 bits in total which is below max bit count for the poly_modulus_degree: CoeffModulus::MaxBitCount(8192) returns 218. The initial scale is set to 2^40. At the last level, this leaves us 60-40 = 20 bits of precision before the decimal point and around 10 to 20 bits of precision after the decimal point. Since our intermediate primes are 40 bits which is very close to 2^40 we are able to achieve stabilization as described earlier.\nIn the example, we\'re evaluating the polynomial PI*x^3 + 0.4x + 1. When computing x^2 (to compute x^3 later), you will notice that the scale will grow to 2^80. After rescaling the new scale should be close to 2^40 (NOT equal).\n5 - Rotation\nRotation can be used in both BFV and CKKS schemes. It is a mechanism that allows you to rotate the encrypted vectors cyclically. It requires special keys called Galois keys which can be generated from the KeyGenerator class. It is possible to rotate the columns and rotate the rows. Rotations usually don\'t consume any noise budget. However, this is only the case when the special prime is at least as large as the other primes. The same applies for relinearization.\nBugs\n\n Parameter mismatch CKKS LR -> update_weights() \n Ciphertext transparent in CKKS LR -> update_weights()\n\n'], 'url_profile': 'https://github.com/MarwanNour', 'info_list': ['11', 'Jupyter Notebook', 'Updated Nov 14, 2020', '10', 'MATLAB', 'MIT license', 'Updated Jun 30, 2020', '3', 'TypeScript', 'MIT license', 'Updated Feb 7, 2021', '26', 'R', 'Updated Feb 23, 2021', '11', 'Jupyter Notebook', 'Updated Jan 31, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '7', 'R', 'Updated Aug 19, 2020', '6', 'C++', 'GPL-3.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Dec 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['11', 'Jupyter Notebook', 'Updated Nov 14, 2020', '10', 'MATLAB', 'MIT license', 'Updated Jun 30, 2020', '3', 'TypeScript', 'MIT license', 'Updated Feb 7, 2021', '26', 'R', 'Updated Feb 23, 2021', '11', 'Jupyter Notebook', 'Updated Jan 31, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '7', 'R', 'Updated Aug 19, 2020', '6', 'C++', 'GPL-3.0 license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Dec 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TeX', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Python', 'Updated Apr 6, 2020', '2', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'Finland', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Linear Rgression Part 1.\n'], 'url_profile': 'https://github.com/hoangnguyen7699', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TeX', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Python', 'Updated Apr 6, 2020', '2', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'Guwahati', 'stats_list': [], 'contributions': '367 contributions\n        in the last year', 'description': ['Predicting-Stock-Market\n   \nApplying the Linear Regression Algorithm to predict the Closing price of stock in  S&P500 stock market index dataset from year 1950 to 2015.\n\n\nFor more infomation take a look into .ipynb file.\n'], 'url_profile': 'https://github.com/pcsingh', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TeX', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Python', 'Updated Apr 6, 2020', '2', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'Chengdu, China', 'stats_list': [], 'contributions': '311 contributions\n        in the last year', 'description': ['<script type=""text/x-mathjax-config"">\n    MathJax.Hub.Config({\n      tex2jax: {\n        skipTags: [\'script\', \'noscript\', \'style\', \'textarea\', \'pre\'],\n        inlineMath: [[\'$\',\'$\']]\n      }\n    });\n</script>\n<script src=""https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"" type=""text/javascript""></script>\n\nÈ´òÊñØËøáÁ®ãÂõûÂΩí\nGaussian Process Regression\n\n‰∏Ä„ÄÅÈ´òÊñØÂàÜÂ∏É\nÈ´òÊñØËøáÁ®ãÔºàGaussian Process, GPÔºâÊòØÈöèÊú∫ËøáÁ®ã‰πã‰∏ÄÔºåÊòØ‰∏ÄÁ≥ªÂàóÁ¨¶ÂêàÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÈöèÊú∫ÂèòÈáèÂú®‰∏ÄÊåáÊï∞ÈõÜÔºàindex setÔºâÂÜÖÁöÑÈõÜÂêà„ÄÇËØ•Ëß£Èáä‰∏≠ÁöÑ‚ÄúÊåáÊï∞‚ÄùÂèØ‰ª•ÁêÜËß£‰∏∫‚ÄúÁª¥Â∫¶‚ÄúÔºåÊåâÁÖßÊú∫Âô®Â≠¶‰π†ÁöÑËßíÂ∫¶ÔºåÂêÑ‰∏™ÊåáÊï∞‰∏äÁöÑÈöèÊú∫ÂèòÈáèÂèØ‰ª•ÂØπÂ∫îÂú∞ÁêÜËß£‰∏∫ÂêÑ‰∏™Áª¥Â∫¶‰∏äÁöÑÁâπÂæÅ„ÄÇ\n1.1 ‰∏ÄÂÖÉÈ´òÊñØÂàÜÂ∏É\n$$\nX \\sim N(\\mu, \\sigma^2)\n$$\nÂÖ∂Ê¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞‰∏∫Ôºö\n$$\nf(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp({-(x - \\mu)^2} / ({2 \\sigma^2}))\n$$\nÊ†áÂáÜÊ≠£ÊÄÅÂàÜÂ∏ÉÔºö\n$$\n\\mu = 0, \\sigma = 1\n$$\nÊ≠£ÊÄÅÂàÜÂ∏ÉÂÖ∑ÊúâÂ¶Ç‰∏ãÊÄßË¥®Ôºö\n\n\nÂ¶ÇÊûú$X \\sim N(\\mu, \\sigma^2)$Ôºå‰∏î$a$„ÄÅ$b$Âùá‰∏∫ÂÆûÊï∞ÔºåÂàô$aX + b \\sim N(a \\mu + b, (a \\sigma)^2)$Ôºõ\n\n\nÂ¶ÇÊûú$X \\sim N(\\mu_x, \\sigma_x^2)$‰∏é$Y \\sim N(\\mu_y, \\sigma_y^2)$Áã¨Á´ãÔºåÂàôÔºö\n\n$U = X + Y \\sim N(\\mu_x + \\mu_y, \\sigma_x^2 + \\sigma_y^2)$Ôºõ\n$V = X - Y \\sim N(\\mu_x - \\mu_y, \\sigma_x^2 + \\sigma_y^2)$Ôºõ\n\n\n\nËã•‰ª•‰∏ä$X$‰∏é$Y$Áõ∏‰∫íÁã¨Á´ãÔºåÂàôÔºö\n\n\n$XY$Á¨¶Âêà‰ª•‰∏ãÊ¶ÇÁéáÂØÜÂ∫¶ÂàÜÂ∏ÉÔºö\n$$\np(z)=\\frac{1}{\\pi \\sigma_x \\sigma_y}K_0(\\frac{|z|}{\\sigma_x \\sigma_y})\n$$\nÂÖ∂‰∏≠$K_0$‰∏∫‰øÆÊ≠£Ë¥ùÂ°ûÂ∞îÂáΩÊï∞Ôºõ\n\n\n$X/Y$Á¨¶ÂêàÊüØË•øÂàÜÂ∏ÉÔºö\n$$\nX/Y \\sim {\\rm Cauchy}(0, \\sigma_x / \\sigma_y)\n$$\n\n\n\n\nËã•$X_1, ..., X_n$ÂêÑËá™Áã¨Á´ãÔºåÁ¨¶ÂêàÊ≠£ÊÄÅÂàÜÂ∏ÉÔºåÂàô$X_1^2 + X_2^2 + ... + X_n^2$Á¨¶ÂêàËá™Áî±Â∫¶‰∏∫$n$ÁöÑÂç°ÊñπÂàÜÂ∏ÉÔºõ\n\n\n1.2 ‰∫åÂÖÉÈ´òÊñØÂàÜÂ∏É\n$$\nf(x,y) = A \\exp (-(\\frac{x - x_0)^2}{2\\sigma_x^2} + \\frac{(y - y_0)^2}{2\\sigma_y^2}))\n$$\n1.3 Â§öÂÖÉÈ´òÊñØÂàÜÂ∏É\n$$\np(x) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}} \\ \\exp(-\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu))\n$$\nÂÖ∂‰∏≠Ôºå$\\mu$‰∏∫ÂêÑÈöèÊú∫ÂèòÈáèÁöÑÂùáÂÄºÁªÑÊàêÁöÑ$n \\times 1$ÂêëÈáèÔºå$\\Sigma$Ë°®Á§∫ÈöèÊú∫ÂèòÈáèÈó¥ÁöÑ$n \\times n$ÂçèÊñπÂ∑ÆÁü©ÈòµÔºåÊ≠£ÂÆö„ÄÇ\n\n‰∫å„ÄÅÂ§öÂÖÉÈ´òÊñØÂàÜÂ∏ÉÁöÑÊù°‰ª∂Ê¶ÇÁéáÂØÜÂ∫¶\n‰ª§ÈöèÊú∫ÂêëÈáè$X = [x_1, x_2, ..., x_n]$Êúç‰ªéÂ§öÂÖÉÈ´òÊñØÂàÜÂ∏É$X \\sim N(\\mu, \\Sigma)$Ôºå‰ª§$X_1 = [x_1, ..., x_m]$‰∏∫Â∑≤ÁªèËßÇÊµãÂèòÈáèÔºå$X_2 = [x_{m+1}, ..., x_n]$‰∏∫Êú™Áü•ÂèòÈáèÔºåÂàôÔºö\n$$\n\\begin{aligned}\nX = \\left(\n\\begin{array}{c}\nX_1 \\\nX_2\n\\end{array}\n\\right)\n\\end{aligned}\n$$\n‰ªéËÄåÊúâÔºö\n$$\n\\begin{aligned}\n\\mu = \\left(\n\\begin{array}{c}\n\\mu_1 \\\n\\mu_2\n\\end{array}\n\\right)\n\\end{aligned}\n$$\n$$\n\\begin{aligned}\n\\Sigma = \\left[\n\\begin{aligned}{}\n\\Sigma_{11}, &\\Sigma_{12} \\\n\\Sigma_{21}, &\\Sigma_{22}\n\\end{aligned}\n\\right]\n\\end{aligned}\n$$\nÁªôÂÆö$X_1$Ê±Ç$X_2$ÁöÑÂêéÈ™åÂàÜÂ∏ÉÔºàËøôÈÉ®ÂàÜÊé®ÂØºÂèØ‰ª•‰ªéÁõ∏ÂÖ≥ÊñáÁåÆ‰∏≠Êü•Âà∞ÔºåÊ≠§Â§ÑÁï•ÔºâÔºö\n$$\n\\mu_{2|1} = \\mu_2 + \\Sigma_{21} \\Sigma_{11}^{-1}(X_1 - \\mu_1)\n$$\n$$\n\\Sigma_{2|1} = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}\n$$\n\n‰∏â„ÄÅÈ´òÊñØËøáÁ®ãÂõûÂΩí\nËÆæÈöèÊú∫ÂèòÈáè$X = [x_1, x_2, x_3, ..., x_n]^T$ÔºåÂÖ∂Êúç‰ªéÊ≠£ÊÄÅÂàÜÂ∏ÉÔºö\n$$\nX \\sim N(\\mu, \\Sigma)\n$$\nÂÖ∂‰∏≠$\\mu = [\\mu_0, \\mu_1, ..., \\mu_n]^T$‰∏∫ÂùáÂÄºÂêëÈáèÔºå$\\Sigma$ÊòØËøô$n$‰∏™ÁâπÂæÅ‰πãÈó¥ÁöÑÂçèÊñπÂ∑ÆÁü©ÈòµÔºåÂ∞Ü$\\Sigma$Â±ïÂºÄÊúâÔºö\n$$\n\\Sigma =\n\\left[\n\\begin{matrix}\n& cov_{1, 1}, & cov_{1, 2}, ..., & cov_{1, n} \\\n& cov_{2, 1}, & cov_{2, 2}, ..., & cov_{2, n} \\\n& ..., & ..., &... \\\n& cov_{n, 1}, & cov_{n, 2}, ..., & cov_{n, n} \\\n\\end{matrix}\n\\right]\n$$\nÂÖ∂‰∏≠$cov_{i,j}$Ë°®Á§∫ÁâπÂæÅ$i$ÂíåÁâπÂæÅ$j$‰πãÈó¥ÁöÑÂçèÊñπÂ∑ÆÔºàcovarianceÔºâ„ÄÇ\nÈ´òÊñØËøáÁ®ãÊ†∑Êú¨‰∏é‰∏ÄËà¨Êú∫Âô®Â≠¶‰π†ÁöÑÊ†∑Êú¨Âå∫Âà´Âú®‰∫éÔºåÈ´òÊñØËøáÁ®ã‰∏≠Ê†∑Êú¨ÂêÑÁâπÂæÅ‰πãÈó¥Â≠òÂú®Áõ∏ÂÖ≥ÂÖ≥Á≥ªÔºåËøôÁßçÁõ∏ÂÖ≥ÂÖ≥Á≥ªÊòØÈÄöËøáÂçèÊñπÂ∑ÆÁü©Èòµ$\\Sigma$Êù•‰ΩìÁé∞ÁöÑ„ÄÇÊØîÂ¶ÇÂú®‰∏Ä‰∫õÊó∂Èó¥Â∫èÂàóÊ®°ÂûãÈáåÈù¢ÔºåÂêÑ‰∏™ÂèòÈáèËæìÂá∫ÁöÑÊó∂Èó¥Â∫èÂàóÂú®Êó∂Èó¥ÂâçÂêéÈÉΩ‰ºö‰ΩìÁé∞Âá∫‰∏ÄÁßçÁõ∏ÂÖ≥ÊÄßÔºàÊØîÂ¶ÇÂπ≥ÊªëËøáÊ∏°Á≠âÔºâÔºåËøôÁßçÊ®°ÂûãËæìÂá∫Â∞±ÂæàÈÄÇÂêà‰ΩøÁî®È´òÊñØËøáÁ®ãÊù•Ê®°Êãü„ÄÇ\n3.1 ÂçèÊñπÂ∑ÆÁü©ÈòµËÆ°ÁÆó\n$\\Sigma$ÂèØ‰ª•ÈÄöËøáÈ´òÊñØËøáÁ®ãÊ†∏ËøõË°åÊ±ÇËß£ÔºåÂ∏∏ËßÅÁöÑÈ´òÊñØËøáÁ®ãÊ†∏ÊúâÔºö\nRBF kernel:\n$$\nk = \\sigma^2 \\exp(-\\frac{||t_a - t_b||^2}{2l^2})\n$$\n\nperiodic kernel:\n$$\nk = \\sigma^2 \\exp(-\\frac{2}{l^2} \\sin(\\frac{\\pi}{p})|t_a - t_b|)\n$$\n\nlinear_kernel:\n$$\nk = \\sigma_b^2 + \\sigma^2 * (t_a - c)(t_b - c)\n$$\n\nËøôÊ†∑ÂΩìÁü•ÈÅì‰∏§‰∏™ÈöèÊú∫ÂèòÈáèÊåáÊï∞$t_a$Âíå$t_b$ÂêéÔºå‰æøÂèØÈÄöËøáÊ†∏ÂáΩÊï∞ËÆ°ÁÆó‰∏§‰∏™ÂèòÈáèÈó¥ÁöÑÂçèÊñπÂ∑Æ„ÄÇÂ¶ÇÊûúÂØπÊâÄÊúâÈöèÊú∫ÂèòÈáèÂùáËøõË°å‰∏äËø∞ËÆ°ÁÆó‰æøÂèØËé∑ÂæóÂçèÊñπÂ∑ÆÁü©Èòµ$\\Sigma$„ÄÇÊúâ‰∫ÜÂçèÊñπÂ∑ÆÁü©Èòµ$\\Sigma$Âêé‰æøÂèØÂØπÈ´òÊñØËøáÁ®ãËøõË°åÈááÊ†∑Ôºà‰∏ÄËà¨ËÆ§‰∏∫È´òÊñØËøáÁ®ãÂÖàÈ™åÂàÜÂ∏ÉÂùáÂÄº$\\mu$Â∫îÊó†ÂÅè‰∏∫0Ôºâ„ÄÇ\n3.2 È´òÊñØËøáÁ®ãÈááÊ†∑\nËé∑Âæó‰∫ÜÂêÑÈöèÊú∫ÂèòÈáè$x$ÁöÑÂùáÂÄº‰ø°ÊÅØ$\\mu$ÂíåËÅîÂêàÂàÜÂ∏ÉÁöÑÂçèÊñπÂ∑ÆÁü©Èòµ$\\Sigma$ÂêéÔºå‰æøÂèØÂØπËØ•È´òÊñØËøáÁ®ãËøõË°åÈöèÊú∫ÈááÊ†∑„ÄÇÈááÊ†∑Ê≠•È™§Â¶Ç‰∏ãÔºö\n\n\nÈ¶ñÂÖàÂØπÂçèÊñπÂ∑ÆÁü©Èòµ$\\Sigma$ËøõË°åSVDÂàÜËß£ÔºåËé∑ÂæóÁü©Èòµ$\\rm U$„ÄÅ$\\rm S$Âíå$\\rm V$Ôºõ\n\n\nÁîüÊàê$N$‰∏™Áã¨Á´ãÂêåÂàÜÂ∏ÉÁöÑÈ´òÊñØÈöèÊú∫ÂèòÈáèÔºàÂùáÂÄº‰∏∫0ÔºåÊ†áÂáÜÂ∑Æ‰∏∫1ÔºâÔºåÁªÑÊàêÂêëÈáè$y$Ôºõ\n\n\nÊåâÁÖßÂ¶Ç‰∏ãÂÖ¨ÂºèËé∑ÂæóÈ´òÊñØËøáÁ®ãÊ†∑Êú¨Ôºö\n$$\nx = \\mu + {\\rm U} \\sqrt{\\rm S}y\n$$\n\n\nÈááÊ†∑ÁªìÊûúÂ¶Ç‰∏ãÂõæÊâÄÁ§∫ÔºåÂõæ‰∏≠ÊØèÊù°ÁÅ∞Ëâ≤Êõ≤Á∫ø‰æøÂØπÂ∫î‰∏ÄÊù°È´òÊñØËøáÁ®ãÊ†∑Êú¨Ôºà$n=100$Ôºâ,ËìùËâ≤Êõ≤Á∫øË°®Á§∫Ê†∑Êú¨ÂùáÂÄºÔºåÂõ†‰∏∫Êàë‰ª¨ËÆæÂÆöÂÖàÈ™åÂàÜÂ∏ÉÂêÑÁª¥Â∫¶‰∏äÂùáÂÄº$\\mu_i=0$ÔºåÊâÄ‰ª•ËìùËâ≤Êõ≤Á∫øÂú®0ÈôÑËøëÊ≥¢Âä®„ÄÇ\n\n3.3 ÂêéÈ™åÂàÜÂ∏ÉÂíåÈááÊ†∑\n3.2‰∏≠Ëé∑ÂæóÁöÑÈ´òÊñØËøáÁ®ãÊ†∑Êú¨‰∏∫ÂÖàÈ™åÊ†∑Êú¨„ÄÇ‰ΩÜÊòØÂΩìÊàë‰ª¨Âú®Êüê‰∫õÊåáÊï∞$t$‰∏äËé∑Âæó‰∫Ü‰∏ÄÊâπËßÇÊµãÊ†∑Êú¨ÂêéÔºåËøôÊâπËßÇÊµãÊ†∑Êú¨Â∞ÜÊúâÂä©‰∫éÊàë‰ª¨ÂØπÂÖ∂‰ªñÊåáÊï∞ÈõÜ‰∏äÁöÑÊ†∑Êú¨ÂàÜÂ∏ÉËøõË°å‰º∞ËÆ°ÔºàÂêéÈ™åÔºâ„ÄÇÊàë‰ª¨Â∞ÜËøôÊâπÂ∑≤ËßÇÊµãÊåáÊï∞ÈõÜËÆæ‰∏∫$X_1$ÔºåÊú™ËßÇÊµãÂà∞ÁöÑÊåáÊï∞ÈõÜËÆæ‰∏∫$X_2$„ÄÇÊé•‰∏ãÊù•‰æøÂèØ‰ΩøÁî®Á¨¨‰∫åËäÇ‰∏≠ÁöÑÊñπÊ≥ïËé∑ÂæóÂú®$X_2$‰∏äÊ†∑Êú¨ÂàÜÂ∏ÉÂêéÈ™åÊ¶ÇÁéáÂèÇÊï∞$\\mu_{2|1}$Âíå$\\Sigma_{2|1}$ÔºåÊúÄÂêéÈáçÊñ∞ÂØπ$X_2$‰∏äÁöÑÈöèÊú∫ÂèòÈáèËøõË°åÈááÊ†∑„ÄÇ‰∏ãÂõæÊòæÁ§∫‰∫ÜÂêéÈ™åÂàÜÂ∏ÉÊ†∑Êú¨Ôºö\n\n'], 'url_profile': 'https://github.com/Ulti-Dreisteine', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TeX', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Python', 'Updated Apr 6, 2020', '2', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'Florida', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/muditpaliwal', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TeX', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Python', 'Updated Apr 6, 2020', '2', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'Thunder Bay', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/vidhipitroda', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TeX', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Python', 'Updated Apr 6, 2020', '2', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['regression\n'], 'url_profile': 'https://github.com/dvgrass', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TeX', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Python', 'Updated Apr 6, 2020', '2', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kushal2022', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TeX', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Python', 'Updated Apr 6, 2020', '2', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression\nPerform both linear and nonlinear regression and compare results from the same data. I derived the diffusion coefficient of the target molecule BSA (bovine serum albumin) in 10kD PEGMD hydrogel network. The technique used to derive an empirical relationship between two or more variables based on data obtained from experimental observations is called regression .\nLinear regression depends on a strict linear dependency of the functional for of the parameters to be determined and is a method for modeling the relation between unknown parameters or coefficients of the function are estimated from the data. It is thus desirable to obtain an approximate best fit function that agrees with the data overall.  Nonlinear regression is also a form of regression except the observed data are modeled by a nonlinear function which is a nonlinear combination of model parameters and relies on one or multiple variables.\n'], 'url_profile': 'https://github.com/michikofeehan', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TeX', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Python', 'Updated Apr 6, 2020', '2', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'Munich, Germany', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/nabilbabai', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'TeX', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', '4', 'Python', 'Updated Apr 6, 2020', '2', 'Python', 'Updated Jun 4, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 1, 2020']}"
"{'location': 'Mysterious_Universe', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Regression\npsig prediction\n'], 'url_profile': 'https://github.com/bcrDas', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'Updated Jan 30, 2020', 'JavaScript', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '972 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shakeebanwar', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'Updated Jan 30, 2020', 'JavaScript', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vaibhav9511', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'Updated Jan 30, 2020', 'JavaScript', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020']}","{'location': 'Berlin', 'stats_list': [], 'contributions': '133 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jmdtc', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'Updated Jan 30, 2020', 'JavaScript', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression\nPlease run the test_model codes\n'], 'url_profile': 'https://github.com/sumanp31', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'Updated Jan 30, 2020', 'JavaScript', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hubertlapsa', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'Updated Jan 30, 2020', 'JavaScript', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/RachitBhatt92', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'Updated Jan 30, 2020', 'JavaScript', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression\nThis project contains all the regression model that will predict the best result for our dataset.\nThis model has possible regression model in it. YOU CAN CHECK OUT WHICH MODEL YOU NEED FOR YOUR DATASET.\n'], 'url_profile': 'https://github.com/Shravs2523', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'Updated Jan 30, 2020', 'JavaScript', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020']}","{'location': 'Russia, Moscow', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['valence-arousal-recognition\nEmotion recognition with Keras library. Uses AffectNet dataset and valence-arousal labels. Implements CNN architecture with regression.\n\n\n\n'], 'url_profile': 'https://github.com/katedukhnai', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'Updated Jan 30, 2020', 'JavaScript', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Machine-Learning\nImplementation of popular machine learning algorithms.\nContents:\n\nLinear regression\nLocally weighted linear regression\nLogistic regression\nGDA\nNaive Bayes classifier\nSVM\n\n'], 'url_profile': 'https://github.com/rajankita', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'Updated Jan 30, 2020', 'JavaScript', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', '5', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dostojewskyi', 'info_list': ['Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'MIT license', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 28, 2020', '3', 'Python', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dostojewskyi', 'info_list': ['Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'MIT license', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 28, 2020', '3', 'Python', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'Morocco', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['LinearRegression_GradientDescent\nPython script for linear regression ""Using Gradient Descent""\nWarning\nThis script is not mine I just uploaded it for testing and studying ML and linear regression üòÑ\n'], 'url_profile': 'https://github.com/farhatizakaria', 'info_list': ['Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'MIT license', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 28, 2020', '3', 'Python', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'Essen, Germany', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['thrreg\nOverview\nthrreg estimates the threshold regression model by Hansen (2000).\nIt provides three functions:\n\nthr_test_hom() tests for a threshold in a linear regression under homoskedasticity.\nthr_test_het() tests for a threshold in a linear regression under heteroskedasticity.\nthr_est() estimates the threshold and the regression parameters of the threshold model.\n\nInstallation\nInstall development version from GitHub:\n# install.packages(""devtools"")\ndevtools::install_github(""mlkremer/thrreg"")\nUsage\nThis example performs the empirical work reported in Hansen (2000) and prints\nthe LaTeX output to output/output.tex:\nlibrary(thrreg)\n\n## Load data\ndata <- dur_john\n\n## Define variable names for output (optional)\nvar_names <- c(""GDP\\\\_Gwth"", ""Log\\\\_GDP\\\\_1960"", ""Log\\\\_Inv/GDP"",\n               ""Log\\\\_Pop\\\\_Gwth"", ""Log\\\\_School"", ""GDP\\\\_1960"", ""Literacy"")\nxi <- 2:5\nh <- 1\n\n\n## First Level\nsink(""output/output.tex"")\ncat(""\\\\section{First Sample Split}"", ""\\n\\n"")\n\n## Test for a Sample Split\ntest_1_gdp <- thr_test_het(data, ""GDPGwth"", xi, ""GDP1960"", var_names)\ntest_1_lit <- thr_test_het(data, ""GDPGwth"", xi, ""Literacy"", var_names)\n\n## Estimate Sample Split\nqhat_1 <- thr_est(data, ""GDPGwth"", xi, ""GDP1960"", h, test_1_gdp$p_value,\n                  var_names, digits.thr = 0, header = var_names[1])\nsink()\n\n\n## Second Level\ndat_2l <- subset(data, GDP1960 <= qhat_1)\ndat_2u <- subset(data, GDP1960 > qhat_1)\n\nsink(""output/output.tex"", append = T)\ncat(paste0(""\\\\section{Second Sample Split: Subsample, Incomes below "", qhat_1,\n           ""}"", ""\\n\\n""))\n\n## Test for a Sample Split\ntest_2l_gdp <- thr_test_het(dat_2l, ""GDPGwth"", xi, ""GDP1960"", var_names)\ntest_2l_lit <- thr_test_het(dat_2l, ""GDPGwth"", xi, ""Literacy"", var_names)\n\ncat(paste0(""\\\\section{Second Sample Split: Subsample, Incomes above "", qhat_1,\n           ""}"", ""\\n\\n""))\n\n## Test for a Sample Split\ntest_2u_gdp <- thr_test_het(dat_2u, ""GDPGwth"", xi, ""GDP1960"", var_names)\ntest_2u_lit <- thr_test_het(dat_2u, ""GDPGwth"", xi, ""Literacy"", var_names)\n\n## Estimate Sample Split\nqhat_2 <- thr_est(dat_2u, ""GDPGwth"", xi, ""Literacy"", h, test_2u_lit$p_value,\n                  var_names, digits.thr = 0, header = var_names[1])\nsink()\n\n\n## Third Level\ndat_3l <- subset(dat_2u, Literacy <= qhat_2)\ndat_3u <- subset(dat_2u, Literacy > qhat_2)\n\nsink(""output/output.tex"", append = T)\ncat(paste0(""\\\\section{Third Sample Split: Subsample, Incomes above "", qhat_1,\n           "", Literacy below "", qhat_2, ""}"", ""\\n\\n""))\n\n## Test for a Sample Split\ntest_3l_gdp <- thr_test_het(dat_3l, ""GDPGwth"", xi, ""GDP1960"")\ntest_3l_lit <- thr_test_het(dat_3l, ""GDPGwth"", xi, ""Literacy"")\n\ncat(paste0(""\\\\section{Third Sample Split: Subsample, Incomes above "", qhat_1,\n           "", Literacy above "", qhat_2, ""}"", ""\\n\\n""))\n\n## Test for a Sample Split\ntest_3u_gdp <- thr_test_het(dat_3u, ""GDPGwth"", xi, ""GDP1960"")\ntest_3u_lit <- thr_test_het(dat_3u, ""GDPGwth"", xi, ""Literacy"")\n\nsink()\nCompile LaTeX output in directory output/ via:\n\\documentclass[a4paper]{scrartcl} %twocolumn %scrartcl, scrreprt, scrbook\n\\usepackage[margin=1in]{geometry}\n\\usepackage{hyperref}\n\\usepackage{booktabs}\n\\usepackage[table]{xcolor}\n\\usepackage{siunitx}\n\n\\setlength{\\parindent}{0in}\n\n\\newcommand{\\RedA}{red}\n\\newcommand{\\RedB}{red!60}\n\\newcommand{\\RedC}{red!30}\n\\newcommand{\\BlueA}{blue!75}\n\\newcommand{\\BlueB}{blue!55}\n\\newcommand{\\BlueC}{blue!30}\n\n\\begin{document}\n\\input{output.tex}\n\\end{document}\nReferences\nthrreg is based on the\nsource code\nprovided by Bruce E. Hansen.\nHansen, B. E. (2000). Sample splitting and threshold estimation.\nEconometrica, 68(3):575--603.\nhttps://onlinelibrary.wiley.com/doi/10.1111/1468-0262.00124.\nOpen access: https://www.ssc.wisc.edu/~bhansen/papers/ecnmt_00.pdf.\n'], 'url_profile': 'https://github.com/mlkremer', 'info_list': ['Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'MIT license', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 28, 2020', '3', 'Python', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': [""DORN implemented in Pytorch 0.4.1\nIntroduction\nThis is a PyTorch(0.4.1) implementation of Deep Ordinal Regression Network for Monocular Depth Estimation. At present, we can provide train script in NYU Depth V2 dataset and Kitti Dataset!\nNote: we modify the ordinal layer using matrix operation, making trianing faster.\nTODO\n\n DORN model in nyu and kitti\n Training DORN on nyu and kitti datasets\n Results evaluation on nyu test set\n the script to generate nyu and kitti dataset.\n Calculate alpha and beta in nyu dataset and kitti dataset\n Realize the ordinal loss in paper\n\nDatasets\nNYU Depth V2\nDORN need to use all the Images (about 120k) in the dataset, but if you just want to test the code, you can use the nyu_depth_v2_labeled.mat and turn it to a h5 file. The convert script is 'create_nyu_h5.py' and you need to change the file paths to yours.\n\nModify create_nyu_h5.py with your path and run the script.\n\npython create_nyu_h5.py\nKitti\nThe kitti dataset contains 23488 images from 32 scenes for training and 697 images from the remaining 29 scenes for testing.\n\nRaw dataset (about 175 GB) can be downloaded by running:\n\nwget -i kitti_archives_to_download.txt -P ~/kitti-raw-data/\n\nUnzip the compressed files:\n\ncd ~/kitti-raw-data\nfind . -name '*.zip' -exec unzip {} \\;\n\nRun the script to generate the kitti_ground_truth\n\npython gen_kitti_dataset.py\n\n""], 'url_profile': 'https://github.com/HieuPhan33', 'info_list': ['Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'MIT license', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 28, 2020', '3', 'Python', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/viksarang', 'info_list': ['Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'MIT license', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 28, 2020', '3', 'Python', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ColbyLe', 'info_list': ['Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'MIT license', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 28, 2020', '3', 'Python', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/likhita26', 'info_list': ['Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'MIT license', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 28, 2020', '3', 'Python', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Sales-Prediction\nSales prediction of a Shop using Linear Regression, Lasso Regression, Ridge regression\n'], 'url_profile': 'https://github.com/SHARANU-ULLEGADDI', 'info_list': ['Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'MIT license', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 28, 2020', '3', 'Python', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'Beijing', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Regression-model\nÂÆûÁé∞Â§öÁßçÂõûÂΩíÊ®°Âûã\n‰∏Ä„ÄÅÁÆÄ‰ªã\nÂú®ÁªüËÆ°Â≠¶‰∏≠ÔºåÂõûÂΩíÂàÜÊûêÔºàregression analysisÔºâÊåáÁöÑÊòØÁ°ÆÂÆö‰∏§ÁßçÊàñ‰∏§Áßç‰ª•‰∏äÂèòÈáèÈó¥Áõ∏‰∫í‰æùËµñÁöÑÂÆöÈáèÂÖ≥Á≥ªÁöÑ‰∏ÄÁßçÁªüËÆ°ÂàÜÊûêÊñπÊ≥ï„ÄÇÂõûÂΩíÂàÜÊûêÊåâÁÖßÊ∂âÂèäÁöÑÂèòÈáèÁöÑÂ§öÂ∞ëÔºåÂàÜ‰∏∫‰∏ÄÂÖÉÂõûÂΩíÂíåÂ§öÂÖÉÂõûÂΩíÂàÜÊûêÔºõÊåâÁÖßÂõ†ÂèòÈáèÁöÑÂ§öÂ∞ëÔºåÂèØÂàÜ‰∏∫ÁÆÄÂçïÂõûÂΩíÂàÜÊûêÂíåÂ§öÈáçÂõûÂΩíÂàÜÊûêÔºõÊåâÁÖßËá™ÂèòÈáèÂíåÂõ†ÂèòÈáè‰πãÈó¥ÁöÑÂÖ≥Á≥ªÁ±ªÂûãÔºåÂèØÂàÜ‰∏∫Á∫øÊÄßÂõûÂΩíÂàÜÊûêÂíåÈùûÁ∫øÊÄßÂõûÂΩíÂàÜÊûê„ÄÇÂú®Â§ßÊï∞ÊçÆÂàÜÊûê‰∏≠ÔºåÂõûÂΩíÂàÜÊûêÊòØ‰∏ÄÁßçÈ¢ÑÊµãÊÄßÁöÑÂª∫Ê®°ÊäÄÊúØÔºåÂÆÉÁ†îÁ©∂ÁöÑÊòØÂõ†ÂèòÈáèÔºàÁõÆÊ†áÔºâÂíåËá™ÂèòÈáèÔºàÈ¢ÑÊµãÂô®Ôºâ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇËøôÁßçÊäÄÊúØÈÄöÂ∏∏Áî®‰∫éÈ¢ÑÊµãÂàÜÊûêÔºåÊó∂Èó¥Â∫èÂàóÊ®°Âûã‰ª•ÂèäÂèëÁé∞ÂèòÈáè‰πãÈó¥ÁöÑÂõ†ÊûúÂÖ≥Á≥ª„ÄÇ\nËØæÁ®ã‰∏≠‰ªãÁªç‰∫ÜÂ∏∏ËßÅÁöÑ‰∏Ä‰∫õÂõûÂΩíÂàÜÊûêÊñπÊ≥ïÔºåÂåÖÊã¨Ôºö\n‰∏ÄÂÖÉÁ∫øÊÄßÂõûÂΩíÔºåÂ§öÂÖÉÁ∫øÊÄßÂõûÂΩíÔºåÈÄªËæëÂõûÂΩíÔºåÂ±ÄÈÉ®ÂõûÂΩíÔºåÂ≤≠ÂõûÂΩíÔºåLASSOÂõûÂΩíÁ≠â„ÄÇ\n‰∫å„ÄÅÊï∞ÊçÆ‰∏éÈóÆÈ¢ò\nÂà©Áî®ÁΩëÁªúÁà¨Ëô´ÊàñËÄÖ‰∏ãËΩΩÂ∑≤ÂÖ¨Â∏ÉÁöÑÊï∞ÊçÆÈõÜÔºåÂÆûÁé∞‰∏äËØæ‰ªãÁªçÁöÑÂá†Á±ªÂü∫Êú¨ÁÆóÊ≥ïÔºåÂπ∂ÊØîËæÉÂêÑÁßçÁÆóÊ≥ïÁöÑÊÄßËÉΩ„ÄÇ\n'], 'url_profile': 'https://github.com/agnJason', 'info_list': ['Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'MIT license', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 28, 2020', '3', 'Python', 'Updated Sep 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', '2', 'Python', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Jupyter Notebook', 'Updated Jan 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Model-X knockoffs\nLinear Regression\nThe report simulated gaussian data with p>n, then constructed Model-X knockoffs by ASDP, equicorrelated method and exact construction, and then compared FDP with the package ‚Äúknockoff‚Äù. The result of my code and the package are almost the same, and the difference is caused by the choice of W.\nLogistic Regression\nThe report uses Model_X knockoff filter to select variables in logistic regression, with small amplitude. When the amplitude is as small as sqrt(log p / n), the filter is conservative.\n'], 'url_profile': 'https://github.com/yangmeng96', 'info_list': ['Updated Feb 2, 2020', 'Updated Jan 30, 2020', 'V', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020']}","{'location': 'Bern, Switzerland', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['regression_2_mean\nproject on ""regression to the mean"" effect - linear regression analysis\n'], 'url_profile': 'https://github.com/severintroesch', 'info_list': ['Updated Feb 2, 2020', 'Updated Jan 30, 2020', 'V', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020']}","{'location': 'Ankara', 'stats_list': [], 'contributions': '358 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kemalcanbora', 'info_list': ['Updated Feb 2, 2020', 'Updated Jan 30, 2020', 'V', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ashish-ECE', 'info_list': ['Updated Feb 2, 2020', 'Updated Jan 30, 2020', 'V', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020']}","{'location': 'Porvorim, Goa, India', 'stats_list': [], 'contributions': '139 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pkenaudekar', 'info_list': ['Updated Feb 2, 2020', 'Updated Jan 30, 2020', 'V', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020']}","{'location': 'Warszawa ', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rozek1szymon', 'info_list': ['Updated Feb 2, 2020', 'Updated Jan 30, 2020', 'V', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Assignment 3 - Logistic Regression\nIn this assignment, you'll be implementing a basic logistic regression model to solve a binary classification problem. This model will be used to fit the given 2D data that comprises of marks of some students. Aim here is to predict whether the student will get admission in a college based upon these marks or not. The fits would be visualized. So the goal here is to find the decision boundary that fits the data best.\nWorking files:\nYou will be restricted to work with 'model.py' and 'Assignment3-Logistic Regression.ipynb'. On running and working through the 'Assignment3-Logistic Regression.ipynb', a file named 'hyper_param.json' would be generated.\nData:\nData is already stored in a csv file and provided to you. You are not allowed to change the CSV file in any manner.\nActual Work:\n\nComplete some functions related to logistic regression (in models.py)\n\ncomputing sigmoid - sigmoid\nloss and gradient - loss\npredicting values - assign_predictions\ncalculating accuracy - accuracy\ntraining the model - train\n\n\nTuning various parameters of the models in order to achieve the best results (in Assignment3-Logistic Regression.ipynb)\n\nSteps:\n\nOpen 'Assignment3-Logistic Regression.ipynb' via Jupyter Notebook.\nWork through the 'Assignment3-Logistic Regression.ipynb' and follow the instructions therein.\nYou need to submit the files: 'models.py', 'Assignment3-Logistic Regression.ipynb' and 'hyper_param.json'.\n\nNOTE: We will be testing your results with the help of 'model.py' and the hyperparameters in 'hyper_param.json'.\nHint File:\nThere is also a hint.pdf, that contains the mathematical workout for logistic\nregression's. Try to complete the assignment without looking at it, but do\nconsult it if you get stuck.\nAll the best.\n""], 'url_profile': 'https://github.com/CSO-241-Spring20', 'info_list': ['Updated Feb 2, 2020', 'Updated Jan 30, 2020', 'V', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SmitParekh17', 'info_list': ['Updated Feb 2, 2020', 'Updated Jan 30, 2020', 'V', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': [""Student_marks_Predictions\nMultiple Linear Regression\nLINEAR REGRESSION\nProblem Statement: Predict student performance in secondary education (high school) for final grade scores.\nLinear Regression is a supervised learning algorithm, meaning we'll have labeled data and try to predict new labels on unlabeled data.\nGetting our Data\nWe will use the Student Performance Data Set from UC Irvine's Machine Learning Repository!\nlink: https://archive.ics.uci.edu/ml/datasets/Student+Performance\nDownload this data and save it as 'student-mat.csv'\nNOTE: The delimiter is a semi-colon.\nAttribute Information\nAttributes for student-mat.csv (Math course) datasets:\n1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\n2 sex - student's sex (binary: 'F' - female or 'M' - male)\n3 age - student's age (numeric: from 15 to 22)\n4 address - student's home address type (binary: 'U' - urban or 'R' - rural)\n5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 √¢‚Ç¨‚Äú 5th to 9th grade, 3 √¢‚Ç¨‚Äú secondary education or        4 √¢‚Ç¨‚Äú higher education)\n8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 √¢‚Ç¨‚Äú 5th to 9th grade, 3 √¢‚Ç¨‚Äú secondary education or         4 √¢‚Ç¨‚Äú higher education)\n9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or    'other')\n10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')\n13 traveltime - home to school travel time (numeric: 1 - less than 15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - more than 1 hour)\n14 studytime - weekly study time (numeric: 1 - less than 2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - more than 10 hours)\n15 failures - number of past class failures (numeric: n if between 1 and 3 , else 4)\n16 schoolsup - extra educational support (binary: yes or no)\n17 famsup - family educational support (binary: yes or no)\n18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n19 activities - extra-curricular activities (binary: yes or no)\n20 nursery - attended nursery school (binary: yes or no)\n21 higher - wants to take higher education (binary: yes or no)\n22 internet - Internet access at home (binary: yes or no)\n23 romantic - with a romantic relationship (binary: yes or no)\n24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n29 health - current health status (numeric: from 1 - very bad to 5 - very good)\n30 absences - number of school absences (numeric: from 0 to 93)\nThese grades are related with the course subject, Math:\n31 G1 - first period grade (numeric: from 0 to 20)\n31 G2 - second period grade (numeric: from 0 to 20)\n32 G3 - final grade (numeric: from 0 to 20, output target)\n""], 'url_profile': 'https://github.com/Sohelshaikh7', 'info_list': ['Updated Feb 2, 2020', 'Updated Jan 30, 2020', 'V', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['linreg\nlinear regression materials\n'], 'url_profile': 'https://github.com/statcodes', 'info_list': ['Updated Feb 2, 2020', 'Updated Jan 30, 2020', 'V', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020']}"
"{'location': 'Russia', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Linear regression\n\nExample of linear regression implementation\nPrerequisites\n\n\njupyterlab\n\n\nnumpy\n\n\nmatplotlib\n\n\nimageio\n\n\ntqdm\n\n\nipywidgets\n\n\nUsage\nTo get started, write the following commands\n\nClone repository\n\n~$ git clone https://github.com/Venderst/linear_regression.git\n~$ cd linear_regression/\n\n\nUpdate and install necessary packages\n\n~$ python -m pip install -Ur requirements.txt\n\n\nRun notebook\n\n~$ jupyter notebook linear_regression.ipynb\n\nLicense\n\n\nGNU AGPL V3 license\nCopyright 2020 ¬© Venderst.\n\n'], 'url_profile': 'https://github.com/Venderst', 'info_list': ['Jupyter Notebook', 'AGPL-3.0 license', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SmitParekh17', 'info_list': ['Jupyter Notebook', 'AGPL-3.0 license', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': [""Student_marks_Predictions\nMultiple Linear Regression\nLINEAR REGRESSION\nProblem Statement: Predict student performance in secondary education (high school) for final grade scores.\nLinear Regression is a supervised learning algorithm, meaning we'll have labeled data and try to predict new labels on unlabeled data.\nGetting our Data\nWe will use the Student Performance Data Set from UC Irvine's Machine Learning Repository!\nlink: https://archive.ics.uci.edu/ml/datasets/Student+Performance\nDownload this data and save it as 'student-mat.csv'\nNOTE: The delimiter is a semi-colon.\nAttribute Information\nAttributes for student-mat.csv (Math course) datasets:\n1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\n2 sex - student's sex (binary: 'F' - female or 'M' - male)\n3 age - student's age (numeric: from 15 to 22)\n4 address - student's home address type (binary: 'U' - urban or 'R' - rural)\n5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 √¢‚Ç¨‚Äú 5th to 9th grade, 3 √¢‚Ç¨‚Äú secondary education or        4 √¢‚Ç¨‚Äú higher education)\n8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 √¢‚Ç¨‚Äú 5th to 9th grade, 3 √¢‚Ç¨‚Äú secondary education or         4 √¢‚Ç¨‚Äú higher education)\n9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or    'other')\n10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')\n13 traveltime - home to school travel time (numeric: 1 - less than 15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - more than 1 hour)\n14 studytime - weekly study time (numeric: 1 - less than 2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - more than 10 hours)\n15 failures - number of past class failures (numeric: n if between 1 and 3 , else 4)\n16 schoolsup - extra educational support (binary: yes or no)\n17 famsup - family educational support (binary: yes or no)\n18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n19 activities - extra-curricular activities (binary: yes or no)\n20 nursery - attended nursery school (binary: yes or no)\n21 higher - wants to take higher education (binary: yes or no)\n22 internet - Internet access at home (binary: yes or no)\n23 romantic - with a romantic relationship (binary: yes or no)\n24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n29 health - current health status (numeric: from 1 - very bad to 5 - very good)\n30 absences - number of school absences (numeric: from 0 to 93)\nThese grades are related with the course subject, Math:\n31 G1 - first period grade (numeric: from 0 to 20)\n31 G2 - second period grade (numeric: from 0 to 20)\n32 G3 - final grade (numeric: from 0 to 20, output target)\n""], 'url_profile': 'https://github.com/Sohelshaikh7', 'info_list': ['Jupyter Notebook', 'AGPL-3.0 license', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['linreg\nlinear regression materials\n'], 'url_profile': 'https://github.com/statcodes', 'info_list': ['Jupyter Notebook', 'AGPL-3.0 license', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['DTR\nDecision Tree Regression\n'], 'url_profile': 'https://github.com/dhirajk100', 'info_list': ['Jupyter Notebook', 'AGPL-3.0 license', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['21-Machine-Learning-HW\nlogistic regression & deep learning\nDeep learning model accuracy = 88%\nLogistic Regression model accuracy = 88%, hyperparameter tuning does not improve this.\nthe majority of loss is due to difficulty differentiating between ""confirmed"" and ""candidate"" exoplanets. An additional metric (whatever is used to confirm a candidate as an exoplanet) would help improve the model.\nI believe that 88% accuracy is good enough to predict new exoplanets. And most likely that number will be higher once the ""candidates"" are confirmed as real exoplanets.\n'], 'url_profile': 'https://github.com/Romina-R', 'info_list': ['Jupyter Notebook', 'AGPL-3.0 license', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['StoryTeller\nRegression StoryTeller Test Cases\n'], 'url_profile': 'https://github.com/AppsQaStack', 'info_list': ['Jupyter Notebook', 'AGPL-3.0 license', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['library(apaTables)\napa.cor.table(attitude, filename=""Table1_APA.doc"", table.number=1)\n'], 'url_profile': 'https://github.com/NicolaasLangbroek', 'info_list': ['Jupyter Notebook', 'AGPL-3.0 license', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'Jalandhar,Punjab,India', 'stats_list': [], 'contributions': '261 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression using Sklearn\nClassifiers used: Linear Regression And SVR\n'], 'url_profile': 'https://github.com/ChiragSaini', 'info_list': ['Jupyter Notebook', 'AGPL-3.0 license', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'Porvorim, Goa, India', 'stats_list': [], 'contributions': '139 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pkenaudekar', 'info_list': ['Jupyter Notebook', 'AGPL-3.0 license', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 31, 2020', 'Apache-2.0 license', 'Updated Apr 29, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 29, 2020']}"
"{'location': 'India', 'stats_list': [], 'contributions': '146 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aman33459', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', '1', 'R', 'BSD-3-Clause license', 'Updated Jul 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 12, 2021', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Julia', 'MIT license', 'Updated Mar 2, 2021', 'Python', 'Updated Jul 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['This is a workshop to showcase the way in which a trend line can be calculated\nto fit a data set. This trend line can then be used to make predictions.\nMaking predictions from a data model is a crucial part of Artificial\nIntelligence and Machine Learning.\nWhat is linear regression?\nLinear regression is a way to explore the relationship between a dependent\nvariable and an independent one.\nExplanation of terms:\nEpochs - the number of times the process is run on the data set, more\nrepeats results in a more accurate model. An excessive number of repeats\ncan result in overfitting, which means the model is specific to only the\ndata present, and can not neccesarily make accurate predictions.\nWeight - the coefficient of a variable in the resulting equation.\nLearning Rate - the rate at which the gradient of the line is altered.\nSetting this too low results in convergence being too slow. Setting it too\nhigh means the changes will be unstable.\n'], 'url_profile': 'https://github.com/hackathonsforschools', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', '1', 'R', 'BSD-3-Clause license', 'Updated Jul 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 12, 2021', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Julia', 'MIT license', 'Updated Mar 2, 2021', 'Python', 'Updated Jul 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['Inverse Sparse Regression (inspre)\ninspre is an R package for calculating a sparse approximate matrix inverse with\nper-entry weights. Applications include directed and undirected graphical\nmodel inference. For more information on the method please consult the paper\nPhenome-scale causal network discovery with bidirectional mediated Mendelian\nrandomization.\nInstallation\nAt the moment the package must be installed using devtools::install_github(""brielin/inspre"").\nEventually we intend to make the package available on CRAN.\nIf you want to run tests, you will need to add the install option to download\nthe tests as well.\n> devtools::install_github(""brielin/inspre"", INSTALL_opts=""--install-tests"")\n> library(testthat)\n> library(inspre)\n> test_package(""inspre"")\n\nRequirements\nTo run the main function inspre::inspre,\n\nR (>3.3.0)\nRcpp\nRcppEigen\nRcppArmadillo\nforeach\ndoMC\nRlinsolve\n\nTo run tests, run GGM.Rmd and make plots,\n\nhuge\negg\ndevtools\ntestthat\n\n'], 'url_profile': 'https://github.com/brielin', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', '1', 'R', 'BSD-3-Clause license', 'Updated Jul 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 12, 2021', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Julia', 'MIT license', 'Updated Mar 2, 2021', 'Python', 'Updated Jul 23, 2020']}","{'location': 'nagpur', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SomyKamble', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', '1', 'R', 'BSD-3-Clause license', 'Updated Jul 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 12, 2021', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Julia', 'MIT license', 'Updated Mar 2, 2021', 'Python', 'Updated Jul 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': [""regression_models\nCoursera. Regression Models project\nIn this document a fuel efficiency of 1970's automobiles is the main subject of interest. In particular, there are 2 major questions of the research:\n\nIs an automatic transmission better than manual in regard to US miles per gallon (MPG) travelled?\nAnd what is the actual MPG difference between automatic and manual transmissions?\n\nJudging by the results of the study, it appears that there is in fact a significant difference in MPG between the 2 types of transmissions. Unfortunately, the data for this study does not provide satisfactory basis for more or less reliable quantification of the difference in fuel economy between 2 groups.\n""], 'url_profile': 'https://github.com/Kasenkow', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', '1', 'R', 'BSD-3-Clause license', 'Updated Jul 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 12, 2021', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Julia', 'MIT license', 'Updated Mar 2, 2021', 'Python', 'Updated Jul 23, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear regression models are used to show or predict the relationship between two variables or factors. The factor that is being predicted (the factor that the equation solves for) is called the dependent variable. The factors that are used to predict the value of the dependent variable are called the independent variables.\nIn linear regression, each observation consists of two values. One value is for the dependent variable and one value is for the independent variable. In this simple model, a straight line approximates the relationship between the dependent variable and the independent variable.\nWhen two or more independent variables are used in regression analysis, the model is no longer a simple linear one. This is known as multiple regression.\nThe simple linear regression model is represented by:\ny = Œ≤0 +Œ≤1x+Œµ\n'], 'url_profile': 'https://github.com/Ranjitkumarsahu1436', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', '1', 'R', 'BSD-3-Clause license', 'Updated Jul 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 12, 2021', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Julia', 'MIT license', 'Updated Mar 2, 2021', 'Python', 'Updated Jul 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BenDevo20', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', '1', 'R', 'BSD-3-Clause license', 'Updated Jul 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 12, 2021', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Julia', 'MIT license', 'Updated Mar 2, 2021', 'Python', 'Updated Jul 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression example\n\n\nAssumptions of Linear Regression\n1. The regression model is linear in the coefficients and the error term\n\nThis assumption addresses the functional form of the model. In statistics, a regression model is linear when all terms in the model are either the constant or a parameter multiplied by an independent variable. Y = mx + b\n\n2. The error term has a population mean of zero\n\n\nThe error term accounts for the variation in the dependent variable that the independent variables do not explain. Random chance should determine the values of the error term. For your model to be unbiased, the average value of the error term must equal zero.\n\n\nSuppose the average error is +7. This non-zero average error indicates that our model systematically underpredicts the observed values.\n\n\n3. All independent variables are uncorrelated with the error term\n\nIf an independent variable is correlated with the error term, we can use the independent variable to predict the error term, which violates the notion that the error term represents unpredictable random error.\n\n4. Observations of the error term are uncorrelated with each other\n\n\nOne observation of the error term should not predict the next observation.\n\n\nIf you have information that allows you to predict the error term for an observation, you need to incorporate that information into the model itself.\n\n\n5. The error term has a constant variance (no heteroscedasticity)\n\n\nThe variance of the errors should be consistent for all observations. In other words, the variance does not change for each observation or for a range of observations. This preferred condition is known as homoscedasticity (same scatter). If the variance changes, we refer to that as heteroscedasticity (different scatter).\n\n\nThe easiest way to check this assumption is to create a residuals versus fitted value plot. On this type of graph, heteroscedasticity appears as a cone shape where the spread of the residuals increases in one direction. In the graph below, the spread of the residuals increases as the fitted value increases.\n\n\n\nResiduals by fitted values plot that displays heteroscedasticity, which violates an OLS assumption.\n\n\n6. No independent variable is a perfect linear function of other explanatory variables\n\nPerfect correlation occurs when two variables have a Pearson‚Äôs correlation coefficient of +1 or -1. When one of the variables changes, the other variable also changes by a completely fixed proportion. The two variables move in unison.\n\n7. The error term is normally distributed (optional)\n\nOLS does not require that the error term follows a normal distribution to produce unbiased estimates with the minimum variance. However, satisfying this assumption allows you to perform statistical hypothesis testing and generate reliable confidence intervals and prediction intervals.\n\n\n\nThe easiest way to determine whether the residuals follow a normal distribution is to assess a normal probability plot. If the residuals follow the straight line on this type of graph, they are normally distributed. They look good on the plot below!\n\n\n\nhttps://statisticsbyjim.com/regression/ols-linear-regression-assumptions/\n\n\nDefinitions\nDependent variable\n\nThe value you want to predict\nThe y value\n\nIndependent variable\n\nEach parameter given as input to the model can be called an independent variable\nAlso known as the x values or predictors\n\nR squared\n\nDetermines how well your model can predict the dependent variable\n\nNull hypothesis\n\nStates that the model with no independent variables fits the data as well as your model\n\nF-test\n\nTests if your model has statiscal significant or fits the null hypothesis\n\nSignificant value\n\nThe probability of rejecting the null hypothesis. This value is usually 0.05 or 5%.\n\nP-value\n\nDetermines the statistical significance of each independent variable. A P value < the the signicant value means that the independent variable has statistical significance.\n\nCoeficients\n\nThe coeficient for an independent variable determines the rate of change for the dependent variable. Meaning that a coeficient of 2 for a variable x will change y by 2x for every unit in change of x (y = 2x) .\n\n\n\n'], 'url_profile': 'https://github.com/RenierVeiga', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', '1', 'R', 'BSD-3-Clause license', 'Updated Jul 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 12, 2021', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Julia', 'MIT license', 'Updated Mar 2, 2021', 'Python', 'Updated Jul 23, 2020']}","{'location': 'Palo Alto, CA', 'stats_list': [], 'contributions': '436 contributions\n        in the last year', 'description': ['SigmaRidgeRegression.jl\n\n\n\nAutomatically and optimally-tuned Ridge regression when the features may be partitioned into groups.\nSee the manuscript below for a theoretical description of the method.\n\nIgnatiadis, Nikolaos, and Panagiotis Lolas. ""Group-regularized ridge regression via\nempirical Bayes noise level cross-validation."" arXiv:2010.15817 (2020+)\n\nThe folder reproduction_code in this repository contains code to reproduce the results of the paper.\nInstallation\nThe package is available on the Julia registry (for Julia version 1.5) and may be installed as follows:\nusing Pkg\nPkg.add(""SigmaRidgeRegression"")\nExample usage\nSigmaRidgeRegression.jl can be used alongside the MLJ framework for machine learning in Julia.\nusing MLJ\nusing SigmaRidgeRegression\nusing Random\n\n# Suppose we have three groups of features, each with n observations\n# and 25, 50 and 100 features respectively\nn = 400\nRandom.seed!(1)\np1 = 25 ; X1 = randn(n, p1)\np2 = 50 ; X2 = randn(n, p2)\np3 = 100; X3 = randn(n, p3)\n\n# The signal in the regression of the coefficients across these groups varies\nŒ±1_sq = 4.0 ;  Œ≤s1 = randn(p1) .* sqrt(Œ±1_sq / p1)\nŒ±2_sq = 8.0 ;  Œ≤s2 = randn(p2) .* sqrt(Œ±2_sq / p2)\nŒ±3_sq = 12.0;  Œ≤s3 = randn(p3) .* sqrt(Œ±3_sq / p3)\n\n# Let us concatenate the results and create a response\nX = [X1 X2 X3]\nŒ≤s = [Œ≤s1; Œ≤s2; Œ≤s3]\nœÉ = 4.0\nY = X*Œ≤s .+ œÉ .* randn(n)\n\n# Let us make a `GroupedFeatures` object that describes the feature grouping\n# !!NOTE!! Right now the features are expected to be ordered consecutively in groups\n# i.e., the first p1 features belong to group 1 etc.\ngroups = GroupedFeatures([p1;p2;p3])\n\n# Create MLJ machine and fit SigmaRidgeRegression:\nsigma_model = LooSigmaRidgeRegressor(;groups=groups)\nmach_sigma_model = machine(sigma_model,  MLJ.table(X), Y)\nfit!(mach_sigma_model)\n\n# How well are we estimating the true X*Œ≤s in mean squared error?\nmean(abs2, X*Œ≤s .- predict(mach_sigma_model))  # 4.612726430034071\n\n# In this case we may compare also to the Bayes risk\nŒªs_opt = œÉ^2 ./ [Œ±1_sq; Œ±2_sq; Œ±3_sq] .* groups.ps ./n\nbayes = MultiGroupRidgeRegressor(;groups=groups, Œªs=Œªs_opt, center=false, scale=false)\n\n\nmach_bayes = machine(bayes, MLJ.table(X), Y)\nfit!(mach_bayes)\nmean(abs2, X*Œ≤s .- predict(mach_bayes)) #4.356913540118585\nTODOs\n\nFully implement the MLJ interface.\nWait for the following MLJ issue to be fixed: https://github.com/alan-turing-institute/MLJBase.jl/issues/428#issuecomment-708141459, in the meantime this package uses type piracy as in the linked comment to accommodate a large number of features.\n\n'], 'url_profile': 'https://github.com/nignatiadis', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', '1', 'R', 'BSD-3-Clause license', 'Updated Jul 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 12, 2021', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Julia', 'MIT license', 'Updated Mar 2, 2021', 'Python', 'Updated Jul 23, 2020']}","{'location': 'Los Angeles, CA ', 'stats_list': [], 'contributions': '341 contributions\n        in the last year', 'description': ['Car-Price-Prediction\nThe dataset was provided by https://www.kaggle.com/hellbuoy/car-price-prediction. Multi-linear Regression model was used to predict the price of the cars after analysis the dataset and observing linear relationships with the features.\n'], 'url_profile': 'https://github.com/ashrestha11', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', '1', 'R', 'BSD-3-Clause license', 'Updated Jul 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'MIT license', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 12, 2021', 'Jupyter Notebook', 'Updated Feb 2, 2020', '2', 'Julia', 'MIT license', 'Updated Mar 2, 2021', 'Python', 'Updated Jul 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '187 contributions\n        in the last year', 'description': ['ML LINEAR REGRESSION MODEL\n'], 'url_profile': 'https://github.com/rakki-18', 'info_list': ['Python', 'Updated Mar 31, 2020', 'CSS', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '202 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/deeprajbasu', 'info_list': ['Python', 'Updated Mar 31, 2020', 'CSS', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': [""House Price prediction Regression project\nKeywords:\nData pre-processing, Data Analysis, Model building, Hyperparameter Optimization\nSummary:\n\n\nPredicting the Sale Price of a House based on 81 different Features, out of which some Features had more than 80% Null values\n\n\nUsed Statistical Analysis to handle missing data and Categorical Features\n\n\nApplied XGBoost Regression algorithm to build an efficient model for prediction\n\n\nRandomizedSearchCV to get the best parameters for XGBoost\n\n\nClick on 'house_price_prediction.ipynb' file for the Analysis code\n""], 'url_profile': 'https://github.com/sandesh-shinde', 'info_list': ['Python', 'Updated Mar 31, 2020', 'CSS', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['LINEAR REGRESSION\nOBJECTIVE\nIn respect to predict bakery sales - linear regression is groundwork and indispensable.\nThis repo gives a simple solultion how linear regression works.\nAPPLICATION\nThis application is written in Python.\nUTILIZED INTERPRETER\nPython Interpreter: v3.6\nCONTRIBUTION\nContributions are what make the open source community such an amazing place to be learn, inspire, and create.\n\nThank you for helping and improving this work.\nLICENCE\nGNU GENERAL PUBLIC LICENSE. Feel free to view and fork this project for personal use.\nGET IN CONTACT\nGithub - gremarsl\nE-Mail:  startwitharduino@gmail.com \n'], 'url_profile': 'https://github.com/gremarsl', 'info_list': ['Python', 'Updated Mar 31, 2020', 'CSS', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['multiple-regression-\nmultiple regression on startup dataset\n'], 'url_profile': 'https://github.com/Shagunaawasthi', 'info_list': ['Python', 'Updated Mar 31, 2020', 'CSS', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Phoneix-15', 'info_list': ['Python', 'Updated Mar 31, 2020', 'CSS', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Boston-House-Pricing-Linear-Regression\nBoston House Pricing Linear Regression\n'], 'url_profile': 'https://github.com/mayank171986', 'info_list': ['Python', 'Updated Mar 31, 2020', 'CSS', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'France', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['intellij-ant-sql-test\nSimple test project to check Jetbrains Intellij ant sql task support.\nIt used to work in older versions (2019.1) but does not work with latest Intellij version (2019.3.2)\nUsage\n\nclone repo\nbuild custom jar with build.xml\nopen build.test.xml\n\n'], 'url_profile': 'https://github.com/mguessan', 'info_list': ['Python', 'Updated Mar 31, 2020', 'CSS', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amuge1997', 'info_list': ['Python', 'Updated Mar 31, 2020', 'CSS', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['HousePrice\nPredict house sales prices using Linear Regression, Random Forests and LASSO regression.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in a city in the USA, this project is to predict the final price of each home by using advanced data wrangling in Python, creative feature engineering and applying machine learning techniques.\n'], 'url_profile': 'https://github.com/ashleylixin', 'info_list': ['Python', 'Updated Mar 31, 2020', 'CSS', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}"
"{'location': 'Dallas, Texas', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['Linear Regression\nExtend ordinary least squares regression, which uses the hypothesis class of linear regression functions, to non-linear regression functions modeled using polynomial basis functions and radial basis functions\nSteps to run\n\nRun the linearRegression.py file under the src folder\nObserver the plots generated. One such instance is available in src\\plots\n\n'], 'url_profile': 'https://github.com/amtul-nazneen', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Jan 29, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', '1', 'Updated Jun 26, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['polynomial_regression\npolynomial regression on employee salary data set . A regression template is also present in the repository.\n'], 'url_profile': 'https://github.com/Shagunaawasthi', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Jan 29, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', '1', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Required dependencies:\npip install numpy pandas h5py scikit-learn\n'], 'url_profile': 'https://github.com/zeratul87', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Jan 29, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', '1', 'Updated Jun 26, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Machine-Learning-Live-Project\nHouse Sale Price Analysis using Linear Regression based on https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/nileshjoshic', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Jan 29, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', '1', 'Updated Jun 26, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '233 contributions\n        in the last year', 'description': ['Machine-Learning\nCSE4020: Machine Learning\nList of programs in my Machine Learning Course in Winter Semester 2019-20\n\nLinear Regression\nMultiple Linear Regression\nDiff between Linear and Polynomial Regression\nClassification - Decision Tree\nClassification - K-Nearest Neighbours\nDiff between Linear and Polynomial Regression\n\n'], 'url_profile': 'https://github.com/satyam9090', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Jan 29, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', '1', 'Updated Jun 26, 2020']}","{'location': 'Galway', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/seshadri95', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Jan 29, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', '1', 'Updated Jun 26, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Machine-Learning-With-Python-Linear-Regression\nMachine Learning With Python, Linear Regression\n'], 'url_profile': 'https://github.com/yashmahes', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Jan 29, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', '1', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['LinearRegression\nLinear regression built from scratch in python\n'], 'url_profile': 'https://github.com/sperek27', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Jan 29, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', '1', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['simple-Linear-Regression\nSimple Linear implementation with python\n'], 'url_profile': 'https://github.com/SithiasmaBasheer', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Jan 29, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', '1', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '99 contributions\n        in the last year', 'description': [""Happiness Description (2020)\nThe purpose of this project is to predict the happiness score of a country. The happiness score is determined by the 2017 world happiness report. There are two models, a regression and classification model. I use a support vector machine for a classifications and a penalized ridge regression. The variables descriptions and sources are in the file happiness_variables file.\nRMSE is used to validate the regression and accuracy is used to validate the classification model. Other metrics derived from the confusion matrix are included as well. A residual analysis is at the end of the markdown file.\nAbortion Survey Description (2019)\nThis school project is a classification meant to understant/predict people that would vote to ban/permit abortion. The dataset is from the 2011 Canadian National Election Study by York University. Methods of unsupersived learning such as, association rules and clustering are used to explore the relationship between charateristics and a person's stance. A Neural Network model predict an individual's vote. AbortionSurvey_variables has the descriptions of the variables and the data preprocessing.\n""], 'url_profile': 'https://github.com/arunagossai', 'info_list': ['Python', 'Updated May 10, 2020', 'Python', 'Updated Jan 29, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', '1', 'Updated Jun 26, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '293 contributions\n        in the last year', 'description': ['bayesian_linear_regression_bandit\nBayesian Linear Regression using different methods\n'], 'url_profile': 'https://github.com/adisetyop', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'TypeScript', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yoannpruvot', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'TypeScript', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ma2b0043', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'TypeScript', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 2, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Apprentice_Chef.Regression_model\nDeliverables: Predicting revenue for first year users through simple machine learning processes and present 2 insights\nand 1 actionable recommendation.\nCourse Case: Apprentice Chef\nCase Challenge Part I\nAfter three years serving customers across the San Francisco Bay Area, the executives at\nApprentice Chef have come to realize that over 90% of their revenue comes from customers\nthat have been ordering meal sets for 12 months or less. Given this information, they would like\nto better understand how much revenue to expect from each customer within their first year of\norders. Thus, they have hired you on a full-time contract to analyze their data, develop your top\ninsights, and build a machine learning model to predict revenue over the first year of each\ncustomer‚Äôs life cycle.\n'], 'url_profile': 'https://github.com/markus-proesch', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'TypeScript', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 2, 2020']}","{'location': 'Guangzhou, China', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rin9o', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'TypeScript', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 2, 2020']}","{'location': 'chennai', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['RegressionModel-houseprice\nRegression model to predict the price of the house based upon the given parameters .\n'], 'url_profile': 'https://github.com/Ayush4087', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'TypeScript', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Fairness-Improving-Machine-Learning\nML in linear logistic regression in python\n‚Ä¢ Implemented a convolutional neural network with Python PyTorch module to perform a binary\nclassification on a multi-feature dataset using logistic loss function\n‚Ä¢ Increased the fairness of the classification by adjusting weights on different subgroups and adding\nregularization terms to the classifier to lower the FNR discrepancy\n‚Ä¢ The final model achieved 88% prediction accuracy and 8.5% FNR discrepancy\n'], 'url_profile': 'https://github.com/gchenra', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'TypeScript', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 2, 2020']}","{'location': 'Argentina', 'stats_list': [], 'contributions': '149 contributions\n        in the last year', 'description': ['car-prices-regression\nSimple linear regression to predict car prices\n'], 'url_profile': 'https://github.com/federicobaiocco', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'TypeScript', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 2, 2020']}","{'location': 'INDIA', 'stats_list': [], 'contributions': '126 contributions\n        in the last year', 'description': ['Regression-Tree-Without-Scikit\nRegression Tree in Python from Scratch\nLanguage Used:      Python\nPreRequisite:       Data Preprocessing,Python Basic,Basic Algebra(weighted mean square error)\n\nTarget:             We need to compute price of the House based on the 81 feature provided in Training Date.\n\nDataSets Provided:  Training Data of Houses.\n                    Testing data and Actual value to compare with your predictions.\n                    \nRunning Guidelenes: Download the Testing Data ,Training Data, Test_labels(correct answers)\n                    Provide the correct path of your Data files\n                    \nHow to run:          Linux terminal : python3 filename.py\n                     or any python IDE\n\n'], 'url_profile': 'https://github.com/jeeveshkataria', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'TypeScript', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['kaggle-house\nkaggle-house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/prafullkatiyar', 'info_list': ['1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'TypeScript', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['kaggle-house\nkaggle-house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/prafullkatiyar', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Phoneix-15', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['WineQuality\nTried to fit different regression models\nThe dataset was downloaded from the Kaggle.\nhttps://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009\nAttribute Information:\nFor more information, read [Cortez et al., 2009]. Input variables (based on physicochemical tests): 1 - fixed acidity 2 - volatile acidity 3 - citric acid 4 - residual sugar 5 - chlorides 6 - free sulfur dioxide 7 - total sulfur dioxide 8 - density 9 - pH 10 - sulphates 11 - alcohol Output variable (based on sensory data): 12 - quality (score between 0 and 10)\nAcknowledgements:\nP. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n'], 'url_profile': 'https://github.com/soumya1103', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020']}","{'location': 'Bloomington', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Bayesian Linear Regression\nTask :\nGoals for this project are :\n(i) to investigate the effect of the number of examples, the numberof features, and the regularization parameter on the performance of the corresponding algorithms\n(ii) to investigate two methods for model selection in linear regression (evidence maximization and cross validation).\nPerformance is evaluated on the basis of Mean Square Error\nExample on sequence of parameter:\n\\python3 pp2.py train-1000-100.csv trainR-1000-100.csv test-1000-100.csv testR-1000-100.csv\\\n\nNumber of Parameter : 4\nOutput:\nFirst graph is for task 1\nSecond Graph is for task 2\nAfter that on output screen results for task 3.1 and task 3.2\nRefer report for more details\n'], 'url_profile': 'https://github.com/tolia-deepali', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/piyushjain967', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['machine learning algorithm to calculate line equation from a dataset with linear regression and gradient descent\nDependancies\n\nPython 3\nnumpy\nmatplotlib\n\nStart by training your model by providing it your datatset to learn.py\nusage: learn.py [-h] [-r RATE] [-e EPOCH] [-v] [-ve] dataset\n\npositional arguments:\n  dataset               your dataset file in csv format\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -r RATE, --rate RATE  set the learning rate, defaul: 0.01\n  -e EPOCH, --epoch EPOCH\n                        set the epoch number, default: 4\n  -v, --visualise       get a visual feedback\n  -ve, --verror         visualise error evolution\n\nThen use estimate.py to estimate values based on the previously computed line equation\nVisual results\n\n\n'], 'url_profile': 'https://github.com/angauber', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020']}","{'location': 'jaipur', 'stats_list': [], 'contributions': '168 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sv6375261073', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kevineleven6677', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020']}","{'location': 'cairo ,egypt', 'stats_list': [], 'contributions': '295 contributions\n        in the last year', 'description': ['machine-learning-regression-models\nmachine learning regression projects (6 models)\n'], 'url_profile': 'https://github.com/MAHMOUDRR707', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['HousePrices\nFor Kaggle "" House Prices: Advanced Regression Techniques""\n'], 'url_profile': 'https://github.com/Kronenbouh', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020']}"
"{'location': 'Winnipeg, Manitoba', 'stats_list': [], 'contributions': '688 contributions\n        in the last year', 'description': ['House-Price-Prediction\nThe objecive of this project is to predict house price from different features.\nData Set Info:\nCheck train.csv\nNumber of Instances: 1460\nNumber of Features: 80\nApplied Algorithms:\nLinear Regression\nDecision Tree\nSVM\nRandom Forest\nAdaBoost\nGradientBoost\nXGBoost\nFeature Engineering\n\n\nPerformace:\nCheck performance.csv for details.\n\n\nScripts:\nCheck housePricePredict.py\n'], 'url_profile': 'https://github.com/ShahedSabab', 'info_list': ['Python', 'Updated May 28, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2019', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Feb 2, 2020', 'JavaScript', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Linear regression based on QR decomposition\nGram-Schmidt Method is used for implementation of QR decomposition.\nIf we consider our formula as :\ny = b*X\nParameters\nX  : 1d array\ninput values\ny : 1d array\nconsidered output values of the model\nExample\n    # Training input data\n\tX = np.array([\n    [1],\n    [2],\n    [-1]])            \n\t\n    # Training target data\n    y = np.array([\n    [0],\n    [1],\n    [-2]])  \n    \n\t# Create a model\n    mdl = LinearRegression()\n    \n\t# Train model with training data\n    b = mdl.fit(X,y)\n    \n    # Inut test for testing the model with new values\n    Xtest = np.array([\n    [3],\n    [-2]])  \n    \n    # Target values of the input test\n    Ytarget = np.array([\n    [2],\n    [-3]])  \n\t\n    # Apply test value to the model\n    Ytest = mdl.predict(Xtest)\n\t\n    # MSE,RMSE Calculation\n    MSE,RMSE = mdl.model_accuray(Ytarget, Ytest)   \n\t\nNotes\n\n\nIf you do not have good value for MSE and RMSE, try to add more point to X and y based on your equation or system to help the model for finding a better model.\n\n\nPlease pay attention that adding irrelevant data will lead to a worse model.\n\n\nThis model is a one-dimensional polynomial equation\n\n\n'], 'url_profile': 'https://github.com/PymatFlow', 'info_list': ['Python', 'Updated May 28, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2019', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Feb 2, 2020', 'JavaScript', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '496 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/robert-cronin', 'info_list': ['Python', 'Updated May 28, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2019', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Feb 2, 2020', 'JavaScript', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Linear_regression_Construction_data\nThe following is an outline for a simple data analysis project.\nPlease download the dataset https://s3.amazonaws.com/cc-analytics-datasets/Building_Permits.csv. The provided dataset comes from the City of Raleigh Open Data website and is\nbased upon pending/granted building permits.\nHere are the following concepts I have worked on\n\nLoad the data from the provided source via a web request rather than downloading a local copy and loading from disk.\nReview the summary statistics for the included features.\no Number of rows and columns in the dataset\no Total different types of construction\no Mean and median number of stories\no Standard deviation for the X and Y coordinates of the permits\nPlot the distributions for each of the following features: Estimated Project Cost and Issue Date Month. Describe the distributions for these fields and explain what insights you might be able to gather.\nA hypothetical executive team is interested in the behavior between Permit Issue Year and Estimated Project Cost, but only for the ""New"" construction work class, with a construction type of ""V B"", and with less than 3 stories. Perform a simple regression analysis of this relationship and describe what insights we can gleam from this using success metrics. (Hint: Implement handling for missing values and explain your reasoning.)\nWhat additional techniques or methodologies could be used to improve the results from the previous step?\n\n\n'], 'url_profile': 'https://github.com/thsalikiprasanna', 'info_list': ['Python', 'Updated May 28, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2019', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Feb 2, 2020', 'JavaScript', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Dublin, ireland', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tipson007', 'info_list': ['Python', 'Updated May 28, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2019', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Feb 2, 2020', 'JavaScript', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression-Python\nContains small and large regression problems\n'], 'url_profile': 'https://github.com/AGK-notebook', 'info_list': ['Python', 'Updated May 28, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2019', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Feb 2, 2020', 'JavaScript', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'New Delhi,India', 'stats_list': [], 'contributions': '677 contributions\n        in the last year', 'description': ['LinearRegression\nPredicting Stock Prices using Linear Regression\n'], 'url_profile': 'https://github.com/radioactive11', 'info_list': ['Python', 'Updated May 28, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2019', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Feb 2, 2020', 'JavaScript', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PoobalanMuthu', 'info_list': ['Python', 'Updated May 28, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2019', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Feb 2, 2020', 'JavaScript', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Th-John', 'info_list': ['Python', 'Updated May 28, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2019', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Feb 2, 2020', 'JavaScript', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Telangana', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/apurvi96', 'info_list': ['Python', 'Updated May 28, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2019', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Feb 2, 2020', 'JavaScript', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Regression-\n'], 'url_profile': 'https://github.com/PriyankaBad', 'info_list': ['Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Feb 3, 2020', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2021', 'Jupyter Notebook', 'ODbL-1.0 license', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ashish-ECE', 'info_list': ['Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Feb 3, 2020', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2021', 'Jupyter Notebook', 'ODbL-1.0 license', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Dont_Call_Me_Turkey_Logistic\nLogistic_Regression\n'], 'url_profile': 'https://github.com/SunnyJia1010', 'info_list': ['Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Feb 3, 2020', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2021', 'Jupyter Notebook', 'ODbL-1.0 license', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['ApplyKernel\nA Python (PySpark) Package to extend linear mllib regression models to process non-linear data using Kernels.\nInstall the package via:\npip install git+https://github.com/heikowagner/ApplyKernel.git\n\nTheoretical Insights\nThe theoretical foundation for the kernel method can be found at https://www.thebigdatablog.com/non-linear-support-vector-machines-svm-in-spark/ implementation details at https://www.thebigdatablog.com/non-linear-classification-methods-in-spark/.\nUsage\nTo train a model:\nTrainedModel= ApplyKernel(<Model>, <Kernel>, <Bandwidth>).train(<LabeledPointVector>).\nFor prediction:\nTrainedModel.predict(<FeatureVector>).\n##Example:\nfrom ApplyKernel import ApplyKernel, RadialKernel\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n##Generate data\n#Simulation\nN=500\nY= np.random.randint(0,2,N)\ndegree=np.random.normal(0,1,N)*2*np.pi\nX= [0+ (0.5 + Y*0.5)* np.cos(degree)+ np.random.normal(0,2,N)*0.05, 0 + (0.5 + Y*0.5)*np.sin(degree)+ np.random.normal(0,2,N)*0.05   ]\n\n#plot data\nplt.scatter(X[0], X[1], c=Y)\nplt.show()\n\n#Create LabeledPoint Vector\nfrom pyspark.mllib.regression import LabeledPoint\nX_par=sc.parallelize(np.transpose(X)).zipWithIndex().map(lambda(x,y) : (y,x) )\nY_par=sc.parallelize(np.transpose(Y)).zipWithIndex().map(lambda(x,y) : (y,x) )\nY_X= Y_par.join(X_par).map(lambda(y,x) : LabeledPoint(x[0], x[1])  )\n\nfrom pyspark.mllib.regression import LinearRegressionModel, LinearRegressionWithSGD\nfrom pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\nfrom pyspark.mllib.classification import SVMModel, SVMWithSGD\n\n##Train the Models\n#KernelRegression= ApplyKernel(LinearRegressionWithSGD, RadialKernel, 0.5).train(Y_X)\n#KernelLogit= ApplyKernel(LogisticRegressionWithLBFGS, RadialKernel, 0.5).train(Y_X)\n\nKernelSVM= ApplyKernel(SVMWithSGD, RadialKernel, 0.5).train(Y_X)\n\n##Simulate Test Set\nN=200\n\nY_test= np.array( np.random.randint(0,2,N) )\ndegree=np.random.normal(0,1,N)*2*np.pi\nX_test= np.array( [0+ (0.5 + Y_test*0.5)* np.cos(degree)+ np.random.normal(0,2,N)*0.05, 0 + (0.5 + Y_test*0.5)*np.sin(degree)+ np.random.normal(0,2,N)*0.05   ])\nX_par= sc.parallelize( X_test.transpose() )\n\n##Predict Group\nPreds = KernelSVM.predict(X_par)\n\n##Evaluate Model\nsc_Y=sc.parallelize( Y_test ).zipWithIndex().map(lambda (x,y): (y,x))\nlabelsAndPreds=Preds.zipWithIndex().map(lambda (x,y): (y,x)).join( sc_Y ).map(lambda (x,y): y)\n\ntestErr = labelsAndPreds.filter(lambda (x,y): y != x).count() / float(labelsAndPreds.count())\nprint(""Training Error = "" + str(testErr))\nplt.scatter(X_test[0], X_test[1], c=Preds.collect() )\nplt.show()\n'], 'url_profile': 'https://github.com/heikowagner', 'info_list': ['Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Feb 3, 2020', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2021', 'Jupyter Notebook', 'ODbL-1.0 license', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Linear-Polynominal-Regression\nI am Creating a tutorial on Linear Polynominal Regression\nThe work is in Development.\n'], 'url_profile': 'https://github.com/Junayed-Rafi', 'info_list': ['Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Feb 3, 2020', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2021', 'Jupyter Notebook', 'ODbL-1.0 license', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['Car Price Prediction\nProblem Statement:\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\nWhich variables are significant in predicting the price of a car?\nHow well those variables describe the price of a car?\n\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\nBusiness Goal:\nwe are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with respect to the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. The model will act as a better medium for management to understand the pricing dynamics of a new market.\nFollowing steps will be followed to reach our goal:\n\nImporting libraries\nReading the concerned dataset\nData Understanding\nData handling\nData visualization\nData preparation\nSplitting the Data and feature scaling\nBuilding a linear regression model\nResidual analysis of the train data\nMaking Predictions Using the Final Model\nModel Evaluation\nConclusion\n\n'], 'url_profile': 'https://github.com/sailyshah', 'info_list': ['Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Feb 3, 2020', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2021', 'Jupyter Notebook', 'ODbL-1.0 license', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021']}","{'location': 'Minneapolis, MN', 'stats_list': [], 'contributions': '169 contributions\n        in the last year', 'description': ['Gradient Descent for Logistic Regression in Spark\nJoel Stremmel\nSee final_report.docx for the final write-up of my analysis.\nProject Goal:\nI implement a simple Gradient Descent function in Spark from scratch, using the Python Spark interface in order to train a Logistic Regression model.  I compare this to a from-scratch, sequential implementation of Logistic Regression in Python which runs on a single machine.  I  also examine the MLlib implementation of Logistic Regression as a benchmark.  In my final report I compare the execution time required to train these models on 2, 4, and 8 worker nodes on an AWS spark cluster to achieve the same level of accuracy.\nDataset Requirements:\nFor this analysis, I use the Chicago Food Inspections dataset from HealthData.gov, containing food inspection information for food facilities in Chicago since 2010.  The classification task is simple: predict if an establishment will pass or fail a food inspection based on the establishment category (bakery, cafe, etc.), the standardized latitude and longitude, and the documented (anticipated) risk of failing.  Before processing, the dataset is 219 MB in CSV format.\nCompute Resources:\nTo complete this analysis, I use my local Python environment, as well as AWS Spark clusters of 2, 4, and 8 m5.xlarge workers with 4vCores and 16 GiB memory.\nProgramming Resources Used\nDocumentation for Python can be found here: https://docs.python.org/3.7/ and documentation for Jupyter Notebook can be found here: http://jupyter-notebook.readthedocs.io/en/latest/.\nThe following Python packages were used for this analysis:\n\npandas 0.24.2\nrequests 2.21.0\nmatplotlib 3.0.3\nnumpy 1.17.2\npyspark 2.3.2\nscikit-learn 0.21.3\n\nData Detail and Licenses\nChicago Food Inspections data from HealthData.gov: https://healthdata.gov/dataset/food-inspections.\nAt the time of writing, this dataset has 194,684 records of food inspections, and represents food inspections in the city of Chicago going back to 2010 as individual records.  Each record contains a facility type, name, address, date, zip code, longitude, latitude, risk status of the business, and inspection result, in addition to specific violations.  The dataset is licensed under the ODbL (Open Database License): http://opendefinition.org/licenses/odc-odbl/.  Of particular note, the dataset provides detailed location information allowing me to identify geographic patterns associated with passing or failing food inspections in the city of Chicago, and link to additional information about city neighborhoods to assess correlations with economic status.\nWhile this dataset has been made freely available by the city of Chicago, one ethical consideration is that the names of restaurants are made public in this dataset.  That said, it is precisely for public health reasons that this information is made public, so I do not remove these identifiers from my analysis.  However, I do not call attention to or disparage specific establishments over others, instead focusing on group trends and considering assumptions and possible confounding effects when summarizing my findings.\nTo address the potential confounding effects of economic status, I include median household income by zip code using the American Community Five Year Survey which I access through the US Census Data API.  See details on these data resources here:\n\nUS Census Data API Terms of Service: https://www.census.gov/data/developers/about/terms-of-service.html\nAPI Key Signup: https://api.census.gov/data/key_signup.html\nAPI User Guide: https://www.census.gov/data/developers/guidance/api-user-guide.html\nAmerican Community and Five Year Survey: https://www.census.gov/data/developers/data-sets/acs-5year.html\nAmerican Community and Five Year Survey Variables: https://api.census.gov/data/2017/acs/acs5/variables.html\nBlogpost I Read to Help with Setup: https://towardsdatascience.com/getting-census-data-in-5-easy-steps-a08eeb63995d\n\nDisclaimers\n\nAs required by census.gov: ""This product uses the Census Bureau Data API but is not endorsed or certified by the Census Bureau.""\nThe data directory includes data licensed under the odbl license and that license is reproduced in LICENSE.txt in accordance with the requirements of the odbl license.\n\nExample Records\n\n\n\nInspection ID\nDBA Name\nAKA Name\nLicense #\nFacility Type\nRisk\nAddress\nCity\nState\nZip\nInspection Date\nInspection Type\nResults\nViolations\nLatitude\nLongitude\nLocation\n\n\n\n\n2320831\nOGDEN PLAZA INC.\nOGDEN PLAZA INC.\n2475982.0\nGrocery Store\nRisk 3 (Low)\n3459 W OGDEN AVE\nCHICAGO\nIL\n60623.0\n10/31/19\nCanvass\nOut of Business\n\n41.85526591\n-87.71240156\n(-87.71240156240032, 41.85526590922669)\n\n\n2320793\nTACO MARIO\'S LIMITED\nTACO MARIO\'S LIMITED\n2622418.0\nMobile Food Preparer\nRisk 2 (Medium)\n2300 S THROOP ST\nCHICAGO\nIL\n60608.0\n10/30/19\nLicense\nPass\n\n41.85045102\n-87.65879786\n(-87.65879785567869, 41.85045102427)\n\n\n2320830\nTHE HOXTON, CHICAGO\nTHE HOXTON, CHICAGO\n2694640.0\nRestaurant\nRisk 2 (Medium)\n200 N GREEN ST\nCHICAGO\nIL\n60607.0\n10/31/19\nLicense\nPass\n36. THERMOMETERS PROVIDED & ACCURATE - Comments: MUST PROVIDE THERMOMETERS IN ALL REFRIGERATION UNITS AND MAINTAIN.\n41.885699200000005\n-87.64878909\n(-87.64878908937915, 41.885699197163355)\n\n\n2320717\nROCKS LAKEVIEW\nROCKS LAKEVIEW\n2304161.0\nRestaurant\nRisk 1 (High)\n3463-3467 N BROADWAY\nCHICAGO\nIL\n60657.0\n10/29/19\nCanvass Re-Inspection\nPass\n47. FOOD & NON-FOOD CONTACT SURFACES CLEANABLE, PROPERLY DESIGNED, CONSTRUCTED & USED - Comments: NOTED TORN RUBBER GASKET INSIDE THE PREP SERVICE COOLER AT THE KITCHEN PREP. INSTRUCTED TO DETAIL REPAIR AND MAINTAIN AND/OR REPLACE.\n41.94497417\n-87.64565976\n(-87.64565975587642, 41.94497417145062)\n\n\n2320618\nA BEAUTIFUL RIND\nA BEAUTIFUL RIND\n2670347.0\n\nRisk 1 (High)\n2211 N MILWAUKEE AVE\nCHICAGO\nIL\n60647.0\n10/28/19\nLicense\nNot Ready\n\n41.92107616\n-87.69413786\n(-87.69413785909323, 41.921076157561416)\n\n\n\nReferences\n\n\nSpark RDD paper with pseudocode for parallelized Logistic Regression: M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J. Franklin, S. Shenker, and I. Stoica. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. In Proceedings of NSDI, pages 15‚Äì28, 2012.\n\n\nModeling techniques and ideas come from: Hastie, Trevor, Tibshirani, Robert and Friedman, Jerome. The Elements of Statistical Learning. New York, NY, USA: Springer New York Inc., 2001.\n\n\nThe problem of identifying establishments likely to fail inspections has been addressed in part by the city of Chicago, using the data they have collected and made available.  Their approach focuses primarily on predicting establishments likely to fail inspections as a way of triaging which establishments need attention.  Their research is located here: https://github.com/Chicago/food-inspections-evaluation.\n\n\nBlogposts have covered the way the city of Chicago breaks up food establishments by risk of foodborne illness according to the type of food they serve and the way they serve it.  This blogpost details the way in which inspectors visit high-risk establishments more often than low-risk ones and has informed my thinking about the problem at hand: http://redlineproject.org/foodinspections.php.\n\n\n'], 'url_profile': 'https://github.com/jstremme', 'info_list': ['Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Feb 3, 2020', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2021', 'Jupyter Notebook', 'ODbL-1.0 license', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['H1 title ""README: Seetharaman Gudetee Git Practice""Àú\nH2 ""My first change""\nH3 ""My Second change""\n'], 'url_profile': 'https://github.com/essramm', 'info_list': ['Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Feb 3, 2020', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2021', 'Jupyter Notebook', 'ODbL-1.0 license', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['KARMA4\nKalman Auto-Regression Moving-Average Fore[4]casting\n'], 'url_profile': 'https://github.com/msalloum80', 'info_list': ['Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Feb 3, 2020', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2021', 'Jupyter Notebook', 'ODbL-1.0 license', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['lmpy\nIntroduction\nThe lmpy package implements some basic linear regression techniques, and complements existing Python implementations. It contains basic linear regression tools, including heteroskedasticity robust and clustered standard errors, as well as basic bootstrapping tools, including the wild bootstrap and the cluster robust bootstrap by Cameron, Gelbach, and Miller (2008). This project is in its very early stages and documentation is virtually nonexistent, so use it at your own risk.\nInstallation\nTo install this package, such that you can import it into Python using import lmpy as lm for example, simply clone the repo. Then, add the path you cloned it into to your Pythonpath environment variable.\n'], 'url_profile': 'https://github.com/maxhuppertz', 'info_list': ['Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Feb 3, 2020', 'Updated Jun 13, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2021', 'Jupyter Notebook', 'ODbL-1.0 license', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', '1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['KARMA4\nKalman Auto-Regression Moving-Average Fore[4]casting\n'], 'url_profile': 'https://github.com/msalloum80', 'info_list': ['1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['lmpy\nIntroduction\nThe lmpy package implements some basic linear regression techniques, and complements existing Python implementations. It contains basic linear regression tools, including heteroskedasticity robust and clustered standard errors, as well as basic bootstrapping tools, including the wild bootstrap and the cluster robust bootstrap by Cameron, Gelbach, and Miller (2008). This project is in its very early stages and documentation is virtually nonexistent, so use it at your own risk.\nInstallation\nTo install this package, such that you can import it into Python using import lmpy as lm for example, simply clone the repo. Then, add the path you cloned it into to your Pythonpath environment variable.\n'], 'url_profile': 'https://github.com/maxhuppertz', 'info_list': ['1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Toulouse', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BrunoPlzk', 'info_list': ['1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Ahmedabad', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Stock_Market_LR\nSimple Demo Usage of Linear Regression for Stock Market Prediction\nCreate a virtual Environment\ninstall the dependencies using requirements.txt\nRun the python file and graph and confidence score will be created.\nCredits: sentdex\n'], 'url_profile': 'https://github.com/Archan2607', 'info_list': ['1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear-Regression\nThis repository is a basic explanation of linear regression.\n'], 'url_profile': 'https://github.com/AiSaturdaysNairobi', 'info_list': ['1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['linear_regression\nImplemented linear regression to understand the internal working of SGD.\nPlease look at the top left corner in pdf file for the exact date of my work.\n'], 'url_profile': 'https://github.com/hanishsairohit', 'info_list': ['1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '221 contributions\n        in the last year', 'description': [""Predicting How Many Acres Will Burn in a California Wildfire\nProblem Statement\nFor the past 7 years (2013-2019), an average of 900k acres burn every year due to wildfires in California [source]. That's about the size of the San Francisco Bay Area! Furthermore, AccuWeather estimated the total damage and economic loss caused by wildfires cost California $400 billion in 2018 and $85 billion in 2017 [source].\nThe goal of this project is to predict the amount of acres burned given a California wildfire, using regression.\nAcres Burned per Wildfire\n\nFinal Pipeline Diagram\n\n""], 'url_profile': 'https://github.com/corralm', 'info_list': ['1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Greater Boston Area', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Logistic-Regression-to-Predict-Cancer-Risk\nThe project objective is to predict the probability of someone getting cancer based on a given data set containing some medical history information for patients at risk for cancer. Each patient in the data set has been diagnosed their actual cancer status. I built classifiers predicting whether a patient has cancer based on other features of that patient.\n'], 'url_profile': 'https://github.com/baovinhnguyen', 'info_list': ['1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Copenhagen', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['MScThesis-Conor-Manu\n'], 'url_profile': 'https://github.com/manu675', 'info_list': ['1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JoshMelendez310', 'info_list': ['1', 'C++', 'Updated Jan 28, 2020', 'Python', 'MIT license', 'Updated Jan 23, 2021', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Nov 26, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['MontrealLocalTV\nLinear Regression Model of Montreal Local TV channels Market Share\nThis model based on Pytorch Neural Network Module, PreProcessing and dataset normalization mention on Jupyter Notebook.\n\n\n\n\nMetric  \\  Models\nNeural Network\nLinear Regression\nPolynominal Regression\n\n\n\n\nMAE\n1.845284\n2.33\n1.88\n\n\nR-Squared\n0.546092\n0.34\n0.61\n\n\n\n\nMetrics based on test data on Real Information\n\n\n\nMy Prediction of Test Dataset can find in the test_pred.csv file\n\n'], 'url_profile': 'https://github.com/tsadr', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jun 29, 2020', '1', 'R', 'Updated Nov 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sohank6721', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jun 29, 2020', '1', 'R', 'Updated Nov 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['customerchurn\ncustomer churn prediction using logistic regression on telecom dataset\n'], 'url_profile': 'https://github.com/shreya-001', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jun 29, 2020', '1', 'R', 'Updated Nov 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['AppFeatureSelection\nGUI Performing Feature Selection for both Regression and Classification Problems!\n'], 'url_profile': 'https://github.com/vbucaj', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jun 29, 2020', '1', 'R', 'Updated Nov 9, 2020']}","{'location': 'paris france', 'stats_list': [], 'contributions': '892 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Metalesaek', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jun 29, 2020', '1', 'R', 'Updated Nov 9, 2020']}","{'location': 'Islamabad, Pakistan 44000', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': [""Polynomial Regression from Scratch\nIntroduction\nThe task is to implement and train a linear regression model on the given data for predicting housing prices.\nEnvironment\nMATLAB R2018a is used for model implementation and training.\nFiles\nThe attached zip file contains the following files:\n\n\nmain.m\nMain file of the program\n\n\ncreateHyp.m\nTo create models with higher order polynomials\n\n\ncreatePoly.m\nTo create higher order polynomial features of a feature vector\n\n\nlearningCurve.m\nTo calculate training and validation errors for learning curve\n\n\nvalidationCurve.m\nTo calculate training and testing errors with various lambda values and produce the validation curve\n\n\ndataCleaning_featureEngg.m\nImplementation of Data Cleaning and Feature Engineering process\n\n\nfeatureNorm.m\nFeature Normalization\n\n\nGD.m\nImplementation of Gradient Descent Algorithm\n\n\ncalCost.m\nImplementation of Cost Function\n\n\nPrediction.m\nImplementation of Prediction Function\n\n\nmodelEval.m\nImplementation of Model Evaluation Metrics (RMSE, R-Squared)\n\n\nsumFunc.m\nTo calculate sum of columns of input matrices.\n\n\nmulFunc.m\nTo multiply two given matrices.\n\n\nmeanFunc.m\nCalculates the mean of of individual columns of input matrix. Returns a row matrix.\n\n\ncreateMatrix.m\nCreate a Matrix of input rows, columns, element value.\n\n\ndata3.csv\nGiven Dataset\n\n\nData Set\nThe data is provided in a (.csv) format. To make the data processing task easier, the header of the file (which is the 1st row) and the first two columns containing Id and date features are removed manually (Further explained in section 2.2). The new data is saved in the file (data.csv) and is provided with the source code.\nData Headers\nThe data headers in the given (.csv) file are described in Table [tab:table1]\n[H]\n[tab:table1]\n\n\n\nSr. No\nVariable\nDescription\n\n\n\n\n1\nId\nUnique ID of each home sold\n\n\n2\nDate\nDate of the home sale\n\n\n3\nPrice\nPrice of each home sold\n\n\n4\nBedrooms\nNumber of Bedrooms\n\n\n5\nBathrooms\nNumber of Bathrooms\n\n\n6\nSqft_living\nSquare footage of apartment interior living space\n\n\n7\nSqft_lot\nSquare footage of land space\n\n\n8\nFloors\nNumber of floors\n\n\n9\nWaterfront\nA dummy variable whether the apartment was overlooking the waterfront or not\n\n\n10\nView\nAn index from 0-4 of how good the view is\n\n\n11\nCondition\nAn index from 1-5 on condition of the apartment\n\n\n12\nGrade\nAn index regarding quality level of construction and design\n\n\n13\nSqft_above\nThe square footage of the interior housing space that is above ground level\n\n\n14\nSqft_basement\nThe square footage of the interior house space that is below ground level\n\n\n15\nYr_built\nThe year the house was initially built\n\n\n16\nYr_renovated\nThe year of the house‚Äôs last renovation\n\n\n17\nZip Code\nWhat zip code area are the house is in\n\n\n18\nLat\nLatitude\n\n\n19\nLong\nLongitude\n\n\n20\nSqft_living15\nThe square footage of interior housing living space for the nearest 15 neighbours\n\n\n21\nSqft_lot15\nThe square footage of the land lots of the nearest 15 neighbours\n\n\n\nData Processing\nData Cleaning\nBy just scrolling through the data, I found out that there are some training examples in which the number of bedrooms is 0 or 33, which is certainly not possible in reality. Therefore, it is pertinent to remove such training examples. I manually removed every training example which has bedrooms either 0 or greater than 11.\nFeature Selection and Engineering\nThe Id feature is not relevant in training the linear regression model. Similarly, the date on which a house sold cannot merely tell us about the price as there can be a case when the seller only found a buyer after a long time and this did not have any effect on the price of the house whatsoever.\nAlso, the feature ‚ÄòSqft_above‚Äô is linearly dependent on ‚ÄòSqft_living‚Äô and ‚ÄòSqft_basement‚Äô features. The relationship is given in equation [eq1]\n[\\label{eq1}\nSqft_above= Sqft_{living} - Sqft_basement]\nHaving correlated features in the data can lead to the poor performance of the linear regression model. Hence, the feature vector ‚ÄòSqft_above‚Äô is removed from the data set.\nThe lat, long coordinates represent a point in 3-dimensional space. Thus, these two features can not be treated independently in training the regression model. One way to address this problem is by using the long lat coordinates of the house to calculate the distance from the lat, long coordinates of a reference location. One such reference location can be the economic hub/center of the city. As this was not given, I took the reference location of the house, which has the maximum price. If the price of a house is higher, chances are the house located in its vicinity will also tend to have higher prices. A house that is farther from the location of the house of the highest price will tend to have a lesser price.\nThe lat, long coordinates of a house which has a maximum price in the given data can be easily obtained and are implemented in (dataCleaning_featureEngg.m) file. Once the reference lat, long coordinates are known, we can use the Haversine formula to calculate the distance between the two given sets of lat-long coordinates. The Haversine formula is given in equation [eq2]\n[\\label{eq2}\ndist = 2r  \\arcsin(\\sqrt{\\sin^{2}(\\frac{lat2-lat1}{2}) + \\cos(lat1)\\cos(lat2)\\sin^{2}(\\frac{long2-long1}{2})})]\nHere (r) is the radius of sphere i.e., Earth. (lat1, long1) And (lat2, long2) are the latitudes and longitudes of two locations. A new feature vector is created (called dist), which is the distance of a house from the house of the highest price. This new feature vector is appended to the given data, and the feature vectors containing lat, long coordinates were removed as there is no need to include them in the data.\nThe ‚ÄòZipcode‚Äô feature represent categorical data, and this type of data is more useful in models such as Decision Trees and K-Means Clustering. Since we already have encoded the geographic information of the house using its lat, long coordinates, I decided to remove this feature vector too. Furthermore, Yr_built and Yr_renovated feature vectors also represent categorical data, and hence these were removed from the data set. Table [tab:table2] shows the selected features for the model.\n[H]\n[tab:table2]\n\n\n\nSr. No\nFeature\nDescription\n\n\n\n\n1\nBedrooms\nNumber of Bedrooms\n\n\n2\nBathrooms\nNumber of Bathrooms\n\n\n3\nSqft_living\nSquare footage of apartment interior living space\n\n\n4\nSqft_lot\nSquare footage of land space\n\n\n5\nFloors\nNumber of floors\n\n\n6\nWaterfront\nA dummy variable whether the apartment was overlooking the waterfront or not\n\n\n7\nView\nAn index from 0-4 of how good the view is\n\n\n8\nCondition\nAn index from 1-5 on condition of the apartment\n\n\n9\nGrade\nAn index regarding quality level of construction and design\n\n\n10\nSqft_basement\nThe square footage of the interior house space that is below ground level\n\n\n11\nSqft_living15\nThe square footage of interior housing living space for the nearest 15 neighbours\n\n\n12\nSqft_lot15\nThe square footage of the land lots of the nearest 15 neighbours\n\n\n13\ndist\nDistance of house from the house of highest price\n\n\n\nTesting and Training Dataset\nNow, the data is split in a random manner into the training and testing set. The training set contains 80% of the given data, while the testing set contains the remaining 20% of the provided data. After data splitting, corresponding feature and label vectors are created.\nFeature Normalization\nAs the given data set contains features that vary highly in their ranges, it is necessary to normalize these. If features are not normalized and have different ranges, the result is in the slow convergence of the Gradient Descent algorithm. To address this, all the features are normalized using Standardization. Feature normalization is done for all the training examples (both testing and training). Standardization basically replaces the values by their corresponding Z scores given by equation [eq3]\n[\\label{eq3}\nx^{'} = \\frac{x - \\bar{x}}{\\sigma}]\nThe new features created have their mean (\\mu = 0) and standard deviation (\\sigma =1).\nRegularized Polynomial Regression\nIn polynomial regression, the relationship between the independent variable x and the dependent variable y is modeled as nth degree polynomial in x. The general form of p-th order polynomial regression for a feature vector x is given in equation [eq4]\n[\\label{eq4}\nh_{\\theta}(x) = \\theta_0,x+\\theta_1,x+\\theta_2,x^2+\\theta_3,x^3+\\theta_4,x^4+\\cdots+\\theta_n,x^p]\nWhere (x0 = 1). (h_{\\theta}(x)) is the dependent variable, and (x), (x^2), (x^3) . . . , (x^p) are the independent variables. (\\theta_0), (\\theta_1), (\\theta_2), (\\theta_3), . . . , (\\theta_n), are regression coefficients/parameters. We need to find the values of regression parameters that best fit our training set. This can be done using the Gradient Descent algorithm\nGenerating higher-order polynomial features\nThe generation of higher-order polynomial features is implemented in a file createPoly.m. This file takes a matrix of size m by n, and a variable poly(which represent the degree of polynomial), and returns a matrix that contains features vectors up-to their poly-th order. Further explanation of the working of code is in the code file.\nRegularized linear regression cost function\nThe cost function for regularized linear regression is given as follows:\n[\\label{eqCF}\n( J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^2 ) +\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^2]\nHere (\\lambda) is a regularization parameter, and it puts a penalty on the cost J. It basically helps prevent overfitting our model to the training data.\nGradient Descent Algorithm\nGradient Descent is an iterative optimization algorithm. The objective of linear regression is to minimize the cost function given in equation [eqCF]. In gradient descent, we simultaneously update (\\theta_0,\\dots,\\theta_{n}) for a specific number of iterations for every (j = 0,\\dots,n). In case of regularized linear regression, update rule for (\\theta) is as follows:\nfor j = 0;\n[\\label{eq5b}\n(\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^2 x_j^{(i)})]\nfor (j\\geq1)\n[\\label{eq5}\n(\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^2 x_j^{(i)}) + \\frac{\\lambda}{m}\\theta_j]\nDefinition of some related terms is as follows:\n\n\nLearning rate ((\\alpha)): It determines how fast and slow the gradient descent algorithm converges. I tested multiple learning rates (0.1, 0.01, 0.001) and settled on using 0.02.\n\n\nm = number of examples in the training set.\n\n\n(\\lambda): Regularization parameter.\n\n\nThe gradient descent algorithm is implemented in (GD.m), and the computation of cost function is being implemented in (calCost.m)\nModel Selection\nHere, the model selection means choosing the degree of a polynomial of our model/hypothesis. Model selection is done using the graph of validation and training error vs. degree of a polynomial. Various models up to pth order are evaluated, and their respective validation and training errors are calculated.\nModel Selection Curve\nThe model selection curve in figure [fig:fig2a] shows the training error, and validation error as a function of the complexity(degree of polynomial) of the model considered. Up to 20th-degree polynomial based models/hypotheses are evaluated in our case. (\\lambda) is assumed zero for the model selection process.\nAs shown in the figure, the training error is decreasing as we increase the complexity of our model. The cross-validation error, on the other hand, first decreases and then starts to increase, which means we are starting to suffer from overfitting.\n\nThe Model Selection Curve shows that the cross-validation is minimum for a model of 7th-degree polynomial. Hence, this 7th-degree polynomial regression model is chosen for further process.\nConvergence of Cost Function\nFigure [fig:fig2] shows the graph of Cost Function Values vs. Number of iterations for a 7th-degree polynomial based model. The graph also confirms that our cost function is decreasing with the number of iterations, which means that our algorithm is converging.\n\nBias/Variance\nTo find out whether our model is suffering from high bias(underfit) or high variance(overfit) problem, we will plot the learning curve for our model.\nLearning Curve\nThe learning curve function is implemented in learningCruve.m file and it basically calculates regression parameters based on the first ith training examples, where i is an iterative variable (say 3000, 6000, 9000, 12000). The respective cross-validation error is also calculated for the obtained regression parameters for first ith training examples. The learning curve is shown in figure [fig:figLC]. For obtaining the learning curve, we increase the size of the training set while calculating the validation and training error using the obtained thetas.\n\nFrom figure [fig:figLC], we can conclude that our model is suffering from high bias (underfitting) issue as the performance of the cross-validation and training set end up being similar. (Regularization would not help in case of high bias, but it is being implemented as the project requirement)\nValidation Curve (Selecting (\\lambda) using cross-validation set)\nFigure [fig:figVC] shows graph of cross-validation error and training error vs different values of (\\lambda). As we can see, increasing the (\\lambda) reduces the complexity of our model, which further reduces the validation and training error. As mentioned before, the model is suffering from high bias, so regularization would not help here.\n\nDealing with high bias\nHere are a few ways to deal with the problem of underfitting or high bias:\n\n\nAdding additional features: Additional features such as the distance of each house from the house of highest price has already been incorporated. Domain knowledge might be required for further feature engineering.\n\n\nAdd polynomial terms: As evident from figure [fig:fig2a], the model with 7th order polynomial gives the lowest validation error. Another solution is to increase the polynomial variables(x1,x2) in our model, but just for cubic terms, the feature grows as (O(n^3)).\n\n\nDecreasing (\\lambda): As discussed in section 3.5, (\\lambda) is already taken zero for our model.\n\n\nPrediction\nAs required, the (prediction()) function is implemented in (prediction.m) file. The function takes two arguments as input ((\\theta_n) and features) and returns a vector containing predicted prices.\nModel Evaluation Metrics\nFor a regression model, the following metrics can be used to determine the accuracy of the model :\n\n\nMSE (Mean Square Error): It is basically the cost function given in equation [eq5a] with (\\lambda = 0).\n[\\label{eq5a}\n( MSE = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^2 )]\n\n\nRMSE (Root Mean Square Error): RMSE measures the average error performed by the model in predicting the outcome for an observation. The lower the RMSE, the better the model.\n[\\label{eq6}\n{RMSE} = \\sqrt{MSE}]\n\n\n(R^2) value: (R^2) corresponds to the squared correlation between the actual outcome values and the predicted values by the model. The higher the (R^2), the better the model. (R^2) value range is [0, 1].\n[\\label{eq6}\nR^{2} = 1 - \\frac{\\sum_{i=1}^m (x_i - \\hat{x_i}) }{\\sum_{i=1}^m (x_i - \\bar{x}) }]\nHere, (x_i) is the actual(observed), (\\hat{x_i}) is predicted output and (\\bar{x}) is the mean of actual(observed) output of the model.\n\n\nBoth the above evaluation metrics are implemented in file (modelEval.m). The RMSE and (R^2) values for training and testing set are shown in Table [tab:table4]\n[tab:table4]\n\n\n\nDataset\nMSE\nRMSE\nR-Squared\n\n\n\n\nTraining\n14828388231.48\n121771.86\n0.781840\n\n\nTesting\n17664794402.45\n132908.97\n0.728440\n\n\nValidation\n14877776739.51\n121974.49\n0.782433\n\n\n\n""], 'url_profile': 'https://github.com/umerjamil16', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jun 29, 2020', '1', 'R', 'Updated Nov 9, 2020']}","{'location': 'South Carolina, USA', 'stats_list': [], 'contributions': '234 contributions\n        in the last year', 'description': ['FuncReg\nBayesian nonparametric functional regression with non-functional covariate\n'], 'url_profile': 'https://github.com/tahmid-usc', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jun 29, 2020', '1', 'R', 'Updated Nov 9, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['ML-Logistic-Regression-Multiclass-Classification-and-digits-dataset\nML Logistic Regression, Multiclass Classification and digits dataset\n'], 'url_profile': 'https://github.com/yashmahes', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jun 29, 2020', '1', 'R', 'Updated Nov 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Implemented Logistic Regression (LR) and Linear Discriminate Analysis (LDA) from scratch\nUsed 5-fold cross validation to estimate perfromance in all of the experiments\nDataset1: Wine Quality\nLink: https://archive.ics.uci.edu/ml/datasets/Wine+Quality\nDataset2: Breast Cancer Diagnosis\nLink: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n'], 'url_profile': 'https://github.com/Zoe327', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jun 29, 2020', '1', 'R', 'Updated Nov 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ineswilms', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'MIT license', 'Updated Feb 6, 2020', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Jun 29, 2020', '1', 'R', 'Updated Nov 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['MORE\nMORE (Multi-Omics REgulation) is an R package for the application of Generalized Linear Models (GLM)\nto multi-omics data. The MORE method applies GLMs to model gene expression as a function of experimental variables,\nsuch as diseases or treatments, and the potential regulators of a given gene. The aim is to obtain specific candidate\nregulators for the biological system under study.\nInstalling\nCurrently, the package can be installed directly from GitHub using the devtools R package:\ninstall.packages(""devtools"")\ndevtools::install_github(""ConesaLab/MORE"")\n\nBefore installation, it might be necessary to install the required dependencies:\n\npbapply\nglmnet\nigraph\nMASS\nparallel\npsych\ncar\n\nUsage\nYou can find the User Guide for the package in the vignettes folder or access it directly here.\nMORE can also be run as a Shiny application. In this directory\nyou will find .zip file containing all the required inputs to run it.\nYou can find more information on how to run MORE as a Shiny app in the User Guide,\nand also in the following video.\n'], 'url_profile': 'https://github.com/ConesaLab', 'info_list': ['R', 'Updated Feb 23, 2021', 'HTML', 'Updated Jan 30, 2020', 'R', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '178 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/calzzone', 'info_list': ['R', 'Updated Feb 23, 2021', 'HTML', 'Updated Jan 30, 2020', 'R', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Customer Churn Prediction at a Telecom Company\nCase Summary:\nMobicom is a telecom carrier, which is experiencing high customer churn rate. When an Industry Survey report showed that the market is experiencing falling ARPU and high customer churn rate, the senior management at Mobicom is concerned that it will effect Mobicom even harder as they are already experiencing high customer churn.\nIn order to mitigate this industry phenomenon, at Mobicom, the senior management wanted to retain customers with the help of targeted proactive retention programs as opposed to the current case based reactive approach. In order to come up with different retention programs and target customers for each of these programs, Mobicom wanted to understand their customers‚Äô churn behavior and factors influencing it.\nProblem Statement:\nIdentify customers who would likely to churn and factors influencing their churn behavior; in order to help Mobicom retain them with the help of various proactive retention programs.\n'], 'url_profile': 'https://github.com/AjayprathapCh', 'info_list': ['R', 'Updated Feb 23, 2021', 'HTML', 'Updated Jan 30, 2020', 'R', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'Islamabad, Pakistan 44000', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': [""Linear Regression from Scratch\nIntroduction\nThe task is to implement and train a linear regression model on the given data for predicting housing prices.\nEnvironment\nMATLAB R2018a is used for model implementation and training.\nFiles\nThe attached zip file contains the following files:\n\n\nmain.m\nMain file of the program\n\n\ndataCleaning_featureEngg.m\nImplementation of Data Cleaning and Feature Engineering process\n\n\nfeatureNorm.m\nFeature Normalization\n\n\nGD.m\nImplementation of Gradient Descent Algorithm\n\n\ncalCost.m\nImplementation of Cost Function\n\n\nPrediction.m\nImplementation of Prediction Function\n\n\nmodelEval.m\nImplementation of Model Evaluation Metrics (RMSE, R-Squared)\n\n\nsumFunc.m\nTo calculate sum of columns of input matrices.\n\n\nmulFunc.m\nTo multiply two given matrices.\n\n\nmeanFunc.m\nCalculates the mean of of individual columns of input matrix. Returns a row matrix.\n\n\ncreateMatrix.m\nCreate a Matrix of input rows, columns, element value.\n\n\ndata3.csv\nGiven Dataset\n\n\nData Set\nThe data is provided in a (.csv) format. To make the data processing task easier, the header of the file (which is the 1st row) and first two columns containing Id and date features are removed manually (Further explained in Sectio 2.2). The new data is saved in the file (data.csv) and is provided with the source code.\nData Headers\nThe data headers in the given (.csv) file are described in follwoing table.\n\n\n\nSr. No\nVariable\nDescription\n\n\n\n\n1\nId\nUnique ID of each home sold\n\n\n2\nDate\nDate of the home sale\n\n\n3\nPrice\nPrice of each home sold\n\n\n4\nBedrooms\nNumber of Bedrooms\n\n\n5\nBathrooms\nNumber of Bathrooms\n\n\n6\nSqft_living\nSquare footage of apartment interior living space\n\n\n7\nSqft_lot\nSquare footage of land space\n\n\n8\nFloors\nNumber of floors\n\n\n9\nWaterfront\nA dummy variable whether the apartment was overlooking the waterfront or not\n\n\n10\nView\nAn index from 0-4 of how good the view is\n\n\n11\nCondition\nAn index from 1-5 on condition of the apartment\n\n\n12\nGrade\nAn index regarding quality level of construction and design\n\n\n13\nSqft_above\nThe square footage of the interior housing space that is above ground level\n\n\n14\nSqft_basement\nThe square footage of the interior house space that is below ground level\n\n\n15\nYr_built\nThe year the house was initially built\n\n\n16\nYr_renovated\nThe year of the house‚Äôs last renovation\n\n\n17\nZip Code\nWhat zip code area are the house is in\n\n\n18\nLat\nLatitude\n\n\n19\nLong\nLongitude\n\n\n20\nSqft_living15\nThe square footage of interior housing living space for the nearest 15 neighbours\n\n\n21\nSqft_lot15\nThe square footage of the land lots of the nearest 15 neighbours\n\n\n\nData Processing\nData Cleaning\nBy just scrolling through the data, I found out that there are some training examples in which the number of bedrooms are 0 or 33, which is certainly not possible in reality. Therefore, it is pertinent to remove such training examples. I manually removed every training example which has bedrooms either 0 or greater than 11.\nFeature Selection and Engineering\nThe Id feature is not relevant in training the linear regression model. Similarly, the date on which a house sold cannot simply tell us about the price as there can be a case when the seller only found a buyer after a long time and this did not had any effect on the price of the house whatsoever.\nAlso, the feature ‚ÄòSqft_above‚Äô is linearly dependent on ‚ÄòSqft_living‚Äô and ‚ÄòSqft_basement‚Äô features. The relationship is given in equation [eq1]\nSqft_above= Sqft_{living} - Sqft_basement]\nHaving correlated features in the data can lead to the poor performance of the linear regression model. Hence, the feature vector ‚ÄòSqft_above‚Äô is removed from the data set.\nThe lat, long coordinates represent a point in 3-dimensional space. Thus, these two features can not be treated independently in training the regression model. One way to address this problem is by using the lat long coordinates of the house to calculate the distance from the lat, long coordinates of a reference location. One such reference location can be the economic hub/center of the city. As this was not given, I took the reference location of the house, which has the maximum price. If the price of a house is higher, chances are the house located in its vicinity will also tend to have higher prices. A house that is farther from the location of the house of the highest price will tend to have a lesser price.\nThe lat, long coordinates of a house which has a maximum price in the given data can be easily obtained and is implemented in (dataCleaning_featureEngg.m) file. Once the reference lat, long coordinates are known, we can use the Haversine formula to calculate the distance between the two given set of lat long coordinates. The Haversine formula is given in equation 2 in PDF file.\nHere (r) is the radius of sphere i.e Earth. (lat1, long1) and (lat2, long2) are the latitudes and longitudes of two locations. A new feature vector is created (called dist), which is the distance of a house from the house of the highest price. This new feature vector is appended to the given data, and the feature vectors containing lat, long coordinates were removed as there is no need to include them in the data.\nThe ‚ÄòZipcode‚Äô feature represent categorical data, and this type of data is more useful in models such as Decision Trees and K-Means Clustering. Since we already have encoded the geographic information of the house using its lat, long coordinates, I decided to remove this feature vector too. Furthermore, Yr_built and Yr_renovated feature vectors also represent categorical data, and hence these were removed from the data set. Table [tab:table2] shows the selected features for the model.\n\n\n\nSr. No\nFeature\nDescription\n\n\n\n\n1\nBedrooms\nNumber of Bedrooms\n\n\n2\nBathrooms\nNumber of Bathrooms\n\n\n3\nSqft_living\nSquare footage of apartment interior living space\n\n\n4\nSqft_lot\nSquare footage of land space\n\n\n5\nFloors\nNumber of floors\n\n\n6\nWaterfront\nA dummy variable whether the apartment was overlooking the waterfront or not\n\n\n7\nView\nAn index from 0-4 of how good the view is\n\n\n8\nCondition\nAn index from 1-5 on condition of the apartment\n\n\n9\nGrade\nAn index regarding quality level of construction and design\n\n\n10\nSqft_basement\nThe square footage of the interior house space that is below ground level\n\n\n11\nSqft_living15\nThe square footage of interior housing living space for the nearest 15 neighbours\n\n\n12\nSqft_lot15\nThe square footage of the land lots of the nearest 15 neighbours\n\n\n13\ndist\nDistance of house from the house of highest price\n\n\n\nLogarithmic Transformation\nThe distribution of prices variable in the given data is highly skewed positively as shown in Figure [fig:test1]. To transform the prices variable into a more symmetrically distribution (Normal Distribution), I used logarithmic transformation . The resulting distribution is shown in Figure [fig:test2].\n\n\nTesting and Training Dataset\nNow, the data is split in a random manner into the training and testing set. The training set contains 80% of the given data, while the testing set contains the remaining 20% of the provided data. After data splitting, corresponding feature and label vectors are created.\nFeature Normalization\nAs the given data set contains features that vary highly in their ranges, it is necessary to normalize these. If features are not normalized and have different ranges, the result is in slow convergence of the Gradient Descent algorithm. To address this, all the features are normalized using Standardization. Feature normalization is done for all the training examples (both testing and training). Standardization basically replaces the values by their corresponding Z scores given by equation [eq3]\n[\\label{eq3}\nx^{'} = \\frac{x - \\bar{x}}{\\sigma}]\nThe new features created have their mean (\\mu = 0) and standard deviation (\\sigma =1).\nMultivariate Linear Regression Model\nMultivariate Linear regression represents a linear mathematical model for determining the value of one dependent variable from the values of given independent variables. The linear regression model for multivariate is given in equation [eq4]\n[\\label{eq4}\nh_{\\theta}(x) = \\theta_0,x_{0}+\\theta_1,x_{1}+\\cdots+\\theta_n,x_{n}]\nWhere (x_0 = 1). In our case, the number of features (n) are 13. (h_{\\theta}(x)) is the dependent variable, and (x_1,\\dots,x_n) are the independent variables. (\\theta_0,\\dots,\\theta_n) are regression coefficients/parameters. We need to find the values of (\\theta_0,\\dots,\\theta_n) that best fit our training set. This can be done using the Gradient Descent algorithm.\nGradient Descent Algorithm\nGradient Descent is an iterative optimization algorithm. The objective of linear regression is to minimize the cost function given by:\n[\\label{eq5}\n( \\min_{\\theta_0,\\cdots,\\theta_n}\\frac{1}{2m}\\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^2 )]\nIn gradient descent, we simultaneously update (\\theta_0,\\dots,\\theta_{n}) for a specific number of iterations for every (j = 0,\\dots,n)\n[\\label{eq5}\n(\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^2 x_j^{(i)})]\nDefinition of some related terms is as follows:\n\n\nLearning rate ((\\alpha)): It determines how fast and slow the gradient descent algorithm converges. I tested multiple learning rates (0.1, 0.01, 0.001) and settled on using 0.01.\n\n\nm = number of examples in the training set.\n\n\nEpochs: In one epoch, an entire dataset is passed through the model only once. I used 3000 epochs for training the model. After 3000 epochs, the cost function value was 0.031229\n\n\nThe gradient descent algorithm is implemented in (GD.m), and the computation of cost function is being implemented in (calCost.m) The parameters obtained using gradient descent are shown in Table [tab:table3].\n[H]\n[tab:table3]\n\n\n\n(\\theta_n)\nValue\n\n\n\n\n(\\theta_0)\n13.047217\n\n\n(\\theta_1)\n-0.010757\n\n\n(\\theta_2)\n0.033670\n\n\n(\\theta_3)\n0.174719\n\n\n(\\theta_4)\n0.032227\n\n\n(\\theta_5)\n-0.003825\n\n\n(\\theta_6)\n0.035045\n\n\n(\\theta_7)\n0.039553\n\n\n(\\theta_8)\n0.049751\n\n\n(\\theta_9)\n0.141792\n\n\n(\\theta_{10})\n-0.029253\n\n\n(\\theta_{11})\n0.091982\n\n\n(\\theta_{12})\n0.017286\n\n\n(\\theta_{13})\n-0.242666\n\n\n\nFigure [fig:fig2] shows the graph of Cost Function Values vs. Number of epochs. The graph also confirms that our cost function is decreasing with the number of epochs, which means that our algorithm is converging.\n\nPrediction\nAs required, the (prediction()) function is implemented in (prediction.m) file. The function takes two arguments as input ((\\theta_n) and features) and returns a vector containing predicted prices.\nModel Evaluation Metrics\nFor a regression model, the following metrics can be used to determine the accuracy of the model :\n\n\nRMSE (Root Mean Square Error): RMSE measures the average error performed by the model in predicting the outcome for an observation. The lower the RMSE, the better the model.\n[\\label{eq6}\n{RMSE} = \\sqrt{\\frac{1}{m}\\sum_{i=1}^m (x_i - \\hat{x_i})}]\n\n\n(R^2) value: (R^2) corresponds to the squared correlation between the actual outcome values and the predicted values by the model. The higher the (R^2), the better the model. (R^2) value range is [0, 1].\n[\\label{eq6}\nR^{2} = 1 - \\frac{\\sum_{i=1}^m (x_i - \\hat{x_i}) }{\\sum_{i=1}^m (x_i - \\bar{x}) }]\nHere, (x_i) is the actual(observed), (\\hat{x_i}) is predicted output and (\\bar{x}) is the mean of actual(observed) output of the model.\n\n\nBoth the above evaluation metrics are implemented in file (modelEval.m). The RMSE and (R^2) values for training and testing set are shown in Table [tab:table4]\n[tab:table4]\n\n\n\nDataset\nRMSE\nR-Squared\n\n\n\n\nTraining\n0.249918\n0.774634\n\n\nTesting\n0.249404\n0.776507\n\n\n\nSummary\nSo, I was able to achieve the (R^2) value of around 0.77 and RMSE value of 0.24 for both the training set and the testing set.\n""], 'url_profile': 'https://github.com/umerjamil16', 'info_list': ['R', 'Updated Feb 23, 2021', 'HTML', 'Updated Jan 30, 2020', 'R', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yanielc', 'info_list': ['R', 'Updated Feb 23, 2021', 'HTML', 'Updated Jan 30, 2020', 'R', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'Oklahoma City', 'stats_list': [], 'contributions': '212 contributions\n        in the last year', 'description': ['Psych5013\nRepository for homework from Psych5013: Regression for Psychologists\n'], 'url_profile': 'https://github.com/adrose', 'info_list': ['R', 'Updated Feb 23, 2021', 'HTML', 'Updated Jan 30, 2020', 'R', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'Newbury', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['TDS-LR-DT\nCombined forces of logistic regression and decision tree\n'], 'url_profile': 'https://github.com/AndrzejSzymanski', 'info_list': ['R', 'Updated Feb 23, 2021', 'HTML', 'Updated Jan 30, 2020', 'R', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'Utrecht, Netherlands', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['MLR-Ovarian-Cancer\nMultinomial Logistic Regression Models for Predicting Malignancy of Ovarian Cancer\n'], 'url_profile': 'https://github.com/VMTdeJong', 'info_list': ['R', 'Updated Feb 23, 2021', 'HTML', 'Updated Jan 30, 2020', 'R', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Gurneet21', 'info_list': ['R', 'Updated Feb 23, 2021', 'HTML', 'Updated Jan 30, 2020', 'R', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Random-Forest-Regression\nRandom Forest Regression implementation on employee salary data set\n'], 'url_profile': 'https://github.com/Shagunaawasthi', 'info_list': ['R', 'Updated Feb 23, 2021', 'HTML', 'Updated Jan 30, 2020', 'R', 'Updated Jan 30, 2020', 'MATLAB', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'R', 'Updated May 12, 2020', 'Jupyter Notebook', 'Updated Aug 30, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020']}"
"{'location': 'Beijing,China', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['simple_liner_regression\nsimple_liner_regression\nÁÆÄÂçïÁöÑÁ∫øÊÄßÂõûÂΩíÔºåÁî®Êù•ÂÖ•Èó®ÁöÑ„ÄÇ_2ÁöÑ‰ª£Á†ÅÂπ∂Ê≤°Êúâ‰ΩøÁî®tensorflowÔºõ_3ÁöÑ‰ª£Á†Å‰ΩøÁî®tensorflow2.0\n'], 'url_profile': 'https://github.com/numpyuncle3', 'info_list': ['Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '307 contributions\n        in the last year', 'description': ['Circle_LinearRegression\n'], 'url_profile': 'https://github.com/ShunLu91', 'info_list': ['Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ArishSayyed', 'info_list': ['Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 3, 2020']}","{'location': 'Columbus, OH', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wu3976', 'info_list': ['Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 3, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['house-price-prediction\nPredicting house prices using Linear Regression and GBR.\nThe tutorial and write up for the code can be found here\nhttps://medium.com/towards-data-science/create-a-model-to-predict-house-prices-using-python-d34fe8fad88f\nThank you\n'], 'url_profile': 'https://github.com/gauravbatra14', 'info_list': ['Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/linli1724647576', 'info_list': ['Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['LogisticRegression\n'], 'url_profile': 'https://github.com/Nayanakulkarni09', 'info_list': ['Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rahuls0599', 'info_list': ['Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '163 contributions\n        in the last year', 'description': ['DeeplearningRegression\n'], 'url_profile': 'https://github.com/SalmanKhanK', 'info_list': ['Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MujtabaArfat', 'info_list': ['Jupyter Notebook', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['logistic-regression\n'], 'url_profile': 'https://github.com/mohammedwork98', 'info_list': ['Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sankethvk', 'info_list': ['Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/morepriya', 'info_list': ['Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 28, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Linear Regression\nLinear Regression is a statistical model which is used for predicting values based on one or several independent values. Inspite of being super easy, Linear Regression is a very efficient Machine Learning algorithm.\n1. Salary v Experience\nThis is a simple linear regression with a single independent variable.\n2. 50 Startups\nThis problem includes multiple independent variables that may require Backward Elimination to remove the predictors that are not significant in the process of carrying out the result.\n'], 'url_profile': 'https://github.com/AshHasib', 'info_list': ['Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 28, 2020']}","{'location': 'Johannesburg, South Africa', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rishen-Gopaldass', 'info_list': ['Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 28, 2020']}","{'location': 'Bangalore, India ', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Linear_regression\n'], 'url_profile': 'https://github.com/akshita2k', 'info_list': ['Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/viveksood', 'info_list': ['Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['regression-regularization\n'], 'url_profile': 'https://github.com/MaryamSyed', 'info_list': ['Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/piyushr385', 'info_list': ['Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/a-guy-named-Philip', 'info_list': ['Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/varsha-jai', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 29, 2020', 'R', 'Updated Jan 28, 2020', 'JavaScript', 'MIT license', 'Updated Feb 2, 2020', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'Hirschau', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Basic_Regression\nThis repository deals with the basic data pre-processing, building up some regression models for an unknown data\nData Given:\ndataset known.csv: a dataset containing 2000 data samples, 200 input variables and 1 target variable.\ndataset unknown.csv: a dataset containing 500 data samples in similar format, but where the target values are unknown.\nObjectives The 3 main objectives of the task are:\nExplore the data, understand its structure and identify the key input variables that drive the target.\nTrain and test a model that predicts, to the best extent possible, the target value from some or all the input variables.\nGenerate a prediction for the unknown dataset.\n'], 'url_profile': 'https://github.com/ChanduPriya01', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 29, 2020', 'R', 'Updated Jan 28, 2020', 'JavaScript', 'MIT license', 'Updated Feb 2, 2020', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Contains code and data for the book\n""Quantile Regression for Cross-sectional and Time Series Data Applications in Energy Markets using R""\nby Jorge M. Uribe (Faculty of Economics and Business, Open University of Catalonia) and Montserrat Guillen (Department of Econometrics, University of Barcelona).\n'], 'url_profile': 'https://github.com/montserrat-guillen', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 29, 2020', 'R', 'Updated Jan 28, 2020', 'JavaScript', 'MIT license', 'Updated Feb 2, 2020', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 29, 2020', 'R', 'Updated Jan 28, 2020', 'JavaScript', 'MIT license', 'Updated Feb 2, 2020', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'Philadelphia', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['Automating your QA with Visual Regression Testing Example Repository\nThis repository is an example for the workshop Automating your QA with Visual Regression Testing. The slides that accompany this repository can be found here.\nBackstopJS is used for the visual regression testing. The app itself is built with Node JS, commander.js, and Inquirer.js.\nPrerequisites\nYou will need:\n\nA local development environment with Node JS/NPM\nA live, web-accessible WordPress site\nAnother environment of the WordPress site above (e.g. local, staging, etc.)\n\nGetting The Code\nCreate a new repository from this template and then either use Git to clone the repository or download the .zip file.\nInstructions\nAfter setting up the repository locally (see above) you will need to:\n\nRun the command npm ci to download dependencies\n\nThis only needs to be done once\n\n\nRun the command npm run start\n\nSelect the site you want to test from the list\nNote: npm run start can be used anytime you want to run the app\n\n\nCheck out the results from the sample test\n\nThey should open in your browser automatically\n\n\nEdit inc/sitesToTest.js\n\nThis is where the list of sites to test is stored\nTry changing to one (or more) of your sites\nnonProductionBaseUrl is your non-production environment (local, staging, etc.) URL\nproductionBaseUrl is your production site URL\nAdjust pathsToTest, which is the array of URIs to test for each site\n\n\nEdit inc/backstopConfig.js to adjust viewports, delay, hidden selectors, etc.\nRun the command npm run start.\n\nSelect the site you want to test from the list\n\n\n\nTroubleshooting\nIf you are having issues with the script hanging or BackstopJS taking a long time there may be headless Chrome instances that didn\'t close properly.\nTry pkill -f ""(chrome)?(--headless)"" on Mac/Linux or Get-CimInstance Win32_Process -Filter ""Name = \'chrome.exe\' AND CommandLine LIKE \'%--headless%\'"" | %{Stop-Process $_.ProcessId} in Windows PowerShell.\n'], 'url_profile': 'https://github.com/metalandcoffee', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 29, 2020', 'R', 'Updated Jan 28, 2020', 'JavaScript', 'MIT license', 'Updated Feb 2, 2020', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'Hyderabad , Telangana , India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['logistic_regression\n'], 'url_profile': 'https://github.com/mdw-hue', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 29, 2020', 'R', 'Updated Jan 28, 2020', 'JavaScript', 'MIT license', 'Updated Feb 2, 2020', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Logistic-regression\nLogistic Regression is a classification algorithm. It is used to predict a binary outcome (1 / 0, Yes / No, True / False) given a set of independent variables. To represent binary / categorical outcome, we use dummy variables.\nYou can also think of logistic regression as a special case of linear regression when the outcome variable is categorical, where we are using log of odds as dependent variable. In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function.\n'], 'url_profile': 'https://github.com/Ranjitkumarsahu1436', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 29, 2020', 'R', 'Updated Jan 28, 2020', 'JavaScript', 'MIT license', 'Updated Feb 2, 2020', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PriyankaBad', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 29, 2020', 'R', 'Updated Jan 28, 2020', 'JavaScript', 'MIT license', 'Updated Feb 2, 2020', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '327 contributions\n        in the last year', 'description': ['Linear Regression Project\nUsed Student Score.csv to perform Linear Regression to predict Score of students based on # of Hours studied.\n'], 'url_profile': 'https://github.com/Suneet-M', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 29, 2020', 'R', 'Updated Jan 28, 2020', 'JavaScript', 'MIT license', 'Updated Feb 2, 2020', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'Kyiv, Ukraine', 'stats_list': [], 'contributions': '278 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/And1sS', 'info_list': ['Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'R', 'Updated Jan 29, 2020', 'R', 'Updated Jan 28, 2020', 'JavaScript', 'MIT license', 'Updated Feb 2, 2020', 'Updated Feb 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/meechapooch', 'info_list': ['Java', 'Updated Feb 1, 2020', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Python', 'Updated Feb 2, 2020', 'R', 'Updated Feb 2, 2020', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['logistic_regression\nThis is the code file for logistic regression in R-programming language which is developed without any library dependencies.\n'], 'url_profile': 'https://github.com/TarunWuyyuru26', 'info_list': ['Java', 'Updated Feb 1, 2020', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Python', 'Updated Feb 2, 2020', 'R', 'Updated Feb 2, 2020', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/mkshailesh100', 'info_list': ['Java', 'Updated Feb 1, 2020', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Python', 'Updated Feb 2, 2020', 'R', 'Updated Feb 2, 2020', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/Akshayy708', 'info_list': ['Java', 'Updated Feb 1, 2020', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Python', 'Updated Feb 2, 2020', 'R', 'Updated Feb 2, 2020', 'Updated Feb 1, 2020']}","{'location': 'Johannesburg,ZA', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/boitshepo97', 'info_list': ['Java', 'Updated Feb 1, 2020', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Python', 'Updated Feb 2, 2020', 'R', 'Updated Feb 2, 2020', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/linli1724647576', 'info_list': ['Java', 'Updated Feb 1, 2020', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Python', 'Updated Feb 2, 2020', 'R', 'Updated Feb 2, 2020', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': ['logistic-regression\n'], 'url_profile': 'https://github.com/radhikascs', 'info_list': ['Java', 'Updated Feb 1, 2020', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Python', 'Updated Feb 2, 2020', 'R', 'Updated Feb 2, 2020', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/inescps', 'info_list': ['Java', 'Updated Feb 1, 2020', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Python', 'Updated Feb 2, 2020', 'R', 'Updated Feb 2, 2020', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '383 contributions\n        in the last year', 'description': ['linear-regression\nbeginning of ds\n#This data set include selling of news paper on a daily basis.As a data scientist i was given a task to find out the sales on weekend and also find out is it a profitable business or not with the sample space.\n'], 'url_profile': 'https://github.com/priyanshu-data', 'info_list': ['Java', 'Updated Feb 1, 2020', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Python', 'Updated Feb 2, 2020', 'R', 'Updated Feb 2, 2020', 'Updated Feb 1, 2020']}","{'location': 'Nashik, Maharashtra, India', 'stats_list': [], 'contributions': '355 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amit17133129', 'info_list': ['Java', 'Updated Feb 1, 2020', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Python', 'Updated Feb 2, 2020', 'R', 'Updated Feb 2, 2020', 'Updated Feb 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/mkshailesh100', 'info_list': ['Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 1, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'Kyiv, Ukraine', 'stats_list': [], 'contributions': '278 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/And1sS', 'info_list': ['Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 1, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Karthik1912', 'info_list': ['Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 1, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic-Regression\nPython Code and Files\n'], 'url_profile': 'https://github.com/shrikantagrawal', 'info_list': ['Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 1, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Regression Analysis\nAnalysis of tides in Brisbane\nAbbreviations\n\nMAE: Mean Absolute Error\nMSE: Mean Squared Error\n\nLiterature Review\nFrom Tensorflow [1]:\n\nMean Squared Error (MSE) is a common loss function used for regression problems (different loss functions are used for classification problems).\nSimilarly, evaluation metrics used for regression differ from classification. A common regression metric is Mean Absolute Error (MAE).\nWhen numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\nIf there is not much training data, one technique is to prefer a small network with few hidden layers to avoid overfitting.\nEarly stopping is a useful technique to prevent overfitting.\n\nUse Pandas to normalize data [4]\nDataset\nTide and current data for northern hemisphere [2]\nThe World Bank provides a Climate Data API [3].\nFarmsense moon api [5]\nMateomatics moon API [6]\nNaval Oceanography Portal [9]\nCode\nMethod for API response from [8]\nFor real-time-tide-readings.py, the text response can be character limited using the following, which can possibly be used to print specific things from the api, like the name of the port:\nprint(response.text[:100])\nPandas can be used to read json [10] [11]\nReading a json array [12]\nChangelog\nChangelog guide at [7]\nReferences\n\nAvailable from: https://www.tensorflow.org/tutorials/keras/regression\nAvailable from: https://tidesandcurrents.noaa.gov/astronomical.html\nAvailable from: https://datahelpdesk.worldbank.org/knowledgebase/articles/902061-climate-data-api\nAvailable from: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#normalization\nAvailable from: http://www.farmsense.net/api/\nAvailable from https://www.meteomatics.com/en/api/available-parameters/moon/#moon_phase\nAvailable from: https://keepachangelog.com/en/1.0.0/\nAvailable from: http://swcarpentry.github.io/web-data-python/01-getdata/\nAvailable from: https://www.usno.navy.mil/USNO/astronomical-applications/data-services\nAvailable from: https://www.marsja.se/how-to-read-and-write-json-files-using-python-and-pandas/\nAvailable from: https://pandas.pydata.org/docs/user_guide/io.html#json\nAvailable in: https://stackoverflow.com/questions/48189684/how-to-parse-json-array-of-objects-in-python\n\n'], 'url_profile': 'https://github.com/wasp-codes', 'info_list': ['Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 1, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/varvaderishikesh', 'info_list': ['Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 1, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'Ukraine', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zavhorodnia', 'info_list': ['Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 1, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['LinearRegression\nVisualization of data done by sklearn and seaborn package\n'], 'url_profile': 'https://github.com/saurabhgrade2010', 'info_list': ['Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 1, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SiddharthKalla', 'info_list': ['Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 1, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020']}","{'location': 'Sydney, Australia', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sik819', 'info_list': ['Jupyter Notebook', 'Updated May 26, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 2, 2020', 'Python', 'Updated May 10, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'HTML', 'Updated Feb 1, 2020', 'Python', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': ['linear-regression\n'], 'url_profile': 'https://github.com/radhikascs', 'info_list': ['Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'San Francisco Bay Area', 'stats_list': [], 'contributions': '557 contributions\n        in the last year', 'description': ['Linear-Regression\nQuick intro to regression with NumPy, scikit-learn, and TensorFlow.\n'], 'url_profile': 'https://github.com/aadypillai', 'info_list': ['Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/viveksood', 'info_list': ['Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/morepriya', 'info_list': ['Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'San Francisco Bay Area', 'stats_list': [], 'contributions': '557 contributions\n        in the last year', 'description': ['Tobit-Regression\nExercise involving implementation of Tobit Regression using TensorFlow, in addition to the use of neural networks to overcome non-linearities in data.\n'], 'url_profile': 'https://github.com/aadypillai', 'info_list': ['Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sundaram117', 'info_list': ['Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Multiple Regression\n'], 'url_profile': 'https://github.com/Shaninigans', 'info_list': ['Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['logistic-regression\nCode for the titanic dataset competition on Kaggle.\nWhat I learned\n\nhandling missing data\nfeature engineering on existing variables\npreparing data for the logistic regresssion\nvisualization using matplotlib\n\n'], 'url_profile': 'https://github.com/RJamesMaier', 'info_list': ['Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Johannesburg, South Africa', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rishen-Gopaldass', 'info_list': ['Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'nagpur', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SomyKamble', 'info_list': ['Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Jan 28, 2020', '1', 'Jupyter Notebook', 'Updated Jan 31, 2020']}"
"{'location': 'Tampa', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Regression-Project\nThis individual project was submitted to Prof. Ron Satterfield during the course Analytical methods for business at University of South Florida.\nTools used: R Studio, R Markdown\nThe aim of this project was to find identify factors affercting the percentage of elderly poverty in the American midwest. For detailed outcome, check out the final report.\n'], 'url_profile': 'https://github.com/arpitsrivastavaece', 'info_list': ['1', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'MIT license', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Java', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Phoneix-15', 'info_list': ['1', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'MIT license', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Java', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '818 contributions\n        in the last year', 'description': ['Regression_Optimization\nDefina uma fun√ß√£o linear que melhor represente os dados disponibilizado utilizando o m√©todo do gradiente descendente para atualizar os par√¢metros aprend√≠veis. Compare os resultados com o gradiente descendente estoc√°stico e o mini batch gradiente descendente. O banco de dados pode ser baixo em: https://www.dropbox.com/s/zfa4zx9a4uf5f2a/weight_height_edit.csv?dl=0\n'], 'url_profile': 'https://github.com/Gabriel3421', 'info_list': ['1', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'MIT license', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Java', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/82midnight', 'info_list': ['1', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'MIT license', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Java', 'Updated Feb 1, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Multiple-Regression\nMultiple regression is a statistical technique that aims to predict a variable of interest from several other variables. The variable that\'s predicted is known as the criterion. The variables that predict the criterion are known as predictors.\nIn general, the multiple regression equation of Y on X1, X2, ‚Ä¶, Xk is given by:\nY = b0 + b1 X1 + b2 X2 + ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶ + bk Xk\ny = dependent variable\nx = expanatory variables\nb0 = y-intercept (constant term)\nbk = slope coefficients for each explanatory variable\nœµ = the model‚Äôs error term (also known as the residuals)\nThe multiple regression model is based on the following assumptions:\n#There is a linear relationship between the dependent variables and the independent variables.\n#The independent variables are not too highly correlated with each other.\n#yi observations are selected independently and randomly from the population.\n#Residuals should be normally distributed with a mean of 0 and variance œÉ.\nThe coefficient of determination (R-squared) is a statistical metric that is used to measure how much of the variation in outcome can be explained by the variation in the independent variables. R2 always increases as more predictors are added to the MLR model even though the predictors may not be related to the outcome variable.\nWhen interpreting the results of a multiple regression, beta coefficients are valid while holding all other variables constant (""all else equal""). The output from a multiple regression can be displayed horizontally as an equation, or vertically in table form.\nMultiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable.\nMultiple regression is an extension of linear (OLS) regression that uses just one explanatory variable.\nMLR is used extensively in econometrics and financial inference.\n'], 'url_profile': 'https://github.com/Ranjitkumarsahu1436', 'info_list': ['1', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'MIT license', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Java', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ale1995co', 'info_list': ['1', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'MIT license', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Java', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Engineer-Hit-Songs\nTrying to understand the factors that make song popular by using linear regression with Lasso Regularization, regression tree, random forest, and K-NN\n'], 'url_profile': 'https://github.com/Lazr-Galstyan', 'info_list': ['1', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'MIT license', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Java', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bsoni08', 'info_list': ['1', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'MIT license', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Java', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aksh323', 'info_list': ['1', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'MIT license', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Java', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/meechapooch', 'info_list': ['1', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Python', 'MIT license', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 12, 2020', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Java', 'Updated Feb 1, 2020']}"
"{'location': 'Hyderabad', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Regression-Tree-From-Scratch\nThe Dataset has Categorical as well as Numeric(Continuous) data. Thus, regression decision tree has  been used.\nThe difference in the logic for categotical and numerical data has been handled accordingly in the code\nDocumentation, in the form of Jupyter Notebook, is also present\n'], 'url_profile': 'https://github.com/aru233', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated May 31, 2020', '1', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'R', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['linear_regression_project\nRegression in Python¬∂\nThis is a very quick run-through of some basic statistical concepts, adapted from Lab 4 in Harvard\'s CS109 course. Please feel free to try the original lab if you\'re feeling ambitious :-) The CS109 git repository also has the solutions if you\'re stuck.\nLinear Regression Models\nPrediction using linear regression\nLinear regression is used to model and predict continuous outcomes with normal random errors. There are nearly an infinite number of different types of regression models and each regression model is typically defined by the distribution of the prediction errors (called ""residuals"") of the type of data. Logistic regression is used to model binary outcomes whereas Poisson regression is used to predict counts. In this exercise, we\'ll see some examples of linear regression as well as Train-test splits.\nThe packages we\'ll cover are: statsmodels, seaborn, and scikit-learn. While we don\'t explicitly teach statsmodels and seaborn in the Springboard workshop, those are great libraries to know.\n'], 'url_profile': 'https://github.com/kumarravindra', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated May 31, 2020', '1', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'R', 'Updated Jan 28, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '407 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rnburn', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated May 31, 2020', '1', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'R', 'Updated Jan 28, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['Ft_linear_regression\nSupervised learning introduction\n\nSetup\nvirtualenv venv\nsource venv/bin/activate\npip install -r requirements.txt\n./train.py\n./predict.py\nUsage\n‚ùØ ./train.py\nUsage: train.py [options] dataset_file\n\nOptions:\n  -h, --help            show this help message and exit\n  -v, --visualize       show cost history & regression graph\n  -q, --quiet           hides every stdout output\n  -a ALPHA, --alpha=ALPHA, --learning-rate=ALPHA\n                        train using specified learning rate\n  -i MAX_ITER, --max-iter=MAX_ITER\n                        train using specified max_iter\n  -l, --least-square    train using the least square algorithm (more precise\n                        but slow with big datasets)\n‚ùØ ./predict.py -h\nUsage: predict.py [options] km_value\n\nOptions:\n  -h, --help            show this help message and exit\n  -f FILE, --theta-file=FILE\n                        file containing theta values\nPropreties\nDataset\n> cat data.csv\nkm(feature x1),price(target y)\n240000,3650\n139800,3800\n150500,4400\n185530,4450\n176000,5250\n114800,5350\n166800,5800\n89000,5990\n144500,5999\n84000,6200\n82029,6390\n63060,6390\n74000,6600\n97500,6800\n67000,6800\n76025,6900\n48235,6900\n93000,6990\n60949,7490\n65674,7555\n54000,7990\n68500,7990\n22899,7990\n61789,8290\nm (number of elements in the dataset) = 23\nn (number of features / element) = 1\n\nModel\nf(x) = ax + b\n\nCost function\nmean squared error\nj(a, b) = (1 / 2m) * (sum of: for i in dataset: (f(xi) - yi) )^2\n\nError minimisation algorithm\nGradient descent by default\nor least square method using the -l option on train.py\n'], 'url_profile': 'https://github.com/jjaniec', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated May 31, 2020', '1', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'R', 'Updated Jan 28, 2020']}","{'location': 'iran', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alinowshad', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated May 31, 2020', '1', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'R', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Statistical Analysis / Regression\n1. Distinguish between categorical and continuous independent variables\n\nThe idea was to split the dataset into two sub-datasets one of which consists of the continuous variables and the other of the categorical.\n\n2. Found the top continuous regressors based on the correlation coefficient\n\nPerformed some tests to find which features should be selected\n\n3. Found the top categorical regressors\n\n\nAt this point columns of dummy variables were created in order to be able to perform linear model regression analysis.\n\n\nAfter having found the highest ones were able to reduce the number of categorical features.\n\n\n4. Last step\n\n\nMerged both the top continuous and the top categorical features into one single dataframe along with the sale prices.\n\n\nPerformed a final test where the rsquared value could be obtained for the whole feature list.\n\n\n'], 'url_profile': 'https://github.com/georpap', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated May 31, 2020', '1', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'R', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Diabetes-Dataset-Linear-Regression\n#Linear Regression model of sklearn has been implemented on diabetes dataset\n'], 'url_profile': 'https://github.com/Sankalp0797', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated May 31, 2020', '1', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'R', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Regression-and-Classification\nLinear Regression with non-linear functions, bias-variance tradeoff. Classification for Gaussians and Iris data.\n'], 'url_profile': 'https://github.com/Suyash9', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated May 31, 2020', '1', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'R', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['Zindi Challenge: Sendy, in partnership with insight2impact facility, is hosting a Zindi challenge to predict the estimated time of delivery of orders, from the point of driver pickup to the point of arrival at final destination.\nThe solution will help Sendy enhance customer communication and improve the reliability of its service; which will ultimately improve customer experience. In addition, the solution will enable Sendy to realise cost savings, and ultimately reduce the cost of doing business, through improved resource management and planning for order scheduling.\nSendy helps men and women behind every type of business to trade easily, deliver more competitively, and build extraordinary businesses.\n'], 'url_profile': 'https://github.com/MathewNhari', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated May 31, 2020', '1', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'R', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['logisticRegressionWithR\n'], 'url_profile': 'https://github.com/DavidPalomeque', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Python', 'Updated May 31, 2020', '1', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2021', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'R', 'Updated Jan 28, 2020']}"
"{'location': 'Banglore', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['MultipleLinearRegression_model\nMultipleLinearRegression using backward elimination\n'], 'url_profile': 'https://github.com/saurabhgrade2010', 'info_list': ['HTML', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 2, 2020', '1', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'HTML', 'Updated Feb 2, 2020', 'C#', 'Updated Feb 2, 2020']}","{'location': 'Montreal, Quebec', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/brendo-k', 'info_list': ['HTML', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 2, 2020', '1', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'HTML', 'Updated Feb 2, 2020', 'C#', 'Updated Feb 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['video-game-sales-regression\nBefore running files in /Web Scraping and Cleaning, must change working directory to directory of Datasets\n'], 'url_profile': 'https://github.com/wlg1', 'info_list': ['HTML', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 2, 2020', '1', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'HTML', 'Updated Feb 2, 2020', 'C#', 'Updated Feb 2, 2020']}","{'location': 'Paris, France', 'stats_list': [], 'contributions': '2,115 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/arcanis', 'info_list': ['HTML', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 2, 2020', '1', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'HTML', 'Updated Feb 2, 2020', 'C#', 'Updated Feb 2, 2020']}","{'location': 'Santiago, Chile', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juanmadepalacios', 'info_list': ['HTML', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 2, 2020', '1', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'HTML', 'Updated Feb 2, 2020', 'C#', 'Updated Feb 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/creichle12', 'info_list': ['HTML', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 2, 2020', '1', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'HTML', 'Updated Feb 2, 2020', 'C#', 'Updated Feb 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/oswaldhanesh59', 'info_list': ['HTML', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 2, 2020', '1', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'HTML', 'Updated Feb 2, 2020', 'C#', 'Updated Feb 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/varvaderishikesh', 'info_list': ['HTML', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 2, 2020', '1', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'HTML', 'Updated Feb 2, 2020', 'C#', 'Updated Feb 2, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['PolynomialRegression_model\nPolynomialRegression plot by sklearn.model_selection\n'], 'url_profile': 'https://github.com/saurabhgrade2010', 'info_list': ['HTML', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 2, 2020', '1', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'HTML', 'Updated Feb 2, 2020', 'C#', 'Updated Feb 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KillerBOB999', 'info_list': ['HTML', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 12, 2020', 'Python', 'Updated Feb 2, 2020', '1', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'HTML', 'Updated Feb 2, 2020', 'C#', 'Updated Feb 2, 2020']}"
"{'location': 'Santiago, Chile', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juanmadepalacios', 'info_list': ['R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/creichle12', 'info_list': ['R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vamsiabbireddy', 'info_list': ['R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['LinearRegressionDS\n'], 'url_profile': 'https://github.com/amarquezmazzeo', 'info_list': ['R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Naifraz', 'info_list': ['R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020']}","{'location': 'Hyderabad , Telangana , India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['linear_regression_model\n'], 'url_profile': 'https://github.com/mdw-hue', 'info_list': ['R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020']}","{'location': 'Germany', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Basic_linear_regression for predicting salary based on job experience\n'], 'url_profile': 'https://github.com/shashank8794', 'info_list': ['R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020']}","{'location': 'Houston', 'stats_list': [], 'contributions': '685 contributions\n        in the last year', 'description': ['Regression-Analysis-with-R\nMy homework solutions (in R) of the Ph.D. level statistics course (STAT 608) at Texas A&M University.\nReference Book : A Modern Approach to Regression with R by Simon J. Sheather\nTopics covered:\n\nSimple Linear Regression\nDiagnostics and Transformations for Simple Linear Regression\nWeighted Least Squares\nMultiple Linear Regression\nDiagnostics and Transformations for Multiple Linear Regression\nVariable Selection\n\n\nR^2\nAIC, AIC_c, BIC\nAll possible subsets\nStepwise (Backward Eimination, Forward Selection)\nLASSO\n\n\nLogistic Regression\nSerially Correlated Errors (AR(1) Models)\n\nThe solutions are written in LaTeX and given in pdf format. All the codes are generated in R programming language. I believe there are imperfect answers; thus, some of them needs to be improved, since this course was my first encounter with R and Statistics :)\n'], 'url_profile': 'https://github.com/salihkilicli', 'info_list': ['R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['fuzzy-matlab-regression-classification\nAirfoil Self-Noise Data Set, Superconductivty Data Data Set\n'], 'url_profile': 'https://github.com/anna-kay', 'info_list': ['R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/oswaldhanesh59', 'info_list': ['R', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 29, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '386 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/carlosezmz', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Haskell', 'MIT license', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'Stevens Institute of Technology', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Linear Regression in python\nSimple python program that predicts price of square footage based on given data.\nUses sympy python library to compute derivatives and find local minimums.\n'], 'url_profile': 'https://github.com/Matthew-Viafora', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Haskell', 'MIT license', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['ECE580-HW2\nGeneral tasks about learning how to explore and manage data as well as analyze said data with techniques like regression\n'], 'url_profile': 'https://github.com/InnocentK', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Haskell', 'MIT license', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'Lagos', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Machine Learning - Linear Regression App\ninstall\nyarn install\nAdd parcel\nyard global add parcel-bundler\nBuild\nTo build\nparcel index.html\nGo to http://localhost:1234 to view\nThanks\n'], 'url_profile': 'https://github.com/drtobbyas', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Haskell', 'MIT license', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PhathuAuti', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Haskell', 'MIT license', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'France', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Danilouli', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Haskell', 'MIT license', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'Tehran, Iran', 'stats_list': [], 'contributions': '711 contributions\n        in the last year', 'description': ['HousePricing\nUniversity of Tehran\nArtificial Intelligence Fall 98 Project 0\nPredicting house prices using regression methods including Gradient Descent, Normal Equation and KNN\n'], 'url_profile': 'https://github.com/armanr99', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Haskell', 'MIT license', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/oswaldhanesh59', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Haskell', 'MIT license', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ["" 1. OVERVIEW \n 1.1 About the data: \n\nFor each purpose I used different dataset:\n1. Data engineering: Titanic dataset, I will predict the chance of survival of one person based on the factors such as sex, class, age...\n2. Logistic Regression: cat images will be used to train a model to classify cat or not. I will write the Logistic Regression code from scratch.\n\n 1.2 What you can expect in this github: \n\nDemo of how to use data engineering to better predict the outcome\nHow to build a logistic regression class\n\n\n\n 2. THE RESULT \n2.1 Data engineering\n- By doing some simple data enginerring, I can increase my accuracy score form 73% to about 80%. Let's check how I did that in the ipynb file.\n\n2.2 Logistic regression\n- A Logistic regression class = algorithms + code\n- What we do is to learn how to code these algorithms into Python and make sure it runs in the right order.\n\n\n\n""], 'url_profile': 'https://github.com/baokhanh92', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Haskell', 'MIT license', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression-Code\nJust a collection of linear regression code. Nothing fancy for future reference.\n'], 'url_profile': 'https://github.com/kyouheijames', 'info_list': ['Jupyter Notebook', 'Updated Feb 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 10, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Haskell', 'MIT license', 'Updated Mar 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'R', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Python', 'Updated Jan 31, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vamsiabbireddy', 'info_list': ['MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'JavaScript', 'MIT license', 'Updated Feb 20, 2020', 'R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'JavaScript', 'Updated Feb 3, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['LinearRegressionDS\n'], 'url_profile': 'https://github.com/amarquezmazzeo', 'info_list': ['MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'JavaScript', 'MIT license', 'Updated Feb 20, 2020', 'R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'JavaScript', 'Updated Feb 3, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Naifraz', 'info_list': ['MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'JavaScript', 'MIT license', 'Updated Feb 20, 2020', 'R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'JavaScript', 'Updated Feb 3, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['optimized-regression-model\nThis algorithm finds the best predictive model using a forward selection strategy to maximize the adjusted R squared. It then prints the diagnostic plots and relevant descriptive statistics.\n'], 'url_profile': 'https://github.com/felipehlvo', 'info_list': ['MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'JavaScript', 'MIT license', 'Updated Feb 20, 2020', 'R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'JavaScript', 'Updated Feb 3, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ruangomes93', 'info_list': ['MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'JavaScript', 'MIT license', 'Updated Feb 20, 2020', 'R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'JavaScript', 'Updated Feb 3, 2021']}","{'location': 'Guatemala', 'stats_list': [], 'contributions': '502 contributions\n        in the last year', 'description': ['Gradient Descent implementation using Linear Regression with Python\nAuthor: Alejandro Madrazo\nClass: Elements of Machine Learning @UFM 2020\n'], 'url_profile': 'https://github.com/rmadrazo97', 'info_list': ['MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'JavaScript', 'MIT license', 'Updated Feb 20, 2020', 'R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'JavaScript', 'Updated Feb 3, 2021']}","{'location': 'Vermont, USA', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Automating your QA with Visual Regression Testing Example Repository\nThis repository was originally an example from the workshop Automating your QA with Visual Regression Testing, presented by @ataylorme at WordCamp US 2019 in St Louis. The slides that accompany this repository can be found here.\n\nImportant Note About This Repo Copy\nUpdated: Feb 20, 2020\nThe original repository was removed from GitHub by the author, but having found it to be an excellent starting point after attending the presentation in November, 2019, I pushed my local here since I had foolishly not forked it at the time of the presentation. I do not plan to do additional maintenance on this repository. If you have questions you should watch the video of the presentation, Andrew Taylor: Automating Your QA with Visual Regression Testing on WordPress.tv - part 1 and part 2\n\nUpdate: Feb 20, 2020\nLooks like @davidneedham is going to carry the torch on this presentation & repo! Look there for updates & improvements.\n\nTools & Technologies\nBackstopJS is used for the visual regression testing. The app itself is built with Node JS, commander.js, and Inquirer.js.\nPrerequisites\nYou will need:\n\nA local development environment with Node JS/NPM\nA live, web-accessible WordPress site\nAnother environment of the WordPress site above (e.g. local, staging, etc.)\n\nGetting The Code\nCreate a new repository from this template and then either use Git to clone the repository or download the .zip file.\nInstructions\nAfter setting up the repository locally (see above) you will need to:\n\nRun the command npm ci to download dependencies\n\nThis only needs to be done once\n\n\nRun the command npm run start\n\nSelect the site you want to test from the list\nNote: npm run start can be used anytime you want to run the app\n\n\nCheck out the results from the sample test\n\nThey should open in your browser automatically\n\n\nEdit inc/sitesToTest.js\n\nThis is where the list of sites to test is stored\nTry changing to one (or more) of your sites\nnonProductionBaseUrl is your non-production environment (local, staging, etc.) URL\nproductionBaseUrl is your production site URL\nAdjust pathsToTest, which is the array of URIs to test for each site\n\n\nEdit inc/backstopConfig.js to adjust viewports, delay, hidden selectors, etc.\nRun the command npm run start.\n\nSelect the site you want to test from the list\n\n\n\nTroubleshooting\nIf you are having issues with the script hanging or BackstopJS taking a long time there may be headless Chrome instances that didn\'t close properly.\nTry pkill -f ""(chrome)?(--headless)"" on Mac/Linux or Get-CimInstance Win32_Process -Filter ""Name = \'chrome.exe\' AND CommandLine LIKE \'%--headless%\'"" | %{Stop-Process $_.ProcessId} in Windows PowerShell.\n'], 'url_profile': 'https://github.com/terriann', 'info_list': ['MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'JavaScript', 'MIT license', 'Updated Feb 20, 2020', 'R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'JavaScript', 'Updated Feb 3, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['linearRegressionWithR\nSimple Linear Regression & Multiple Linear Regression examples.\n'], 'url_profile': 'https://github.com/DavidPalomeque', 'info_list': ['MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'JavaScript', 'MIT license', 'Updated Feb 20, 2020', 'R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'JavaScript', 'Updated Feb 3, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NBA_Regression_Predictions\nPredicts NBA game matchups using both teams\' ""Four Factors"" described by Dean Oliver\nhttps://www.basketball-reference.com/about/factors.html\nFour Factors:\nField Goals (Shooting)\nTurnovers\nRebounds\nFree Throws\n\nUses polynomial features to form regression curves for predicting how well the 2 teams will do against each other\n'], 'url_profile': 'https://github.com/hd-tran', 'info_list': ['MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'JavaScript', 'MIT license', 'Updated Feb 20, 2020', 'R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'JavaScript', 'Updated Feb 3, 2021']}","{'location': 'Leeds ', 'stats_list': [], 'contributions': '293 contributions\n        in the last year', 'description': ['Visual Regression Demo\nIntroduction\nTest application demonstrating how BackstopJS can be utilised in order to perform visual regression tests against an\nexample application.\nBackstopJS was created and is maintained by Garris Shipon\nGetting the test app running\n$ cd visual-regression-demo/laceup\n$ yarn install\n$ yarn run start\n\nNavigate to http://localhost:3000 or http://localhost:3000?async for async version\nBackstop Setup\n$ npm i backstopjs -g\n\nRun Backstop JS Scenarios\nComplete the section Getting the test app running\nEach scenario in the presentation can be run by directing backstop to the corresponding config file\nAll the below are run from visual-regression-demo\nBasic Example\nShows the minimal configuration required to run a visual-regression test using BackstopJS\nbackstop test --config backstop/config/backstop--basic-example.js\nMobile First\nDemonstrates the capturing of the test application rendered with multiple dimensions\nbackstop test --config backstop/config/backstop--mobile-first.js\nAsync\nDemonstrates how a delay can be used in order to wait for events to occur\nbackstop test --config backstop/config/backstop--async.js\nAsyncII\nDemonstrates the weaknesses of delay, and how it can be mitigated using readySelector\nbackstop test --config backstop/config/backstop--asyncII.js\nAsyncIII\nDemonstrates the weaknesses of delay, and how it can be mitigated using readyEvent\nbackstop test --config backstop/config/backstop--asyncIII.js\nHover\nDemonstrates how backstopJS can hover over an element prior to capturing a screenshot\nbackstop test --config backstop/config/backstop--hover.js\nClick\nDemonstrates how backstopJS can click on an element prior to capturing a screenshot\nbackstop test --config backstop/config/backstop--click.js\n'], 'url_profile': 'https://github.com/sjoedwards', 'info_list': ['MATLAB', 'Updated Jan 27, 2020', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'JavaScript', 'MIT license', 'Updated Feb 20, 2020', 'R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'JavaScript', 'Updated Feb 3, 2021']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abdulazeezb', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '1', 'Python', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'CC-BY-SA-4.0 license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Andyy13', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '1', 'Python', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'CC-BY-SA-4.0 license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Tanyashree', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '1', 'Python', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'CC-BY-SA-4.0 license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SumaHuddar', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '1', 'Python', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'CC-BY-SA-4.0 license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '199 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/omrastogi', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '1', 'Python', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'CC-BY-SA-4.0 license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 28, 2020']}","{'location': 'Tamil Nadu, India', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Overview\nThis is the code for this blog post.\nDependencies\nKeras\nPandas\nMatplotlib\n'], 'url_profile': 'https://github.com/niranjanbsubramanian', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '1', 'Python', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'CC-BY-SA-4.0 license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': [""Titanic-Logistic-Regression\nLogistic Regression has been used on Titanic Dataset\nAt first the dataset has been cleaned using numpy and pandas library followed by sklearn's inbuilt Logistic Regression module.\n""], 'url_profile': 'https://github.com/Sankalp0797', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '1', 'Python', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'CC-BY-SA-4.0 license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 28, 2020']}","{'location': 'Seattle', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Linear-and-Logistic-Regression\nLinear Regression and Logistic Regression are implemented from scratch using python. Gradient Descent algorithm is implemented to find out the best parameters. Analyzing the results by plots using matplotlib and seaborn\nRegularization to avoid overfitting\nFeature Selection\n'], 'url_profile': 'https://github.com/AlekhyaRanabothu', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '1', 'Python', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'CC-BY-SA-4.0 license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 28, 2020']}","{'location': 'Earth...', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': [""Awesome Visual Regression Testing \n\nCurated list of awesome visual regression testing resources.\n\nRegression testing is a type of software testing which verifies that software which was previously developed and tested still performs the same way after it was changed or interfaced with other software. The purpose of regression testing is to ensure that changes to the software have not introduced new faults.\nForeword\nThis is intended to be an incomplete list of resources about visual regression testing. It is not tailored to a specific area or role (Developer/QA/UX-Designer). Note that this is for all areas of regression software testing after the code in question is written. For a awesome list on general software testing see e.g. awesome-testing.\nFinally, I'm sure everyone who reads this list has one thing they want to add. Please read the How to Contribute page and Feel free to add to the list!!. If you think this is helpful Please give a Star ‚òÖ.\nContents\n\nGeneral information\nBrowser automation\nTools and frameworks\nOnline services\nBlog posts\nSlideshows, talks and videos\nDeprecated\nMiscellaneous\n\nContributing\nCode of Conduct\nLicense\n\n\n\nGeneral information\n\nWikipedia: Regression testing\nSurvey of screenshot-based CSS testing tools\nvisualregressiontesting.com - Basic collection of links.\n\nBrowser automation\n\nSelenium - Browser automation framework and ecosystem.\nSlimerJS - Scriptable browser like PhantomJS, based on Firefox.\ntrifleJS - Headless automation for Internet Explorer.\nCasperJS - Navigation scripting and testing utility for PhantomJS and SlimerJS.\nWebdriver.io - Node.js bindings implementation for the W3C WebDriver protocol.\nNavalia - Browser Automation based on headless Chrome and GraphQL.\nChromeless - Chrome automation made simple. Runs locally or headless on AWS Lambda.\nCypress.io - An automation framework that runs in-browser.\n\nTools and frameworks\n\nbasset - Open source platform for generating and reviewing visual differences. Supports multiple browsers, integrations for github and slack.\nAyeSpy - 44 image comparisons in 90 seconds.\nWraith - Easy to use ruby tool with docker support.\nBackstopJS - Config-driven automated screenshot test framework.\nGalen - Java framework based on Selenium.\nGemini - Feature rich framework with support for Selenium and  CasperJS.\nHuxley - Python framework based on Selenium Webdriver.\nPhantomFlow - Experimental approach to UI testing, based on Decision Trees.\nCSSCritic - Lightweight CSS regression testing.\nSpectre - Provides image comparison capabilities and an admin interface for managing screenshots.\nShoov - UI regression and functional testing focused on Drupal 7 sites.\nqd_screenshottests - CasperJS-based UI regression and functional testing focused on Drupal 8 sites.\nOcularJS - uses PhantomJS.\nWebdriverCSS - WebdriverCSS sits on top of Webdriver.io and hooks into Selenium.\nLook-alike - Chrome Extension for taking and comparing screenshots.\nHardy - Selenium-driven, cucumber-powered CSS testing.\nTestCafe - Automated browser testing for the modern web development stack.\nNeedle - Needle is a tool for testing visuals with Selenium and nose (Python).\ngatling - Integrated visual RSpec matcher which makes real visual testing easy (Ruby).\ngrunt-photobox - Plugin to prevent your project of broken layout via screenshot photo sessions of your site.\nvrtest - JavaScript library for running visual regression tests on your components cross browser via selenium.\nHappo - Visual diffing in CI for user interfaces.\nreg-cli - Visual regression test tool which output easy-to-read single file html report.\nNightmare - High-level browser automation library based on Electron.\nPuppeteer - Headless Google Chrome Node API.\nVisual Review - A human-friendly tool for testing and reviewing visual regressions.\nreg-suit - Visual regression testing suite which compares images, stores snapshots, and notifies the difference to your GitHub repo.\nChimp - Develop acceptance tests & end-to-end tests with realtime feedback.\nDifferencify - A library for visual regression testing using Puppeteer.\nResembleJS - Analyse and compare images with Javascript and HTML5.\nMuppeteer - Visual regression testing framework for Chrome using Mocha and Puppeteer.\nember-visual-test - Simple visual regression testing for Ember.\nAET - Scalable testing tool providing visual regression testing, accessibility and performance validation, markup analysis and more.\nWendigo - Test-oriented browser automation library based on Puppeteer.\nLoki - Visual regression testing for Storybook using Chrome in docker et al.\nZombie.js - Insanely fast, headless full-stack testing using Node.js.\nCodeceptJS - Modern Era Acceptance Testing Framework for NodeJS.\nFuncUnit - A functional test suite based on jQuery\nNightwatch - Automated testing and continuous integration framework based on Node.js and using the Webdriver protocol.\nProtractor - E2E test framework for Angular apps.\njest-puppeteer-react - Visual regression testing with Jest and puppeteer for React components\ntest-crawler - Visual regression testing, by crawling a website and providing snapshot comparison reports.\n\nOnline services\n\nBrowserStack - Free for Open Source. Supports Selenium Webdriver.\nLambdaTest - Perform Automated and Live Interactive Cross Browser Testing on 2000+ Real Browsers and Operating Systems Online.\nscreener.io - For React, looks open source.\napplitools - Cloud base visual tests.\npercy.io - Continuous visual reviews for web apps.\nscreenster.io - Cloud based automation testing platform for web and mobile UI.\nMogoTest - Website Browser Testing Tool For SEO Experts.\nbrowserling - LIVE interactive cross-browser testing.\nBrowser Shots - Screenshots only.\nGhost Inspector - See introduction video.\nCrossBrowserTesting - Manual & exploratory testing on 1500+ real browsers and mobile devices.\nArgos-CI - Automate visual regression testing.\nDiffy - Cloud based visual regression tool. Uses puppeteer and proprietary comparison algorithm (detect layout shifts). Great scalability with AWS Lambda.\nChromatic - Visual testing and UI review for component libraries. Cloud-based. Video\nVisWiz.io - Flexible visual regression testing service.\nHappo - Cloud-based screenshot testing service with support for multiple browsers.\nVisual Knight - Cloud-based visual testing platform with realtime results for testing tools.\nAxcept - Testing for the whole team. Up to 100 tests in parallel. Endpoint Mocking. Code Coverage.\nFluxguard - Screenshot pixel and DOM change comparisons and regressions.\nVidiff - Cloud-based visual regression testing across stages.\nReflect - Visual regression testing and test automation tool.\n\nBlog posts\n\nKevin Lamping: The 5 best visual regression testing tools - Compares: Wraith, PhantomCSS, Gemini, WebdriverCSS and Spectre.\nGarris Shipon: Visual Regression Testing For Angular Applications -  Tutorial using BackstopJS.\nAngela Riggs: Visual Regression Testing with BackstopJS - Tutorial using BackstopJS.\nGarris Shipon: Automating CSS Regression Testing - Tutorial using BackstopJS.\nPhillip Gourley: Making visual regression useful - Why you should use BackstopJS.\nPavels Jelisejevs: Visual Regression Testing with PhantomCSS - Introduction to PhantomCSS.\nChromeless, Chrominator, Chromy, Navalia, Lambdium, GhostJS, AutoGCD - Headless Chrome is shaking up traditional approaches to test automation.\nVisual regression testing using Jest, Chromeless and AWS Lambda - Tutorial using Chromeless and jest-image-snapshot.\nMake visual regression testing easier - Introduction to Differencify and how to use it.\nVisual Regression Testing with Puppeteer & Jest - Tutorial to setup visual testing with Puppeteer, Jest and VisWiz.io.\nKeeping a React Design System consistent: using visual regression testing to save time and headaches - Using percy, and jest puppeteer to visually test a React component library.\nVisual Regression Test with WebdriverIO & WebdriverCSS - Tutorial using WebdriverIO and WebdriverCSS with Spec Reporter\n\nSlideshows, talks and videos\n\nCSS Regression Testing with Wraith - Screencast: Basic introduction to wraith, a screenshot comparison tool.\nVisual Regression Testing with Shoov - How to setup shoov and get your first test written.\nVisual Regression Testing with PhantomCSS - Talk by Jon Bellah on how to use PhantomCSS during wordpress development.\nVisual Regression Testing: Sanity Checks With BackstopJS - Screencast with code demo and best practices.\nScreenster Tutorial - Tutorial on how to create visual automated tests with Screenster.\nLook-alike - visual regression testing tool - Demo what the Look-alike Chrome extension is, how it works and how and why it was build.\nScreencast on CSS critic - a lightweight testing framework for CSS - How to write your first CSS test with CSS critic, make it pass, break it, and make it pass again.\nVisual Regression Testing - from a tool to a process by Nikhil Verma - How the Mobile Web team in Badoo converted and integrated PhantomCSS into their workflow and connected it to their CI process.\n\nDeprecated\nThe following projects are no longer maintained actively but are still worth mentioning because of their user base.\n\nPhantomJS - Scriptable Headless WebKit. No longer maintained since 2 June 2018.\nPhantomCSS - Visual/CSS regression testing with PhantomJS or SlimerJS. No longer maintained since 22 Dec 2017.\nDalekJS - Automated cross browser testing with JavaScript. No longer maintained since 4 Jun 2017.\ndpxdt - End-to-end testing with Python.\n\nMiscellaneous\nContributing\nSee the Contribution Guide for details on how to contribute.\nCode of Conduct\nSee the Code of Conduct for details. Basically it comes down to:\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of experience,\nnationality, personal appearance, race, religion, or sexual identity and orientation.\n\nLicense\n\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\nLicense holders are all contributors.\n""], 'url_profile': 'https://github.com/eric-erki', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '1', 'Python', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'CC-BY-SA-4.0 license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 28, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rahulromilkeswani', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', '1', 'Python', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'CC-BY-SA-4.0 license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/berkayaskar06', 'info_list': ['Python', 'Updated Feb 1, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'BANGALORE', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vatsmanish', 'info_list': ['Python', 'Updated Feb 1, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Logistic_Regression_Titanic\nTitanic Dataset\n\nLogistic Regression task Link\n\n'], 'url_profile': 'https://github.com/Zeinab-Haroon', 'info_list': ['Python', 'Updated Feb 1, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Remote', 'stats_list': [], 'contributions': '299 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MoAmrYehia', 'info_list': ['Python', 'Updated Feb 1, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Oregon', 'stats_list': [], 'contributions': '121 contributions\n        in the last year', 'description': ['""Helpful"" reviews: a regression approach\nA machine learning mini-project (in progress)\nThis project aims to predict the ""helpfulness"" (as rated by readers) of over 300,000 product reviews from Amazon. In another project, I approached this as a classification problem using NLP to focus on the texts of the reviews themselves. Here I use other features, some from the original dataset, others engineered by me. You can view the original dataset on Kaggle.\nThe key packages used in this project are scikit-learn, pandas, NumPy, and matplotlib.\nThis is a work in progress, so check back occasionally for updates.\n'], 'url_profile': 'https://github.com/jrkreiger', 'info_list': ['Python', 'Updated Feb 1, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'hyderabad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/balajibalu999', 'info_list': ['Python', 'Updated Feb 1, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\n'], 'url_profile': 'https://github.com/sivakrishna96', 'info_list': ['Python', 'Updated Feb 1, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Dearborn, Michigan, USA', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pobalasumanth', 'info_list': ['Python', 'Updated Feb 1, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression-Models-Project\n'], 'url_profile': 'https://github.com/WNHAN', 'info_list': ['Python', 'Updated Feb 1, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RushikSubba', 'info_list': ['Python', 'Updated Feb 1, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}"
"{'location': 'Tehran/Iran', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Regression-for-Interview\nThis program aims to predict the ""Market Share_total"" of Montreal citizen\'s film taste.\nAt first, I imported the test and training data as a data frame then I needed to change the format of some columns from string to integer to make the prediction much easier for ""sklearn"" library.\nUnfortunately, some columns were not complete and some rows in the dataset had null values. I could give them a number like mean of features but I decided to delete those rows.\nAt last, I used ""sklearn"" library to use linear regression and printed the predicted ""Market Share_total"".\n'], 'url_profile': 'https://github.com/farrokhikasra', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Dec 6, 2020', 'Updated Jan 28, 2020', '1', 'HTML', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': [""ml-simple-logistic-regression\nA simple logistic regression practice. The dataset is obtained from here - Andrew Ng's dataset for the Coursera ML course.\n""], 'url_profile': 'https://github.com/dandycheng', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Dec 6, 2020', 'Updated Jan 28, 2020', '1', 'HTML', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anthonysaitta', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Dec 6, 2020', 'Updated Jan 28, 2020', '1', 'HTML', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['House-Price-Regression\nThe 2008 financial crisis was one of the worst in the history of mankind. A key driver of the crisis was\nsomething that analysts and financial experts called the ‚Äúhousing bubble‚Äù. Put in short, during the early\n2000s there was a sharp decrease in interest rates for home loans. As Investopedia puts it, ‚Äú\u200bThe vast\nmajority of loans were adjustable-rate\u200b \u200bmortgages with low initial rates\u200b‚Äù.\nThe subsequent madness of buying homes was followed by house prices being driven up. When it became apparent to homeowners\nthat prices might fall soon and they were living in a ‚Äúbubble‚Äù, unprecedented selling of houses started,\nwhich drastically buried the prices, with people across the country increasingly finding themselves unable\nto pay off mortgages. And the rest is history.\nHouse prices are a crucial moving part of every economy since it talks about the spending power of\npeople and serves as a key economic indicator. For our project, we have chosen a dataset that provides us\nwith housing prices and multiple other features/characteristics associated with a house that are potential\ndrivers. Being able to identify the key drivers and subsequently predict house prices is what our project\nwill aim to tackle.\n'], 'url_profile': 'https://github.com/pahal2007', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Dec 6, 2020', 'Updated Jan 28, 2020', '1', 'HTML', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Durham, NC', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/FranciscoReveriano', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Dec 6, 2020', 'Updated Jan 28, 2020', '1', 'HTML', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Model-based-stockprediction\nIt is a model based stock prediction program using Linear Regression\n'], 'url_profile': 'https://github.com/milindthakur177', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Dec 6, 2020', 'Updated Jan 28, 2020', '1', 'HTML', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bingyi2020', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Dec 6, 2020', 'Updated Jan 28, 2020', '1', 'HTML', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Karnal, Haryana', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MuskanKaushik', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Dec 6, 2020', 'Updated Jan 28, 2020', '1', 'HTML', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/swathirekha37', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Dec 6, 2020', 'Updated Jan 28, 2020', '1', 'HTML', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}","{'location': 'Charlottesville, VA', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['StackOverflow_Tag_Prediction\nPredict tags for stack overflow questions using Logistic Regression and LSTM\n'], 'url_profile': 'https://github.com/AkankshaNichrelay', 'info_list': ['Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Dec 6, 2020', 'Updated Jan 28, 2020', '1', 'HTML', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Customer Churn Analysis\nCustomer churn also known as Customer attrition is the loss of customers. Customer analytics is a process by which data from customer behavior is used to make key business decisions via predictive analytics.\nTelephone service companies or Internet service providers are using analysis of customer attrition because cost of retaining an existing customer is far less as compared to acquiring the new customer. We are trying to Predict customer behavior to retain customers.\nProblem Description:\nThe Telecom company marketing team wants to identify the customers who has the high probability of churning (Shifting to other telecom provider) so that they can offer some perks to hold back the customers.\n‚Ä¢\tThe information they have are the details of the customers such as gender, if senior citizen, do they have dependents, do they have partners, tenure they are with current provider, which services they are using from the provider, on what contract they are, do they enroll for paperless billing or not, what payment method they are using, monthly and total charges they are paying or used to pay.\n‚Ä¢\tThe team has the information of about 7043 customers with a good mixture of people who have churned and who have not left the company.\n'], 'url_profile': 'https://github.com/Anusha-Kokkinti', 'info_list': ['R', 'Updated May 14, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['samTEMsel (R package version 0.1.0)\nSparse Additive models for Treatment Effect-Modifier Selection\nAn implementation of a constrained sparse additive regression for modeling interaction effects between a categorical treatment variable and a set of pretreatment covariates on a scalar-valued outcome; the regression simultaneousely conducts treatment effect-modifier variable selection. The method can effectively identify treatment effect-modifiers exhibiting possibly nonlinear interactions with the treatment. The selected pretreatment characteristics and the associated nonzero component functions can be used as a new set of data-driven features for making individualized treatment recommendations in further analysis. We refer to Park, Petkova, Tarpey, and Ogden (2020) doi:10.1016/j.jspi.2019.05.008 and Park, Petkova, Tarpey, and Ogden (2020) ""A constrained sparse additive model for treatment effect-modifier selection"" (pre-print) for detail of the method. The wrapper function of this package is cv.samTEMsel().\nDescription\n\nsamTEMsel - samTEMsel main function\ncv.samTEMsel - samTEMsel cross-validation function for tuning parameter selection\npredict_samTEMsel - samTEMsel prediction function\nmake_ITR - make individualized treatment recommendations (ITRs) based on a samTEMsel object\nplot_samTEMsel -  plot component functions from a samTEMsel object\n\nTo run:\nTo install an R package, start by installing the ""devtools"" package (from CRAN). On R, type:\ninstall.packages(""devtools"")  # install the devtools package from CRAN\nlibrary(devtools)\n\nTo install the ""samTEMsel"" package from github, type:\ndevtools::install_github(""syhyunpark/samTEMsel"")  # install the samTEMsel package from github \nlibrary(samTEMsel)   # load the samTEMsel package to R \n\nTo see some of the example codes appearing in the ""help"" menu, type:\n?samTEMsel   \n?cv.samTEMsel\n\n'], 'url_profile': 'https://github.com/syhyunpark', 'info_list': ['R', 'Updated May 14, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'Mumbai, Maharashtra', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': [""SUV-Predictions\nPredicting the sale of SUV's using Logistic Regression Classification Algorithm\n""], 'url_profile': 'https://github.com/Devvrat53', 'info_list': ['R', 'Updated May 14, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'St Johns, NL, Canada.', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BaruaSourav', 'info_list': ['R', 'Updated May 14, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '754 contributions\n        in the last year', 'description': ['Edited README\nThis is a fork of the GNU regression, econometrics and time-series library.\n\nHomepage\nSource\nUser\'s Manual\n\nOriginal README\nGretl (GNU regression, econometrics and time-series library) comprises\nlibgretl, a shared library which provides various functions relating to\neconometric estimation, a command-line client program and a gui client,\nusing GTK+.\nGretl is free software under the GNU General Public License, a copy\nof which is provided in the source package (""COPYING""), and comes\nwith abolutely no warranty.\nLibgretl is based on the stand-alone command-line econometrics program\nESL, originally written by Ramu Ramanathan of the Department of Economics\nat UC-San Diego.\nTo find out more about gretl, please look at the manual (under ""doc"" in\nsource package) and/or the online help, which is available in two somewhat\ndifferent versions for the command-line and gui client programs.  Also,\ngretl has a web home at\nhttp://gretl.sourceforge.net/\nFor configuration and installation guidance, see the file INSTALL.\nAllin Cottrell\nDepartment of Economics\nWake Forest University\ncottrell@wfu.edu\n'], 'url_profile': 'https://github.com/adam-hanna', 'info_list': ['R', 'Updated May 14, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gcarrghub', 'info_list': ['R', 'Updated May 14, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Analysis of Logistic Regression and Linear Discriminant Analysis for Wine Quality Prediction and Breast Cancer Diagnostic\nThe main objective of this project is to apply linear machine learning classification techniques to classify\nthe quality of wine and the malignancy of breast cancer based on benchmark data sets.\n'], 'url_profile': 'https://github.com/adambabs', 'info_list': ['R', 'Updated May 14, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akselkohen', 'info_list': ['R', 'Updated May 14, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'Gwalior', 'stats_list': [], 'contributions': '486 contributions\n        in the last year', 'description': ['Used_Car_Prediction\nPrediciton of Used Car prices using regression techniques and some analysis with some feature engineering.\n'], 'url_profile': 'https://github.com/skshashankkumar41', 'info_list': ['R', 'Updated May 14, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 1, 2020']}","{'location': 'Berkeley, CA', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mwarady22', 'info_list': ['R', 'Updated May 14, 2020', '1', 'R', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', '1', 'Python', 'Updated Feb 24, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', '1', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Python', 'Updated Feb 1, 2020']}"
"{'location': 'Richmond, VA', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/crescentini', 'info_list': ['RMarkdown', 'Updated Jan 28, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 17, 2020', 'Python', 'MIT license', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'R', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['HWnet\nÂü∫‰∫é‚ÄúÁ∫øÊÄßÂµåÂÖ•ÂºèÂêëÈáè‚ÄùÂèØÂú®Á∫ø‰º∏Áº©‚ÄúÁ∫øÊÄßÂõûÂΩíÁΩëÁªú‚Äù\nLinear regression network can expand and shrink on line based on ""Linear Embedding""\n1. ÊÄßËÉΩÊèêÂçá\n1. Performance\n\n\nÂ∑¶‰æß:Â§öÈ°πÂºèÁ∫øÊÄßÂõûÂΩí„ÄÅTanhÊøÄÊ¥ªÂáΩÊï∞; Âè≥‰æß:Á∫øÊÄßÂµåÂÖ•ÂºèÂêëÈáè„ÄÇ\nLeft figure: polynomial linear regression, tanh activation; right figure: Linear Embedding.\nËÆ≠ÁªÉÊÄßËÉΩÊèêÂçáËøëÁôæÂÄç„ÄÇ\nNearly hundred times improvement in training.\nÈùûÂ∏∏Êé•Ëøë‚ÄúÂÖ®Â±ÄÊúÄ‰ºòÁÇπ‚Äù„ÄÇ\nVery close to ""global best""\n\n2. Á∫øÊÄßÂõûÂΩíÂú®Â§ßËßÑÊ®°ÁΩëÁªú‰∏≠ÁöÑÁº∫Èô∑\n2. Defects of linear regression in large net\n\nÊØè‰∏Ä‰∏™Êï∞ÊçÆËÆ≠ÁªÉÊâÄÊúâÂèÇÊï∞„ÄÇ\nEvery data train All parameters; All parameters are trained by Each data.\nÊØè‰∏Ä‰∏™ÂèÇÊï∞ÂèÇ‰∏éÊØè‰∏ÄÊ¨°Êé®Êñ≠„ÄÇ\nEvery parameter participates in Each inference; Each inference is base on All parameters.\n\n3. ÁÆ±ÂºèÁâπÂæÅÁöÑÁº∫Èô∑\n3. Defects of box features\n\nÂàÜ‚ÄúÁÆ±‚ÄùÂêéÔºåÁõ∏ÈÇªÁÆ±Ëøá‰∫éÁã¨Á´ãÔºåÂØºËá¥‚Äú‰∏çÂúÜÊªë‚Äù„ÄÇ\nAfter the ""box"" is divided, the boxes nearby are too independent, which makes results in ""not smooth"".\nÁ®ÄÁñèÁâπÂæÅÂØºËá¥ÂêéÁ´ØÁΩëÁªúËßÑÊ®°ËøáÂ§ßÔºåÂÆπÊòìËøáÊãüÂêà„ÄÇ\nSparse features make back-end to large and overfitting easily.\n\n4. Á∫øÊÄßÂµåÂÖ•ÂºèÂêëÈáè\n4. Linear Embedding\n\nÊØè‰∏™ÂÄºÂüüÊúâÂêÑËá™ÁöÑÂêëÈáè„ÄÇ\nEach value field has its own vector.\nÂØπÊØè‰∏Ä‰∏™ËæìÂÖ•ÂÄºÔºåÁî®ÊâÄÂú®ÂÄºÂüü„ÄÅÂë®ËæπÂÄºÂüüÁöÑ‚ÄúÂêëÈáè‚Äù‰πò‰ª•‚ÄúÊùÉÈáç‚ÄùÂíåË°®ÂæÅ„ÄÇ\nEach input can be described by the sum of ""vector"" multiply ""weight"".\nÊØè‰∏™ÂêëÈáèÁöÑ‚ÄúÊùÉÈáç‚Äù‰∏éËæìÂÖ•ÂÄº‰∏éÂÄºÂüü‰∏≠ÁÇπÁöÑË∑ùÁ¶ªÁõ∏ÂÖ≥„ÄÇ\nThe ""weight"" of each vector is related to the distance between the input and the middle of the value field.\nÈÄöËøáËÆæÂÆö‚Äútakecare‚ÄùÂèÇÊï∞ÔºåÊéßÂà∂Âë®ËæπÂÄºÂüü‚ÄúÂêëÈáè‚ÄùÁöÑ""ÊùÉÈáç""„ÄÇ\nBy setting the ""takecare"" parameter can control the ""weight"" of ""vector"".\n\nÈò≤Ê≠¢Ëøá‰∫é‚ÄúÁã¨Á´ã‚ÄùÂØºËá¥ÁöÑ‚ÄúËøáÊãüÂêà‚Äù„ÄÅ‚Äú‰∏çÂúÜÊªë‚Äù„ÄÇ\nPrevent ""over fitting"" and ""non smoothness"" caused by less takecare.\nÈò≤Ê≠¢Ëøá‰∫é‚Äútakecare‚ÄùÂØºËá¥ÊãüÂêàËÉΩÂäõ‰∏çË∂≥„ÄÇ\nPrevent ""under fitting"" caused by over takecare.\n\nÊ†πÊçÆ‰∏äÂõæÂèØËßÅÔºåÂΩìtakcareÂΩìÂâçÂÄºÂüü60%Êó∂ÔºåÂè™ÈúÄ5‰∏™ÂÄºÂüüÁöÑÂêëÈáèÂèÇ‰∏éËÆ≠ÁªÉ„ÄÅÊé®Êñ≠„ÄÇ\nAccording to the figure above, when takcare is 60%, only 5 vectors are needed in training and inference.\n\n\n\n5. Â§öÁª¥ËæìÂÖ•‰∏éÂêéÁ´ØÁΩëÁªú\n5. Multiple inputs and back-end\n\nÊúâÂ§ö‰∏™ËæìÂÖ•Êó∂ÔºåÊØè‰∏™ËæìÂÖ•ÂêÑËá™‚ÄúÂêëÈáèÂåñ‚Äù, Â∞ÜÁªìÊûúÂêàÂπ∂ËæìÂÖ•ÂêéÁ´ØÁΩëÁªú„ÄÇ\nWhen there are multiple inputs, each input is ""vectorized"" to merge and feed into the back-end.\nÂêéÁ´ØÂèØ‰ª•ÊòØÂêÑÁßçÁΩëÁªú, ‰æãÂ¶ÇÔºöÂÖ®ËøûÊé•„ÄÅLSTM......\nBack-end can be all kinds of net, like full connection, LSTM......\n\n6. Âú®Á∫ø‰º∏Áº©\n6. Online expand and shrink\n\nÁ¥ØËÆ°ÂêÑÂÄºÂüüÊçüÂ§±: Â§ß‰∫éÈ¢ÑÊúüÁöÑÂÄºÂüüËøõË°åÂàÜË£ÇÔºåÂ∞è‰∫éÈ¢ÑÊúüÁöÑÂÄºÂüü‰∏éÂë®Âõ¥ÂÄºÂüüÂêàÂπ∂„ÄÇ\nBy accumulating the loss of each value field: larger than expected will be split, less than expected will be merged.\nÁ∫øÊÄßÂµåÂÖ•ÂºèÂêëÈáèÁöÑËæìÂá∫Áª¥Â∫¶Âõ∫ÂÆöÔºåÂâçÁ´ØÁΩëÁªú‰º∏Áº©ÂêéÔºåÂêéÁ´ØÁΩëÁªúÊó†ÈúÄÈáçÊñ∞ÊûÑÂª∫ÔºåÂè™ÈúÄËøõË°åÂ∞ëÈáèËÆ≠ÁªÉ„ÄÇ\nWhen the output dimension of linear embedding is fixed , after the front-end expand and shrink, the back-end do not need rebuild, only need a little training.\n\n7. To Do\n\n2Âë®ÂÜÖÂçáÁ∫ßPLUSÁâàÊú¨„ÄÇ\nUpgrade PLUS version in 2 weeks.\n\n'], 'url_profile': 'https://github.com/FFiot', 'info_list': ['RMarkdown', 'Updated Jan 28, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 17, 2020', 'Python', 'MIT license', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'R', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Brazil', 'stats_list': [], 'contributions': '549 contributions\n        in the last year', 'description': ['\nA Gaussian Process Regression (GPR) library that can work with complex numbers.\nDevelopment\nFor dependency management and publishing to Pypi we use\npoetry.\nIf you want to extend gpr_complex, clone it from the\ngit repository, run\npoetry install to create the virtual environment with the required\ndependencies and run pre-commit install to install the commit hooks.\n'], 'url_profile': 'https://github.com/darcamo', 'info_list': ['RMarkdown', 'Updated Jan 28, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 17, 2020', 'Python', 'MIT license', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'R', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Haldia, West Bengal', 'stats_list': [], 'contributions': '169 contributions\n        in the last year', 'description': ['Expander\nDeveloped various regression model using this dataset and calculated MSE for different regressio models.\nlink to dataset: https://b.ctdo.app/80b2e\nor\nlink to dataset: https://drive.google.com/open?id=1gxZoubZFUbJUBdXpGNr9m95sW7QW9RQs\n'], 'url_profile': 'https://github.com/NIKsaurabh', 'info_list': ['RMarkdown', 'Updated Jan 28, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 17, 2020', 'Python', 'MIT license', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'R', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Krak√≥w', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['online-news-popularity-prediction\nUsing Lasso regression to predict popularity of online news. Dataset is available at UCI repository.\n'], 'url_profile': 'https://github.com/Geecek', 'info_list': ['RMarkdown', 'Updated Jan 28, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 17, 2020', 'Python', 'MIT license', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'R', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'Madrid', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mirbeck', 'info_list': ['RMarkdown', 'Updated Jan 28, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 17, 2020', 'Python', 'MIT license', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'R', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kinshukdua', 'info_list': ['RMarkdown', 'Updated Jan 28, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 17, 2020', 'Python', 'MIT license', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'R', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'El Buhaira, Egypt', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['Regression-Model-in-Keras\nBuild a Regression Model in Keras - Coursera introduction to deep learning with keras final project\n'], 'url_profile': 'https://github.com/ahmedmedrah', 'info_list': ['RMarkdown', 'Updated Jan 28, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 17, 2020', 'Python', 'MIT license', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'R', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Fitbit Prediction Project\nBlog post reporting results located here.\nThis notebook is for a project after spending time on predictive modeling using machine learning.\nI used the same dataset of my activity levels and resting heart rate for 2019 as well as adding in what I had from 2018. I added in this extra data to increase the observations for the machine learning to utilize.\nLike the activity levels project I will return to this project in the future and include additional data and skills.\n'], 'url_profile': 'https://github.com/NealWhitlock', 'info_list': ['RMarkdown', 'Updated Jan 28, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 17, 2020', 'Python', 'MIT license', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'R', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['TensorflowExercises\nAll Tensorflow Exercises covering basic tensforflow methods such as constant to regression and classification\n'], 'url_profile': 'https://github.com/nishultomar', 'info_list': ['RMarkdown', 'Updated Jan 28, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 17, 2020', 'Python', 'MIT license', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'R', 'Updated Jan 30, 2020', 'Python', 'Updated Feb 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Trial-lab-project\nLittle showcase of exercises on decision trees, linear regression, clustering and neural networks.\n'], 'url_profile': 'https://github.com/dgovi6', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 30, 2020', '1', 'MATLAB', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 2, 2020']}","{'location': 'Phoenix area, Arizona', 'stats_list': [], 'contributions': '129 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shantanu-93', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 30, 2020', '1', 'MATLAB', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 2, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['TensorflowExercises\nAll Tensorflow Exercises covering basic tensforflow methods such as constant to regression and classification\n'], 'url_profile': 'https://github.com/nishultomar', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 30, 2020', '1', 'MATLAB', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Airbnb Predictions\nIn this project, we are interested in understanding the AirBnB Seattle dataset better through the lens of classification, regression and clustering algorithms. We want to predict AirBnB Seattle listings prices with the lowest error in order to help people price new listings. Also, each AirBnB listing also has a review score or undesirable or desirable, and our goal is to design a classifier for this variable that has the highest accuracy on new data.\nThe data is sourced from the \u200bAirbnb\u200b site. Please see the PDF attached.\n'], 'url_profile': 'https://github.com/ilitwin', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 30, 2020', '1', 'MATLAB', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Customer-Churn\nPredicting customer churn by using logistic regression, gradient boosting, and neural network.\n'], 'url_profile': 'https://github.com/Lazr-Galstyan', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 30, 2020', '1', 'MATLAB', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 2, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Iowa-house-prices\nDeveloping a regression model to predict the house prices in Ames, Iowa\nThis repo contains a Jupyter Notebook in which I have developed a regression model to predict the house prices in Ames, then evaluated the\npredictions of the model using Shapley values (from the SHAP library).\n'], 'url_profile': 'https://github.com/tomukmatthews', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 30, 2020', '1', 'MATLAB', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 2, 2020']}","{'location': 'Greater Boston Area', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': [""Logistic-Regression-for-Digit-Classification\nThe project objective is to use logistic regression to classify hand-written digits (8 and 9 in particular). First, I fitted logistic regression models to the training data using sklearn's implementation of the model with the liblinear solver. I explored what happens when limiting the iterations allowed for the solver when converging to its solution, leaving all other parameters with default values.\nI also explored different values of the inverse penalty strength C using a regularly-spaced grid of values logspace(-9, 6, 31). Then I determined what C value gives the least log loss on the test data. I also included accuracy score of the model and a confusion matrix.\nI analyzed the false positives and the false negatives and provided some thoughts about what mistake the classifier is making.\nFinally, I analyzed all of the final weights produced by the classifier. I reshaped the weight coefficients into a (28 √ó 28) matrix, corresponding to the pixels of the original images, and plot the result using imshow(), with colormap RdYlBu, vmin=-0.5, and vmax=0.5. I will discuss the plot regarding which pixels correspond to an 8 (have negative weights), and which correspond to a 9 (have positive weights).\n""], 'url_profile': 'https://github.com/baovinhnguyen', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 30, 2020', '1', 'MATLAB', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 2, 2020']}","{'location': 'Madrid', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['multiple-linear-model-analysis\nStatistical analysis of biomedical data using multiple regression models and mixed effects\n'], 'url_profile': 'https://github.com/vfanjul', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 30, 2020', '1', 'MATLAB', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '658 contributions\n        in the last year', 'description': [""PredictPRoNTo\nPredict using the PRoNTo toolbox. Supports both regression and classification, and different predictive models.\nA typical example of when these type of predictions could be interesting is when multiple subjects' spatially normalised tissue segmentations from brain MRI data, obtained with for example the SPM12 software, are available. If the subjects have age and/or sex labels, then the approach implemented here can be used for prediciting between subjects.\nFor detailed information see PredictPRoNTo.m.\nExample\nCall PredictPRoNTo(data), where data is a cell array with the following form:\ndata{n,1} = 'subj1feature.nii' or {'subj1feature1.nii','subj1feature2.nii'}\ndata{n,2} = floating point number (e.g., 42)\ndata{n,3} = logical value (e.g., true)\nand n is the subject index going from 1 to N, the total number of subjects.\nRequirements\nRequires that SPM12 and PRoNTo v2 are on the MATLAB path:\n\nSPM12:  https://www.fil.ion.ucl.ac.uk/spm/software/download/\nPRoNTo: http://www.mlnl.cs.ucl.ac.uk/pronto/prtsoftware.html (.zip file in this repo!)\n\n""], 'url_profile': 'https://github.com/brudfors', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 30, 2020', '1', 'MATLAB', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 2, 2020']}","{'location': 'Belo Horizonte - MG, Brazil', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['Machine learning with PyTorch framework\nThe purpose of this project is to learn the PyTorch framework by implementing everything from traditional machine learning algorithms to state-of-the-art deep learning algorithms.\nTable of Contents\n\n\nLinear Regression\n\n\nLogistic Regression\n\n\nMultilayer Perceptron (MLP)\n\n\nConvolutional Neural Networks (CNNs)\n\nLeNet-5\n\n\n\nRecurrent Neural Networks (RNNs)\n\nSimple RNN\nGated Recurrent Unit (GRU)\nLong Short-Term Memory (LSTM)\n\n\n\nEach folder contains a jupyter notebook with a practical example.\nRequirements\n\nNumpy 1.18.0\nPandas 0.24.2\nMatplotlib 3.0.3\nPillow 6.2.0\nscikit-learn 0.21.3\nTensorflow 2.1.0\ntorch 1.4.0\ntorchvision 0.4.2\n\nCitation\n{@misc{pedbrgs-pytorch-models,\n       author = {Pedro Vinicius A. B. Venancio},\n       title = {Machine learning models with PyTorch framework},\n       year = {2020},\n       howpublished = {\\url{https://github.com/pedbrgs/Machine-Learning-with-PyTorch/}}}\n\n'], 'url_profile': 'https://github.com/pedbrgs', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 15, 2021', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'R', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 30, 2020', '1', 'MATLAB', 'Updated Jun 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Oct 2, 2020']}"
"{'location': 'Earth...', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': [""Awesome Decision Tree Research Papers\n\n\n  \n\n\n\nA curated list of decision, classification and regression tree research papers with implementations from the following conferences:\n\nMachine learning\n\nNeurIPS\nICML\nICLR\n\n\nComputer vision\n\nCVPR\nICCV\nECCV\n\n\nNatural language processing\n\nACL\nNAACL\nEMNLP\n\n\nData\n\nKDD\nCIKM\nICDM\nSDM\nPAKDD\nPKDD/ECML\nSIGIR\nWWW\nWSDM\n\n\nArtificial intelligence\n\nAAAI\nAISTATS\nICANN\nIJCAI\nUAI\n\n\n\nSimilar collections about graph classification, gradient boosting, fraud detection, Monte Carlo tree search, and community detection papers with implementations.\n2019\n\n\nMulti-Level Deep Cascade Trees for Conversion Rate Prediction in Recommendation System (AAAI 2019)\n\nHong Wen, Jing Zhang, Quan Lin, Keping Yang, Pipei Huang\n[Paper]\n\n\n\nInduction of Non-Monotonic Logic Programs to Explain Boosted Tree Models Using LIME (AAAI 2019)\n\nFarhad Shakerin, Gopal Gupta\n[Paper]\n\n\n\nLearning Optimal and Fair Decision Trees for Non-Discriminative Decision-Making (AAAI 2019)\n\nSina Aghaei, Mohammad Javad Azizi, Phebe Vayanos\n[Paper]\n\n\n\nDesiderata for Interpretability: Explaining Decision Tree Predictions with Counterfactuals (AAAI 2019)\n\nKacper Sokol, Peter A. Flach\n[Paper]\n\n\n\nWeighted Oblique Decision Trees (AAAI 2019)\n\nBin-Bin Yang, Song-Qing Shen, Wei Gao\n[Paper]\n\n\n\nLearning Optimal Classification Trees Using a Binary Linear Program Formulation (AAAI 2019)\n\nSicco Verwer, Yingqian Zhang\n[Paper]\n\n\n\nOptimization of Hierarchical Regression Model with Application to Optimizing Multi-Response Regression K-ary Trees (AAAI 2019)\n\nPooya Tavallali, Peyman Tavallali, Mukesh Singhal\n[Paper]\n\n\n\nXBART: Accelerated Bayesian Additive Regression Trees (AISTATS 2019)\n\nJingyu He, Saar Yalov, P. Richard Hahn\n[Paper]\n\n\n\nInteraction Detection with Bayesian Decision Tree Ensembles (AISTATS 2019)\n\nJunliang Du, Antonio R. Linero\n[Paper]\n\n\n\nAdversarial Training of Gradient-Boosted Decision Trees (CIKM 2019)\n\nStefano Calzavara, Claudio Lucchese, Gabriele Tolomei\n[Paper]\n\n\n\nInterpretable MTL from Heterogeneous Domains using Boosted Tree (CIKM 2019)\n\nYa-Lin Zhang, Longfei Li\n[Paper]\n\n\n\nInterpreting CNNs via Decision Trees (CVPR 2019)\n\nQuanshi Zhang, Yu Yang, Haotian Ma, Ying Nian Wu\n[Paper]\n\n\n\nIncorporating Grouping Information into Bayesian Decision Tree Ensembles (ICML 2019)\n\nJunliang Du, Antonio R. Linero\n[Paper]\n\n\n\nAdaptive Neural Trees (ICML 2019)\n\nRyutaro Tanno, Kai Arulkumaran, Daniel C. Alexander, Antonio Criminisi, Aditya V. Nori\n[Paper]\n[Code]\n\n\n\nRobust Decision Trees Against Adversarial Examples (ICML 2019)\n\nHongge Chen, Huan Zhang, Duane S. Boning, Cho-Jui Hsieh\n[Paper]\n[Code]\n\n\n\nLearn Smart with Less: Building Better Online Decision Trees with Fewer Training Examples (IJCAI 2019)\n\nAriyam Das, Jin Wang, Sahil M. Gandhi, Jae Lee, Wei Wang, Carlo Zaniolo\n[Paper]\n\n\n\nFAHT: An Adaptive Fairness-aware Decision Tree Classifier (IJCAI 2019)\n\nWenbin Zhang, Eirini Ntoutsi\n[Paper]\n[Code]\n\n\n\nInter-node Hellinger Distance based Decision Tree (IJCAI 2019)\n\nPritom Saha Akash, Md. Eusha Kadir, Amin Ahsan Ali, Mohammad Shoyaib\n[Paper]\n[Matlab Code]\n[R Code]\n\n\n\nGradient Boosting with Piece-Wise Linear Regression Trees (IJCAI 2019)\n\nYu Shi, Jian Li, Zhize Li\n[Paper]\n[Code]\n\n\n\nA Gradient-Based Split Criterion for Highly Accurate and Transparent Model Trees (IJCAI 2019)\n\nKlaus Broelemann, Gjergji Kasneci\n[Paper]\n\n\n\nCombining Decision Trees and Neural Networks for Learning-to-Rank in Personal Search (KDD 2019)\n\nPan Li, Zhen Qin, Xuanhui Wang, Donald Metzler\n[Paper]\n\n\n\nPartitioning Structure Learning for Segmented Linear Regression Trees (NeurIPS 2019)\n\nXiangyu Zheng, Song Xi Chen\n[Paper]\n\n\n\nProvably Robust Boosted Decision Stumps and Trees against Adversarial Attacks (NeurIPS 2019)\n\nMaksym Andriushchenko, Matthias Hein\n[Paper]\n[Code]\n\n\n\nOptimal Decision Tree with Noisy Outcomes (NeurIPS 2019)\n\nSu Jia, Viswanath Nagarajan, Fatemeh Navidi, R. Ravi\n[Paper]\n[Code]\n\n\n\nRegularized Gradient Boosting (NeurIPS 2019)\n\nCorinna Cortes, Mehryar Mohri, Dmitry Storcheus\n[Paper]\n\n\n\nOptimal Sparse Decision Trees (NeurIPS 2019)\n\nXiyang Hu, Cynthia Rudin, Margo Seltzer\n[Paper]\n[Code]\n\n\n\nCalibrating Probability Estimation Trees using Venn-Abers Predictors (SDM 2019)\n\nUlf Johansson, Tuwe L√∂fstr√∂m, Henrik Bostr√∂m\n[Paper]\n\n\n\nFast Training for Large-Scale One-versus-All Linear Classifiers using Tree-Structured Initialization (SDM 2019)\n\nHuang Fang, Minhao Cheng, Cho-Jui Hsieh, Michael P. Friedlander\n[Paper]\n\n\n\nBlock-distributed Gradient Boosted Trees (SIGIR 2019)\n\nTheodore Vasiloudis, Hyunsu Cho, Henrik Bostr√∂m\n[Paper]\n\n\n\nEntity Personalized Talent Search Models with Tree Interaction Features (WWW 2019)\n\nCagri Ozcaglar, Sahin Cem Geyik, Brian Schmitz, Prakhar Sharma, Alex Shelkovnykov, Yiming Ma, Erik Buchanan\n[Paper]\n\n\n\n2018\n\n\nAdapting to Concept Drift in Credit Card Transaction Data Streams Using Contextual Bandits and Decision Trees (AAAI 2018)\n\nDennis J. N. J. Soemers, Tim Brys, Kurt Driessens, Mark H. M. Winands, Ann Now√©\n[Paper]\n\n\n\nMERCS: Multi-Directional Ensembles of Regression and Classification Trees (AAAI 2018)\n\nElia Van Wolputte, Evgeniya Korneva, Hendrik Blockeel\n[Paper]\n[Code]\n\n\n\nDifferential Performance Debugging With Discriminant Regression Trees (AAAI 2018)\n\nSaeid Tizpaz-Niari, Pavol Cern√Ω, Bor-Yuh Evan Chang, Ashutosh Trivedi\n[Paper]\n[Code]\n\n\n\nEstimating the Class Prior in Positive and Unlabeled Data Through Decision Tree Induction (AAAI 2018)\n\nJessa Bekker, Jesse Davis\n[Paper]\n\n\n\nMDP-Based Cost Sensitive Classification Using Decision Trees (AAAI 2018)\n\nShlomi Maliah, Guy Shani\n[Paper]\n\n\n\nGenerative Adversarial Image Synthesis With Decision Tree Latent Controller (CVPR 2018)\n\nTakuhiro Kaneko, Kaoru Hiramatsu, Kunio Kashino\n[Paper]\n[Code]\n\n\n\nEnhancing Very Fast Decision Trees with Local Split-Time Predictions (ICDM 2018)\n\nViktor Losing, Heiko Wersing, Barbara Hammer\n[Paper]\n[Code]\n\n\n\nRealization of Random Forest for Real-Time Evaluation through Tree Framing (ICDM 2018)\n\nSebastian Buschj√§ger, Kuan-Hsun Chen, Jian-Jia Chen, Katharina Morik\n[Paper]\n\n\n\nFinding Influential Training Samples for Gradient Boosted Decision Trees (ICML 2018)\n\nBoris Sharchilev, Yury Ustinovskiy, Pavel Serdyukov, Maarten de Rijke\n[Paper]\n[Code]\n\n\n\nLearning Optimal Decision Trees with SAT (IJCAI 2018)\n\nNina Narodytska, Alexey Ignatiev, Filipe Pereira, Jo√£o Marques-Silva\n[Paper]\n\n\n\nExtremely Fast Decision Tree (KDD 2018)\n\nChaitanya Manapragada, Geoffrey I. Webb, Mahsa Salehi\n[Paper]\n[Code]\n\n\n\nRapidScorer: Fast Tree Ensemble Evaluation by Maximizing Compactness in Data Level Parallelization (KDD 2018)\n\nTing Ye, Hucheng Zhou, Will Y. Zou, Bin Gao, Ruofei Zhang\n[Paper]\n\n\n\nCatBoost: Unbiased Boosting with Categorical Features (NIPS 2018)\n\nLiudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, Andrey Gulin\n[Paper]\n[Code]\n\n\n\nActive Learning for Non-Parametric Regression Using Purely Random Trees (NIPS 2018)\n\nJack Goetz, Ambuj Tewari, Paul Zimmerman\n[Paper]\n\n\n\nAlternating Optimization of Decision Trees with Application to Learning Sparse Oblique Trees (NIPS 2018)\n\nMiguel √Å. Carreira-Perpi√±√°n, Pooya Tavallali\n[Paper]\n\n\n\nMulti-Layered Gradient Boosting Decision Trees (NIPS 2018)\n\nJi Feng, Yang Yu, Zhi-Hua Zhou\n[Paper]\n[Code]\n\n\n\nTransparent Tree Ensembles (SIGIR 2018)\n\nAlexander Moore, Vanessa Murdock, Yaxiong Cai, Kristine Jones\n[Paper]\n\n\n\nPrivacy-aware Ranking with Tree Ensembles on the Cloud (SIGIR 2018)\n\nShiyu Ji, Jinjin Shao, Daniel Agun, Tao Yang\n[Paper]\n\n\n\n2017\n\n\nStrategic Sequences of Arguments for Persuasion Using Decision Trees (AAAI 2017)\n\nEmmanuel Hadoux, Anthony Hunter\n[Paper]\n\n\n\nBoostVHT: Boosting Distributed Streaming Decision Trees (CIKM 2017)\n\nTheodore Vasiloudis, Foteini Beligianni, Gianmarco De Francisci Morales\n[Paper]\n\n\n\nLatency Reduction via Decision Tree Based Query Construction (CIKM 2017)\n\nAman Grover, Dhruv Arya, Ganesh Venkataraman\n[Paper]\n\n\n\nEnumerating Distinct Decision Trees (ICML 2017)\n\nSalvatore Ruggieri\n[Paper]\n\n\n\nGradient Boosted Decision Trees for High Dimensional Sparse Output (ICML 2017)\n\nSi Si, Huan Zhang, S. Sathiya Keerthi, Dhruv Mahajan, Inderjit S. Dhillon, Cho-Jui Hsieh\n[Paper]\n[Code]\n\n\n\nConsistent Feature Attribution for Tree Ensembles (ICML 2017)\n\nScott M. Lundberg, Su-In Lee\n[Paper]\n[Code]\n\n\n\nExtremely Fast Decision Tree Mining for Evolving Data Streams (KDD 2017)\n\nAlbert Bifet, Jiajin Zhang, Wei Fan, Cheng He, Jianfeng Zhang, Jianfeng Qian, Geoff Holmes, Bernhard Pfahringer\n[Paper]\n\n\n\nCatBoost: Gradient Boosting with Categorical Features Support (NIPS 2017)\n\nAnna Veronika Dorogush, Vasily Ershov, Andrey Gulin\n[Paper]\n[Code]\n\n\n\nLightGBM: A Highly Efficient Gradient Boosting Decision Tree (NIPS 2017)\n\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu\n[Paper]\n[Code]\n\n\n\nVariable Importance Using Decision Trees (NIPS 2017)\n\nJalil Kazemitabar, Arash Amini, Adam Bloniarz, Ameet S. Talwalkar\n[Paper]\n\n\n\nA Unified Approach to Interpreting Model Predictions (NIPS 2017)\n\nScott M. Lundberg, Su-In Lee\n[Paper]\n[Code]\n\n\n\nPruning Decision Trees via Max-Heap Projection (SDM 2017)\n\nZhi Nie, Binbin Lin, Shuai Huang, Naren Ramakrishnan, Wei Fan, Jieping Ye\n[Paper]\n\n\n\nA Practical Method for Solving Contextual Bandit Problems Using Decision Trees (UAI 2017)\n\nAdam N. Elmachtoub, Ryan McNellis, Sechan Oh, Marek Petrik\n[Paper]\n\n\n\nComplexity of Solving Decision Trees with Skew-Symmetric Bilinear Utility (UAI 2017)\n\nHugo Gilbert, Olivier Spanjaard\n[Paper]\n\n\n\nGB-CENT: Gradient Boosted Categorical Embedding and Numerical Trees (WWW 2017)\n\nQian Zhao, Yue Shi, Liangjie Hong\n[Paper]\n\n\n\n2016\n\n\nSparse Perceptron Decision Tree for Millions of Dimensions (AAAI 2016)\n\nWeiwei Liu, Ivor W. Tsang\n[Paper]\n\n\n\nLearning Online Smooth Predictors for Realtime Camera Planning Using Recurrent Decision Trees (CVPR 2016)\n\nJianhui Chen, Hoang Minh Le, Peter Carr, Yisong Yue, James J. Little\n[Paper]\n\n\n\nOnline Learning with Bayesian Classification Trees (CVPR 2016)\n\nSamuel Rota Bul√≤, Peter Kontschieder\n[Paper]\n\n\n\nAccurate Robust and Efficient Error Estimation for Decision Trees (ICML 2016)\n\nLixin Fan\n[Paper]\n\n\n\nMeta-Gradient Boosted Decision Tree Model for Weight and Target Learning (ICML 2016)\n\nYury Ustinovskiy, Valentina Fedorova, Gleb Gusev, Pavel Serdyukov\n[Paper]\n\n\n\nBoosted Decision Tree Regression Adjustment for Variance Reduction in Online Controlled Experiments (KDD 2016)\n\nAlexey Poyarkov, Alexey Drutsa, Andrey Khalyavin, Gleb Gusev, Pavel Serdyukov\n[Paper]\n\n\n\nXGBoost: A Scalable Tree Boosting System (KDD 2016)\n\nTianqi Chen, Carlos Guestrin\n[Paper]\n[Code]\n\n\n\nYggdrasil: An Optimized System for Training Deep Decision Trees at Scale (NIPS 2016)\n\nFiras Abuzaid, Joseph K. Bradley, Feynman T. Liang, Andrew Feng, Lee Yang, Matei Zaharia, Ameet S. Talwalkar\n[Paper]\n\n\n\nA Communication-Efficient Parallel Algorithm for Decision Tree (NIPS 2016)\n\nQi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhiming Ma, Tie-Yan Liu\n[Paper]\n[Code]\n\n\n\nExploiting CPU SIMD Extensions to Speed-up Document Scoring with Tree Ensembles (SIGIR 2016)\n\nClaudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Nicola Tonellotto, Rossano Venturini\n[Paper]\n[Code]\n\n\n\nPost-Learning Optimization of Tree Ensembles for Efficient Ranking (SIGIR 2016)\n\nClaudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Fabrizio Silvestri, Salvatore Trani\n[Paper]\n[Code]\n\n\n\n2015\n\n\nParticle Gibbs for Bayesian Additive Regression Trees (AISTATS 2015)\n\nBalaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh\n[Paper]\n\n\n\nDART: Dropouts Meet Multiple Additive Regression Trees (AISTATS 2015)\n\nKorlakai Vinayak Rashmi, Ran Gilad-Bachrach\n[Paper]\n[Code]\n\n\n\nSingle Target Tracking Using Adaptive Clustered Decision Trees and Dynamic Multi-level Appearance Models (CVPR 2015)\n\nJingjing Xiao, Rustam Stolkin, Ales Leonardis\n[Paper]\n\n\n\nFace Alignment Using Cascade Gaussian Process Regression Trees (CVPR 2015)\n\nDonghoon Lee, Hyunsin Park, Chang Dong Yoo\n[Paper]\n[Code]\n\n\n\nTracking-by-Segmentation with Online Gradient Boosting Decision Tree (ICCV 2015)\n\nJeany Son, Ilchae Jung, Kayoung Park, Bohyung Han\n[[Paper]](Tracking-by-Segmentation with Online Gradient Boosting Decision Tree)\n\n\n\nEntropy Evaluation Based on Confidence Intervals of Frequency Estimates : Application to the Learning of Decision Trees (ICML 2015)\n\nMathieu Serrurier, Henri Prade\n[Paper]\n\n\n\nLarge-scale Distributed Dependent Nonparametric Trees (ICML 2015)\n\nZhiting Hu, Qirong Ho, Avinava Dubey, Eric P. Xing\n[Paper]\n\n\n\nOptimal Action Extraction for Random Forests and Boosted Trees (KDD 2015)\n\nZhicheng Cui, Wenlin Chen, Yujie He, Yixin Chen\n[Paper]\n\n\n\nA Decision Tree Framework for Spatiotemporal Sequence Prediction (KDD 2015)\n\nTaehwan Kim, Yisong Yue, Sarah L. Taylor, Iain A. Matthews\n[Paper]\n\n\n\nEfficient Non-greedy Optimization of Decision Trees (NIPS 2015)\n\nMohammad Norouzi, Maxwell D. Collins, Matthew Johnson, David J. Fleet, Pushmeet Kohli\n[Paper]\n\n\n\nQuickScorer: A Fast Algorithm to Rank Documents with Additive Ensembles of Regression Trees (SIGIR 2015)\n\nClaudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Nicola Tonellotto, Rossano Venturini\n[Paper]\n[Code]\n\n\n\n2014\n\n\nA Mixtures-of-Trees Framework for Multi-Label Classification (CIKM 2014)\n\nCharmgil Hong, Iyad Batal, Milos Hauskrecht\n[Paper]\n\n\n\nOn Building Decision Trees from Large-scale Data in Applications of On-line Advertising (CIKM 2014)\n\nShivaram Kalyanakrishnan, Deepthi Singh, Ravi Kant\n[Paper]\n\n\n\nFast Supervised Hashing with Decision Trees for High-Dimensional Data (CVPR 2014)\n\nGuosheng Lin, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, David Suter\n[Paper]\n\n\n\nOne Millisecond Face Alignment with an Ensemble of Regression Trees (CVPR 2014)\n\nVahid Kazemi, Josephine Sullivan\n[Paper]\n\n\n\nThe return of AdaBoost.MH: multi-class Hamming trees (ICLR 2014)\n\nBal√°zs K√©gl\n[Paper]\n\n\n\nDiagnosis Determination: Decision Trees Optimizing Simultaneously Worst and Expected Testing Cost (ICML 2014)\n\nFerdinando Cicalese, Eduardo Sany Laber, Aline Medeiros Saettler\n[Paper]\n\n\n\nLearning Multiple-Question Decision Trees for Cold-Start Recommendation (WSDM 2013)\n\nMingxuan Sun, Fuxin Li, Joonseok Lee, Ke Zhou, Guy Lebanon, Hongyuan Zha\n[Paper]\n\n\n\n2013\n\n\nWeakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria (ICCV 2013)\n\nChristoph N. Straehle, Ullrich K√∂the, Fred A. Hamprecht\n[Paper]\n\n\n\nRevisiting Example Dependent Cost-Sensitive Learning with Decision Trees (ICCV 2013)\n\nOisin Mac Aodha, Gabriel J. Brostow\n[Paper]\n\n\n\nConformal Prediction Using Decision Trees (ICDM 2013)\n\nUlf Johansson, Henrik Bostr√∂m, Tuve L√∂fstr√∂m\n[Paper]\n\n\n\nFocal-Test-Based Spatial Decision Tree Learning: A Summary of Results (ICDM 2013)\n\nZhe Jiang, Shashi Shekhar, Xun Zhou, Joseph K. Knight, Jennifer Corcoran\n[Paper]\n\n\n\nTop-down Particle Filtering for Bayesian Decision Trees (ICML 2013)\n\nBalaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh\n[Paper]\n\n\n\nQuickly Boosting Decision Trees - Pruning Underachieving Features Early (ICML 2013)\n\nRon Appel, Thomas J. Fuchs, Piotr Doll√°r, Pietro Perona\n[Paper]\n\n\n\nKnowledge Compilation for Model Counting: Affine Decision Trees (IJCAI 2013)\n\nFr√©d√©ric Koriche, Jean-Marie Lagniez, Pierre Marquis, Samuel Thomas\n[Paper]\n\n\n\nUnderstanding Variable Importances in Forests of Randomized Trees (NIPS 2013)\n\nGilles Louppe, Louis Wehenkel, Antonio Sutera, Pierre Geurts\n[Paper]\n\n\n\nRegression-tree Tuning in a Streaming Setting (NIPS 2013)\n\nSamory Kpotufe, Francesco Orabona\n[Paper]\n\n\n\nLearning Max-Margin Tree Predictors (UAI 2013)\n\nOfer Meshi, Elad Eban, Gal Elidan, Amir Globerson\n[Paper]\n\n\n\n2012\n\n\nRegression Tree Fields - An Efficient, Non-parametric Approach to Image Labeling Problems (CVPR 2012)\n\nJeremy Jancsary, Sebastian Nowozin, Toby Sharp, Carsten Rother\n[Paper]\n\n\n\nConfDTree: Improving Decision Trees Using Confidence Intervals (ICDM 2012)\n\nGilad Katz, Asaf Shabtai, Lior Rokach, Nir Ofek\n[Paper]\n\n\n\nImproved Information Gain Estimates for Decision Tree Induction (ICML 2012)\n\nSebastian Nowozin\n[Paper]\n\n\n\nLearning Partially Observable Models Using Temporally Abstract Decision Trees (NIPS 2012)\n\nErik Talvitie\n[Paper]\n\n\n\nSubtree Replacement in Decision Tree Simplification (SDM 2012)\n\nSalvatore Ruggieri\n[Paper]\n\n\n\n2011\n\n\nIncorporating Boosted Regression Trees into Ecological Latent Variable Models (AAAI 2011)\n\nRebecca A. Hutchinson, Li-Ping Liu, Thomas G. Dietterich\n[Paper]\n\n\n\nSyntactic Decision Tree LMs: Random Selection or Intelligent Design (EMNLP 2011)\n\nDenis Filimonov, Mary P. Harper\n[Paper]\n\n\n\nDecision Tree Fields (ICCV 2011)\n\nSebastian Nowozin, Carsten Rother, Shai Bagon, Toby Sharp, Bangpeng Yao, Pushmeet Kohli\n[Paper]\n\n\n\nConfidence in Predictions from Random Tree Ensembles (ICDM 2011)\n\nSiddhartha Bhattacharyya\n[Paper]\n\n\n\nSpeeding-Up Hoeffding-Based Regression Trees With Options (ICML 2011)\n\nElena Ikonomovska, Jo√£o Gama, Bernard Zenko, Saso Dzeroski\n[Paper]\n\n\n\nDensity Estimation Trees (KDD 2011)\n\nParikshit Ram, Alexander G. Gray\n[Paper]\n\n\n\nBagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models (SIGIR 2011)\n\nYasser Ganjisaffar, Rich Caruana, Cristina Videira Lopes\n[Paper]\n\n\n\nOn the Complexity of Decision Making in Possibilistic Decision Trees (UAI 2011)\n\nH√©l√®ne Fargier, Nahla Ben Amor, Wided Guezguez\n[Paper]\n\n\n\nAdaptive Bootstrapping of Recommender Systems Using Decision Trees (WSDM 2011)\n\nNadav Golbandi, Yehuda Koren, Ronny Lempel\n[Paper]\n\n\n\nParallel Boosted Regression Trees for Web Search Ranking (WWW 2011)\n\nStephen Tyree, Kilian Q. Weinberger, Kunal Agrawal, Jennifer Paykin\n[Paper]\n\n\n\n2010\n\n\nDiscrimination Aware Decision Tree Learning (ICDM 2010)\n\nFaisal Kamiran, Toon Calders, Mykola Pechenizkiy\n[Paper]\n\n\n\nDecision Trees for Uplift Modeling (ICDM 2010)\n\nPiotr Rzepakowski, Szymon Jaroszewicz\n[Paper]\n\n\n\nLearning Markov Network Structure with Decision Trees (ICDM 2010)\n\nDaniel Lowd, Jesse Davis\n[Paper]\n\n\n\nMultivariate Dyadic Regression Trees for Sparse Learning Problems (NIPS 2010)\n\nHan Liu, Xi Chen\n[Paper]\n\n\n\nFast and Accurate Gene Prediction by Decision Tree Classification (SDM 2010)\n\nRong She, Jeffrey Shih-Chieh Chu, Ke Wang, Nansheng Chen\n[Paper]\n\n\n\nA Robust Decision Tree Algorithm for Imbalanced Data Sets (SDM 2010)\n\nWei Liu, Sanjay Chawla, David A. Cieslak, Nitesh V. Chawla\n[Paper]\n\n\n\n2009\n\n\nStochastic Gradient Boosted Distributed Decision Trees (CIKM 2009)\n\nJerry Ye, Jyh-Herng Chow, Jiang Chen, Zhaohui Zheng\n[Paper]\n\n\n\nFeature Selection for Ranking Using Boosted Trees (CIKM 2009)\n\nFeng Pan, Tim Converse, David Ahn, Franco Salvetti, Gianluca Donato\n[Paper]\n\n\n\nThai Word Segmentation with Hidden Markov Model and Decision Tree (PAKDD 2009)\n\nPoramin Bheganan, Richi Nayak, Yue Xu\n[Paper]\n\n\n\nParameter Estimdation in Semi-Random Decision Tree Ensembling on Streaming Data (PAKDD 2009)\n\nPei-Pei Li, Qianhui Liang, Xindong Wu, Xuegang Hu\n[Paper]\n\n\n\nDTU: A Decision Tree for Uncertain Data (PAKDD 2009)\n\nBiao Qin, Yuni Xia, Fang Li\n[Paper]\n\n\n\n2008\n\n\nPredicting Future Decision Trees from Evolving Data (ICDM 2008)\n\nMirko B√∂ttcher, Martin Spott, Rudolf Kruse\n[Paper]\n\n\n\nBayes Optimal Classification for Decision Trees (ICML 2008)\n\nSiegfried Nijssen\n[Paper]\n\n\n\nA New Credit Scoring Method Based on Rough Sets and Decision Tree (PAKDD 2008)\n\nXiYue Zhou, Defu Zhang, Yi Jiang\n[Paper]\n\n\n\nA Comparison of Different Off-Centered Entropies to Deal with Class Imbalance for Decision Trees (PAKDD 2008)\n\nPhilippe Lenca, St√©phane Lallich, Thanh-Nghi Do, Nguyen-Khang Pham\n[Paper]\n\n\n\nBOAI: Fast Alternating Decision Tree Induction Based on Bottom-Up Evaluation (PAKDD 2008)\n\nBishan Yang, Tengjiao Wang, Dongqing Yang, Lei Chang\n[Paper]\n\n\n\nA General Framework for Estimating Similarity of Datasets and Decision Trees: Exploring Semantic Similarity of Decision Trees (SDM 2008)\n\nIrene Ntoutsi, Alexandros Kalousis, Yannis Theodoridis\n[Paper]\n\n\n\nROC-tree: A Novel Decision Tree Induction Algorithm Based on Receiver Operating Characteristics to Classify Gene Expression Data (SDM 2008)\n\nM. Maruf Hossain, Md. Rafiul Hassan, James Bailey\n[Paper]\n\n\n\n2007\n\n\nTree-based Classifiers for Bilayer Video Segmentation (CVPR 2007)\n\nPei Yin, Antonio Criminisi, John M. Winn, Irfan A. Essa\n[Paper]\n\n\n\nAdditive Groves of Regression Trees (ECML 2007)\n\nDaria Sorokina, Rich Caruana, Mirek Riedewald\n[Paper]\n\n\n\nDecision Tree Instability and Active Learning (ECML 2007)\n\nKenneth Dwyer, Robert Holte\n[Paper]\n\n\n\nEnsembles of Multi-Objective Decision Trees (ECML 2007)\n\nDragi Kocev, Celine Vens, Jan Struyf, Saso Dzeroski\n[Paper]\n\n\n\nSeeing the Forest Through the Trees: Learning a Comprehensible Model from an Ensemble (ECML 2007)\n\nAnneleen Van Assche, Hendrik Blockeel\n[Paper]\n\n\n\nSample Compression Bounds for Decision Trees (ICML 2007)\n\nMohak Shah\n[Paper]\n\n\n\nA Tighter Error Bound for Decision Tree Learning Using PAC Learnability (IJCAI 2007)\n\nChaithanya Pichuka, Raju S. Bapi, Chakravarthy Bhagvati, Arun K. Pujari, Bulusu Lakshmana Deekshatulu\n[Paper]\n\n\n\nKeep the Decision Tree and Estimate the Class Probabilities Using its Decision Boundary (IJCAI 2007)\n\nIsabelle Alvarez, Stephan Bernard, Guillaume Deffuant\n[Paper]\n\n\n\nReal Boosting a la Carte with an Application to Boosting Oblique Decision Tree (IJCAI 2007)\n\nClaudia Henry, Richard Nock, Frank Nielsen\n[Paper]\n\n\n\nScalable Look-ahead Linear Regression Trees (KDD 2007)\n\nDavid S. Vogel, Ognian Asparouhov, Tobias Scheffer\n[Paper]\n\n\n\nMining Optimal Decision Trees from Itemset Lattices (KDD 2007)\n\nSiegfried Nijssen, √âlisa Fromont\n[Paper]\n\n\n\nA Hybrid Multi-group Privacy-Preserving Approach for Building Decision Trees (PAKDD 2007)\n\nZhouxuan Teng, Wenliang Du\n[Paper]\n\n\n\n2006\n\n\nDecision Tree Methods for Finding Reusable MDP Homomorphisms (AAAI 2006)\n\nAlicia P. Wolfe, Andrew G. Barto\n[Paper]\n\n\n\nA Fast Decision Tree Learning Algorithm (AAAI 2006)\n\nJiang Su, Harry Zhang\n[Paper]\n\n\n\nAnytime Induction of Decision Trees: An Iterative Improvement Approach (AAAI 2006)\n\nSaher Esmeir, Shaul Markovitch\n[Paper]\n\n\n\nWhen a Decision Tree Learner Has Plenty of Time (AAAI 2006)\n\nSaher Esmeir, Shaul Markovitch\n[Paper]\n\n\n\nDecision Trees for Functional Variables (ICDM 2006)\n\nSuhrid Balakrishnan, David Madigan\n[Paper]\n\n\n\nCost-Sensitive Decision Tree Learning for Forensic Classification (ECML 2006)\n\nJason V. Davis, Jungwoo Ha, Christopher J. Rossbach, Hany E. Ramadan, Emmett Witchel\n[Paper]\n\n\n\nImproving the Ranking Performance of Decision Trees (ECML 2006)\n\nBin Wang, Harry Zhang\n[Paper]\n\n\n\nA General Framework for Accurate and Fast Regression by Data Summarization in Random Decision Trees (KDD 2006)\n\nWei Fan, Joe McCloskey, Philip S. Yu\n[Paper]\n\n\n\nConstructing Decision Trees for Graph-Structured Data by Chunkingless Graph-Based Induction (PAKDD 2006)\n\nPhu Chien Nguyen, Kouzou Ohara, Akira Mogi, Hiroshi Motoda, Takashi Washio\n[Paper]\n\n\n\nVariable Randomness in Decision Tree Ensembles (PAKDD 2006)\n\nFei Tony Liu, Kai Ming Ting\n[Paper]\n\n\n\nGeneralized Conditional Entropy and a Metric Splitting Criterion for Decision Trees (PAKDD 2006)\n\nDan A. Simovici, Szymon Jaroszewicz\n[Paper]\n\n\n\nDecision Trees for Hierarchical Multilabel Classification: A Case Study in Functional Genomics (PKDD 2006)\n\nHendrik Blockeel, Leander Schietgat, Jan Struyf, Saso Dzeroski, Amanda Clare\n[Paper]\n\n\n\nk-Anonymous Decision Tree Induction (PKDD 2006)\n\nArik Friedman, Assaf Schuster, Ran Wolff\n[Paper]\n\n\n\n2005\n\n\nRepresenting Conditional Independence Using Decision Trees (AAAI 2005)\n\nJiang Su, Harry Zhang\n[Paper]\n\n\n\nUse of Expert Knowledge for Decision Tree Pruning (AAAI 2005)\n\nJingfeng Cai, John Durkin\n[Paper]\n\n\n\nModel Selection in Omnivariate Decision Trees (ECML 2005)\n\nOlcay Taner Yildiz, Ethem Alpaydin\n[Paper]\n\n\n\nCombining Bias and Variance Reduction Techniques for Regression Trees (ECML 2005)\n\nYuk Lai Suen, Prem Melville, Raymond J. Mooney\n[Paper]\n\n\n\nSimple Test Strategies for Cost-Sensitive Decision Trees (ECML 2005)\n\nShengli Sheng, Charles X. Ling, Qiang Yang\n[Paper]\n\n\n\nEffective Estimation of Posterior Probabilities: Explaining the Accuracy of Randomized Decision Tree Approaches (ICDM 2005)\n\nWei Fan, Ed Greengrass, Joe McCloskey, Philip S. Yu, Kevin Drummey\n[Paper]\n\n\n\nExploiting Informative Priors for Bayesian Classification and Regression Trees (IJCAI 2005)\n\nNicos Angelopoulos, James Cussens\n[Paper]\n\n\n\nRanking Cases with Decision Trees: a Geometric Method that Preserves Intelligibility (IJCAI 2005)\n\nIsabelle Alvarez, Stephan Bernard\n[Paper]\n\n\n\nMaximizing Tree Diversity by Building Complete-Random Decision Trees (PAKDD 2005)\n\nFei Tony Liu, Kai Ming Ting, Wei Fan\n[Paper]\n\n\n\nHybrid Cost-Sensitive Decision Tree (PKDD 2005)\n\nShengli Sheng, Charles X. Ling\n[Paper]\n\n\n\nTree2 - Decision Trees for Tree Structured Data (PKDD 2005)\n\nBj√∂rn Bringmann, Albrecht Zimmermann\n[Paper]\n\n\n\nBuilding Decision Trees on Records Linked through Key References (SDM 2005)\n\nKe Wang, Yabo Xu, Philip S. Yu, Rong She\n[Paper]\n\n\n\nDecision Tree Induction in High Dimensional, Hierarchically Distributed Databases (SDM 2005)\n\nAmir Bar-Or, Ran Wolff, Assaf Schuster, Daniel Keren\n[Paper]\n\n\n\nBoosted Decision Trees for Word Recognition in Handwritten Document Retrieval (SIGIR 2005)\n\nNicholas R. Howe, Toni M. Rath, R. Manmatha\n[Paper]\n\n\n\n2004\n\n\nOn the Optimality of Probability Estimation by Random Decision Trees (AAAI 2004)\n\nWei Fan\n[Paper]\n\n\n\nOccam's Razor and a Non-Syntactic Measure of Decision Tree Complexity (AAAI 2004)\n\nGoutam Paul\n[Paper]\n\n\n\nUsing Emerging Patterns and Decision Trees in Rare-Class Classification (ICDM 2004)\n\nHamad Alhammady, Kotagiri Ramamohanarao\n[Paper]\n\n\n\nOrthogonal Decision Trees (ICDM 2004)\n\nHillol Kargupta, Haimonti Dutta\n[Paper]\n\n\n\nImproving the Reliability of Decision Tree and Naive Bayes Learners (ICDM 2004)\n\nDavid George Lindsay, Si√¢n Cox\n[Paper]\n\n\n\nCommunication Efficient Construction of Decision Trees Over Heterogeneously Distributed Data (ICDM 2004)\n\nChris Giannella, Kun Liu, Todd Olsen, Hillol Kargupta\n[Paper]\n\n\n\nDecision Tree Evolution Using Limited Number of Labeled Data Items from Drifting Data Streams (ICDM 2004)\n\nWei Fan, Yi-an Huang, Philip S. Yu\n[Paper]\n\n\n\nLookahead-based Algorithms for Anytime Induction of Decision Trees (ICML 2004)\n\nSaher Esmeir, Shaul Markovitch\n[Paper]\n\n\n\nDecision Trees with Minimal Costs (ICML 2004)\n\nCharles X. Ling, Qiang Yang, Jianning Wang, Shichao Zhang\n[Paper]\n\n\n\nTraining Conditional Random Fields via Gradient Tree Boosting (ICML 2004)\n\nThomas G. Dietterich, Adam Ashenfelter, Yaroslav Bulatov\n[Paper]\n\n\n\nDetecting Structural Metadata with Decision Trees and Transformation-Based Learning (NAACL 2004)\n\nJoungbum Kim, Sarah E. Schwarm, Mari Ostendorf\n[Paper]\n\n\n\nOn the Adaptive Properties of Decision Trees (NIPS 2004)\n\nClayton D. Scott, Robert D. Nowak\n[Paper]\n\n\n\nA Metric Approach to Building Decision Trees Based on Goodman-Kruskal Association Index (PAKDD 2004)\n\nDan A. Simovici, Szymon Jaroszewicz\n[Paper]\n\n\n\n2003\n\n\nRademacher Penalization over Decision Tree Prunings (ECML 2003)\n\nMatti K√§√§ri√§inen, Tapio Elomaa\n[Paper]\n\n\n\nEnsembles of Cascading Trees (ICDM 2003)\n\nJinyan Li, Huiqing Liu\n[Paper]\n\n\n\nPostprocessing Decision Trees to Extract Actionable Knowledge (ICDM 2003)\n\nQiang Yang, Jie Yin, Charles X. Ling, Tielin Chen\n[Paper]\n\n\n\nK-D Decision Tree: An Accelerated and Memory Efficient Nearest Neighbor Classifier (ICDM 2003)\n\nTomoyuki Shibata, Takekazu Kato, Toshikazu Wada\n[Paper]\n\n\n\nIdentifying Markov Blankets with Decision Tree Induction (ICDM 2003)\n\nLewis J. Frey, Douglas H. Fisher, Ioannis Tsamardinos, Constantin F. Aliferis, Alexander R. Statnikov\n[Paper]\n\n\n\nComparing Naive Bayes, Decision Trees, and SVM with AUC and Accuracy (ICDM 2003)\n\nJin Huang, Jingjing Lu, Charles X. Ling\n[Paper]\n\n\n\nBoosting Lazy Decision Trees (ICML 2003)\n\nXiaoli Zhang Fern, Carla E. Brodley\n[Paper]\n\n\n\nDecision Tree with Better Ranking (ICML 2003)\n\nCharles X. Ling, Robert J. Yan\n[Paper]\n\n\n\nSkewing: An Efficient Alternative to Lookahead for Decision Tree Induction (IJCAI 2003)\n\nDavid Page, Soumya Ray\n[Paper]\n\n\n\nEfficient Decision Tree Construction on Streaming Data (KDD 2003)\n\nRuoming Jin, Gagan Agrawal\n[Paper]\n\n\n\nPaintingClass: Interactive Construction Visualization and Exploration of Decision Trees (KDD 2003)\n\nSoon Tee Teoh, Kwan-Liu Ma\n[Paper]\n\n\n\nAccurate Decision Trees for Mining High-Speed Data Streams (KDD 2003)\n\nJo√£o Gama, Ricardo Rocha, Pedro Medas\n[Paper]\n\n\n\nNear-Minimax Optimal Classification with Dyadic Classification Trees (NIPS 2003)\n\nClayton D. Scott, Robert D. Nowak\n[Paper]\n\n\n\nImproving Performance of Decision Tree Algorithms with Multi-edited Nearest Neighbor Rule (PAKDD 2003)\n\nChenzhou Ye, Jie Yang, Lixiu Yao, Nian-yi Chen\n[Paper]\n\n\n\nArbogodai: a New Approach for Decision Trees (PKDD 2003)\n\nDjamel A. Zighed, Gilbert Ritschard, Walid Erray, Vasile-Marian Scuturici\n[Paper]\n\n\n\nCommunication and Memory Efficient Parallel Decision Tree Construction (SDM 2003)\n\nRuoming Jin, Gagan Agrawal\n[Paper]\n\n\n\nDecision Tree Classification of Spatial Data Patterns from Videokeratography using Zernicke Polynomials (SDM 2003)\n\nMichael D. Twa, Srinivasan Parthasarathy, Thomas W. Raasch, Mark Bullimore\n[Paper]\n\n\n\n2002\n\n\nMulticlass Alternating Decision Trees (ECML 2002)\n\nGeoffrey Holmes, Bernhard Pfahringer, Richard Kirkby, Eibe Frank, Mark A. Hall\n[Paper]\n\n\n\nHeterogeneous Forests of Decision Trees (ICANN 2002)\n\nKrzysztof Grabczewski, Wlodzislaw Duch\n[Paper]\n\n\n\nSolving the Fragmentation Problem of Decision Trees by Discovering Boundary Emerging Patterns (ICDM 2002)\n\nJinyan Li, Limsoon Wong\n[Paper]\n\n\n\nSolving the Fragmentation Problem of Decision Trees by Discovering Boundary Emerging Patterns (ICDM 2002)\n\nJinyan Li, Limsoon Wong\n[Paper]\n\n\n\nLearning Decision Trees Using the Area Under the ROC Curve (ICML 2002)\n\nC√©sar Ferri, Peter A. Flach, Jos√© Hern√°ndez-Orallo\n[Paper]\n\n\n\nFinding an Optimal Gain-Ratio Subset-Split Test for a Set-Valued Attribute in Decision Tree Induction (ICML 2002)\n\nFumio Takechi, Einoshin Suzuki\n[Paper]\n\n\n\nEfficiently Mining Frequent Trees in a Forest (KDD 2002)\n\nMohammed Javeed Zaki\n[Paper]\n\n\n\nSECRET: a Scalable Linear Regression Tree Algorithm (KDD 2002)\n\nAlin Dobra, Johannes Gehrke\n[Paper]\n\n\n\nInstability of Decision Tree Classification Algorithms (KDD 2002)\n\nRuey-Hsia Li, Geneva G. Belford\n[Paper]\n\n\n\nExtracting Decision Trees From Trained Neural Networks (KDD 2002)\n\nOlcay Boz\n[Paper]\n\n\n\nDyadic Classification Trees via Structural Risk Minimization (NIPS 2002)\n\nClayton D. Scott, Robert D. Nowak\n[Paper]\n\n\n\nApproximate Splitting for Ensembles of Trees using Histograms (SDM 2002)\n\nChandrika Kamath, Erick Cant√∫-Paz, David Littau\n[Paper]\n\n\n\n2001\n\n\nJapanese Named Entity Recognition based on a Simple Rule Generator and Decision Tree Learning (ACL 2001)\n\nHideki Isozaki\n[Paper]\n\n\n\nMessage Length as an Effective Ockham's Razor in Decision Tree Induction (AISTATS 2001)\n\nScott Needham, David L. Dowe\n[Paper]\n\n\n\nSQL Database Primitives for Decision Tree Classifiers (CIKM 2001)\n\nKai-Uwe Sattler, Oliver Dunemann\n[Paper]\n\n\n\nA Unified Framework for Evaluation Metrics in Classification Using Decision Trees (ECML 2001)\n\nRicardo Vilalta, Mark Brodie, Daniel Oblinger, Irina Rish\n[Paper]\n\n\n\nBackpropagation in Decision Trees for Regression (ECML 2001)\n\nVictor Medina-Chico, Alberto Su√°rez, James F. Lutsko\n[Paper]\n\n\n\nConsensus Decision Trees: Using Consensus Hierarchical Clustering for Data Relabelling and Reduction (ECML 2001)\n\nBranko Kavsek, Nada Lavrac, Anuska Ferligoj\n[Paper]\n\n\n\nMining Decision Trees from Data Streams in a Mobile Environment (ICDM 2001)\n\nHillol Kargupta, Byung-Hoon Park\n[Paper]\n\n\n\nEfficient Determination of Dynamic Split Points in a Decision Tree (ICDM 2001)\n\nDavid Maxwell Chickering, Christopher Meek, Robert Rounthwaite\n[Paper]\n\n\n\nA Comparison of Stacking with Meta Decision Trees to Bagging, Boosting, and Stacking with other Methods (ICDM 2001)\n\nBernard Zenko, Ljupco Todorovski, Saso Dzeroski\n[Paper]\n\n\n\nEfficient Algorithms for Decision Tree Cross-Validation (ICML 2001)\n\nHendrik Blockeel, Jan Struyf\n[Paper]\n\n\n\nBias Correction in Classification Tree Construction (ICML 2001)\n\nAlin Dobra, Johannes Gehrke\n[Paper]\n\n\n\nBreeding Decision Trees Using Evolutionary Techniques (ICML 2001)\n\nAthanassios Papagelis, Dimitrios Kalles\n[Paper]\n\n\n\nObtaining Calibrated Probability Estimates from Decision Trees and Naive Bayesian Classifiers (ICML 2001)\n\nBianca Zadrozny, Charles Elkan\n[Paper]\n\n\n\nTemporal Decision Trees or the lazy ECU vindicated (IJCAI 2001)\n\nLuca Console, Claudia Picardi, Daniele Theseider Dupr√©\n[Paper]\n\n\n\nData Mining Criteria for Tree-based Regression and Classification (KDD 2001)\n\nAndreas Buja, Yung-Seop Lee\n[Paper]\n\n\n\nA Decision Tree of Bigrams is an Accurate Predictor of Word Sense (NAACL 2001)\n\nTed Pedersen\n[Paper]\n\n\n\nRule Reduction over Numerical Attributes in Decision Tree Using Multilayer Perceptron (PAKDD 2001)\n\nDaeEun Kim, Jaeho Lee\n[Paper]\n\n\n\nA Scalable Algorithm for Rule Post-pruning of Large Decision Trees (PAKDD 2001)\n\nTrong Dung Nguyen, Tu Bao Ho, Hiroshi Shimodaira\n[Paper]\n\n\n\nOptimizing the Induction of Alternating Decision Trees (PAKDD 2001)\n\nBernhard Pfahringer, Geoffrey Holmes, Richard Kirkby\n[Paper]\n\n\n\nInteractive Construction of Decision Trees (PAKDD 2001)\n\nJianchao Han, Nick Cercone\n[Paper]\n\n\n\nBloomy Decision Tree for Multi-objective Classification (PKDD 2001)\n\nEinoshin Suzuki, Masafumi Gotoh, Yuta Choki\n[Paper]\n\n\n\nA Fourier Analysis Based Approach to Learning Decision Trees in a Distributed Environment (SDM 2001)\n\nByung-Hoon Park, Rajeev Ayyagari, Hillol Kargupta\n[Paper]\n\n\n\n2000\n\n\nIntuitive Representation of Decision Trees Using General Rules and Exceptions (AAAI 2000)\n\nBing Liu, Minqing Hu, Wynne Hsu\n[Paper]\n\n\n\nTagging Unknown Proper Names Using Decision Trees (ACL 2000)\n\nFr√©d√©ric B√©chet, Alexis Nasr, Franck Genet\n[Paper]\n\n\n\nClustering Through Decision Tree Construction (CIKM 2000)\n\nBing Liu, Yiyuan Xia, Philip S. Yu\n[Paper]\n\n\n\nHandling Continuous-Valued Attributes in Decision Tree with Neural Network Modelling (ECML 2000)\n\nDaeEun Kim, Jaeho Lee\n[Paper]\n\n\n\nInvestigation and Reduction of Discretization Variance in Decision Tree Induction (ECML 2000)\n\nPierre Geurts, Louis Wehenkel\n[Paper]\n\n\n\nNonparametric Regularization of Decision Trees (ECML 2000)\n\nTobias Scheffer\n[Paper]\n\n\n\nExploiting the Cost (In)sensitivity of Decision Tree Splitting Criteria (ICML 2000)\n\nChris Drummond, Robert C. Holte\n[Paper]\n\n\n\nMulti-agent Q-learning and Regression Trees for Automated Pricing Decisions (ICML 2000)\n\nManu Sridharan, Gerald Tesauro\n[Paper]\n\n\n\nGrowing Decision Trees on Support-less Association Rules (KDD 2000)\n\nKe Wang, Senqiang Zhou, Yu He\n[Paper]\n\n\n\nEfficient Algorithms for Constructing Decision Trees with Constraints (KDD 2000)\n\nMinos N. Garofalakis, Dongjoon Hyun, Rajeev Rastogi, Kyuseok Shim\n[Paper]\n\n\n\nInteractive Visualization in Mining Large Decision Trees (PAKDD 2000)\n\nTrong Dung Nguyen, Tu Bao Ho, Hiroshi Shimodaira\n[Paper]\n\n\n\nVQTree: Vector Quantization for Decision Tree Induction (PAKDD 2000)\n\nShlomo Geva, Lawrence Buckingham\n[Paper]\n\n\n\nSome Enhencements of Decision Tree Bagging (PKDD 2000)\n\nPierre Geurts\n[Paper]\n\n\n\nCombining Multiple Models with Meta Decision Trees (PKDD 2000)\n\nLjupco Todorovski, Saso Dzeroski\n[Paper]\n\n\n\nInduction of Multivariate Decision Trees by Using Dipolar Criteria (PKDD 2000)\n\nLeon Bobrowski, Marek Kretowski\n[Paper]\n\n\n\nDecision Tree Toolkit: A Component-Based Library of Decision Tree Algorithms (PKDD 2000)\n\nNikos Drossos, Athanassios Papagelis, Dimitrios Kalles\n[Paper]\n\n\n\n1999\n\n\nModeling Decision Tree Performance with the Power Law (AISTATS 1999)\n\nLewis J. Frey, Douglas H. Fisher\n[Paper]\n\n\n\nCausal Mechanisms and Classification Trees for Predicting Chemical Carcinogens (AISTATS 1999)\n\nLouis Anthony Cox Jr.\n[Paper]\n\n\n\nPOS Tags and Decision Trees for Language Modeling (EMNLP 1999)\n\nPeter A. Heeman\n[Paper]\n\n\n\nLazy Bayesian Rules: A Lazy Semi-Naive Bayesian Learning Technique Competitive to Boosting Decision Trees (ICML 1999)\n\nZijian Zheng, Geoffrey I. Webb, Kai Ming Ting\n[Paper]\n\n\n\nThe Alternating Decision Tree Learning Algorithm (ICML 1999)\n\nYoav Freund, Llew Mason\n[Paper]\n[Code]\n\n\n\nBoosting with Multi-Way Branching in Decision Trees (NIPS 1999)\n\nYishay Mansour, David A. McAllester\n[Paper]\n\n\n\n1998\n\n\nLearning Sorting and Decision Trees with POMDPs (ICML 1998)\n\nBlai Bonet, Hector Geffner\n[Paper]\n\n\n\nUsing a Permutation Test for Attribute Selection in Decision Trees (ICML 1998)\n\nEibe Frank, Ian H. Witten\n[Paper]\n\n\n\nA Fast and Bottom-Up Decision Tree Pruning Algorithm with Near-Optimal Generalization (ICML 1998)\n\nMichael J. Kearns, Yishay Mansour\n[Paper]\n\n\n\n1997\n\n\nPessimistic Decision Tree Pruning Based Continuous-Time (ICML 1997)\n\nYishay Mansour\n[Paper]\n\n\n\nPAC Learning with Constant-Partition Classification Noise and Applications to Decision Tree Induction (ICML 1997)\n\nScott E. Decatur\n[Paper]\n\n\n\nOption Decision Trees with Majority Votes (ICML 1997)\n\nRon Kohavi, Clayton Kunz\n[Paper]\n\n\n\nIntegrating Feature Construction with Multiple Classifiers in Decision Tree Induction (ICML 1997)\n\nRicardo Vilalta, Larry A. Rendell\n[Paper]\n\n\n\nFunctional Models for Regression Tree Leaves (ICML 1997)\n\nLu√≠s Torgo\n[Paper]\n\n\n\nThe Effects of Training Set Size on Decision Tree Complexity (ICML 1997)\n\nTim Oates, David D. Jensen\n[Paper]\n\n\n\nUnsupervised On-line Learning of Decision Trees for Hierarchical Data Analysis (NIPS 1997)\n\nMarcus Held, Joachim M. Buhmann\n[Paper]\n\n\n\nData-Dependent Structural Risk Minimization for Perceptron Decision Trees (NIPS 1997)\n\nJohn Shawe-Taylor, Nello Cristianini\n[Paper]\n\n\n\nGeneralization in Decision Trees and DNF: Does Size Matter (NIPS 1997)\n\nMostefa Golea, Peter L. Bartlett, Wee Sun Lee, Llew Mason\n[Paper]\n\n\n\n1996\n\n\nSecond Tier for Decision Trees (ICML 1996)\n\nMiroslav Kubat\n[Paper]\n\n\n\nNon-Linear Decision Trees - NDT (ICML 1996)\n\nAndreas Ittner, Michael Schlosser\n[Paper]\n\n\n\nLearning Relational Concepts with Decision Trees (ICML 1996)\n\nPeter Geibel, Fritz Wysotzki\n[Paper]\n\n\n\n1995\n\n\nA Hill-Climbing Approach for Optimizing Classification Trees (AISTATS 1995)\n\nXiaorong Sun, Steve Y. Chiu, Louis Anthony Cox Jr.\n[Paper]\n\n\n\nAn Exact Probability Metric for Decision Tree Splitting (AISTATS 1995)\n\nJ. Kent Martin\n[Paper]\n\n\n\nOn Pruning and Averaging Decision Trees (ICML 1995)\n\nJonathan J. Oliver, David J. Hand\n[Paper]\n\n\n\nOn Handling Tree-Structured Attributed in Decision Tree Learning (ICML 1995)\n\nHussein Almuallim, Yasuhiro Akiba, Shigeo Kaneda\n[Paper]\n\n\n\nRetrofitting Decision Tree Classifiers Using Kernel Density Estimation (ICML 1995)\n\nPadhraic Smyth, Alexander G. Gray, Usama M. Fayyad\n[Paper]\n\n\n\nIncreasing the Performance and Consistency of Classification Trees by Using the Accuracy Criterion at the Leaves (ICML 1995)\n\nDavid J. Lubinsky\n[Paper]\n\n\n\nEfficient Algorithms for Finding Multi-way Splits for Decision Trees (ICML 1995)\n\nTruxton Fulton, Simon Kasif, Steven Salzberg\n[Paper]\n\n\n\nTheory and Applications of Agnostic PAC-Learning with Small Decision Trees (ICML 1995)\n\nPeter Auer, Robert C. Holte, Wolfgang Maass\n[Paper]\n\n\n\nBoosting Decision Trees (NIPS 1995)\n\nHarris Drucker, Corinna Cortes\n[Paper]\n\n\n\nUsing Pairs of Data-Points to Define Splits for Decision Trees (NIPS 1995)\n\nGeoffrey E. Hinton, Michael Revow\n[Paper]\n\n\n\nA New Pruning Method for Solving Decision Trees and Game Trees (UAI 1995)\n\nPrakash P. Shenoy\n[Paper]\n\n\n\n1994\n\n\nA Statistical Approach to Decision Tree Modeling (ICML 1994)\n\nMichael I. Jordan\n[Paper]\n\n\n\nIn Defense of C4.5: Notes Learning One-Level Decision Trees (ICML 1994)\n\nTapio Elomaa\n[Paper]\n\n\n\nAn Improved Algorithm for Incremental Induction of Decision Trees (ICML 1994)\n\nPaul E. Utgoff\n[Paper]\n\n\n\nDecision Tree Parsing using a Hidden Derivation Model (NAACL 1994)\n\nFrederick Jelinek, John D. Lafferty, David M. Magerman, Robert L. Mercer, Adwait Ratnaparkhi, Salim Roukos\n[Paper]\n\n\n\n1993\n\nUsing Decision Trees to Improve Case-Based Learning (ICML 1993)\n\nClaire Cardie\n[Paper]\n\n\n\n1991\n\nContext Dependent Modeling of Phones in Continuous Speech Using Decision Trees (NAACL 1991)\n\nLalit R. Bahl, Peter V. de Souza, P. S. Gopalakrishnan, David Nahamoo, Michael Picheny\n[Paper]\n\n\n\n1989\n\nPerformance Comparisons Between Backpropagation Networks and Classification Trees on Three Real-World Applications (NIPS 1989)\n\nLes E. Atlas, Ronald A. Cole, Jerome T. Connor, Mohamed A. El-Sharkawi, Robert J. Marks II, Yeshwant K. Muthusamy, Etienne Barnard\n[Paper]\n\n\n\n1988\n\nMultiple Decision Trees (UAI 1988)\n\nSuk Wah Kwok, Chris Carter\n[Paper]\n\n\n\n1987\n\nDecision Tree Induction Systems: A Bayesian Analysis (UAI 1987)\n\nWray L. Buntine\n[Paper]\n\n\n\n""], 'url_profile': 'https://github.com/eric-erki', 'info_list': ['Python', 'CC0-1.0 license', 'Updated Feb 2, 2020', '1', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 31, 2020', '1', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', '1', 'R', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anushatsatish', 'info_list': ['Python', 'CC0-1.0 license', 'Updated Feb 2, 2020', '1', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 31, 2020', '1', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', '1', 'R', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/essramm', 'info_list': ['Python', 'CC0-1.0 license', 'Updated Feb 2, 2020', '1', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 31, 2020', '1', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', '1', 'R', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['online_store_session_classification\n\nOnline_store_session_classification.ipynb: Jupyter notebook with exploratory data analysis and logistic regression classifier of sessions from online store.\nrequirements.txt: Anaconda environment packages requirements\n\n'], 'url_profile': 'https://github.com/nadezdam', 'info_list': ['Python', 'CC0-1.0 license', 'Updated Feb 2, 2020', '1', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 31, 2020', '1', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', '1', 'R', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shaw-anugya', 'info_list': ['Python', 'CC0-1.0 license', 'Updated Feb 2, 2020', '1', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 31, 2020', '1', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', '1', 'R', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': [""Boston-Housing-Price-Predictor\nThis is a humble, albeit end-to-end project that I did to increase my knowledge about training and deploying machine learning models. It is a Flask app that, given a set of input parameters describing a given neighborhood in Boston, returns the median house price, using the prediction of a linear regression model. The Boston dataset is available directly through scikit-learn, as well as from the UCI Machine Learning Repository.\nOnce downloaded the app can be called from the command line using a python environment with numpy and flask installed. Simply navigate to the parent directory and type:\npython script.py\n\nThe command line will then run the app locally on the user's machine, giving them a URL to see it. By default, this is\nhttp://127.0.0.1:5000/.\nThe user will then be able to input the known variables. Of course, it is not expected that the user will have any real data to enter. However, the model can be tested against data from the existing dataset.\nFurther information on the variables can be found in Table IV of the original paper by Harrison and Rubinfeld.\n""], 'url_profile': 'https://github.com/evan-fannin', 'info_list': ['Python', 'CC0-1.0 license', 'Updated Feb 2, 2020', '1', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 31, 2020', '1', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', '1', 'R', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'Paris, France', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Incident Management Process\nThis project aims at predicting the estimaed time of completion of an incident fix.\nMotivation\nThis started out as a college project, but I decided to put it on my Github and try to contribute to the open source community,\nby simply sharing my way of doing things!\nTech/framework used\nBuilt with\n\nJupyter Notebook\n\nAlgorithms\nLinear Regression\nRandom Forest\nGridSearch Random Forest\nLibraries\nPandas\nNumpy\nSklearn\nInstallation\n1 - Clone repository\n~$ git clone https://github.com/HanyAkoury/IMP.git\n2 - Go to the working directory and then create a virtual environement\n~/IMP$ python3 -m venv venv\n3 - Install the requirements for this project\n~/IMP$ pip install -r requirements.txt\n4 - Launch the jupyter notebook\n~/IMP$ jupyter notebook\nData Source\nUCI-Machine Learning repsitory\nLicense\nMIT ¬© Hany Akoury\n'], 'url_profile': 'https://github.com/HanyAkoury', 'info_list': ['Python', 'CC0-1.0 license', 'Updated Feb 2, 2020', '1', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 31, 2020', '1', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', '1', 'R', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['UKBiobank-Table-Conversion\nConversion of data-tables extracted from the UKBiobank data-files into new regression-friendly data tables\nThe current R code, containing functions and scripts, is designed to work specifically for the data fields:\nData-Field 20002\nDescription:\tNon-cancer illness code, self-reported\nCategory:\tNon-cancer illness code, self-reported <- Medical conditions <- Verbal interview <- UK Biobank Assessment Centre\nData-Field 20002\nDescription:\tNon-cancer illness code, self-reported\nCategory:\tNon-cancer illness code, self-reported <- Medical conditions <- Verbal interview <- UK Biobank Assessment Centre\nData-Field 6159\nDescription:\tPain type(s) experienced in last month\nCategory:\tPain - Health and medical history - Touchscreen - UK Biobank Assessment Centre\nas well as for some of the additional demographic and geontypic infromation\nThe data available in an original UKBiobank file is presented in a way that allows for space to be saved (fiugratively speaking it is in a somewhat more compressed format). This data, however, is not presented in a format suitable for data analysis (e.g. regressions), which may requires binary column representation of variables of cathegorical nature or numerical column representation of continuous variables.\nThe R code provided performs this conversion.\nHow to Use:\nStep 1: Use script ""First_from_UKB.R"" to extract a raw subtable from the big raw UKB data file;\nThe scripts discussed below, import and use the file of R functions ""Set_of_functions.R"". Make sure to download this file.\nStep 2: After the raw subtatble is being extracted:\n2.1. For field 20002 and related: Use ""Table_20002.R""\n2.2. For field 6159 and related: Use ""Table_6159.R""\n2.3. For fields of demographic data (e.g. Age, Gender, Principal Components, etc): Use ""Table_demogr_data.R""\nStep 3: Further conversions:\n3.1. For a medical condition from field 20002 together with categorization for Age of Onset: Use ""Table_condition_age_grp.R""\n3.2. For cross-tabulation of two tables: Use ""Cross_Tabulation.R""\n3.3. For Chi-Squared test of each pair of variables from two respective lists of variables from two respective tables: Use ""Table_chi_squared.R""\n'], 'url_profile': 'https://github.com/futurologist', 'info_list': ['Python', 'CC0-1.0 license', 'Updated Feb 2, 2020', '1', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 31, 2020', '1', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', '1', 'R', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Marketing-Spend-Analysis-for-Healthcare-Companies\nVariable creation -\nCreate a quadratic term for SGNA (SQ_SGNA)\nCreate growth in profit [(gross profit at t-gross profit at t-1)/ gross profit at t-1].\nCreate two additional variable ‚ÄúLAG_SGNA‚Äù and ‚ÄúSGNA_MKT‚Äù\nFind the lag of SGNA at the company and fyear level.\nFind the moving average of SGNA for each firm for 3 years.\nCovert ‚Äúprivate‚Äù to type=1, if private=public; else 0.\nCreate interaction terms for interaction of SGNA with litigation, numbers of cases, developed, multinational, Australia, \tBelgium, Germany, India, Ireland, Norway, Singapore, Sweden, UK, USA, policy_reform_Australia, policy_reform_Belgium, policy_reform_Germany, policy_reform_India, policy_reform_Ireland, policy_reform_Norway, policy_reform_Singapore, policy_reform_Sweden, policy_reform_UK, policy_reform_USA, gdp per capita growth, life expectancy, mortality, population, Individuals using the Internet, physicians, acquisitions\n\nVariable Descriptions -\nDependent variables (DV): Gross_profit and growth in profit\nIndependent Variables: SGNA and SQ_SGNA\nModerating variables: All that have been created in ‚Äòd‚Äô above. \nControl Variables: current assets, age, ppe, emp, operating expenses, liabilities, book value\n\nEstimate Models -\nEstimate a regression model with SGNA as dependent variable and lag_sgna and sgna_mkt as independent variables. Store the \t\tresidual of the model NAMED AS ‚ÄúR1‚Äù.\nModel 1: DV=gross_profit, IV= SGNA and SQ_SGNA; CONTROL= current assets, age, ppe, emp, operating expenses, liabilities, book \t\tvalue, R1.\nModel 2: DV=growth in profit, IV= SGNA and SQ_SGNA; CONTROL= current assets, age, ppe, emp, operating expenses, liabilities, \t\tbook value, r1\nModel 3: Estimate a set of models for each interaction effect, for example\nExample 1: DV= gross_profit, IV= SGNA and SQ_SGNA; moderating variables: litigation, interaction between litigation and SGNA; \t\tCONTROL= current assets, age, ppe, emp, operating expenses, liabilities, book value, r1\nExample 2: DV= gross_profit, IV= SGNA and SQ_SGNA; moderating variables: acquisition, interaction between acquisition and SGNA; \tCONTROL= current assets, age, ppe, emp, operating expenses, liabilities, book value, r1\nModel 4: DV= gross_profit, IV= SGNA and SQ_SGNA; moderating variables: all of them as discussed in A (e) above; CONTROL= current \tassets, age, ppe, emp, operating expenses, liabilities, book value, r1\n\n'], 'url_profile': 'https://github.com/nishadsharmin', 'info_list': ['Python', 'CC0-1.0 license', 'Updated Feb 2, 2020', '1', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 31, 2020', '1', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', '1', 'R', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'Bloomington', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Generalized Linear Model (GLM) for Classification\nTask :\n‚Ä¢ Performed Linear, Poisson, Ordinal (for 5 levels) regression on given dataset.\n‚Ä¢ Showed that all 3 decrease in Error rate from approx. 45% to 30% as number of iterations increases.\n‚Ä¢ Used Bayesian approach as model selection to find the unknown parameter which was converged after 100 loops for time\nefficiency which showed better results with respect to error rate.\nRun the Code :\nGive the following parameters in the command line with the file name: pp3.py\n\\python3 pp3.py A.csv labels-A.csv bayesian ten\n\n\\python3 pp3.py usps.csv labels-usps.csv bayesian ten\n\n\\python3 pp3.py AP.csv labels-AP.csv poisson ten\n\n\\python3 pp3.py AO.csv labels-AO.csv ordinal ten\n\nExpected results:\nGraph 1 : Error rate graph for given dataset\nGraph 2 : Run time per iteration\nOutput: Total run time for the algorithm etc.\nFor Model Selection run this:\n\\python3 pp3.py A.csv labels-A.csv bayesian bms\n\n\\python3 pp3.py usps.csv labels-usps.csv bayesian bms\n\n\\python3 pp3.py AP.csv labels-AP.csv poisson bms\n\n\\python3 pp3.py AO.csv labels-AO.csv ordinal bms\n\nRefer report for more details\n'], 'url_profile': 'https://github.com/tolia-deepali', 'info_list': ['Python', 'CC0-1.0 license', 'Updated Feb 2, 2020', '1', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 31, 2020', '1', 'HTML', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Oct 23, 2020', '1', 'R', 'Updated Feb 11, 2021', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '273 contributions\n        in the last year', 'description': ['CarMPG_Analysis\nRegression analysis of car MPG as a function of transmission type (manual vs automatic).\n'], 'url_profile': 'https://github.com/Cbflessner', 'info_list': ['HTML', 'Updated Jan 30, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 2, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Nov 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Predict Salary and deploy using Heroku.\nPredict the salary using Linear Regression.\nInput parameters: experience, test_score, interview_score.\nOptput: Salary(in Rupees).\nDeploy using Heroku Platform.\n'], 'url_profile': 'https://github.com/sayakmisra', 'info_list': ['HTML', 'Updated Jan 30, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 2, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Nov 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'Ontario, Canada', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['rcalibration\nImplements Generalized Regression Calibration (and Related) Estimators for Measurement Error Correction.\nThis package implements a generalized regression calibration estimator, for the correction of measurement error in standard modelling contexts. This generalized estimator contains the more common replicate-based estimators (Carroll, et al., 2006) as a special case and incorporates a broad correction that establishes consistency under a broad class of error models. The package further provides utilities related to these corrections, notably the capacity to compute optimal weights when taking a convex combination of error-prone proxies and utilities for estimating the structure of errors observed in data.\nInstallation\nInstall the latest version from github. Note, this requires devtools.\ninstall.packages(""devtools"")\ndevtools::install_github(""dylanspicker/rcalibration"")\n\nUsage\nThe following example shows a brief simulation of the package in use.\n# Using \'MASS\' for \'mvrnorm\'\nlibrary(rcalibration)\nlibrary(MASS)\n\nset.seed(3141592)\n\n# Normal Example\nn <- 500 # number of observations\nreps <- 2500 # number of experiments to run\n\nresults <- matrix(nrow=reps, ncol=9*(4+1))\n\nfor(ii in 1:reps){\n  X <- mvrnorm(n, mu=c(1,2,1,-1), Sigma = matrix(c(1, 0.2, 0.1, 0.3,\n                                                   0.2, 2, 0.1, 0.1,\n                                                   0.1, 0.1, 3, 0.5,\n                                                   0.3, 0.1, 0.5, 0.5), nrow = 4, ncol = 4))\n  \n  W1 <- X + mvrnorm(n, mu=c(0,0,0,0), Sigma = diag(c(0.7, 0.4, 0.8, 0.2)))\n  W2 <- X * replicate(4, runif(n, 0, 2))\n  W3 <- X + replicate(4, rt(n, 8))\n  \n  Y <- cbind(rep(1, n), X) %*% as.matrix(c(-2, 1, 4, 2, -3)) + rnorm(n)\n  \n  ## Selecting \'optimal\' Weights\n  W <- list(W1,W2,W3)\n  \n  # Naive Models\n  mod.naive.1 <- lm(Y~W1)\n  mod.naive.2 <- lm(Y~W2)\n  mod.naive.3 <- lm(Y~W3)\n  \n  Wmean <- (1/3)*(W1+W2+W3)\n  mod.naive.mean <-lm(Y~Wmean)\n  \n  \n  wts.obj <- getOptimalWeights(W)\n  Wopt <- Reduce(""+"", lapply(1:3, function(ii){ wts.obj$weights[[ii]]*W[[ii]] }))\n  mod.naive.opt <- lm(Y~Wopt)\n  \n  ## Grabbing some of the Imputed Values\n  Xhat.opt <- generalizedRC(W)\n  Xhat.eq <- generalizedRC(W, weights=\'equal\')\n  Xhat.rand <- generalizedRC(W, weights=c(0.8, 0.05, 0.15))\n  \n  mod.cor.opt <- lm(Y~Xhat.opt)\n  mod.cor.eq <- lm(Y~Xhat.eq)\n  mod.cor.rand <- lm(Y~Xhat.rand)\n  \n  mod.true <- lm(Y~X)\n  \n  results[ii, ] <- cbind(coef(mod.true),coef(mod.cor.opt),coef(mod.cor.eq),\n                         coef(mod.cor.rand),coef(mod.naive.1),coef(mod.naive.2),\n                         coef(mod.naive.3),coef(mod.naive.mean),coef(mod.naive.opt))\n}\nparams <- c(-2, 1, 4, 2, -3)\n\npar(mfrow=c(3,2))\nfor (ii in 1:(4+1)) {\n  boxplot(results[,seq(ii,9*(4+1),by=5)], outline=F)\n  abline(h=params[ii], lty=3)\n}\n\n'], 'url_profile': 'https://github.com/DylanSpicker', 'info_list': ['HTML', 'Updated Jan 30, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 2, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Nov 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'Greater Boston Area', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Feature-Engineering-for-Sneakers-vs.-Sandals-data-set-\nThe project objective is to apply different feature engineering methods, transforming the input features that are given to the regression classifier. The goal is to classifiy sneakers vs. sandals images (28x28 each). First, I used a simple logistic regression, applying on the original training data set, as a benchmark. Next, I applied a few feature engineering methods: flipping the image horizontally (doubled the training data set), counting the number of black & white pixels (then added to the training data set as 2 new variables), and doing a histogram on the data set (added 10 new variables into the training data set).\nThe result shows that counting the number of black & white pixels earns highest AUROC, following by doing histogram the training data set and flipping the image horizontally.\n'], 'url_profile': 'https://github.com/baovinhnguyen', 'info_list': ['HTML', 'Updated Jan 30, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 2, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Nov 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '146 contributions\n        in the last year', 'description': ['Hyper-Parameter-Tuning-Polynomial-Regression\n'], 'url_profile': 'https://github.com/jayjagtap', 'info_list': ['HTML', 'Updated Jan 30, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 2, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Nov 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '618 contributions\n        in the last year', 'description': ['Salary prediction with using simple linear regression\nThis project include python and R code to implement simple linear regression.You can see how to implement linear regression two different type\nPython Graphic Result\n\n\nR Graphic Result\n\n\n'], 'url_profile': 'https://github.com/SerhatKayaa', 'info_list': ['HTML', 'Updated Jan 30, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 2, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Nov 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Simanta-kalita', 'info_list': ['HTML', 'Updated Jan 30, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 2, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Nov 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Strasser-Pablo', 'info_list': ['HTML', 'Updated Jan 30, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 2, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Nov 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/antoinedelorme', 'info_list': ['HTML', 'Updated Jan 30, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 2, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Nov 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Phoneix-15', 'info_list': ['HTML', 'Updated Jan 30, 2020', 'Python', 'GPL-3.0 license', 'Updated Feb 2, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 30, 2020', 'Updated Nov 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Phoneix-15', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'TypeScript', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kstonny', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'TypeScript', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Bike-Sharing-Demand-Linear-Regression\nThis project focuses on conducting Exploratory Data Analysis and running Linear Regression on Bike Sharing Demand data set which was provided by Hadi Fanaee Tork using data from Capital Bikeshare.\nWhat is Bike Sharing Systems ?\nBike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.\nObjective\nThe main objective of this project is to explore and create a linear Regression Model so as to try to predict bike sharing demand.\nlibrary(ggplot2)\nlibrary(dplyr)\n\nbikeshare <- read.csv(""E:/Udemy Courses/R programming AZ Udemy/bike-sharing-demand/train.csv"")\nView(bikeshare)\n\nFeatures of Data\nprint(head(bikeshare))\n\n\nThe data set contains following features :-\n datetime - hourly date + timestamp \n season - 1 = spring, 2 = summer, 3 = fall, 4 = winter \n holiday - whether the day is considered a holiday \n workingday - whether the day is neither a weekend nor holiday \n weather - \n 1: Clear, Few clouds, Partly cloudy, Partly cloudy \n 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n\n temp - temperature in Celsius \n atemp - ""feels like"" temperature in Celsius \n humidity - relative humidity \n windspeed - wind speed \n casual - number of non-registered user rentals initiated \n registered - number of registered user rentals initiated \n count - number of total rentals \n\nWhat we are trying to predict ?\nWe are trying to predict count variable i.e. number of total rentals which will be behaving as a dependent variable for our analysis.\nExploratory Data Analysis\nFirst I will be conducting exploratory data analysis which is an essential step to undestand the data.\nLoading the ggplot library\nlibrary(ggplot2)\n\nScatter Plot to show the relationship between count (number of total rentals) and temp (temperature in Celsius)\nggplot(data = bikeshare, aes(temp,count)) + geom_point(alpha = 0.3, aes(color = temp)) + theme_bw() +\n  geom_smooth(method=""loess"", se=F) +\n  labs(title=""Bike Count Vs Temperature"", \n       y=""Bike Count"", \n       x=""Temperature"")\n\nThe above scatter plot shows that as the temperature increases the count i.e. the number of total rentals also increases.\nScatter Plot to show the relationship between count (number of total rentals) and date time.\nbikeshare$datetime <- as.POSIXct(bikeshare$datetime)\npl <- ggplot(bikeshare,aes(datetime,count)) + geom_point(aes(color=temp),alpha=0.5)\npl + scale_color_continuous(low = \'#55D8CE\',high = \'#FF6E2E\') + theme_bw()\n\nThere is a clear seasonal trend where the total rental bikes seems to decrease during Winters i.e month of January and February of the year and the total rental bikes seems to increase during summers.\nThe other trend which is quite evident is that the number of rental bike counts is increasing from year 2011 to year 2013.\nCorrelation between temperature and count.\ncor(bikeshare[,c(\'temp\',\'count\')])\n\nThere is not so strong correlation between temp and count.\nBox Plot\nggplot(bikeshare,aes(factor(season),count)) + geom_boxplot(aes(color = factor(season))) + theme_bw()\n\nThe box plot between the number of bike rentals and season shows that the line can not capture the non linear relationship and that there\'s is more rentals in winter as compared to spring.\nFeature Engineering\nAs part of feature engineering I have added an hour column in the dataset.\nbikeshare$hour <- sapply(bikeshare$datetime,function(x){format(x,""%H"")})\nbikeshare$hour <- sapply(bikeshare$hour,as.numeric)\nprint(head(bikeshare))\n\nRelationship between hour of the working day and the count of bikes rented.\npl1 <- ggplot(filter(bikeshare,workingday == 1), aes(hour,count))\npl1 <- pl1+ geom_point()\nprint(pl1)\n\nThis scatter plot shows an interesting trend where count of rented bikes increases during the evening hours when people leave from office i.e. around 5 PM and morning hours when people leave for office i.e. around 8 AM.\npl1 <- pl1 + geom_point(position=position_jitter(w=1,h=0),aes(color = temp),alpah=0.5)\npl1 <- pl1 + scale_color_gradientn(colours = c(\'dark blue\',\'blue\',\'light blue\',\'light green\',\'yellow\',\'orange\',\'red\'))\nprint(pl1 + theme_bw())\n\nThis plot gives an interesting finding regarding temperature and bike rental count.\nAs the temperature increases i.e. gets hotter the count of bike rental increases and for cold temperature there is a decline in count of bike rental.\nRelationship between hour of the non-working day and the count of bikes rented.\npl2 <- ggplot(filter(bikeshare,workingday == 0), aes(hour,count))\npl2 <- pl2+ geom_point()\npl2 <- pl2 + geom_point(position=position_jitter(w=1,h=0),aes(color = temp),alpah=0.5)\npl2 <- pl2 + scale_color_gradientn(colours = c(\'dark blue\',\'blue\',\'light blue\',\'light green\',\'yellow\',\'orange\',\'red\'))\nprint(pl2 + theme_bw())\n\nDuring non working days there is very less bike rental during morning hours and it eventually increases after noon.\nModel Building\nThis model will be predicting the count of the bike rental based on the temp variable.\ntemp.model <- lm(count ~ temp, bikeshare)\nprint(summary(temp.model))\n\nModel Interpretation\n** Based on the value of Intercept which is 6.0462, linear regression model predicts that there will be 6 bike rental when the temperature is 0.\n** For temp variable Estimated Std. value is 9.1705 which signifies that a temperature increase of 1 Celsius holding all things equal is associated with a rental increase of about 9.1 bikes. \n** The above findings is not a Causation and Beta 1 would be negative if an increase in temperature was associated with a decrease in rentals.\nNext we want to know is how many bikes would we predict to be rented if the temperature was 25 degrees Celsius.\nHow many rented bikes at temperature 25 degrees Celsius\n6.0462 + 9.1705 * 25\ntemp.test <- data.frame(temp=c(25))\npredict(temp.model,temp.test)\n\n\nBased on the above calculation we can say that the number of bikes rented at 25 degrees Celsius temperature will be 235.30\nBuilding Second Model with more features\nModel that attempts to predict count based off of the following features :-\nseason\nholiday\nworkingday\nweather\ntemp\nhumidity\nwindspeed\nhour (factor)\nmodel <- lm(count ~ . -casual - registered - datetime - atemp, bikeshare)\nprint(summary(model))\n\nImportant Insights\nThis sort of model doesn\'t work well given our seasonal and time series data. We need a model that can account for this type of trend. We will get thrown off with the growth of our dataset accidentally attributing to the winter season instead of realizing it\'s just overall demand growing.\nAcknowledgements\nThis dataset was provided by Hadi Fanaee Tork using data from Capital Bikeshare . We also thank the UCI machine learning repository for hosting the dataset.\n'], 'url_profile': 'https://github.com/rajenderk18', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'TypeScript', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic-Regression-on-Titanic-Disaster\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.  While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.  With the help of this data set of details of people information who survived or not can summarize on which factors effected their survival.\nWith the help of logistic I got Accuracy Score of 77 with the same specificity and sensitivity.\nI will now woking on this Dataset with the help of Decision Tree or other algorithms.\n'], 'url_profile': 'https://github.com/PriyaKashyap', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'TypeScript', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ["" OVERVIEW \n 1 The context: \n\nAn ecommerce company is trying to decide whether to focus their efforts on their mobile app experience or their website.\n\n\n 2 About the data: \n\nThis is just a sample data: we'll work with the Ecommerce Customers csv file from the company. It has Customer info, suchas Email, Address, and their color Avatar. Then it also has numerical value columns:ers.fy it and start first with only 3 classes.\n\n 3 What you can expect in this github: \n\nHow to see the insights in the data\nHow to use this data to predict user's spending\n\n\n""], 'url_profile': 'https://github.com/baokhanh92', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'TypeScript', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '204 contributions\n        in the last year', 'description': ['House Prices: Advanced Regression Techniques\nPredict sales prices and practice feature engineering, RFs, and gradient boosting.\nKaggle Link: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\nThe notebook ""Loan_Prediction.ipynb"", I have submitted this on ""kaggle"". I have got a public score of 0.13930 on test data with a rank of 2628. I will try to improve the score by using better feature selection, hyperparameter tuning and better model selection.\nFor my first submission I have followed an awesome video ""Kaggle Competition - House Prices: Advanced Regression Techniques Part1"" on this data set by Krish Naik and link for this video is:""https://www.youtube.com/watch?v=vtm35gVP8JU&list=PLZoTAELRMXVPiKOxbwaniXjHJ02bdkLWy&index=2&t=0s"".\n'], 'url_profile': 'https://github.com/rratnakar09', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'TypeScript', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['simpleLinearRegressionBostonExercise\n'], 'url_profile': 'https://github.com/cristian-nicp', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'TypeScript', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Sydney, Australia', 'stats_list': [], 'contributions': '1,663 contributions\n        in the last year', 'description': ['This project was bootstrapped with Create React App.\nAvailable Scripts\nIn the project directory, you can run:\nyarn start\nRuns the app in the development mode.\nOpen http://localhost:3000 to view it in the browser.\nThe page will reload if you make edits.\nYou will also see any lint errors in the console.\nyarn test\nLaunches the test runner in the interactive watch mode.\nSee the section about running tests for more information.\nyarn build\nBuilds the app for production to the build folder.\nIt correctly bundles React in production mode and optimizes the build for the best performance.\nThe build is minified and the filenames include the hashes.\nYour app is ready to be deployed!\nSee the section about deployment for more information.\nyarn eject\nNote: this is a one-way operation. Once you eject, you can‚Äôt go back!\nIf you aren‚Äôt satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project.\nInstead, it will copy all the configuration files and the transitive dependencies (Webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you‚Äôre on your own.\nYou don‚Äôt have to ever use eject. The curated feature set is suitable for small and middle deployments, and you shouldn‚Äôt feel obligated to use this feature. However we understand that this tool wouldn‚Äôt be useful if you couldn‚Äôt customize it when you are ready for it.\nLearn More\nYou can learn more in the Create React App documentation.\nTo learn React, check out the React documentation.\n'], 'url_profile': 'https://github.com/fwouts', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'TypeScript', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'New York, NY ', 'stats_list': [], 'contributions': '205 contributions\n        in the last year', 'description': [""Which Public Health Factors have the Greatest Impact on Life Expectancy?\nMultivariate Linear Regression Analysis on Public Health Factors influencing Global Life Expectancy\n\nBackground Information:\nLife expectancy is the key metric for assessing population health. It tells us the average age of death in a population. Estimates suggest that in a pre-modern, poor world, life expectancy was 30 years in all regions of the world. Since 1900 the global average life expectancy has more than doubled and is now above 70 years. The inequality of life expectancy is still very large across countries. More recently the country with the lowest life expectancy is the Central African Republic at 53 years while in Japan life expectancy is 30 years greater.\nProject Description and Motivaton:\nLife expectancy is the average number of years that a group of persons in a population is expected to live. The intent of this project was to get a better understanding of the relationship between various public health factors and global life expectancy. How can countries better allocate their limited resources to improve their overall life expectancy. To answer that question multiple linear regression model were created and then evaluated to determine best fit.\nWorkflow\n\nExplore the Dataset\n\nAdd Features\nClean the Dataset\n\n\nCreate Models\nEvaluating Models\nRecommendations\n\nData Sources\nThe Global Health Observatory (GHO) data repository under World Health Organization (WHO) keeps track of the health status as well as many other related factors for all countries. The final dataset contains 2939 observations where each row represents a country for a specific year. There are a total of 193 countries with data from years 2000 to 2015. The features include immunization factors, mortality factors, economic factors, social factors and other health related factors.\n\n\n\nFeatures include:\n\n\n\n\n\n\nCountry\nHIV\\AIDS\nMeasles\n\n\nYear\nHepatitis B\nBMI\n\n\nLife expectancy\nPolio\nStatus\n\n\nAdult mortality\nDiphtheria\nPrevalence for malnutrition 5-9\n\n\nInfant mortality\nGDP\nEducation\n\n\nAlcohol consumpton\nPopulation\nTotal expenditure on health\n\n\nExpenditure on health (%)\nPrevalence for malnutrition 1-19\n\n\n\n\nTechnical Description\nTo achieve the goal various Python libraries were used, which include: Pandas to clean and explore the data. Numpy, Scipy, and Sklearn for data analysis, descriptive statistics and modeling.\nNew Features\nInitially for the baseline model, literature review and domain knowledge was used to select which predictors could have the greatest influence on life expectancy. Additionally, 4 more features were created that could effect life expectancy and better explain the data. These 4 engineered features are:\n\nPopulation Size - A population range was created which includes three catagories; Small, Medium, and Large.\nLifestyle - We created an interaction variable that takes alcohol consumption and BMI into consideration.\nEconomy - A interaction variable between population and the gross domestic product (GDP).\nDeath ratio - The ratio between adult and infant mortality.\n\nCleaning the Data\nFirst, all the fragmented observations from the dataset were removed. In order to see if the data met the linearity model assumption scatter plots were generated for each predictor with the target variable and assessed. When necessary, certain features were transformed to achieve a more linear relationship and normal distribution.\n\nNext, the multicollinearity model assumption was assessed between the selected predictors by creating a correlation heat-map. A multicollinearity threshold was assigned at 0.8 and variables above the threshold were cut off.\n\nAfter that, all possible cases of outliers were assessed by looking at box-whisker plots and scatter plots. Extreme observations that were skewing the data were removed.\n\n\nResults\nThe first model to predict life expectancy used the features; BMI, HIV, thinness 1‚Äì19, GDP, mortality ratio, lifestyle, education, infant mortality rate, economy, and population size. With the R squared equal to 0.804. In other words, the initial model explains 80% of variation in life expectancy (when compared to the mean line).\n\nAnother model was built in which the data was scaled and insignificant predictors (p-value >\xa0.05) from the baseline model were removed. Since various features have different units of measurement and the data has been transformed to meet model assumptions, scaling the data allows results to be more comparable and interpretable.\n\nTo test the model for another linear regression model assumption, the distribution of residuals for homoscedasticity was evaluated. The residuals although scattered did suggest a minor positive linear relationship. This heteroscedasticity is likely to due predictor variable distribution being skewed or there might be missing features that the dataset does not have information on.\n\nWe conducted a train, test split test using 80% of our data to to train the model and predict on the other 20%. The model's mean absolute error was 3.022\n\nTrain Test Split\nAdditionally, we tested the model with all the features we previously excluded (BMI, alcohol, GDP, and population size). The model mean absolute error slightly improved to 2.995.\n\nConclusion\nA suggestion for countries looking to increase their global life expectancy is to focus their resources mainly on programs and policies that increase HIV awareness and prevention. It's interesting to see that although the research and maintenance of HIV has increased it is still a major burden in developing nations.\xa0\nAdditionally, another suggestion would be for countries to develop policies and allocate more resources that increase access to education. The results raise another question: Are countries not giving priority to education or do they not have enough resources to provide more education? Are these countries too focused on survival to the point that more education is secondary?\nA possible next step would be to separate 'developing' and 'established' countries as the public health factors effecting each type may be very different. Developing nations still lack fundamental resources while established nations may have completely different issues such as cardiovascular disease and cancer (more data!).\n""], 'url_profile': 'https://github.com/haahussain', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'TypeScript', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/karthikbharadhwajKB', 'info_list': ['Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'HTML', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'TypeScript', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}"
"{'location': 'Paris', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TrieuYu', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 1, 2020', '5', 'Jupyter Notebook', 'Updated Feb 1, 2021', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Logistic-Regression-SUV-Data-Prediction-\nLogistic regression is a technique in statistical analysis that attempts to predict a data value based on prior observations. A logistic regression algorithm looks at the relationship between a dependent variable and one or more dependent variables.\nLogistic regression has a number of applications in machine learning. A logistic regression algorithm might attempt to predict the SUV is selling or not test the prediction.\n'], 'url_profile': 'https://github.com/Ranjitkumarsahu1436', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 1, 2020', '5', 'Jupyter Notebook', 'Updated Feb 1, 2021', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Linear-regression-of-big-data\n'], 'url_profile': 'https://github.com/xim2016', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 1, 2020', '5', 'Jupyter Notebook', 'Updated Feb 1, 2021', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'New York City, New York', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['homework-1-optimization-Harguna\n'], 'url_profile': 'https://github.com/Harguna', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 1, 2020', '5', 'Jupyter Notebook', 'Updated Feb 1, 2021', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': ['Bayesian-Networks-and-Linear-Regression\nRepresenting Linear Models as Bayesian Networks\nhttps://medium.com/analytics-vidhya/gaussian-markov-networks-linear-regression-on-continuous-variables-affddb0643be\n'], 'url_profile': 'https://github.com/Ritvik29', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 1, 2020', '5', 'Jupyter Notebook', 'Updated Feb 1, 2021', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['car-price-prediction-linear-regression\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.      They have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:  Which variables are significant in predicting the price of a car How well those variables describe the price of a car Based on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\n'], 'url_profile': 'https://github.com/hackrudra1234', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 1, 2020', '5', 'Jupyter Notebook', 'Updated Feb 1, 2021', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/teenamary', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 1, 2020', '5', 'Jupyter Notebook', 'Updated Feb 1, 2021', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'Manhattan, New York City', 'stats_list': [], 'contributions': '907 contributions\n        in the last year', 'description': ['The Cold Start Problem\nThe cold start problem refers to the scenario faced by recommendation systems when there isn\'t enough information [either due to lack of users, or due to the items being new / the platform being launched just now] for the system to make a decent recommendation action.\nThus, the system cannot draw inference for users or items for which it has not yet gathered sufficient information.\n\n1] ITEM CARECTERISTICS => CONTENT BASED FILTERING. ""Find another item similar to a given item"".\n2] USER\'s SOCIAL ENVIRONMENT / PAST BEHAVIOR => COLLABORATIVE FILTERING. ""Find another user similar to a given user. Hypothesis: Similar users will buy similar items.""\n\nTypes of problems\nNew Community\nNew Item\nNew User\nMovie Recommendation\nThe use case of Movie Recommendations suffers from  popularity bias . That is, we would have a minority of movies in a dataset which have been viewed / rated / favorited by a lot of people - on the other hand, most of the movies would not have been viewed / rated / favorited by a number of people significant enough for them to be recommended.\nPopularity Bias\nA handful of items receive a large number of interaction, while most of the items recieve only a fraction of them.\nFEW INERACTIONS => POOR RECOMMENDATIONS.\nZipf\'s Law\nZipf\'s law (/z…™f/, not /ts…™pf/ as in German) is an empirical law formulated using mathematical statistics that refers to the fact that for many types of data studied in the physical and social sciences, the rank-frequency distribution is an inverse relation. The Zipfian distribution is one of a family of related discrete power law probability distributions. It is related to the zeta distribution, but is not identical.\nThe same relationship occurs in many other rankings of human created systems, such as the ranks of mathematical expressions or ranks of notes in music, and even in uncontrolled environments, such as the population ranks of cities in various countries, corporation sizes, income rankings, ranks of number of people watching the same TV channel, and so on. The appearance of the distribution in rankings of cities by population was first noticed by Felix Auerbach in 1913. Empirically, a data set can be tested to see whether Zipf\'s law applies by checking the goodness of fit of an empirical distribution to the hypothesized power law distribution with a Kolmogorov‚ÄìSmirnov test.\n\nInformation Retrieval (Scraping)\n\nUsing requests and bs4 (BeautifulSoup) for static web pages (and make use of  Selenium for dynamic pages).\nRefer the Scraper for a basic shell/structure of a scraper.\n\nExample results from bs4 processing:\nfunction lockScroll() {\n            var lockX = window.scrollX;\n            var lockY = window.scrollY;\n\n            function lockIt() {\n                window.scrollTo(lockX, lockY);\n                return false;\n            }\n\n            window.addEventListener(""scroll"", lockIt, false);\n            return {\n                stop: function () {\n                    window.removeEventListener(""scroll"", lockIt, false);\n                }\n            }\n        }\n        window.addEventListener(""load"", function () {\n            $(\'#ResultsScrollable\').bind(""scroll"", function () {\n                if ($(this).scrollTop() + $(this).innerHeight() >= $(this)[0].scrollHeight) {\n                    var locker = lockScroll();\n                    var loadBtn = document.getElementById(\'loadMoreJobs\');\n                    if (loadBtn) loadBtn.click();\n                    locker.stop();\n                } \n            });\n        });\nProblem Statement\nThis project is divided into two parts:\n\nThe Story of Film: This section aims at narrating the history, trivia and facts behind the world of cinema through the lens of data. Extensive Exploratory Data Analysis is performed on Movie Metadata about Movie Revenues, Casts, Crews, Budgets, etc. through the years. Two predictive models are built to predict movie revenues and movie success. Through these models, we also aim at discovering what features have the most significant impact in determining revenue and success.\nMovie Recommender Systems: This part is focused around building various kinds of recommendation engines; namely the Simple Generic Recommender, the Content Based Filter and the User Based Collaborative Filter. The performance of the systems are evaluated in both a qualitative and quantitative manner.\n\nApproach - Workflow\nThe problem was divided into several steps:\n\nData Collection: Data was collected from the MovieLens website and through a script that queried for data from various TMDB Endpoints.\nData Wrangling: The datasets were uploaded to a dataframe and explored. Null values were filled in wherever appropriate and polluted values were discarded or wrangled.\nEDA: Extensive data visualisation and summary statistics were used to extract insights and pattern from the various datasets. The history, facts and trivia behind movies were narrated through data.\nMachine Learning: Gradient Boosting Classifer and Regressor were trained on our feature engineered dataset to predict movie success and revenue respectively. Their feature importances were noted to gain insights into what factors influence the revenues of a movie relative to budget.\nRecommendation Systems: Four different recommendation systems were built using various ideas and algorithms such as IMDB\'s Weighted Rating, Content Based Filtering and Collaborative Filtering.\n\nFinal Results\nA Gradient Boosting Regressor and Classifier were built to predict Film Revenue and Success respectively with a Score of 0.84 and 0.88 respectively.\nIn addition, four recommendation engines were built based on different ideas and algorithms:\n\nSimple Recommender: This system used overall TMDB Vote Count and Vote Averages to build Top Movies Charts, in general and for a specific genre. The IMDB Weighted Rating System was used to calculate ratings on which the sorting was finally performed.\nContent Based Recommender: I built two content based engines; one that took movie overview and taglines as input and the other which took metadata such as cast, crew, genre and keywords to come up with predictions. I also devised a simple filter to give greater preference to movies with more votes and higher ratings.\nCollaborative Filtering: I used the powerful Surprise Library to build a collaborative filter based on singular value decomposition (SVD). The RMSE obtained was less than 1 and the engine gave estimated ratings for a given user and movie.\nHybrid Engine: I brought together ideas from content and collaborative filtering to build an engine that gave movie suggestions to a particular user based on the estimated ratings that it had internally calculated for that user.\n\nRepository Structure\n\nmovies_eda.ipynb: The Jupyter notebook that contains the EDA and narrates the Story of Film.\nmovies_recommender.ipynb: The Jupyter notebook containing code for the recommendation engines\nscrapers: The folder containing all the scrapers used to gather data from TMDB.\n\nExtension - Currently working on:\n\nUse XGBoost for classification + Visualization using Bokeh and Plotly.JS.\nImplement basic recommendation models in R and use shiny App for building a web-app.\nTry Deep collaborative filtering : The matrix factorization model can be represented as a neural network. Can also try (traditional collaborative filtering models) + (auto-encoders).\nA Reinforcement Learning Framework for Explainable Recommendation, ICDM 2018\n\n'], 'url_profile': 'https://github.com/ashwinpn', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 1, 2020', '5', 'Jupyter Notebook', 'Updated Feb 1, 2021', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Freelance_Singapore\nRequirement : I have a dataset containing all the property transactions from Jan 2016 to Jan 2019. Need to generate 2 insights and 1 predictive model using the dataset\nThe model must be either a Linear Regression Logistic Regression Machine Learning using Linear/Logistic Regression K-means clustering model KNN classification model\nInsights:\n\nThe Price range between $730000 to $1983000 is the most frequent rate for housing metrics i.e. approximately 88%.\nThe most popular Unit Price preferred for purchase ranges between $1000 to $2000.\nThe most in demand Sqft sold are in the range of 409 sqft to 1098 sqft.\nThe maximum houses sold are in postal district 17 and 19.\n63.5% of houses sold are Condiminium making it more sort after compared to 36.5% of property sold as Apartments.\nApproximately 49.5% of the Market Segment belongs to OCR.\nThe most prominent Type of Sale s Resale followed by New Sale. Sub Sale is very rare.\nAlmost all of the property sold are Apartments in high-rise buildings and very rarely property on land is purchased.\nThe most popular Floor Level sold ranges from 01 to 05 followed by 06 to 10 and 11 to 15. Floor Level B1 to B5 is statistically significant in variation of the housing Price.\nMarch, April, May and July are the highest sale months. The next set of popular sale months are June, August, September,October and November.\nSale Months July, Aug and November are statistically significant in the variation of the the price.\nThe 36 Project Names listed are statistically significant in variation of the Price.\nStreet Names Draycott Park, Jalan Mariam and Phoenix Avenue are the most staistically significant in determining the Price.\nTenure 99 yrs lease commencing from 1983 and 999999 yrs lease commmencing from 1960 are statistically significant.\nType of Sale as Resale is most statistically significant.\nThe other variables that are statistically signficant are No of Units, Area Sqft.\n\nModel Prediction:\nIn the Gradient Boost Regression Model, apprx. 98% of the variation in Price is well explained by the independent variables Project Name. Street Name, Type, Postal District, Market Segment, Tenure, Type of Sale, No of Units, Sqft Area, Floor Level, Unit Price, Sale Month and Day.\n'], 'url_profile': 'https://github.com/sdavid15', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 1, 2020', '5', 'Jupyter Notebook', 'Updated Feb 1, 2021', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}","{'location': 'Mohali,Punjab', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Breast-Cancer-Prediction-Using-Logistic-Regression\nPredicting whether cancer is benign or malignant using Logistic Regression in Python\nDataset Used: Breast Cancer Wisconsin (Diagnostic) Dataset\nKaggle\n'], 'url_profile': 'https://github.com/lovish1', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Feb 1, 2020', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Feb 1, 2020', '5', 'Jupyter Notebook', 'Updated Feb 1, 2021', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020']}"
"{'location': 'Cincinnati', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': [' Predict Diabetes in Logistic Regression using R  \nThe goal of this project is to build a logistic regression model that would predict the likelihood of diabetes.\nThe dataset was collected and publicly shared by ‚ÄúNational Institute of Diabetes and Digestive and Kidney Diseases‚Äù.\nThe objective of the dataset is to diagnostically predict whether or not a patient has diabetes,\nbased on certain diagnostic measurements included in the dataset. We will start the analysis from basic\ndata cleaning steps such as looking for missing values, duplicate records and identifying for outliers in each covariate.\nRpubs link: https://rpubs.com/soodrk/578110\n'], 'url_profile': 'https://github.com/soodrk', 'info_list': ['HTML', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'MQL5', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['The Boston housing market is highly competitive(because of the location and for very good facilities), and we wants to work for buisness purpose (be the best real estate agent) in the area. To compete with our idea, we decide to use a  basic Regression techniques concepts to assist us and help out our client with finding the best price for home of there choices(with given factors which he likes the most) . Fortunately, we have the Boston Housing dataset which contains the data on various features for houses in Greater Boston communities, which have MEDV (the median value of homes) for each of those areas which here we have to predict. Our task is to build an optimal model(with good prediction accuracy) based on a statistical analysis with different tools available. This model will finally be used to estimate(predict) the best selling price of houses which help for our clients as per the most important features they want for there house.\nOUR PROJECT STRUCTURE\n\n\nIntroduction and Data description\n\n\nObjective\n\n\nData pre-processing\n\n\npre-modelling\n(i)  standardise our regressors.\n(ii) Splitting Data into Train and Testing set.\n(iii) Estimated coefficient, model fitting and RMSE.\n\n\nModelling and Assumption checking\n(i)  Leverage and influencial point removal\n     * Leverage Points\n     Diagnostics for Influential points\n     * Cook‚Äôs Distance\n     * DFFITS\n     * DFBETAS\n     * COVRATIO\n\n(ii) Curvature Checking  and removal.\n     * APR and CPR\n     * Transformation of regressors\n\n(iii) Heteroscedasticity Checking and removal\n     * Plots of residual vs fiited response\n     * Testing of presence of Heteroscedasticity with Breusch Pagan Test\n     * Removal\n\n(iv)  Normality Checking and removal\n     * Checking Through Plots\n     * Box-Cox transformation\n     * Transformed response and Checking Q-Q plot             \n\n\n\nResults\n\n\nReferences\n\n\nRESULTS/Conclusions\nOur conclusion are as follows:\n(1.)  Firstly we fitted data on train set and estimated our RMSE on test set and found RMSE to be 5.9042.\n(2.) Secondly we remove the leverage and influential point from the training data and observed that our RMSE value is decreased after removal of these points and Hence conclude that there is significant effect of ouliers in our model that is previously obtained from training data . Thus , after these diagnosis we found that our model trained has RMSE equal to 5.698279.\n(3.) Now , we check whether there is non-linearity of any regressor with respect to residual . We found that ""rm"" is non-linear with respect to residual and corrected it with the exponential transformation and again find the RMSE value for test data with the help of model trained using training data with transformed ""rm"" regressor . Finally we found that our RMSE has decreased to 5.128485 which suggest that thier is significant effect of non-linearity of ""rm"" regressor in the previous model that is trained without transformation of ""rm"" regressor and Thus after the transformation we have improved the model accuracy .\n(4.) After the outlier detection and dealing with non-linearity , Now we checked whether homoscedastic assumption of linear model is satisfied by our model . We performed Breuschpagan test to test for heteroscedasticity and we found that test statistics Q=16.44206 which is greater than the critical value 12.59159. thus we reject homoscedastic assumption for our model and support the claim of hetroscedasticity is present in our model .Now we transformed our model and with the help of iterative method we estimated\nour coefficient estimates . Now again we calculated RMSE value on test data set found that there is decrease in RMSE value obtained from previous model and now RMSE is 5.106228 which suggest that there is significant effect of heteroscedasticity in our previous model .\n(5.)  Lastly we have validated the normality assumption of response variable with the help of Q-Q plot and we saw that plot does not give us significant evidence of normality of response . Here we use Box-Cox transformation to transform regressor such that normality assumption is satisfied. Again we calculated the RMSE value with transformed regressor and found that RMSE is 5.106228 which is same as previous model and thus we conclude that there is no significant effect of normality of response on the model . Finally after all the diagnosis like oulier detection, dealing with curvature , heteroscedasticity , normality. We get the model as :\nMEDV= 5.32655724+(‚àí0.46115639)‚àócrim+(‚àí0.15854193)‚àóindus+(‚àí0.17175952)‚àónox+ 0.15669411‚àótransformedrm+ (‚àí0.30314938)‚àóage+ (‚àí0.517181305)‚àódis+(‚àí0.02613209)‚àótax+(‚àí0.39179899)‚àóptrratio+0.30797729‚àóblack+(‚àí0.83449504)‚àólstat\ntransformedrm=exp(3.0371211)‚àóexp(0.2978245‚àórm)   ,  RMSE for test data = 5.106228. Thus our aim of building a model with good model accuracy and model interpretability is almost achieved . As in predicting ""MEDV"" RMSE of 5.106228 is considered as small. We have also validated the assumption of linearity , homoscedasticity , normality assumption of General linear model . Now we can predict MEDV(median house price of owner owned house in boston area ) with given feature.\nSOURCE/CITATION\n\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning: with Applications in R. Springer: 2017.\nHarrison, D. and D. L. Rubinfeld (1978). Hedonic prices and the demand for clean air. Journal of Environmental Economics and Management, 5, 81-102.\nintroduction to Linear Regression Analysis by Douglas C Montgomery, Elizabeth APeck, G. Geoffrey Vining.\n\n'], 'url_profile': 'https://github.com/rajatagstats', 'info_list': ['HTML', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'MQL5', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['HTML', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'MQL5', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['baseball_predictive_analytics\n'], 'url_profile': 'https://github.com/bjhammack', 'info_list': ['HTML', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'MQL5', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['HTML', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'MQL5', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'United States of America', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': [""Predicting Office Room Occupancy Using Logistic Regression\nIntroduction\nIn this repository, I used UCI's Occupancy Detection dataset to predict room occupancy prediction using logistic regression. This dataset is provided at https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+. This dataset contains parameters of an office room. Those parameters are Date, Temperature(in Celsius), Light (lux), CO2, Relative Humidity (%), Humidity Ratio and Occupancy, which is binary with 1 if room is not vacant and 0 otherwise. I used single hidden layer neural network to develop a classifier that tells if room is vacant or not.\nData Visualization\nI used Tableau for visualizing data as it is easy. On visualizing various parameters vs occupancy, following is observed:\n\nTemperature vs Occupancy:\nThe picture below shows that temperature remains same between 20C to 25C if room is vacant or not. It makes sense because of possible air conditioning in the office.\n\n\n\nLight vs Occupancy:\nThe picture below shows that Light is more between 7am to 7pm when room is not vacant. And outside these times, light is 0 because office only has activity between 7am to 7pm.\n\n\n\nCO2 vs Occupancy:\nThe picture below shows that CO2 is high between 7am to 7pm when room is not vacant and low outside these times which makes sense as humans exhale CO2. So more CO2 means more humans.\n\n\n\nHumidity vs Occupancy:\nThe relative humidity doesnt change whether room has occupancy or not, as evident in picture below:\n\n\nPreprocessing\nI used Pandas to load the dataset. Following picture shows how our data looks like:\n\nIt can be seen that all columns except Date and Occupancy are continuous values. Since machine learning algorithms perform better when data is scaled, so I used Scikit-learn's Standard Scaler which scales data to have mean 0 and standard deviation 1.\nMachine Learning Algorithm\nI used single hidden layer neural network with keras library for machine learning algorithm. Adam optimizer is used. Train-test split is kept at 80:20. Following accuracy and loss curves are observed:\n\n\nResults\nAfter training the algorithm, it is seen that our model fits data really well. Finally, predictions are computed and mean squared error between predictions and test set is found to be 0.05.\nConclusion\nBy doing this project, it is seen how easy it is to use room parameters to train a neural network based classifier for room occupancy detection.\n""], 'url_profile': 'https://github.com/shayanalibhatti', 'info_list': ['HTML', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'MQL5', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Hungary', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear regression expert for Meta Trader\nAn expert written in the Meta Quotes 5 language that opens and closes trades based on linear regressions and moving avarages.\n'], 'url_profile': 'https://github.com/imre-tomori', 'info_list': ['HTML', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'MQL5', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '196 contributions\n        in the last year', 'description': ['Data-Analysis\nData analysis/machine learning projects\n'], 'url_profile': 'https://github.com/jakechamblee', 'info_list': ['HTML', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'MQL5', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q‚ÄìQ (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['HTML', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'MQL5', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K ‚Äì 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White‚Äôs Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['HTML', 'Updated Mar 25, 2020', '1', 'Jupyter Notebook', 'Updated Dec 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'MQL5', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Feb 16, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}"
"{'location': 'Johannesburg', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['CROSS-VALIDATION and SIMPLE-LINEAR-REGRESSION\nBuilding a simple model to determine how well Years Worked predicts an employee‚Äôs salary.\n'], 'url_profile': 'https://github.com/codeART96', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'HTML', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'HTML', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['KAGGLE : House Prices: Advanced Regression Techniques\nThis contains python codes for kaggle submissions for the house price prediction. The codes begin with concepts learnt from ""kaggle courses"" and then i use all the gained knowledge to make the best possible model. Kaggle ranks achieved is also commented in the codes to keep track of my progress. Codes have been written in a very revision friendly manner.\n'], 'url_profile': 'https://github.com/quickSilverShanks', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'HTML', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'HTML', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'HTML', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic-Regression-on-SuicideChina.csv\nRelazione finale del corso di ""Modelli per Dati Categoriali"" riguardante l\'applicazione di un modello di regressione logistica con l\'obiettivo di stimare le eventuali relazioni esistenti tra: l\'esito fatale e le altre variabili di carattere sociale, culturale e geografico presenti nel dataset ""SuicideChina.csv"".\nPer visionare il report aprire: SuicideChinaSiciliaMario (1).ipynb\n'], 'url_profile': 'https://github.com/mariosicilia', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'HTML', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'HTML', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Florida', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Siddharth-Rawat07', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'HTML', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q‚ÄìQ (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'HTML', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'HTML', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Predicting-A-Pulsar-Star-Logistic-Regression\n'], 'url_profile': 'https://github.com/gideonkarasek', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps ‚Äúdays‚Äù uniquely to ‚Äúnumber of sales‚Äù, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'Germany', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['linearRegressionModel-for-wine-quality-testing\n'], 'url_profile': 'https://github.com/shashank8794', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It‚Äôs a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a‚Äî\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a‚Äî\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '103 contributions\n        in the last year', 'description': ['Lung-Cancer-Life-Expectancy-NN-Regression\nPredicting lung cancer survival time\n'], 'url_profile': 'https://github.com/maxboels', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['Logistic-regression-on-Bank-marketing-dataset\nWas the campaign successful in targetting customers for their campaign?\nDid they reach out to potential customers at the right time?\nHow likely will a customer open a new deposit account?\nMy dataset is- https://drive.google.com/openid=10lxxaNTtMqtNzIyyyUlMMTzwbOF80vYLbZhw00zJPt4\nIt is a dataset of a Portuguese bank‚Äôs marketing campaign which shows whether a person has responded positively by opening a bank account after the campaign or negatively by not responding at all.\nHaving had a peer-to-peer marketing experience and also having interest in the insurance and banking aspects of finance, I felt this to be an interesting dataset ‚Äì not just for this project but also in general. Marketing and thinking better ways to market the bank and it‚Äôs product takes up a huge portion of time of a banker. With new-age working culture, marketing is being more and more ingrained with all other divisions, not just in banks but other industries too. After-all, without anybody buying your service or product or idea, the existence of any institution will have no meaning.  Often, marketing campaigns are designed keeping a pessimistic approach that upon launch- it is possible in 90% of the cases target audience would decline the offer, optimistic being 70%.\nAnd, any campaign creates a budget requirement on the company‚Äôs capital.\nSo, while it is important to assess the target or segment of the market thoroughly while designing and launching your campaigns, it is also important to assess afterwards. Maybe some factors are highlighted from the data which the management could have missed before as they didn‚Äôt have the results from a live experiment.\nHence, this marketing dataset and similar datasets will provide ways to classify ‚Äì ‚Äòhow likely‚Äô will a customer respond positively to your campaign, based upon certain parameters of the customer. Well, I use this word  ‚Äòhow likely‚Äô because prediction and reality are two different things. I might have a prediction and proceed likewise, but the outcome maybe opposite to my prediction.\nBut still, in the world with millions of people and companies trying their level best to cater to global taste everyday, these predictions are helpful navigators as it saves our time and risk levels which could have incurred by swimming through the pool of global customer profiles without any idea.\nAbout the dataset\nThis dataset has the following columns:\nData columns (total 17 columns):\nage 11162 non-null int64\njob 11162 non-null object\nmarital 11162 non-null object\neducation 11162 non-null object\ndefault 11162 non-null object\nbalance 11162 non-null int64\nhousing 11162 non-null object\nloan 11162 non-null object\ncontact 11162 non-null object\nday 11162 non-null int64\nmonth 11162 non-null object\nduration 11162 non-null int64\ncampaign 11162 non-null int64\npdays 11162 non-null int64\nprevious 11162 non-null int64\npoutcome 11162 non-null object\ndeposit 11162 non-null object\ndtypes: int64(7), object(10)\nThis dataset can be treated valid for binary classification problem because, the target output has only two possible values: yes or no.\nHere we have 16 dimensions and 1 target variable column of deposit.\nPreliminary Data Analysis\n1st we check for null values.\nNext, we check mean, quartiles, median and mode. From mode we see that majority answer is ""NO"", which hints that something was wrong with the campaign. We try to find out what?\nNext, we plot charts with each of our dimension parameters vs the target feature we would like to predict.\n\n\nAge vs Deposit - we see that majority age group lied between 25 to 60, with peaks in range 30 to 36. From the catplot, we see that the majority who opened a deposit account belonged to lower age below 60, while majority who said \'NO\' belonged to upper age.\n\n\nDeposit vs job - From this countplot we see that most people in blue-collar jobs responded negatively to the campaign. However, more proportion of students, retired, management and unemployed said Yes than No. This can be indicative of two things -\n\n\na) retired includes people who have taken early retirement.\nb) this deposit scheme suits those who are either not financially secure or dependent or in professions where thare is no financial stability.\nBut, surprisingly, the groups whose response were YES were not the majority contacted by the bank.\n\n\nComapred to marital status - we find higher proportion of singles responding in YES while higher proportion of married people responded in No. Also, looking at the divorced column, we find Yes and No in almost near equal proportion to each other. This can be indicative of the fact that - if a person is married, perhaps he / she is financially secured by the other spouse, else, they find this scheme beneficial to support themselves financially. But, the number contacted more by the bank were married people.\n\n\nComparing to education - people with tertiary education favoured this scheme, while people with primary and secondary education didn\'t quite comply to the campaign. It maybe due to:\n\n\na) Majority with tertiary education are either: students / managers / trying to find a job which suits their qualification / retired\nb) Most people with secondary and primary education contacted by the bank are into blue collar jobs\n\n\nFrom assesing loans and defaults columns aanist deposit, we find that the bank seems to have contacted majorly those people who didn\'t have loans or who aren\'t defaulters. In either case, the response was a higher proportion of ""NO"" compared to YES.\n\n\nIt seems that almost equal propportion of people with or without housing were contacted by the bank. And again, people without housing preferred the deposit scheme more compared to those with housing.\n\n\nIt seems that cellular and digital mode of commuincation is the most preferred mode of contact for customers.\n\n\nFrom months, we find a pattern - most contacts made during the months of September, October, December, February, March, April resulted in successful campaign result because an account was opened. However, the Bank made most of it\'s calls during May when mostly the outcome was a \'NO\' perhaps because it May is usually time for Summer break and people were either traveling or relaxing or just not in mood for banks.\n\n\nFrom comparing days to deposit we see that while the distribution is mostly evenly spread, the proportion of YES for calls made on the 30th of the month is higher while the proportion of NOs is higher is for the calls made during the middle of the month.\n\n\nComaparing duration with the outcome of deposit showws that in cases of YES, the highest duration of a call was a little less than 4000 minutes, while for NOs, the maximum call duration was for 3250 minutes. The median duration of YES is higher than NO and from this, we can infer that - perhaps the marketing personnel had to do a lot of convincing talks for people to open up a deposit account. This might indicate that perhaps either the features in the deposit account were not that good, so it needed some selling effort. Or perhaps the features were not well presented in the marketing manuals created for the campaign.\n\n\nFrom the above, we can infer that - The Bank were not able to segment their potential customer group properly in terms of who would be appealed the most by the benefits of their products. Otherwise, the campaign would have been a huge success.\nRefer to the notebook for code.\nAfter this analysis, a Logistic regression model tells us that given the present situation, if a new person is considered with characteristics under the dimensions of this study, if or not he or she will open a bank account.\n'], 'url_profile': 'https://github.com/Pranjita1', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kes7ragi', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}"
"{'location': 'Boston,MA', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jiajieyuan1010', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', 'R', 'Updated Jan 28, 2020', '1', 'R', 'Updated Aug 12, 2020']}","{'location': 'Greater Boston Area', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': [""Nearest-Neighbors-Regression-to-predict-abalone-age\nThe project objective is to predict abalones' age through the number of rings found in the abalone shell using nearest neighbors regression (abalone age is determined to be 1.5 plus number of rings). The input data includes various features of abalone, such as gender, longest shell measurement, shell diameter, height of shell, weights (entire, meat, guts, dried shell).\nI built a set of classifiers using the following values for the number of neighbors: K = 1, 3, 5, 7, 9, 11, 21, 41, 61, 81, 101, 201, 401, 801. For each such K value, I built a KNeighborsRegressor based upon the training set, and evaluated its predictions on the validation and test sets.\nThen, I made a plot of the MSE for each K value. Finally, I determined the best K value for the validation data.\n""], 'url_profile': 'https://github.com/baovinhnguyen', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', 'R', 'Updated Jan 28, 2020', '1', 'R', 'Updated Aug 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\n'], 'url_profile': 'https://github.com/SithiasmaBasheer', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', 'R', 'Updated Jan 28, 2020', '1', 'R', 'Updated Aug 12, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['Kaggle-Competition---House-Prices-Regression-Techniques\nHouse Prices Regression. Different methods have been used. The main goal is to predict the final price of each home.\n'], 'url_profile': 'https://github.com/NamanGuptacs', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', 'R', 'Updated Jan 28, 2020', '1', 'R', 'Updated Aug 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['TP-Python-for-data-analysis-Regression\n'], 'url_profile': 'https://github.com/Stephane-Bcd', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', 'R', 'Updated Jan 28, 2020', '1', 'R', 'Updated Aug 12, 2020']}","{'location': 'United States ', 'stats_list': [], 'contributions': '376 contributions\n        in the last year', 'description': ['Prediction-of-Taxi-Fare-using-Regression-analysis\nPerformed Statistical analysis of Taxi trip in the City of Chicago Dataset available on Kaggle to identify key Insights.\nPredicted taxi fare using Regression analysis on multiple variables.\nInvestigated relevant Interactions and performed common independent variable transformations on multiple variables and selected optimized model.\nValidated the regression model with LINE assumptions and residual analysis.\nPerformed Dataset Cleaning and exploratory analysis using R in RStudio and documented project in R Markdown\n'], 'url_profile': 'https://github.com/ARGULASAISURAJ', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', 'R', 'Updated Jan 28, 2020', '1', 'R', 'Updated Aug 12, 2020']}","{'location': 'Patiala, Punjab, India', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abhishek94goel', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', 'R', 'Updated Jan 28, 2020', '1', 'R', 'Updated Aug 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Song_Analyser\nPython program (mini) project using linear regression ton determine how top 50 Spotifys songs are impacted by differents parameters ...-\nThis project is here to be reused for other dataframe. The idea is to understand the way the linear regression works for many extends.\n'], 'url_profile': 'https://github.com/MarcBelleperche', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', 'R', 'Updated Jan 28, 2020', '1', 'R', 'Updated Aug 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', 'R', 'Updated Jan 28, 2020', '1', 'R', 'Updated Aug 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Python', 'Updated Feb 16, 2020', '1', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Aug 22, 2020', 'R', 'Updated Jan 28, 2020', '1', 'R', 'Updated Aug 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '113 contributions\n        in the last year', 'description': ['Spotify_popular_or_not_analysis\nUsing the top Spotify songs dataset (2010-2019) from Kaggle predict if the song will be able to make it to hit list (top 30s) or not.\nLibraries Used: Pandas, Numpy, Sklearn, Seaborn, Matplotlib, Scipy,stats, wordcloud.\nData Source: https://www.kaggle.com/leonardopena/top-spotify-songs-from-20102019-by-year/kernels\n'], 'url_profile': 'https://github.com/jaindamini1111', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Mar 6, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Mar 6, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['R package: ""rpart.LAD""\nDescription\nRecursive partitioning for least absolute deviation regression trees. Another algorithm from the 1984 book by Breiman, Friedman, Olshen and Stone in addition to the \'rpart\' package (Breiman, Friedman, Olshen, Stone (1984, ISBN:9780412048418).\nInstallation\nFrom CRAN\nThe easiest way to use any of the functions in the rpart.LAD package is to install the CRAN version. It can be installed from within R using the command:\n#!R\n\ninstall.packages(""rpart.LAD"")\n\nFrom bitbucket\nThe devtools package contains functions that allow you to install R packages directly from bitbucket or github. If you\'ve installed and loaded the devtools package, the installation command is\n#!R\n\ninstall_bitbucket(""sdlugosz/rpart.lad"")\n\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Mar 6, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Mar 6, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Wine-classification-based-on-content-proportion\nThe code is using Logistic Regression to identify the class of wine based on the proportion of different components .\n'], 'url_profile': 'https://github.com/vanshikachanderiya', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Mar 6, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['margZIfit-Package\nA pact of functions to implement marginal zero-inflated regression models for clustered data with excess zeros in R.\n'], 'url_profile': 'https://github.com/tbenesi', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Mar 6, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['bsvs\nAn R package that performs Bayesian iterated screening and/or variable selection for ultra-high dimensional Gaussian linear regression models\n'], 'url_profile': 'https://github.com/Run-Wang', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Mar 6, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['cluscov\nClustered Covariate Regression. This is an R package for the paper Soale & Tsyawo 2019 ""Clustered Covariate Regression"". It contains routines for executing the models in the paper. The link to the webpage is https://estsyawo.github.io/cluscov.\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Mar 6, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""\nHCTR\nThe goal of HCTR is to create a new searching scheme for the\nregularization parameter in penalized regression, such as Lasso,\nadaptive Lasso, SCAD, and MCP.\nExample\nThis is a basic example which shows you how to (1) estimate false null\nhypothesis proportion, and (2) create a new tuning region for the\nregularization parameter.\n## basic example code\nlibrary('HCTR')\n# 1. Estimate proportion\nset.seed(10)\nX <- matrix(runif(n = 10000, min = 0, max = 1), nrow = 100)\nresult <- bounding.seq(p.value = X)\nY <- matrix(runif(n = 100, min = 0, max = 1), nrow = 100)\ntest <- est.prop(p.value = Y, cn = result)\n# 2. Estimate a new tuning region\nset.seed(10)\nX <- matrix(rnorm(20000), nrow = 100)\nbeta <- rep(0, 200)\nbeta[1:100] <- 5\nY <- MASS::mvrnorm(n = 1, mu = X%*%beta, Sigma = diag(100))\nfit <- glmnet::cv.glmnet(x = X, y = Y)\npihat <- 0.01\nresult <- est.lambda(cv.fit = fit, pihat = pihat, p = ncol(X))\n""], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Mar 6, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'C++', 'Updated May 20, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Mar 6, 2020', 'R', 'GPL-3.0 license', 'Updated Dec 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 29, 2020']}"
"{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '3,438 contributions\n        in the last year', 'description': ['mr-mash\nA simple implementation of the multivariate multiple regression method\nwith adaptive shrinkage priors (""mr-mash""), with examples.\n'], 'url_profile': 'https://github.com/pcarbo', 'info_list': ['1', 'R', 'Updated Sep 1, 2020', 'HTML', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', 'R', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'College Station, Texas, USA', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Model-selection-python-practice\nI applied gradient boosting, support vector machines, random forest, and linear regression methods to predict the New York Building Score\n'], 'url_profile': 'https://github.com/jamielaiwang', 'info_list': ['1', 'R', 'Updated Sep 1, 2020', 'HTML', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', 'R', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Galaxy-Morphology-Classification\nPredicting the morphology of galaxy with the help of finding probabilities of 11 specific questions, using regression techniques.\n'], 'url_profile': 'https://github.com/drago8055', 'info_list': ['1', 'R', 'Updated Sep 1, 2020', 'HTML', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', 'R', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['1', 'R', 'Updated Sep 1, 2020', 'HTML', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', 'R', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': [""pulsarstarprediction\nThis project deals with the prediction of pulsar stars using different classifiers like SVM, Random Forests, Logistic Regression etc\nPulsars are a rare type of Neutron star that produce radio emission detectable here on Earth. They are of considerable scientific interest as probes of space-time, the inter-stellar medium, and states of matter. Generally, pulsar search involves looking for periodic radio signals with large radio telescopes. Each pulsar produces a slightly different emission pattern, which varies slightly with each rotation. Thus, a potential signal detection known as a 'candidate', is averaged over many rotations of the pulsar, as determined by the length of an observation. In the absence of additional info, each candidate could potentially describe a real pulsar.\nThere are many pros as Machine learning tools are now being used to automatically label pulsar candidates to facilitate rapid analysis. Hence, we can utilize binary classification to segregate the positive class examples from the spurious examples.\nData Set:\nHTRU2 is the name of the data set. This dataset describes a sample of pulsar candidates collected during the High Time Resolution Universe Survey. It contains 16,259 spurious examples caused by RFI/noise, and 1,639 real pulsar examples.\nTools and technologies used:\nPandas and NumPy are used to manipulate the dataset.\nSupport Vector Machines and Clustering classifiers will be used for the data modelling\n""], 'url_profile': 'https://github.com/vyshnavi97', 'info_list': ['1', 'R', 'Updated Sep 1, 2020', 'HTML', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', 'R', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['1', 'R', 'Updated Sep 1, 2020', 'HTML', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', 'R', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['1', 'R', 'Updated Sep 1, 2020', 'HTML', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', 'R', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'Dublin, Ireland', 'stats_list': [], 'contributions': '445 contributions\n        in the last year', 'description': [""Car_Price_Prediction\nFitting a Multiple Linear Regression model to identify the features that impact the market price of a car.\nKaggle describes the problem as follows:\nGeely Auto, a Chinese automobile company aspires to dive in the US market. They want to discover the factors that affect the costing of cars in the US automobile market.\nWe are given a dataset with 25 possible predictors. Our aim is to identify the most significant variables in order to accurately predict the market price of a car. We fit a multiple Linear Regression on the datasetand follow a top-down approach for finalizing the predictor variables.\nBelow are the steps followed :\n1.Data was uploaded and cleaned-checked for missing values and outliers. We performed the EDA on the data. We considered only company name as the independent variable for model building by splitting the car name.\n2. Dropped Car_ID as it was not needed in the analysis . It just shows the no of observation.\n3. The car name was incorrect in the data set his we replaced it with correct name #'mazda' if x=='maxda' #'porsche' if x=='porcshce' #'toyota' if x=='toyouta' #'volkswagen' if x=='vokswagen' #'nissan' if x=='Nissan' #'volkswagen' if x=='vw'\n4.Fuel System mpfi is same as mfi. Hence we made 'mpfi' if x=='mfi' in data .\n5. Engine type dohcv seemed a typo for dohc.Hence replaced it with correct name. 'dohc' if x=='dohcv'\n6. Visualised the data to check the correlation among the variables. Outcome: Wheelbase and car length are highly correlated. Car length and Car width are highly correlated. Curbweight and car width are highly correlated etc . These may cause multicollinearity in the model.\n7. We then checked the categorical variable. And did on hot encoding .\n8. Then we splitted the data into Test and Train and performed scaling on numerical variables.\n9 We then divided the data into X Train and Y train 10. We then built a linear model using -RFE. We predicted top 12 variables. ['carwidth', 'curbweight', 'enginesize', 'boreratio', 'stroke', 'CarName_bmw', 'CarName_porsche', 'enginelocation_rear', 'enginetype_rotor', 'cylindernumber_three', 'cylindernumber_twelve', 'cylindernumber_two'],\n11.We then found the model is overfitted .So we started manually. We deleted variables where the p value was high as well as the vif was high We selected the 9th model as final.\nBelow are the variables which are significant in predicting the price of a car:\nEnginesize, CarName_bmw, CarName_porsche, enginelocation_rear, cylindernumber_two\nReferences:\n\n\nKaggle\nhttps://www.kaggle.com/ashydv/car-price-prediction-linear-regression/data\n\n\nMachine Learning by Andrew NG, Coursera\nhttps://www.coursera.org/learn/machine-learning\n\n\n""], 'url_profile': 'https://github.com/vaibhavoberoi', 'info_list': ['1', 'R', 'Updated Sep 1, 2020', 'HTML', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', 'R', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'Stockholm', 'stats_list': [], 'contributions': '375 contributions\n        in the last year', 'description': ['DD2434 - Machine Learning Advanced: Final Project\nFinal project of the KTH DD2434 - Machine Learning Advanced course.\nImplementation of the Relevance Vector Machine and comparison with Support Vector Machine for regression and classification tasks.\n'], 'url_profile': 'https://github.com/fernando2393', 'info_list': ['1', 'R', 'Updated Sep 1, 2020', 'HTML', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', 'R', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hugozylberajch', 'info_list': ['1', 'R', 'Updated Sep 1, 2020', 'HTML', 'Updated Jan 30, 2020', 'Python', 'Updated Jan 31, 2020', 'R', 'Updated Nov 25, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Feb 7, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Feb 1, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chenkof', 'info_list': ['R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Feb 1, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anasofiamvaladez', 'info_list': ['R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Feb 1, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'Atlanta, GA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Installation Instructions\n\n\n\nCreate a virtual environment with python 3.7. For example if you are using conda, use statement: conda create env -n some_name python=3.7\n\n\nActivate environment. For example with conda, use statement conda activate some_name\n\n\nInstall poetry [https://python-poetry.org/docs/#installation]\n\n\nRun poetry install\n\n\nOverview - Organizing code for model selection and model evaluation\n\nData scientists generally spend a lot of time training different models before narrowing down on the model that best suits their data. Here I have provided a code structure that makes it easy to implement and evaluate many models using my wrapper function. This simplifies the job of a data scientist and ML engineer to a great extent. To walk throught the code, I used a toy dataset and various regression models to demonstrate while providing a quick background for regression.\nBackground\n\nRegression:\nThe goal of regression is to predict a continuous response variable given some input variables. There are different regression models available. Some are meaningful and relevant for the problem and some not so much. We are interested in learning which models are the best for your problem and dataset. How do we do this? We need to identify relevant models from the non-relevant ones. For example, linear regression for predicting a continuous variable is a relevant model while logistic regression is not. Logistic regression is used for classification.\nModel Selection:\nHow do we say one model is better than another? There are many measures that are used, one of which is r-square. One caveat to call to attention, r-square is not the best metric to evaluate multiple variable regression, but it is the most looked at. If r-square is low there is no linear relationship. We also need to look at the r-square for test and training data. Test r-square is more reliable because training r-square is a result of the data used to build r-square. Test r-square is also an indicator of model generalization, i.e. how well does the model perform on un-seen data.\nWriting code to explore different models is a time-consuming process, not to mention the bulky code it creates. Therefore, it needs automation.\nHere is one such tool which automates all required functions, a wrapper function that creates an architecture for the code, enables less cumbersome and more manageable code that is also easier to debug!\nData:\nFor explaining the steps of the code we will be using the automobile mpg dataset to predict the miles per gallon of the car given other features of the car such as engine size, age, horsepower, weight and acceleration of car.\nThe dataset has been cleaned and preprocessed for the purpose of the tutorial. Data Preprocessing is a topic on its own and if not done correctly can result in misleading conclusions. This topic is not covered here.\nRelevant regression models under consideration: Linear, Ridge, Lasso, Elastic Net, SVR, SGD, KNN, DT (we will try to cover as many as possible in the time frame)\nModel Workflow in real-life:\nModel workflow in real-life is process oriented. Sourcing data -> Exploratory data analysis -> data pre-processing -> training and evaluating best models and choosing best models for your data.\nIt all starts with sourcing data, is there enough data for the model to start detecting patterns? Sourcing data and curating features is the first step (not covered here).\nExploratory data analysis is where you understand the data trends/patterns and outliers in the through visualizations. It provides the context needed to develop appropriate model and interpret results correctly.\nData preparation and pre-processing is massaging the data to make it suitable for model algorithms. This is a common requirement for many machine learning algorithms implemented in sklearn, they might behave badly if the individual features deviate from model assumptions.\nTrain and test splits are performed on data as the goal of ML is to find prediction on un-seen data. The ML model learns patterns from the data that is used to train it and needs to be evaluated on un-seen data to learn if it also generalizes well. The test hold out data serves this purpose of model validation.\nTypically many models are tried on the datasets and the best performing model is chosen for production and deployment. There are many evaluation/ model scoring metrics which allow for comparison between models. The model with best score is considered the best model.\nRegularization:\nRegularization is a technique that is used commonly for model selection although not always necessary. It controls for overfitted models and improves performance for un-seen data. In this process we try to penalize the coefficients of variables by reducing their magnitude. As a result coefficients of some variables are much higher than the coefficients of other variables, which control the predictions. The popular methods for regularization in linear models are Lasso, Ridge and ElasticNet regressions which follow L1, L2 and combination of both methodologies.\nModel Validation:\nThe idea of\xa0building models\xa0works on a constructive feedback principle. You build a model, get feedback from metrics, make improvements and continue until you achieve a desirable accuracy. Evaluation metrics explain the performance of a model. An important aspect of evaluation metrics is their capability to discriminate among model results. After you are finished building your model, these metrics will help you in evaluating your model‚Äôs accuracy.\nRMSE is the square root of average of squared errors. While this is a good metric that gives a sense of how close the predicted values are to actual value. There is no baseline to say a specific value is considered good and no baseline for comparison.\nR-square metric is the proportion of that variation of sum of squared deviations from the mean that is explained by the model. In other words, how good is our regression model compared to a very simple model that just predicts the mean value of target from the train set as predictions? How much of variation of the target is explained by the regression? If it is 0, your model isn‚Äôt explaining anything, if it is closer to 1 your model is getting better at predicting the target.\nR-square doesn‚Äôt consider the number of variables in the model. So it doesn‚Äôt penalize complex models, rather complex models tend to have r-square closer to 1. Hence this metric should be used with caution. For the purpose of our dummy data, we will consider using r-square for evaluation.\nAdjusted r-square takes the number of features into account and penalizes more complex models. This is generally a better metric for multi-variate regression.\nNow you can jump into the python code to see the API.\n'], 'url_profile': 'https://github.com/manasagudimella', 'info_list': ['R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Feb 1, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) ‚Äì mean(x*y)) / ( mean (x)^2 ‚Äì mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Feb 1, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nCovariance and Correlation\nWe start the section by covering covariance and correlation, both of which relate to how likely two variables are to change together. For example, with houses, it wouldn\'t be too surprising if the number of rooms and the price of a house was correlated (in general, more rooms == more expensive).\nStatistical Learning Theory\nWe then explore statistical learning theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Feb 1, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets. Key takeaways include:\n\nThe Pearson Correlation (range: -1 -> 1) is a standard way to describe the correlation between two variables\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Feb 1, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Feb 1, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It‚Äôs a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a‚Äî\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a‚Äî\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Feb 1, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K ‚Äì 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White‚Äôs Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Feb 1, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Feb 22, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Regularized-Linear-Regression-and-Bias-v.s.-Variance\nIn this exercise, you will implement regularized linear regression and use it to study models with different bias-variance properties. Before starting on the programming exercise, we strongly recommend watching the video lectures and completing the review questions for the associated topics. To get started with the exercise, you will need to download the starter code and unzip its contents to the directory where you wish to complete the exercise. If needed, use the cd command in Octave/MATLAB to change to this directory before starting this exercise. You can also find instructions for installing Octave/MATLAB in the ‚ÄúEnvironment Setup Instructions‚Äù of the course website. This assignment helps us in understanding how the bias and variance differ the predictability of the model.\nFiles included in this exercise\nex5.m - Octave/MATLAB script that steps you through the exercise\nex5data1.mat - Dataset\nsubmit.m - Submission script that sends your solutions to our servers\nfeatureNormalize.m - Feature normalization function\nfmincg.m - Function minimization routine (similar to fminunc)\nplotFit.m - Plot a polynomial fit\ntrainLinearReg.m - Trains linear regression using your cost function\n[1] linearRegCostFunction.m - Regularized linear regression cost function\n[2] learningCurve.m - Generates a learning curve\n[3] polyFeatures.m - Maps data into polynomial feature space\n[4] validationCurve.m - Generates a cross validation curve\nThroughout the exercise, you will be using the script ex5.m. These scripts\nset up the dataset for the problems and make calls to functions that you will\nwrite. You are only required to modify functions in other files, by following\nthe instructions in this assignment.\n1. Regularized Linear Regression\nIn the first half of the exercise, you will implement regularized linear regression to predict the amount of water flowing out of a dam using the change of water level in a reservoir. In the next half, you will go through some diagnostics of debugging learning algorithms and examine the effects of bias v.s. variance. The provided script, ex5.m, will help you step through this exercise.\n2. Bias-variance\nAn important concept in machine learning is the bias-variance tradeoff. Models with high bias are not complex enough for the data and tend to underfit, while models with high variance overfit to the training data.\n3. Polynomial regression\nThe problem with our linear model was that it was too simple for the data\nand resulted in underfitting (high bias). In this part of the exercise, you will\naddress this problem by adding more features.\nFor use polynomial regression, our hypothesis has the form:\nhŒ∏(x) = Œ∏0 + Œ∏1 ‚àó (waterLevel) + Œ∏2 ‚àó (waterLevel)2 + ¬∑ ¬∑ ¬∑ + Œ∏p ‚àó (waterLevel)p\n= Œ∏0 + Œ∏1x1 + Œ∏2x2 + ... + Œ∏pxp.\nNotice that by defining x1 = (waterLevel), x2 = (waterLevel)2\n, . . . , xp = (waterLevel)p, we obtain a linear regression model where the features are the\nvarious powers of the original value (waterLevel).\nNow, you will add more features using the higher powers of the existing\nfeature x in the dataset. Your task in this part is to complete the code in\npolyFeatures.m so that the function maps the original training set X of size\nm √ó 1 into its higher powers. Specifically, when a training set X of size m √ó 1\nis passed into the function, the function should return a m√óp matrix X poly,\n7\nwhere column 1 holds the original values of X, column 2 holds the values of\nX.^2, column 3 holds the values of X.^3, and so on. Note that you don‚Äôt\nhave to account for the zero-eth power in this function.\nNow you have a function that will map features to a higher dimension,\nand Part 6 of ex5.m will apply it to the training set, the test set, and the\ncross validation set (which you haven‚Äôt used yet).\n'], 'url_profile': 'https://github.com/git-ash22', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) ‚Äì mean(x*y)) / ( mean (x)^2 ‚Äì mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['illustrative-EDA-on-kaggle-House-Prices-Advanced-Regression-Techniques-dataset\nA handy notebook on\n1) how to extract some useful insight from a Dataset by visualizing features\n2) Using Xgboost to get the top important features and then how to use them in modelling\n3) Ways to tackle multicollinearity\n4) Feature interactions\n5) Lastly how to use XGboost to get desired results\n'], 'url_profile': 'https://github.com/Nikita0108', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nIntroduce Statsmodels for multiple regression\nPresent alternatives for running regression in Scikit Learn\n\nStatsmodels for multiple linear regression\nThis lecture will be more of a code-along, where we will walk through a multiple linear regression model using both Statsmodels and Scikit-Learn.\nRemember that we introduced single linear regression before, which is known as ordinary least squares. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(""auto-mpg.csv"") \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[""acceleration""]\nlogdisp = np.log(data[""displacement""])\nloghorse = np.log(data[""horsepower""])\nlogweight= np.log(data[""weight""])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[""acc""]= scaled_acc\ndata_fin[""disp""]= scaled_disp\ndata_fin[""horse""] = scaled_horse\ndata_fin[""weight""] = scaled_weight\ncyl_dummies = pd.get_dummies(data[""cylinders""], prefix=""cyl"")\nyr_dummies = pd.get_dummies(data[""model year""], prefix=""yr"")\norig_dummies = pd.get_dummies(data[""origin""], prefix=""orig"")\nmpg = data[""mpg""]\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 26 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_3     392 non-null uint8\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_70     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_1    392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(21)\nmemory usage: 23.4 KB\n\nThis was the data we had until now. As we want to focus on model interpretation and still don\'t want to have a massive model for now, let\'s only inlude ""acc"", ""horse"" and the three ""orig"" categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis= 1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_1\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n1\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n1\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n1\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n1\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n1\n0\n0\n\n\n\n\nA linear model using Statsmodels\nNow, let\'s use the statsmodels.api to run our ols on all our data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$, where, with $n$ predictors, X is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = ""mpg ~ acceleration+weight+orig_1+orig_2+orig_3""\nmodel = ols(formula= formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable ""mpg"" out of your data frame, and use the a ""+"".join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nOr even easier, simply use the .OLS-method from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors dataframe so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nInterpretation\nJust like for single multiple regression, the coefficients for our model should be interpreted as ""how does Y change for each additional unit X""? Do note that the fact that we transformed X, interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed X, the actual relationship is ""how does Y change for each additional unit X\'"", where X\' is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit learn\nYou can also repeat this process using Scikit-Learn. The code to do this can be found below. The Scikit-learn is generally known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit learn compared to Statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of Scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -0.71140721, -0.29903267,  1.01043987])\n\nThe intercept of the model is stored in the .intercept_-attribute.\n# intercept\nlinreg.intercept_\n21.472164286075383\n\nWhy are the coefficients different in scikit learn vs Statsmodels?\nYou might have noticed that running our regression in Scikit-learn and Statsmodels returned (partially) different parameter estimates. Let\'s put them side to side:\n\n\n\n\nStatsmodels\nScikit-learn\n\n\n\n\nintercept\n16.1041\n21.4722\n\n\nacceleration\n5.0494\n5.0494\n\n\nweight\n-5.8764\n-5.8764\n\n\norig_1\n4.6566\n-0.7114\n\n\norig_2\n5.0690\n-0.2990\n\n\norig_3\n6.3785\n1.0104\n\n\n\nThese models return equivalent results!\nWe\'ll use an example to illustrate this. Remember that minmax-scaling was used on acceleration, and standardization on log(weight).\nLet\'s assume a particular observation with a value of 0.5 for both acceleration and weight after transformation, and let\'s assume that the origin of the car = orig_3. The predicted value for mpg for this particular value will then be equal to:\n\n16.1041 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 6.3785 = 22.0691 according to the Statsmodels\n21.4722 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 1.0104 = 22.0691 according to the Scikit-learn model\n\nThe eventual result is the same. The extimates for the categorical variables are the same ""up to a constant"", the difference between the categorical variables, in this case 5.3681, is added in the intercept!\nYou can make sure to get the same result in both Statsmodels and Scikit-learn, by dropping out one of the orig_-levels. This way, you\'re essentially forcing the coefficient of this level to be equal to zero, and the intercepts and the other coefficients will be the same.\nThis is how you do it in Scikit-learn:\npredictors = predictors.drop(""orig_3"",axis=1)\nlinreg.fit(predictors, y)\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -1.72184708, -1.30947254])\n\nlinreg.intercept_\n22.482604160455665\n\nAnd Statsmodels:\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    22.4826     0.789    28.504  0.000    20.932    24.033\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1    -1.7218     0.653    -2.638  0.009    -3.005    -0.438\n\n\norig_2    -1.3095     0.688    -1.903  0.058    -2.662     0.043\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               9.59\n\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in both Scikit-Learn and Statsmodels. Before we discuss the model metrics in detail, let\'s go ahead and try out this model on the Boston Housing Data Set!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nIntroduce Statsmodels for multiple regression\nPresent alternatives for running regression in Scikit Learn\n\nStatsmodels for multiple linear regression\nThis lecture will be more of a code-along, where we will walk through a multiple linear regression model using both Statsmodels and Scikit-Learn.\nRemember that we introduced single linear regression before, which is known as ordinary least squares. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(""auto-mpg.csv"") \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[""acceleration""]\nlogdisp = np.log(data[""displacement""])\nloghorse = np.log(data[""horsepower""])\nlogweight= np.log(data[""weight""])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[""acc""]= scaled_acc\ndata_fin[""disp""]= scaled_disp\ndata_fin[""horse""] = scaled_horse\ndata_fin[""weight""] = scaled_weight\ncyl_dummies = pd.get_dummies(data[""cylinders""], prefix=""cyl"")\nyr_dummies = pd.get_dummies(data[""model year""], prefix=""yr"")\norig_dummies = pd.get_dummies(data[""origin""], prefix=""orig"")\nmpg = data[""mpg""]\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 26 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_3     392 non-null uint8\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_70     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_1    392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(21)\nmemory usage: 23.4 KB\n\nThis was the data we had until now. As we want to focus on model interpretation and still don\'t want to have a massive model for now, let\'s only inlude ""acc"", ""horse"" and the three ""orig"" categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis= 1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_1\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n1\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n1\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n1\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n1\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n1\n0\n0\n\n\n\n\nA linear model using Statsmodels\nNow, let\'s use the statsmodels.api to run our ols on all our data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$, where, with $n$ predictors, X is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = ""mpg ~ acceleration+weight+orig_1+orig_2+orig_3""\nmodel = ols(formula= formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable ""mpg"" out of your data frame, and use the a ""+"".join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nOr even easier, simply use the .OLS-method from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors dataframe so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nInterpretation\nJust like for single multiple regression, the coefficients for our model should be interpreted as ""how does Y change for each additional unit X""? Do note that the fact that we transformed X, interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed X, the actual relationship is ""how does Y change for each additional unit X\'"", where X\' is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit learn\nYou can also repeat this process using Scikit-Learn. The code to do this can be found below. The Scikit-learn is generally known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit learn compared to Statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of Scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -0.71140721, -0.29903267,  1.01043987])\n\nThe intercept of the model is stored in the .intercept_-attribute.\n# intercept\nlinreg.intercept_\n21.472164286075383\n\nWhy are the coefficients different in scikit learn vs Statsmodels?\nYou might have noticed that running our regression in Scikit-learn and Statsmodels returned (partially) different parameter estimates. Let\'s put them side to side:\n\n\n\n\nStatsmodels\nScikit-learn\n\n\n\n\nintercept\n16.1041\n21.4722\n\n\nacceleration\n5.0494\n5.0494\n\n\nweight\n-5.8764\n-5.8764\n\n\norig_1\n4.6566\n-0.7114\n\n\norig_2\n5.0690\n-0.2990\n\n\norig_3\n6.3785\n1.0104\n\n\n\nThese models return equivalent results!\nWe\'ll use an example to illustrate this. Remember that minmax-scaling was used on acceleration, and standardization on log(weight).\nLet\'s assume a particular observation with a value of 0.5 for both acceleration and weight after transformation, and let\'s assume that the origin of the car = orig_3. The predicted value for mpg for this particular value will then be equal to:\n\n16.1041 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 6.3785 = 22.0691 according to the Statsmodels\n21.4722 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 1.0104 = 22.0691 according to the Scikit-learn model\n\nThe eventual result is the same. The extimates for the categorical variables are the same ""up to a constant"", the difference between the categorical variables, in this case 5.3681, is added in the intercept!\nYou can make sure to get the same result in both Statsmodels and Scikit-learn, by dropping out one of the orig_-levels. This way, you\'re essentially forcing the coefficient of this level to be equal to zero, and the intercepts and the other coefficients will be the same.\nThis is how you do it in Scikit-learn:\npredictors = predictors.drop(""orig_3"",axis=1)\nlinreg.fit(predictors, y)\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -1.72184708, -1.30947254])\n\nlinreg.intercept_\n22.482604160455665\n\nAnd Statsmodels:\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    22.4826     0.789    28.504  0.000    20.932    24.033\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1    -1.7218     0.653    -2.638  0.009    -3.005    -0.438\n\n\norig_2    -1.3095     0.688    -1.903  0.058    -2.662     0.043\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               9.59\n\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in both Scikit-Learn and Statsmodels. Before we discuss the model metrics in detail, let\'s go ahead and try out this model on the Boston Housing Data Set!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'C', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'C', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) ‚Äì mean(x*y)) / ( mean (x)^2 ‚Äì mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'C', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'C', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'C', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'C', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'Ahmedabad, India', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Linear-regression-curve-fitting-using-Least-square-method-in-c\nSo, This project is helps to find the second order equation and also it will able to find the coffiecient of that equation.\nFirst you have to give the certain amount of point means (x,y)\nSo, It will find those coffiecient and equation.\nAfter that if you provide x then it will predict the value of Y.\nTo run this project First start the Linear_regression.c which includes the whole c programs.\nHere, I atteched some data file and sample plot graph of line fitting. Also i am provided script file for GNUPLOT.\n**So, Enjoy the world of prediction.\n'], 'url_profile': 'https://github.com/Mayanktank', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'C', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'C', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'C', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020']}","{'location': 'Manchester', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['Predicting-stock-prices-using-Linear-Regression-and-ARIMA\nUsing historical price data of Amazon(AMZN) as an example ,can be downloaded easily from Yahoo Finance\n'], 'url_profile': 'https://github.com/cumbersomeamir', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'C', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020']}"
"{'location': 'Bengaluru', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Housing-Price-Prediction--Advance-Regression-Techniques--Kaggle-Competetions\nA Kaggle competition where the object was to predict the housing price using advance regression techniques and performing feature engineering and building predictive models.\n'], 'url_profile': 'https://github.com/Mruga', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', '2', 'C', 'MIT license', 'Updated Feb 11, 2020']}","{'location': 'Roorkee, Uttarakhand, India', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Logistic-Regression-for-image-recognition\nIn this notebook I build your first image recognition model using Logistic Regression on Numpy. This is a cat classifier that recognizes cats with 70% accuracy.\nIn this assignment:\n\nI learned to Build the general architecture of a learning algorithm, including building functions for initializing parameters, calculating the cost function and its gradient, using an optimization algorithm (gradient descent), minimizing cost function and merging all functions into a model function.\nI Worked with logistic regression in a way that builds intuition relevant to neural networks.\nI Learned how to minimize the cost function.\nI Understoood how derivatives of the cost are used to update parameters.\nI brushed up on my Numpy skills\n\n'], 'url_profile': 'https://github.com/SourabhMagadum', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', '2', 'C', 'MIT license', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', '2', 'C', 'MIT license', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['House-Price-Prediction-on-boston-dataset-using-Multivariate-Regression\n'], 'url_profile': 'https://github.com/MaclaurinYudhisthira', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', '2', 'C', 'MIT license', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', '2', 'C', 'MIT license', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', '2', 'C', 'MIT license', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/VineethDumpeti', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', '2', 'C', 'MIT license', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', '2', 'C', 'MIT license', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', '2', 'C', 'MIT license', 'Updated Feb 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Name\npolyfit ‚Äî ""C"" function that fits a polynomial to a set of points\nSynopsis\n#include ""polyfit.h""\nint polyfit( int pointCount, double *xValues, double *yValues,  int coefficientCount, double *coefficientResults );\nDescription\nThe polyfit() function regresses a line or a higher-order polynomial to a set of points, using the Method of Least Squares. Its design goals were simplicity and ease of porting, as opposed to run-time efficiency with large data.\nArguments\npointCount ‚Äî input. The total number of points in the set. For the algorithm to work, this must be greater than or equal to coefficientCount.\nxValues  ‚Äî input. Points to an array of the X coordinates of the points. There should be pointCount X coordinates.\nyValues ‚Äî input. Points to an array of the Y coordinates of the points. There should be pointCount Y coordinates.\ncoefficientCount ‚Äî input. The number of coefficients to be computed, equal to the degree of the polynomial plus 1. For instance, if fitting a line ‚Äî a first degree polynomial ‚Äî then coefficientCount would be 2, and for fitting a parabola ‚Äî a second degree polynomial ‚Äî coefficientCount would be 3.\nresults ‚Äî input. Points to where the computed coefficients will be stored. There should be space for coefficientCount coefficients. These coefficients are ordered from the highest-order monomial term to the lowest, such that for instance the polynomial:\n 5x¬≤ + 3x - 7\n\nis represented as:\n [ 5, 3, -7 ] \n\nReturn Value\nIn addition to setting the coefficient results, the polyfit() function returns 0 on success.\nFILES\n./src/polyfit.c ‚Äî defines the polyfit() function.\n./inc/polyfit.h ‚Äî declares the polyfit() function\'s prototype.\nMISC\nTo support unit testing on Linux with gcc, the repo has these additional files:\n./src/test.c ‚Äî exercises polyfit() and provides examples of usage.\n./Makefile ‚Äî allows the make command to build an executable, ./bin/polytest, that tests polyfit().\n'], 'url_profile': 'https://github.com/henryfo', 'info_list': ['Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', '2', 'C', 'MIT license', 'Updated Feb 11, 2020']}"
"{'location': 'Philippines', 'stats_list': [], 'contributions': '532 contributions\n        in the last year', 'description': ['Machine Learning\nOverview\nThis repository contains exercises, lectures, and homework in machine learning; both supervised and unsupervised learning. Exercises in neural networks and state-of-the-art architectures will be stored in a different repository.\nLicense\nCopyright 2020 Jan Rodolf Espinas\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n'], 'url_profile': 'https://github.com/jrpespinas', 'info_list': ['2', 'Python', 'Apache-2.0 license', 'Updated Jan 21, 2021', '2', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'R', 'Updated Jan 28, 2020', 'C++', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'C++', 'Updated May 8, 2020', 'R', 'Updated Oct 14, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Price-prediction-of-used-cars.\nThis dataset contains the data of 371540 used cars in germany. The goal is to train a linear regression model from this interesting dataset in order to predict the value of a used car.\n'], 'url_profile': 'https://github.com/prateektyagi29', 'info_list': ['2', 'Python', 'Apache-2.0 license', 'Updated Jan 21, 2021', '2', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'R', 'Updated Jan 28, 2020', 'C++', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'C++', 'Updated May 8, 2020', 'R', 'Updated Oct 14, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['2', 'Python', 'Apache-2.0 license', 'Updated Jan 21, 2021', '2', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'R', 'Updated Jan 28, 2020', 'C++', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'C++', 'Updated May 8, 2020', 'R', 'Updated Oct 14, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['2', 'Python', 'Apache-2.0 license', 'Updated Jan 21, 2021', '2', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'R', 'Updated Jan 28, 2020', 'C++', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'C++', 'Updated May 8, 2020', 'R', 'Updated Oct 14, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['House-Prices-in-Australia\nbuild a regression model using regularisation in order to predict the actual value of the prospective House properties and decide whether to invest in them or not.\n'], 'url_profile': 'https://github.com/SurabhiKhare97', 'info_list': ['2', 'Python', 'Apache-2.0 license', 'Updated Jan 21, 2021', '2', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'R', 'Updated Jan 28, 2020', 'C++', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'C++', 'Updated May 8, 2020', 'R', 'Updated Oct 14, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['2', 'Python', 'Apache-2.0 license', 'Updated Jan 21, 2021', '2', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'R', 'Updated Jan 28, 2020', 'C++', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'C++', 'Updated May 8, 2020', 'R', 'Updated Oct 14, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['2', 'Python', 'Apache-2.0 license', 'Updated Jan 21, 2021', '2', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'R', 'Updated Jan 28, 2020', 'C++', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'C++', 'Updated May 8, 2020', 'R', 'Updated Oct 14, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\nDGLMExtPois\nDGLMExtPois is a package that contains statistical functions for the\nmodel estimation, dispersion testing and diagnosis of hyper-Poisson and\nConway-Maxwell-Poisson regression models.\nInstallation\nYou can install the released version of DGLMExtPois from\nCRAN with:\ninstall.packages(""DGLMExtPois"")\nExample\nThis is a basic example which shows you how to solve a common problem:\nlibrary(DGLMExtPois)\nlibrary(Ecdat)\n#> Loading required package: Ecfun\n#> \n#> Attaching package: \'Ecfun\'\n#> The following object is masked from \'package:base\':\n#> \n#>     sign\n#> \n#> Attaching package: \'Ecdat\'\n#> The following object is masked from \'package:datasets\':\n#> \n#>     Orange\nBids$size.sq <- Bids$size ^ 2\nhP.bids <- glm.hP(formula.mu = numbids ~ leglrest + rearest + finrest +\n                    whtknght + bidprem + insthold + size + size.sq +\n                    regulatn,\n                    formula.gamma = numbids ~ 1,\n                    data = Bids)\nsummary(hP.bids)\n#> \n#> Call:\n#> glm.hP(formula.mu = numbids ~ leglrest + rearest + finrest + \n#>     whtknght + bidprem + insthold + size + size.sq + regulatn, \n#>     formula.gamma = numbids ~ 1, data = Bids)\n#> \n#> Mean model coefficients (with log link):\n#>              Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  1.042145   0.387027   2.693  0.00709 ** \n#> leglrest     0.240887   0.109638   2.197  0.02801 *  \n#> rearest     -0.268646   0.144930  -1.854  0.06379 .  \n#> finrest      0.104245   0.163049   0.639  0.52260    \n#> whtknght     0.487929   0.110133   4.430 9.41e-06 ***\n#> bidprem     -0.709086   0.273832  -2.589  0.00961 ** \n#> insthold    -0.363993   0.304749  -1.194  0.23232    \n#> size         0.173023   0.048291   3.583  0.00034 ***\n#> size.sq     -0.007371   0.002479  -2.973  0.00295 ** \n#> regulatn    -0.008751   0.118167  -0.074  0.94097    \n#> ---\n#> Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n#> \n#> Dispersion model coefficients (with logit link):\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -2.6219     0.3489  -7.516 5.67e-14 ***\n#> ---\n#> Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n#> \n#> AIC: 362.3\nAIC(hP.bids)\n#> [1] 362.3072\nBIC(hP.bids)\n#> [1] 393.5063\ncoef(hP.bids)\n#> $mean_model\n#>  (Intercept)     leglrest      rearest      finrest     whtknght \n#>  1.042145307  0.240886951 -0.268645993  0.104245100  0.487928599 \n#>      bidprem     insthold         size      size.sq     regulatn \n#> -0.709086033 -0.363993485  0.173023482 -0.007370863 -0.008751012 \n#> \n#> $dispersion_model\n#> (Intercept) \n#>   -2.621855\nconfint(hP.bids)\n#>                   2.5 %       97.5 %\n#> (Intercept)  0.28358635  1.800704268\n#> leglrest     0.02600000  0.455773898\n#> rearest     -0.55270282  0.015410837\n#> finrest     -0.21532507  0.423815275\n#> whtknght     0.27207160  0.703785596\n#> bidprem     -1.24578669 -0.172385379\n#> insthold    -0.96129006  0.233303088\n#> size         0.07837414  0.267672828\n#> size.sq     -0.01223058 -0.002511151\n#> regulatn    -0.24035456  0.222852539\nhead(fitted(hP.bids))\n#>        1        2        3        4        5        6 \n#> 2.733621 1.331997 2.196977 1.176840 1.231121 2.088129\nhead(residuals(hP.bids))\n#>          1          2          3          4          5          6 \n#> -0.5421896 -1.7307986 -1.0424542 -0.2501029 -0.3175111  0.8264357\nCMP.bids <- glm.CMP(formula.mu = numbids ~ leglrest + rearest + finrest +\n                    whtknght + bidprem + insthold + size + size.sq +\n                    regulatn,\n                    formula.nu = numbids ~ 1,\n                    data = Bids)\nsummary(CMP.bids)\n#> \n#> Call:\n#> glm.CMP(formula.mu = numbids ~ leglrest + rearest + finrest + \n#>     whtknght + bidprem + insthold + size + size.sq + regulatn, \n#>     formula.nu = numbids ~ 1, data = Bids)\n#> \n#> Mean model coefficients (with log link):\n#>              Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  0.990004   0.435140   2.275 0.022898 *  \n#> leglrest     0.267903   0.122808   2.181 0.029148 *  \n#> rearest     -0.173273   0.154704  -1.120 0.262703    \n#> finrest      0.067916   0.174300   0.390 0.696794    \n#> whtknght     0.481172   0.131654   3.655 0.000257 ***\n#> bidprem     -0.685007   0.307470  -2.228 0.025889 *  \n#> insthold    -0.367923   0.346620  -1.061 0.288481    \n#> size         0.179279   0.047604   3.766 0.000166 ***\n#> size.sq     -0.007580   0.002483  -3.052 0.002270 ** \n#> regulatn    -0.037561   0.130235  -0.288 0.773031    \n#> ---\n#> Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n#> \n#> Dispersion model coefficients (with logit link):\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  0.56208    0.04809   11.69   <2e-16 ***\n#> ---\n#> Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n#> \n#> AIC: 382.2\nAIC(CMP.bids)\n#> [1] 382.1753\nBIC(CMP.bids)\n#> [1] 413.3744\ncoef(CMP.bids)\n#> $mean_model\n#>  (Intercept)     leglrest      rearest      finrest     whtknght \n#>  0.990003872  0.267902606 -0.173272510  0.067916284  0.481171553 \n#>      bidprem     insthold         size      size.sq     regulatn \n#> -0.685006796 -0.367923118  0.179279126 -0.007580393 -0.037561467 \n#> \n#> $dispersion_model\n#> (Intercept) \n#>   0.5620821\nconfint(CMP.bids)\n#>                   2.5 %       97.5 %\n#> (Intercept)  0.13714473  1.842863012\n#> leglrest     0.02720360  0.508601611\n#> rearest     -0.47648689  0.129941873\n#> finrest     -0.27370514  0.409537708\n#> whtknght     0.22313460  0.739208506\n#> bidprem     -1.28763746 -0.082376130\n#> insthold    -1.04728529  0.311439049\n#> size         0.08597638  0.272581868\n#> size.sq     -0.01244781 -0.002712976\n#> regulatn    -0.29281835  0.217695412\nhead(fitted(CMP.bids))\n#>        1        2        3        4        5        6 \n#> 2.736143 1.296889 2.140255 1.189666 1.205770 2.092416\nhead(residuals(CMP.bids))\n#>          1          2          3          4          5          6 \n#> -0.5646967 -1.3669047 -0.9753079 -0.2069397 -0.2233074  0.7839429\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['2', 'Python', 'Apache-2.0 license', 'Updated Jan 21, 2021', '2', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'R', 'Updated Jan 28, 2020', 'C++', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'C++', 'Updated May 8, 2020', 'R', 'Updated Oct 14, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\n\n\noutput\n\n\n\n\n\n\n\npdf_document\nhtml_document\n\n\n\n\ndefault\ndefault\n\n\n\n\n\n\n\nALassoSurvIC\nThe ALassoSurvIC package provides penalized variable selection tools for the Cox proportional hazards model with interval censored and possibly left truncated data. The main function alacoxIC performs the variable selection via penalized nonparametric maximum likelihood estimation with an adaptive lasso penalty. The function also finds the optimal thresholding parameter automatically by minimizing the Bayesian information criterion (BIC). The unpenalized Non-Parametric Maximum Likelihood Estimate (NPMLE) for interval censored and possibly left truncated data is also available with another main function unpencoxIC. The asymptotic validity of the methodology is established in Li et al. (2019).\nInstallation\ninstall.packages(""ALassoSurvIC"")\nintsall.packages(""parallel"") # required for parallel computing\nOverview\nThe package contains two main functions (alacoxIC and unpencoxIC) and two methods (baseline and plot) for the objects returned by the main functions. The cluster object, created by makeCluster in the parallel package, can be supplied with the cl argument in the main functions to reduce computation time via parallel computing. The parallel computing will be used when searching the optimal thresholding parameter and calculating the hessian matrix of the log profile likelihood. How to use the parallel computing is illustrated in one of the examples given below.\n\n\nalacoxIC : The function performs variable selection for interval censored data or for interval censored and left truncated data. The users can supply the value of a theresholding parameter with the argument theta in the function. If theta is not supplied by users, the function will automatically find the optimal thresholding parameter using a grid search algorithm, based on the Bayesian information criterion (BIC).\n\n\nunpencoxIC: The function allows users to get unpenalized NPMLEs along with standard errors and 95% confidence intervals.\n\n\nbasline: The method to extract the NPMLEs for the baseline cumulative hazard function from an object returned by the alacoxIC function or the unpencoxIC function.\n\n\nplot: The method to plot the estimated baseline cumulative hazard function or the estimated baseline survival function from an object returned by the alacoxIC function or the unpencoxIC function.\n\n\nExample\nThe examples below show how to use the main functions and the methods with two virtual data sets; ex_IC is interval censored data and ex_ICLT is interval censored and left truncated data. Any inference cannot be drawn from these data sets.\nExample 1: Interval censored data\nlibrary(ALassoSurvIC)\n\ndata(ex_IC) # \'ex_IC\' is interval censored data\nlowerIC <- ex_IC$lowerIC\nupperIC <- ex_IC$upperIC\nX <- ex_IC[, -c(1:2)]\n\n## Performing the variable selection algorithm using a single core\n## Use the `cl` argument to reduce computation time.\nres <- alacoxIC(lowerIC, upperIC, X)\nres           # main result\nbaseline(res) # obtaining the baseline cumulative hazard estimate\nplot(res)     # plotting the estimated baseline cumulative hazard function by default\nplot(res, what = ""survival"")  # plotting the estimated baseline survival hazard function\n\n## Getting the unpenalized NPMLEs for interval censored data\nres2 <- unpencoxIC(lowerIC, upperIC, X)\nres2\n\nExample 2: Interval censored and left truncated data\ndata(ex_ICLT) # \'ex_ICLT\' is interval censored and left truncated data\nlowerIC <- ex_ICLT$lowerIC\nupperIC <- ex_ICLT$upperIC\ntrunc <- ex_ICLT$trunc\nX <- ex_ICLT[, -c(1:3)]\n\n## Performing the variable selection algorithm using a single core\n## Use the `cl` argument to reduce computation time.\nres3 <- alacoxIC(lowerIC, upperIC, X, trunc)\nres3\nbaseline(res3)\nplot(res3)\nplot(res3, what = ""survival"")\n\n## Getting the unpenalized NPMLEs for interval censored data\nres4 <- unpencoxIC(lowerIC, upperIC, X, trunc)\nres4\n\nExample 3: Reducing computation time using parallel computing\ndata(ex_IC) # \'ex_IC\' is interval censored data\nlowerIC <- ex_IC$lowerIC\nupperIC <- ex_IC$upperIC\nX <- ex_IC[, -c(1:2)]\n\nlibrary(parallel)\ncl <- makeCluster(2L)  # making the cluster object \'cl\' with two CPU cores\n# cl <- makeCluster(detectCores()) # run this code instead to use all available CPU cores\n\n## Compare two computation times\n## Note that the `unpencoxIC` function also allows users to use the `cl` argument.\nsystem.time(res_parallel <- alacoxIC(lowerIC, upperIC, X, cl = cl)) # Use two cores\nsystem.time(res <- alacoxIC(lowerIC, upperIC, X)) # Use a single core\n\nReference\nLi, C., Pak, D., & Todem, D. (2019). Adaptive lasso for the Cox regression with interval censored and possibly left truncated data. Statistical methods in medical research, in press.\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['2', 'Python', 'Apache-2.0 license', 'Updated Jan 21, 2021', '2', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'R', 'Updated Jan 28, 2020', 'C++', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'C++', 'Updated May 8, 2020', 'R', 'Updated Oct 14, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['2', 'Python', 'Apache-2.0 license', 'Updated Jan 21, 2021', '2', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'R', 'Updated Jan 28, 2020', 'C++', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'C++', 'Updated May 8, 2020', 'R', 'Updated Oct 14, 2020', 'R', 'Updated May 3, 2020', '1', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 28, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bryangne', 'info_list': ['HTML', 'Updated Jan 27, 2020', '1', 'R', 'Updated Jul 23, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'Updated Feb 2, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Jan 29, 2020', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Feb 28, 2021', 'Updated Feb 1, 2020', 'R', 'Updated Mar 6, 2020']}","{'location': 'Provo, UT', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Titanic-Competition\nThe purpose of this project is to perform classification analysis using logistic regression, random forests, and boosting then compare which method is the most effective. Data and idea comes from the Kaggle Titanic competition.\n'], 'url_profile': 'https://github.com/spebert', 'info_list': ['HTML', 'Updated Jan 27, 2020', '1', 'R', 'Updated Jul 23, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'Updated Feb 2, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Jan 29, 2020', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Feb 28, 2021', 'Updated Feb 1, 2020', 'R', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['HTML', 'Updated Jan 27, 2020', '1', 'R', 'Updated Jul 23, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'Updated Feb 2, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Jan 29, 2020', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Feb 28, 2021', 'Updated Feb 1, 2020', 'R', 'Updated Mar 6, 2020']}","{'location': 'Varanasi, India', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yaduu572', 'info_list': ['HTML', 'Updated Jan 27, 2020', '1', 'R', 'Updated Jul 23, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'Updated Feb 2, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Jan 29, 2020', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Feb 28, 2021', 'Updated Feb 1, 2020', 'R', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\nxtune: Tuning differential shrinkage parameters of penalized regression models based on external information \n\n\nüìó  Introduction\nMotivation\nIn standard Lasso and Ridge regression, a single penalty parameter Œª applied equally to all regression coefficients to control the amount of regularization in the model.\nBetter prediction accuracy may be achieved by allowing a differential amount of shrinkage. Ideally, we want to give a small penalty to important features and a large penalty to unimportant features. We guide the penalized regression model with external data Z that are potentially informative for the importance/effect size of coefficients and allow differential shrinkage modeled as a log-linear function of the external data.\nThe objective function of differential-shrinkage Lasso integrating external information is:\n\nThe objective function of differential-shrinkage Lasso integrating external information is:\n\nThe idea of external data is that it provides us information on the importance/effect size of regression coefficients. It could be any nominal or quantitative feature-specific information, such as the grouping of predictors, prior knowledge of biological importance, external p-values, function annotations, etc. Each column of Z is a variable for features in design matrix X. Z is of dimension, where p is the number of features and q is the number of variables in Z.\nTuning multiple penalty parameters\nPenalized regression fitting consists of two phases: (1) learning the tuning parameter(s) (2) estimating the regression coefficients giving the tuning parameter(s). Phase (1) is the key to achieve good performance. Cross-validation is widely used to tune a single penalty parameter, but it is computationally infeasible to tune more than three penalty parameters. We propose an Empirical Bayes approach to learn the multiple tuning parameters. The individual penalties are interpreted as variance terms of the priors (double exponential prior for Lasso and Gaussian prior for Ridge) in a random effect formulation of penalized regressions. A majorization-minimization algorithm is employed for implementation. Once the tuning parameters Œªs are estimated, and therefore the penalties are known, phase (2) - estimating the regression coefficients is done using glmnet.\nData structure examples\nSuppose we want to predict a person\'s weight loss using his/her weekly dietary intake. Our external information Z could incorporate information about the levels of relevant food constituents in the dietary items.\nPrimary data X and Y: predicting an individual\'s weight loss by his/her weekly dietary items intake\n\nExternal information Z: the nutrition facts about each dietary item\n\nüìô  Installation\nxtune can be installed from Github using the following command:\n# install.packages(""devtools"")\n\nlibrary(devtools)\ndevtools::install_github(""ChubingZeng/xtune"")\n\nlibrary(xtune)\nüìò  Examples\nTo show some examples on how to use this package, we simulated an example of data that contains 100 observations, 200 predictors, and a continuous outcome. The external information Z contains 4 columns, each column is indicator variable (can be viewed as the grouping of predictors).\n## load the example data\ndata(example)\nThe data looks like:\nexample$X[1:3,1:5]\n\nexample$Z[1:5,]\n\nxtune() is the core function to fit the integrated penalized regression model. At a minimum, you need to specify the predictor matrix X, outcome variable Y. If an external information matrix Z is provided, the function will incorporate Z to allow differential shrinkage based on Z. The estimated tuning parameters are returned in $penalty.vector.\nIf you do not provide external information Z, the function will perform empirical Bayes tuning to choose the single penalty parameter in penalized regression, as an alternative to cross-validation. You could compare the tuning parameter chosen by empirical Bayes tuning to that choose by cross-validation (see also cv.glmnet). The default penalty applied to the predictors is the Lasso penalty.\nIf you provide an identify matrix as external information Z to xtune(), the function will estimate a separate tuning parameter  for each regression coefficient .\nxtune.fit <- xtune(example$X,example$Y,example$Z)\n\n## for ridge\n## xtune.fit <- xtune(example$X,example$Y,example$Z,method = ""ridge"")\nTo view the penalty parameters estimated by xtune()\nxtune.fit$penalty.vectors\n\nThe coef and predict functions can be used to extract beta coefficient estimates and predict response on new data.\ncoef(xtune.fit)\npredict(xtune.fit, example$X)\nTwo real-world examples are also described in the vignettes to further illustrate the usage and syntax of this package.\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['HTML', 'Updated Jan 27, 2020', '1', 'R', 'Updated Jul 23, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'Updated Feb 2, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Jan 29, 2020', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Feb 28, 2021', 'Updated Feb 1, 2020', 'R', 'Updated Mar 6, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['4.IoT\nDetecting Wifi Indoor Locationing: Multivariate classification and regression analysis by using PCA (Principle Component Analysis),  Near Zero Variance, RFE (Recursive Feature Elimination) in R\n'], 'url_profile': 'https://github.com/sunin22', 'info_list': ['HTML', 'Updated Jan 27, 2020', '1', 'R', 'Updated Jul 23, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'Updated Feb 2, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Jan 29, 2020', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Feb 28, 2021', 'Updated Feb 1, 2020', 'R', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '199 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vivekchauhan12000', 'info_list': ['HTML', 'Updated Jan 27, 2020', '1', 'R', 'Updated Jul 23, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'Updated Feb 2, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Jan 29, 2020', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Feb 28, 2021', 'Updated Feb 1, 2020', 'R', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['HTML', 'Updated Jan 27, 2020', '1', 'R', 'Updated Jul 23, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'Updated Feb 2, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Jan 29, 2020', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Feb 28, 2021', 'Updated Feb 1, 2020', 'R', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['This GitHub project is aimed at containing RCode, RMD files and Data linked to the chapter of comprehensive chemometrics on ""The essentials on linear regression, ANOVA, general linear and linear mixed models for the chemist"".\nThis material will be available during the first semester of 2020.\nYou can already contact the main author Bernadette.govaerts@uclouvain.be if needed.\n'], 'url_profile': 'https://github.com/bgovaerts', 'info_list': ['HTML', 'Updated Jan 27, 2020', '1', 'R', 'Updated Jul 23, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'Updated Feb 2, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Jan 29, 2020', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Feb 28, 2021', 'Updated Feb 1, 2020', 'R', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['HTML', 'Updated Jan 27, 2020', '1', 'R', 'Updated Jul 23, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 27, 2020', 'Updated Feb 2, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Jan 29, 2020', 'JavaScript', 'Updated Feb 1, 2020', 'R', 'Updated Feb 28, 2021', 'Updated Feb 1, 2020', 'R', 'Updated Mar 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated May 9, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 1, 2020']}","{'location': 'Los Angeles, California', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Classification-using-Neural-Networks\nBuilding a neural networks using ReLU and softmax activation functions and comparing its performance with a logistic regression for a dataset\n'], 'url_profile': 'https://github.com/samarthraizada', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated May 9, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated May 9, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated May 9, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 1, 2020']}","{'location': 'Greater Boston', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rish332', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated May 9, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated May 9, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['blrm\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated May 9, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated May 9, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['SVM\nA project to find the best hyper-parameters for binary classification and linear regression using Support Vector Machines. Please see report attached.\n'], 'url_profile': 'https://github.com/JamesArmer', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated May 9, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 1, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '242 contributions\n        in the last year', 'description': ['UCI-Spambase-spam-or-not-spam-detection\nA simple Logistic regression classification to identify whether email is spam or not spam built using python and scikit learn\nDataset used -\n\nUCI Spambase dataset [https://archive.ics.uci.edu/ml/datasets/spambase]\n\nRequirements\n\nInstall Pandas Numpy and Scikit learn\nPython3.X\n\nHow to run?\n\nPython3.x spam_detection.py\n\nResults\n\nCross validation score is: \n\n[0.93167702 0.90993789 0.9068323  0.9378882  0.94720497 0.92236025\n 0.92236025 0.92857143 0.91304348 0.91925466]\n\n\nClassification results without any parameter tuning or CV are\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.92      0.94       883\n           1       0.87      0.94      0.91       498\n\n    accuracy                           0.93      1381\n   macro avg       0.92      0.93      0.92      1381\nweighted avg       0.93      0.93      0.93      1381\n\n'], 'url_profile': 'https://github.com/ranjeetds', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated May 9, 2020', 'R', 'Updated Jan 28, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Jan 27, 2020', 'MATLAB', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '3,696 contributions\n        in the last year', 'description': ['ECE421\nFor ECE421\nNumpy notation used:\nUse (N, ) as shape of vectors of size N\nDot Product\nA = np.random.random((A, ))\nB = np.random.random((A, ))\nuse A.dot(B) for dot product\nEdit this file https://www.overleaf.com/8958533229qjygrqkhywfn\nfor assignment 1\nEdit this file https://www.overleaf.com/1745191488fstcgyjqccqd\nfor assignment 2\nEdit this file https://www.overleaf.com/2911262193kycqjzgsdnwc\nfor assignment 3\n'], 'url_profile': 'https://github.com/EvanSamaa', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'Chicago, IL, USA', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['Data Science - United-States-Senate-Elections-2018\nPerformed exploratory data analysis and created different regression, classification and clustering to predict the number of votes received for each party in each county and classified the counties based on the elected party.\nDataset\nThis project uses the dataset that is the compilation of 2018 U.S. Census Demographic Data.\nThis dataset provides ample numerical and categorical variables that can be utilized as predictors for our regression and classification models.\nSuch predictors include race, gender, household income, unemployment rate, education and so forth.\nPart 1 (Data Preparation & Hypothesis Testing)\nDependencies:\nIf there are issues running the code for task #10, check to see if pyshp, plotly, shapely, and geopandas packages are properly installed. \nOtherwise run the following commands using the command line interface of your choosing:\n$ conda install -c anaconda pyshp\n$ conda install -c anaconda plotly\n$ conda install -c anaconda shapely\n$ conda install -c anaconda geopandas\n\nInstructions to Run Code:\n\nDownload zip file\nMake sure the folder contains three csv files (demographics_train.csv, election_train.csv, and State_codes.csv).\nOpen the python file in Jupyter Notebook or JupyterLab and run the code segments.\n\nPart 2 (Precditive Modeling - Machine Learning Algorithms)\nDependencies\nCurrently, there is known issue that matplotlib version 3.1.1 does not properly display the heatmap on macOS. If there is an issue doing so, please downgrade to matplotlib version 3.1.0 upgrade to matplotlib 3.1.2.\nhttps://github.com/matplotlib/matplotlib/issues/14675\nIf there are issues running the code for task #06, check to see if pyshp, plotly, shapely, and geopandas packages are properly installed. Otherwise run the following commands using the command line interface of your choosing:\n$ conda install -c anaconda pyshp\n$ conda install -c anaconda plotly\n$ conda install -c anaconda shapely\n$ conda install -c anaconda geopandas\n\nInstructions to Run Code\n\nAfter downloading the zip file, make sure that the following csv files are present \n\n\t- merged_train.csv \n\t- demographics_test.csv\n\t- output.xlsx\n\n\nOpen the python file in Jupyter Notebook or JupyterLab and run the code segments\n\n'], 'url_profile': 'https://github.com/anushasagi', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Python Machine Learning Algorithms\nIntroduction\nThis is a personal project to create rigorous codes that run machine learning algorithms on\nPython3, coded solely using numpy and pandas.\nCurrent progress of project:\n\nSupports linear regression, logistic regression\nSupports kfold validation\nSupports regularization tuning\n\nUsage\nCode is applicable to different data sets, data sets will have to be manually imported and\nmanipulated by the user.\nModel accuracy is computed after the model parameters are obtained. K-fold validation is used\nto increase reliability of computed accuracy.\nCross-validation data can be used for model accuracy comparisons against different hyper-\nparameters. An example would be to tune the regularization parameter lambda.\nEnd-Notes\nThe regression functions similarly to using sklearn regression functions.\nFuture Plans\n\nRigorous code for neural network (successfully coded on MATLAB, pending conversion to Python3)\n\n'], 'url_profile': 'https://github.com/intellimouseftw', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '113 contributions\n        in the last year', 'description': ['Clickthroughrate_predictions\nPredict whether a given ad on a given page, will be clicked by a given user with predictive features such as user information etc.\nLibraries Used: Numpy, Pandas, Seaborn, Matplotlib, Wordcloud, Sklearn.\nData Source: https://www.kaggle.com/fayomi/advertising\n'], 'url_profile': 'https://github.com/jaindamini1111', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'Bhubaneswar', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/swapnilbhat98', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['league-of-legends-ml\ndataset: https://www.kaggle.com/paololol/league-of-legends-ranked-matches\n'], 'url_profile': 'https://github.com/carralas', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PujaSarkar04', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Python', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'R', 'Updated Jan 29, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jan 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nhyland28', 'info_list': ['Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Dec 7, 2020', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 30, 2020']}","{'location': 'Uk', 'stats_list': [], 'contributions': '123 contributions\n        in the last year', 'description': [""\nGalaxy spin and environment\nThis repo contains code to understand how a galaxy spin depends on properties such as stellar mass, halo mass, group membership, and, large-scale environment.\nA galaxy's spin appears to be strongly correlated with morphology, with secondary dependences on stellar mass and inclination (i.e. ŒªR is a biased obervational estimate), however, local and large-scale environment (filamentary structure) are informative (to a lesser degree). This is detailed in chapter 4 of my thesis, and, best quickly summarised here based on results from ./scripts/random_forest/ and ./plots/random_forest/.\nData\nData is taken from various sources and a basic summary of each catalogue is given here, however, for more detail see here and the references therein.\n\n\nIntegral field unit observations are from the MaNGA galaxy survey, which is processed by the internal Data Analysis Pipeline DAP. This is used to compute ŒªR, a flux weighted measure of coherent rotation of a galaxy see here.\n\n\nAdditional information is taken from the NASA-Sloan Atlas targetting catalogue which provides stellar mass, and, galaxy inclination. For all galaxies in MaNGA, morphological classifications from citizen science project galaxyZoo are found.\n\n\nThese catalogues are cross-matched with group catalogues found from galaxies in the SDSS-DR7 spectroscopic sample, which provides halo mass, and, central/satellite definition.\n\n\nCosmic web catalogues are also cross-matched to this data to provide distances to morphological features of the cosmic web such as distances to filaments and nodes.\n\n\nThe total number of MaNGA galaxies (for MPL-9) after cross-matching information from each of these catalogues (top row) and cumulatively cross-matching (bottom row) are given here:\n\n\n\n\nMaNGA (w GalaxyZoo)\nGroup membership\nCosmic-web\n\n\n\n\nCross-matched\n7398\n6343\n6378\n\n\nCumulative cross-matched\n7398\n6343\n6117\n\n\n\nCatalog class object\nData catalogues (and various versions of MaNGA data releases) are brought together by the catalog class object found here, which performs the cross-matching. Catalog class objects store cross-matched information in the form of a pandas.DataFrame object (stored as property catalog.df).\nThe catalog class objects also contain various methods to select galaxy sub-samples based on these properties (./lib/catalog_init.py), for data processing ./lib/catalog_process.py and plotting ./lib/catalog_plot.py. These methods are tied together in ./catalog.py, however, are mainly used for the ./scripts/thesis_plots directory.\nMaNGA data used by the catalog object here is currently proprietary and hence not included in the repo.\nRandom Forest\nTo evaluate the importance of various galaxy properties (including local and large-scale environment) in predicting a galaxy's spin, we generate a random forest to predict ŒªR.\nInput features\n\n(nsersic) sersic index\n(b/a) inclination\n(Mstel) stellar mass\n(Mhalo) halo mass\n(Dnode) distance to nearest node (of the cosmic web)\n(Dskel) distance to nearest filament segment (of the cosmic web)\n(cen/sat) group membership (i.e. if the galaxy is the most massive (central) in the group or not (satellite))\n\nWe test various parameterisations of morphology including one-hot encoding of galaxyZoo classifications (i.e. ETGs, LTGs etc.), empirically estimated morphological T-type (using vote fractions in GZ), however, sersic index appears to be most informative for predicting galaxy spin (see ./plots/random_forest/morphology_importances.pdf.\nHyperparamter tuning\nThe best performing random forest is found using the sklearn optimized (cross-validated) grid-search method GridSearchCV. In contrast to a full grid-search, not all parameter values are tried, however, a fixed number of setting combinations are sampled from the distribution. We find there is no significant drop-off in performance between various hyperparameter combinations, however, our best random forest set-up is found to be:\n{'n_estimators': 1000,\n 'min_samples_split': 5,\n 'min_samples_leaf': 4,\n 'max_features': 'sqrt',\n 'max_depth': 10}\n\nModel fitting\nUsing the following parameters and hyperparameters, a random forest is generated using the cross-matched data from the catalog class (6117 galaxies), where 80% is used for training and 20% for testing. We find that the model predictions have a mean absolute error of 0.12538. To compare the performance of the network (and importance of secondary parameters), we also generate a random forest (with the same hyperparameters) just using nsersic and b/a. We find the model predictions to have a mean absolute error of 0.12937. The ŒªR predicted distributions for the random forests (using all parameters and just the two most important (nsersic, b/a)) and the actual values are shown here:\n\nTo evaluate the performance of the predictions we also plot the correlation (with pearson correlation coeffients) with the actual ŒªR values for the test data. We find that the random forests make reasonably informed predictions of ŒªR, however, struggle to predict low or high values. The correlation plot is shown here:\n\nFeature importances\nA natural output of random forests is the relative importances of input features (based on the frequency of a given feature in nodes across all of the decision trees in the forest). The relative importances (i.e. normalised so they all sum to one) are given here:\n\nWe find that sersic index and inclination are most informative of predicting ŒªR, however there are reasonable contributions from stellar and halo mass along with small but significant contributions from cosmic-web environment. Group membership appears to be insignificant (however is encoded as a binary discrete parameter, which may impact its importance ranking relative to all other features which are continuous).\n""], 'url_profile': 'https://github.com/Chris-Duckworth', 'info_list': ['Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Dec 7, 2020', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PR-Anusha', 'info_list': ['Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Dec 7, 2020', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PR-Anusha', 'info_list': ['Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Dec 7, 2020', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 30, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vamsiabbireddy', 'info_list': ['Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Dec 7, 2020', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Dec 7, 2020', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Dec 7, 2020', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Dec 7, 2020', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 30, 2020']}","{'location': 'Toulouse', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BrunoPlzk', 'info_list': ['Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Dec 7, 2020', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 30, 2020']}","{'location': 'Irvine, California', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/adubidu', 'info_list': ['Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Dec 7, 2020', 'Updated Jan 28, 2020', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Jan 29, 2020', 'R', 'Updated Jan 30, 2020']}"
"{'location': 'Ketchum, ID', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['Denver-AirBnb-Project\nContributors:\n\nWenbin Yang GitHub: wenbin-yang\nAlyson Chen GitHub: alysonchen\nJunji Weiner GitHub: 8Jun\nAldo Peter GitHub: Aldospy\n\nObjective:\nThis project was started in the summer semester during my Masters in Business Analytics program at CU Boulder. It is intended to be an exploratory project into the basics of R.\nData Cleaning\nLet\'s start by uploading the necessary CSV\'s into R and install any necessary packages. For this project the only package used was leaflet.\nlistings <- read.csv(""denver_listings.csv"",stringsAsFactors=FALSE)\nURL <- read.csv(""URLs.csv"",stringsAsFactors=FALSE)\n\ninstall.packages(\'leaflet\')\nrequire(\'leaflet\')\nFor starters, we need to do a little bit of cleaning with the price columns. The data set has information on both ""price"" which is what we assume is nightly vs ""weekly price"" which is what my group assumed was the price for renting this AirBNB for the entire week:\nWe need to take the price out of character form and change it into numeric so it is easy to find means and other stats. Let\'s remove the dollar sign and the quotes, and then change it into numeric for both price and weekly_price.\nlistings2$price <- sub(pattern=""$"",replacement="""",listings2$price,fixed=TRUE)\nlistings2$price <- as.numeric(listings2$price)\n\nlistings2$weekly_price <- sub(pattern=""$"",replacement="""",listings2$weekly_price,fixed=TRUE)\nlistings2$weekly_price <- as.numeric(listings2$weekly_price)\nFilling in ""NA\'s"" with 0\'s.\nlistings2[is.na(listings2)] <- 0\nSome of the weekly values in our data set do not have prices for the week. We could assign NA to them or throw them out, but lets try and give them a value using their corresponding price/night value. We can take that value and multiply to 7 to give us some estimation for the weekly price.\nlistings2$price_WEEK <- rep(0,times=nrow(listings2))\n\nlistings2[listings2$weekly_price == 0,\'price_WEEK\'] <- listings2[listings2$weekly_price == 0,\'price\']*7\n\nlistings2[listings2$weekly_price != 0,\'price_WEEK\'] <- listings2[listings2$weekly_price != 0,\'weekly_price\']\nNow that we have cleaned both our price/night and price/week columns, let\'s see how they look!\nhead((listings2$price),25)\n# [1]  56 140  61  42  70  95  76  65 110 111 300 179 120  75  70  60 125 411\n# [19] 195  35  32  66 235 300  70\nhead((listings2$price_WEEK),25)\n#  [1]  392  980  375  295  490  665  532  455  770  649 2100 1253  840  399\n# [15]  455  364  900 2877 1365  900  350  950 1645 2100  450\nVariable Creation\nNow we need to think about the map in leaflet that we ultimately want to create. We want to highlight each neighbourhood in the dataset and feed different information in like prices, ratings, etc. For each thing we want to feed in, we need to get an aggregate of that item for each neighbourhood.\n\n\nUltimately, we will feed all newly created variables (remember: all are per NB = 78 rows) into a final data set called zDATA. This is the dataset that will be fed into our leaflet mapping tool.\n\n\nNOTE: NB = neighbourhood\n\n\nUsing Aggregate to find Mean Prices per NB and Rounding into variables zNIGHT and zWEEK:\nzNIGHT <- aggregate(price~neighbourhood, FUN=mean, data=listings2)\nzWEEK <- aggregate(price_WEEK~neighbourhood, FUN=mean, data=listings2)\n\nzNIGHT$price <- round(zNIGHT$price)\nzWEEK$price_WEEK <- round(zWEEK$price_WEEK)\n\nzNIGHT\nzWEEK\nUsing Aggregate to find Mean Rating per NB and Rounding into variables zRATING:\nzRATING <- aggregate(review_scores_rating~neighbourhood, FUN=mean, data=listings2)\n\nzRATING$review_scores_rating <- round(zRATING$review_scores_rating)\n\nzRATING\nUsing Aggregate to find Count of Total Properties Listed per NB and put into variable zTYPE:\nzTYPE <- aggregate(property_type~neighbourhood, FUN=length, data=listings2)\nzTYPE\nThe variable NBmeans required more work. To be loaded into leaflet, we needed a MEAN latitude and longitude for each NB. Pulled columns latitude, longitude, and neighbourhood into a temporary dataset named newdata. Then we can find the average latitude and longitude for each neighbourhood here. Then cbind it and put it into NBmeans.\nlats <- c(""latitude"", ""longitude"", ""neighbourhood"")\nnewdata <- listings2[lats]\nlatitudeAverage <- aggregate(latitude ~ neighbourhood, FUN=mean, data=newdata)\nlongitudeAverage <- aggregate(longitude~ neighbourhood, FUN=mean, data=newdata)\nNBmeans <- cbind(latitudeAverage, longitudeAverage) \nLet\'s see how NBmeans looks now:\nhead(NBmeans)\n# neighbourhood latitude neighbourhood longitude\n# 1   Athmar Park 39.70341   Athmar Park -105.0130\n# 2       Auraria 39.74840       Auraria -105.0042\n# 3         Baker 39.71921         Baker -104.9931\n# 4        Barnum 39.71878        Barnum -105.0354\n# 5   Barnum West 39.72107   Barnum West -105.0443\n# 6   Bear Valley 39.66221   Bear Valley -105.0663\nNBmeans looks good but we don\'t need neighbourhood to repeat more than once. Let\'s take out the 3rd column.\nNBmeans <- subset(NBmeans,select=-c(3))\nhead(NBmeans)\n#  neighbourhood latitude longitude\n# 1   Athmar Park 39.70341 -105.0130\n# 2       Auraria 39.74840 -105.0042\n# 3         Baker 39.71921 -104.9931\n# 4        Barnum 39.71878 -105.0354\n# 5   Barnum West 39.72107 -105.0443\n# 6   Bear Valley 39.66221 -105.0663\nBecause this was an exploratory project, we wanted to explore a little bit of text mining and add it into our zDATA data set for the leaflet map. In this code, we search for homes that have text stating ""no dog(s) or no cat(s)"". This text search isn\'t perfect - but it gives a good foundation to build on.\nLet\'s put all of the homes that we find with these ""no pets"" rules into txtPET.\ntxtPET <- listings2[grepl(""no pet|no dog|no dogs|no cats|no cat"",listings2$house_rules),]\nhead(txtPET)\nWe can create another variable called ""notpetfriendly"" that is an aggregate of each NB and how many of these ""no pet"" properties are there.\nnotpetfriendly <- aggregate(property_type~neighbourhood,data=txtPET,FUN=length)\nLet\'s make this interesting and instead of a number for each NB, we can add an emoji of a Dog if pets are allowed, or an emoji of a couple, to suggest that no pets were mentioned in the text.\nzPet <- merge(zRATING, notpetfriendly, by=\'neighbourhood\',all=TRUE)\nzPet[is.na(zPet)] <- 0\n\nfor (i in 1:length(zPet $property_type)){\n\tif(zPet $property_type[i] == 0){\n\t\tzPet $property_type[i] <- ""\\U0001f436""\n\t}else{\n\t\tzPet $property_type[i] <- \'\\U0001f46b\'\n\t\t}\n}\nOur pet text mining information is now housed inside of zPet, which you\'ll notice was merged with neighbourhood and zRATING here. We will clean this up later in the code - and for each instance of zPet will house the emoji code.\nMapping Set Up\nNow we are ready to merge everything into zDATA. We can do a simple cbind here with all of the variables we want to include. Most of these are going to have their own NB column, so we can just keep the first instance of this. The subset command helps us to remove which repeat columns we don\'t want so we can get a cleaner dataset.\n\nNOTE: the variable URL is what was loaded at the beginning of this code. This was manually created to get a URL for each Neighbourhood vs. for each property like in the listings dataset.\n\nzDATA <- cbind(zRATING,zNIGHT,zWEEK,NBmeans,zTYPE,URL,zPet)\nzDATA <- subset(zDATA,select=-c(3,5,7,10,12, 14, 15))\nGreat! Our zDATA is now ready to roll and be placed inside our leaflet mapping structure.\nCall Packages\nlibrary(leaflet)\nlibrary(shiny)\nLet\'s Map!\nThe leaflet mapping structure has a few components to it and I will do my best to explain what each does.\nThis function takes zNIGHT (our price/night variable) and tests it to see which threshold it falls into. If below $100, we can color our popup icon green. If between $101-200, the icon will be orange. If above this, it will be red.\ngetColor <- function(zNIGHT) {\n  sapply(zNIGHT$price, function(price) {\n  if(price <= 100) {\n    ""green""\n  } else if(price <= 200) {\n    ""orange""\n  } else {\n    ""red""\n  } })\n}\nThe icons variable calls awesomeIcons which is built in, and the markerColor is received from our getColor function which we performed above.\nicons <- awesomeIcons(\n  icon = \'ios-close\',\n  iconColor = \'black\',\n  library = \'ion\',\n  markerColor = getColor(zNIGHT)\n)\nThe large for loop below does something similar to our pricing and the colors. Instead, it takes the average rating for each NB, and assigns a ""star"" emoji to it. If 90 or above, for example, it will paste 5 stars into the pop up menu.\nfor (i in 1:length(zDATA$review_scores_rating)){\n\tif(zDATA$review_scores_rating[i] >= 90){\n\t\tzDATA$review_scores_rating[i] <-\'\\u2b50\\u2b50\\u2b50\\u2b50\\u2b50\'\n\t}else if(zDATA$review_scores_rating[i] >= 80){\n\t\tzDATA$review_scores_rating[i] <-\'\\u2b50\\u2b50\\u2b50\\u2b50\'\n\t}else if(zDATA$review_scores_rating[i] >= 70){\n\t\tzDATA$review_scores_rating[i] <-\'\\u2b50\\u2b50\\u2b50\'\n\t}else if(zDATA$review_scores_rating[i] >= 60){\n\t\tzDATA$review_scores_rating[i] <-\'\\u2b50\\u2b50\'\n\t}else{\n\t\tzDATA$review_scores_rating[i] <-\'\\u2b50\'\n\t}\n}\nThe code below is the official leaflet structure. We want to add tiles based on the zNIGHT variable (this is a judgement call, it can be weekly_price too with some alteration to the code above). The addAwesomeMarkers will require a longitude and a latitude, which has been taken from our zDATA. The icon variable = icons from the code above. To add a popup, which we have done, you\'ll need to do ""popup = "" and can put as many or as little things as possible.\nThe format for the pop up, as you can see, is some sort of text label (Neighbourhood: ""), the data where you want it to come from (zDATA$neighbourhood), and """" at the end followed by commas for each variable you want to include. If you want to include a link, as I did, the write up is slightly different.\nleaflet(zNIGHT) %>% addTiles() %>%  \n  addAwesomeMarkers(lng = zDATA$longitude, lat = zDATA$latitude,icon=icons,\n             popup = paste(""Neighbourhood:"", zDATA$neighbourhood, ""<br>"",\n                           ""Avg Score:"", zDATA$review_scores_rating, ""<br>"",\n                           ""Avg Price/Night:"", zDATA$price, ""<br>"",\n                           ""Avg Price/Week:"", zDATA$price_WEEK, ""<br>"",\n\t\t\t\t   ""Number of Properties:"",zDATA$property_type, ""<br>"",\n\t\t\t\t   ""Suitable for:"",zDATA$property_type.1, ""<br>"",\n                           ""<a href=\'"", zDATA$URL , ""\' target=\'_blank\'>"", ""AirBnb Properties in this Neighbourhood:</a>""))\nFinal Result\nImage of the Map:\n\nImage of the Popup Box:\n\nBasic Regression\nThe code below shows some basic exploratory Linear Regression used to predict the price of a home located in Sloane Lake.\nlakeDATA <- subset(listings2, neighbourhood=\'Sloane Lake\')\nsummary(lm(price~bedrooms+bathrooms,data=lakeDATA))\n\n#Price Estimation off beds and baths:\n19.974 +  51.106*(beds) + 24.784*(baths)\n\nPRICE = 19.974 +  51.106*(4) + 24.784*(2)\nPRICE = #$273.97/night\n'], 'url_profile': 'https://github.com/alexqaddourah', 'info_list': ['R', 'Updated Feb 16, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jul 28, 2020', 'R', 'Updated Jan 28, 2020', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Feb 16, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jul 28, 2020', 'R', 'Updated Jan 28, 2020', 'Updated Jan 29, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""spam_vs_email\nIn this R code, I describe and test various machine learning models as applied to statistics such as Linear Discriminant Analysis, KNN, Logistic Regression, and Random Forests to classify 'spam' from 'emails' in a large data set. This code also compares these models to find the one with the predictive accuracy or lowest misclassification error.\n""], 'url_profile': 'https://github.com/soniajshaikh', 'info_list': ['R', 'Updated Feb 16, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jul 28, 2020', 'R', 'Updated Jan 28, 2020', 'Updated Jan 29, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Predicting Course Completion Outcome in VLE\nPurpose\nThe purpose of this research is to determine whether students‚Äô gender, age range, the average number of daily clicks, and the average assessment score can predict the course completion outcome. The secondary research question is whether the student‚Äôs age range has an effect on their average assessment score and whether the effect of student‚Äôs age range on their average assessment score is moderated by the student‚Äôs gender.\nMethodology\nIn the present study, seven anonymized datasets were retrieved from the Open University in the United Kingdom (Kuzilek, Hlosta, & Zdrahal, 2017). The codebook can be found here. They contain data regarding the students, courses, and student interaction with the Virtual Learning Environment (VLE) for seven courses, where students had access to the course materials online and their interactions with the materials were recorded. Three of these datasets were merged.\nParticipants\nThe dataset consists of 32,593 observations, which indicates that data has been gathered for 32,593 students. There are no missing values. However, the data of students, who have withdrawn from the courses, was omitted from the analysis. Thus, the data of only 22,437 students were used for the analyses after the omission of the withdrawn students.\nInstruments\nThe variables of interest are the students‚Äô gender, age range, the average number of daily clicks for each student, and the average assessment score for each student, which served as predictors. Gender is a dichotomous categorical variable with levels ‚ÄúMale‚Äù (coded as ‚Äú0‚Äù) and ‚ÄúFemale‚Äù (coded as ‚Äú1‚Äù). The student‚Äôs age range is also a categorical variable with the following three levels: ‚Äú17-35‚Äù (coded as ‚Äú0‚Äù), ‚Äú35-55‚Äù (coded as ‚Äú1‚Äù), and ‚Äú55 or greater‚Äù (coded as ‚Äú2‚Äù). The student‚Äôs average number of daily clicks and average assessment score are continuous predictors, and the average assessment score is measured in percentage and ranges from 0 to 100. The outcome variable for the first research question is the course completion, which is operationalized as the student‚Äôs final result of the course. Students, who have withdrawn from the course, were omitted from the analysis, while students, who have finished the course with ‚Äúdistinction‚Äù, were considered as the passing students. Thus, the outcome variable, the student‚Äôs final result in the course presentation, is a dichotomous categorical variable with two following groups: ‚ÄúPass‚Äù (coded as ‚Äú1‚Äù) and ‚ÄúFail‚Äù (coded as ‚Äú0‚Äù).\nProcedure\nThere were two following variables created: each student‚Äôs average number of daily clicks in the VLE and each student‚Äôs average assessment score in the seven courses. A binomial logistic regression was performed to answer the primary research question. A two-way ANOVA test for interaction was conducted to examine the secondary research question and followed up with simple main effects with Holm-Bonferroni correction, and the analysis of the main effect of the student‚Äôs age range. All of the analyses were evaluated at a 5% significant level.\nResults\nFactors that Predict Course Completion Outcome\n\n\nModel\nglm1 <- glm(formula=final_result~age_band+gender+mean_click + mean_score+ age_band:gender, data=IVA2, family = ""binomial"" )\nsummary(glm1) # these are logit coefficients\n\n## Interaction between age range and gender is not significant\n\n## Coefficients:\ncoef(glm1) \n\n## Exponentiated (and rounded) coefficients\nround(exp(coef(glm1)), 2) \n\nThe results showed that the student‚Äôs gender, age range, the average number of daily clicks, and average assessment score are statistically significant in predicting the course completion outcome. Being student who is 55 years old or older had the most odds in passing the course. In particular, for every additional click made by a student, there is a 22% increase in the odds of a student passing the course. Similarly, for every additional increase in percentage in the student‚Äôs average assessment score, there is an 8% increase in passing the course. Being a student in a ‚Äú35-55‚Äù age group increases the odds of passing a course by 14%. Likewise, being a student in a ‚Äú55 or greater‚Äù age group increases the odds of passing a course by as much as 106%. Being a female student also increases the odds of passing a course by 22%.\nInteraction of Age Range and Gender on the Average Assessment Score\n\n\nModel\nlm1 <- lm(mean_score ~ age_band*gender, \n          data = IVA2)\n          \n# Two-way interaction is significant, so must look at simple effects.\n\njoint_tests(object = lm1, by = ""age_band"")\n\nemm1 <- emmeans(object = lm1,\n                specs = ~ gender| age_band,\n                adjust = ""none"")\n\nThe results revealed that the student‚Äôs age range has an effect on their average assessment score, and that effect is moderated by the student‚Äôs gender. Specifically, the student‚Äôs gender has a differential effect on their average assessment score for those students who are 55 years old or older. Male students, who are 55 years old or older, scored on average higher on assessments than female students in the same age group.\nReferences\nKuzilek J., Hlosta M., Zdrahal Z. (2017). Open University Learning Analytics dataset Sci. Data 4:170171 doi: 10.1038/sdata.2017.171.\n'], 'url_profile': 'https://github.com/lizarova777', 'info_list': ['R', 'Updated Feb 16, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jul 28, 2020', 'R', 'Updated Jan 28, 2020', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Feb 16, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jul 28, 2020', 'R', 'Updated Jan 28, 2020', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['CIS519-Machine-Learning\nUPenn Applied Machine Learning Course\nHW list:\n-HW0:\n-Escaping The Room: DFS(Depth-first Search) Algorithm + Recursion\n-Titanic Crew Data: Pandas Data Manipulation\n'], 'url_profile': 'https://github.com/YongxinJackGuo', 'info_list': ['R', 'Updated Feb 16, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jul 28, 2020', 'R', 'Updated Jan 28, 2020', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Anatomy of CEOs on Twitter\nIntroduction :\nTwitter is a microblogging and social networking site. With growing twitter users, twitter can act as platform for CEOs to directly interact with customers. So tweets could be possibly influence how stock prices change in the short term like the immediate day etc. Intrigued by this thought, we went out to analyse tweets and see how it is making a difference on the company. Also we looked at tweeting styles of CEOs from various sectors like technology, finance, airline\nTechnology :\nPython 3.7.3\nLibraries used : tweepy, spacy, genism, nltk, textblob\nApproach :\n\nScrapped Twitter accounts of 31 CEOs of top 100 FORTUNE companies using twitter API and also extracted daily stock and financial statistics from Yahoo finance for the respective companies.\nData pre-processing ‚Äì Removed stop words, bigrams, lemmatization, word-frequency filters.\nSentiment analysis - Classified CEOs based on sentiment scores of tweet ‚Äì positive, neutral and negative.\nLDA topic modeling on tweets to discover tweeting styles and topics tweeted about.\nRegressed stock price on - number of CEO attributes like (age, compensation etc), twitter attributes (tweeting style, number of tweets, number of likes, sentiment score) and company attributes (company statistics, daily returns, volume of stock traded etc.)\nEngagement analysis using logistic regression to determine types of topics which increase engagement.\n\nInsights :\n\nIt is unable to distinguish between genuine and sarcastic tweets using the method of  sentiment analysis. John Legere (T-Mobile) has a good number of sarcastic tweets directed towards his competitors but they were classified as positive.\nAaron Levie turned out to be CEO with negative tweets, but with topic modeling he is a very generous and positive person on Twitter.\nSome tweeting styles of CEOs ‚Äì\na.\tElon Musk uses his twitter account to talk about various products and uses it as a marketing platform\nb.\tTim Cook tweets about current events and talks how apple is involved to help.\nWhen CEOs actively use twitter as a platform to show side of their part from being a company CEO who is promoting products customer engagement is high.\nInfluential CEOs can actually impact stock prices through tweets.\n\nRecommendations :\n\nMore company CEOs should take advantage of twitter as a platform to connect with customers directly.\nThey should express about variety of other topics apart from their company or company products‚Äô. This way they can form an emotional connection and increase engagement\n\n'], 'url_profile': 'https://github.com/Asayesha', 'info_list': ['R', 'Updated Feb 16, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jul 28, 2020', 'R', 'Updated Jan 28, 2020', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Feb 16, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jul 28, 2020', 'R', 'Updated Jan 28, 2020', 'Updated Jan 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['ohenery\n\n\n\n\n\n\nPerforms softmax regression for ordered outcomes under the Harville\nand Henery models.\n-- Steven E. Pav, shabbychef@gmail.com\nInstallation\nThis package can be installed\nfrom CRAN,\nvia drat, or\nfrom github:\n# via CRAN:\ninstall.packages(""ohenery"")\n# via drat:\nif (require(drat)) {\n    drat:::add(""shabbychef"")\n    install.packages(""ohenery"")\n}\n# get snapshot from github (may be buggy)\nif (require(devtools)) {\n  install_github(\'shabbychef/ohenery\')\n}\nWhat is it?\n\nThe softmax regression generalizes logistic regression, wherein only one of two\npossible outcomes is observed, to the case where one of many possible outcomes is observed.\nAs in logistic regression, one models the log odds of each outcome as a linear\nfunction of some independent variables.\nMoreover, a softmax regression allows one to model data from events where the number\nof possible outcomes differs.\nSome examples where one might apply a softmax regression:\n\nModel which among five sprinters takes first place in a race,\nhaving observed characteristics of the runners over many races.\nModel which film is awarded the Academy Award for Best Picture,\nbased on genre information, and co-nomination information.\n(The number of nominees has varied over the years.)\nModel which major city in the U.S. experiences the most rain\nin a given calendar year.\n\nNote that in the examples illustrated above, one might be better served\nby modeling some continuous outcome, instead of modeling the winner.\nFor example, one might model the speed of each racer, or the number of\nvotes each film garnered, or the total amount of rain each city experienced.\nDiscarding this information in favor of modeling the binary outcome is\nlikely to cause a loss of statistical power, and is\ngenerally discouraged.\nHowever, in some cases the continuous outcome is not observed, as\nin the case of the Best Picture awards, or in horse racing where finishing\ntimes are often not available.\nSoftmax regression can be used in these cases.\nThe softmax regression can be further generalized to model the case where\none observes place information about participants. For example, one might\nobserve which of first through fifth place each sprinter claims in each race.\nOr one might only observe some limited information, like the\nGold, Silver and Bronze medal winners in an Olympic event.\nThere is more than one way to generalize the softmax to deal with ranked\noutcomes like these:\n\nThe Harville model, where the ratio of probabilities of two participants\ntaking first place is equal to the ratio of the conditional probabilities that they\ntake second place, conditional on neither of them taking first place.\nEffectively in the Harville model once one has observed the first place\nfinisher, the probabilities for second place simply rescale.\nI believe that if finishing times for racers are exponentially distributed,\nthen their finishing places are distributed under a Harville model.\nThe Henery model generalizes the Harville model to the case where the\nratio of probabilities of two participants\ntaking first place is equal to the ratio of the conditional probabilities that they\ntake second place, conditional on neither of them taking first place,\nraised to some power, called gamma.  The Harville model is the Henery model with all\ngamma constants equal to one.\nI believe that if finishing times for racers are (log) normally distributed,\nthen their finishing places are nearly distributed like the Henery model.\n\nThis package supports fitting softmax regressions under both models.\nBasic Usage\nBest Picture\nHere we use softmax regression to model the Best Picture winner odds\nas linear in some features: whether the film also was nominated for\nBest Director, Best Actor or Actress, or Best Film Editing, as well\nas genre designations for Drama, Romance and Comedy.\nWe only observe the winner of the Best Picture award, and not\nrunners-up, so we weight the first place finisher with a one,\nand all others with a zero.\nThis seems odd, but it ensures that the regression does not try\nto compare model differences between the runners-up.\nWe find the strongest absolute effect on odds from the Best Director\nand Film Editing co-nomination variables.\nlibrary(ohenery)\nlibrary(dplyr)\nlibrary(magrittr)\ndata(best_picture)\nbest_picture %<>%\n  mutate(place=ifelse(winner,1,2)) %>%\n  mutate(weight=ifelse(winner,1,0))\n\nfmla <- place ~ nominated_for_BestDirector + nominated_for_BestActor + nominated_for_BestActress + nominated_for_BestFilmEditing + Drama + Romance + Comedy\n\nosmod <- harsm(fmla,data=best_picture,group=year,weights=weight) \nprint(osmod)\n## --------------------------------------------\n## Maximum Likelihood estimation\n## BFGS maximization, 59 iterations\n## Return code 0: successful convergence \n## Log-Likelihood: -92 \n## 7  free parameters\n## Estimates:\n##                                   Estimate Std. error t value Pr(> t)    \n## nominated_for_BestDirectorTRUE       3.294      0.850    3.88 0.00011 ***\n## nominated_for_BestActorTRUE          1.003      0.316    3.18 0.00149 ** \n## nominated_for_BestActressTRUE        0.048      0.348    0.14 0.89048    \n## nominated_for_BestFilmEditingTRUE    1.949      0.399    4.88   1e-06 ***\n## Drama                               -0.659      0.605   -1.09 0.27580    \n## Romance                              0.521      0.974    0.53 0.59291    \n## Comedy                               1.317      1.013    1.30 0.19333    \n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## --------------------------------------------\n##    R2: 0.77 \n## --------------------------------------------\n\nPrediction\nHere we use the predict method to get back predictions under\nthe Harville model produced above.\nThree different types of prediction are supported:\n\nPredictions of the odds in odds space, the \'eta\'.\nPredictions of the probability of taking first place, the \'mu\'.\nThe expected rank under a Harville model.\n\nWe do not currently have the ability to compute the\nexpected rank under the Henery model, as it is too computationally\nintensive. (File an issue if this is important.)\nprd <- best_picture %>%\n  mutate(prd_erank=as.numeric(predict(osmod,newdata=.,group=year,type=\'erank\',na.action=na.pass))) %>%\n  mutate(prd_eta=as.numeric(predict(osmod,newdata=.,group=year,type=\'eta\'))) %>%\n  mutate(prd_mu=as.numeric(predict(osmod,newdata=.,group=year,type=\'mu\'))) \nHorse Racing\nThe package is bundled with a dataset of three weeks of\nthoroughbred and harness race results from tracks around the world.\nFirst, let us compare the \'consensus odds\', as computed from\nthe win pool size, with the probability of winning a race.\nThis is usually plotted as the empirical win probability\nover horses grouped by their consensus win probability.\nWe present this plot below.\nClearly visible are the \'longshot bias\' and\n\'sureshot bias\' (or favorite bias).\nThe longshot bias is the effect where longshots are overbet,\nresulting in elevated consensus odds.\nThis appears as the point off the line in the lower left.\nOne can think of these as points which were moved\nto the right of the plot from a point on the diagonal\nby overbetting.\nThe sureshot bias is the complementary effect where\nfavorites to win are underbet, effectively shifting the\npoints to the left.\nlibrary(ohenery)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\n\ndata(race_data)\n\nph <- race_data %>%\n  group_by(EventId) %>%\n    mutate(mu0=WN_pool / sum(WN_pool)) %>%\n  ungroup() %>%\n  mutate(mubkt=cut(mu0,c(0,10^seq(-2,0,length.out=14)),include.lowest=TRUE)) %>%\n  mutate(tookfirst=as.numeric(coalesce(Finish==1,FALSE))) %>%\n  group_by(mubkt) %>%\n    summarize(winprop=mean(tookfirst),\n              medmu=median(mu0),\n              nrace=length(unique(EventId)),\n              nhorse=n()) %>%\n  ungroup() %>%\n  ggplot(aes(medmu,winprop)) + \n  geom_point(aes(size=nhorse)) + \n  geom_abline(intercept=0,slope=1) + \n  scale_x_log10(labels=scales::percent) + \n  scale_y_log10(labels=scales::percent) +\n  labs(x=\'consensus odds (from win pool)\',\n       y=\'empirical win probability\',\n       title=\'efficiency of the win pool\')\nprint(ph)\n\nHere we use softmax regression under the Harville model to capture\nthe longshot and sureshot biases.\nOur model computes consensus odds as the inverse softmax\nfunction of the consensus probabilities, which are constructed\nfrom the win pool.\nWe define a factor variable that captures longshot, sureshot,\nand \'vanilla\' bets based on the win pool-implied probabilities.\nHere we are only concerned with the win probabilities,\nso we adapt a Harville model and weight only the winning\nfinishers.\n# because of na.action, make ties for fourth\ndf <- race_data %>%\n  mutate(Outcome=coalesce(Finish,4L)) \n\n# create consensus odds eta0\ndf %<>%\n  mutate(weights=coalesce(as.numeric(Finish==1),0) ) %>%\n  group_by(EventId) %>%\n    mutate(big_field=(n() >= 6)) %>%\n    mutate(mu0=WN_pool / sum(WN_pool)) %>%\n    mutate(eta0=inv_smax(mu0)) %>%\n  ungroup() %>%\n  dplyr::filter(big_field) %>%\n  dplyr::filter(!is.na(eta0)) %>%\n  mutate(bettype=factor(case_when(mu0 < 0.025 ~ \'LONGSHOT\',\n                                  mu0 > 0.50  ~ \'SURESHOT\',\n                                  TRUE        ~ \'VANILLA\'))) \n\n# Harville Model with market efficiency\neffmod <- harsm(Outcome ~ eta0:bettype,data=df,group=EventId,weights=weights)\nprint(effmod)\n## --------------------------------------------\n## Maximum Likelihood estimation\n## BFGS maximization, 44 iterations\n## Return code 0: successful convergence \n## Log-Likelihood: -6862 \n## 3  free parameters\n## Estimates:\n##                      Estimate Std. error t value Pr(> t)    \n## eta0:bettypeLONGSHOT   1.2602     0.0977    12.9  <2e-16 ***\n## eta0:bettypeSURESHOT   1.1896     0.0417    28.5  <2e-16 ***\n## eta0:bettypeVANILLA    1.1158     0.0257    43.4  <2e-16 ***\n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## --------------------------------------------\n##    R2: 0.53 \n## --------------------------------------------\n\nWe see that in this model the beta coefficient for consensus odds\nis around 1.12 for \'vanilla\' bets,\nslightly higher for sure bets, and much higher for longshots.\nThe interpretation is that long shots are relatively overbet,\nbut that otherwise bets are nearly efficient.\nOne would like to make a correction to inefficiencies in the\nwin pool by generalizing this idea to multiple cut points along the consensus\nprobabilities.\nHowever, the softmax model works by tweaking the consensus odds;\nthe odds translate into win probabilities in a way that depends on the\nother participants, and so there is no one curve.\nOffsets\nThe print display of the Harville softmax regression includes an \'R-squared\'\nand sometimes a \'delta R-squared\'.\nThe latter is only reported when an offset is used in the model.\nThe R-squared is the improvement in spread in the predicted ranks from the\nmodel compared to the null model which assumes all log odds are equal.\nWhen an offset is given, the R-squared includes the offset,\nbut a \'delta\' R-squared is reported which gives the improvement\nin spread of predicted ranks over the model based on the offset:\n# Harville Model with offset\noffmod <- harsm(Outcome ~ offset(eta0) + bettype,data=df,group=EventId,weights=weights)\nprint(offmod)\n## --------------------------------------------\n## Maximum Likelihood estimation\n## BFGS maximization, 25 iterations\n## Return code 0: successful convergence \n## Log-Likelihood: -6872 \n## 2  free parameters\n## Estimates:\n##                 Estimate Std. error t value Pr(> t)    \n## bettypeSURESHOT    0.816      0.150    5.43 5.5e-08 ***\n## bettypeVANILLA     0.437      0.126    3.46 0.00054 ***\n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## --------------------------------------------\n##    R2: 0.1 \n## delR2: -0.86 \n## --------------------------------------------\n\nUnfortunately, the R-squareds are hard to interpret, and can sometimes give\nnegative values. The softmax regression is not guaranteed to improve the\nresidual sum of squared errors in the rank.\nHenery Model\nHere we fit a Henery model to the horse race data.\nNote that in the data we only observe win, place, and show finishes,\nso we assign zero weight to all other finishers.\nAgain this is so the regression does not attempt to model distinctions between runners-up.\nMoreover, because of how na.action works on the input data,\nthe response variable has to be coalesced with tie-breakers.\n# because of na.action, make ties for fourth\ndf <- race_data %>%\n  mutate(Outcome=coalesce(Finish,4L)) %>%\n  mutate(weights=coalesce(as.numeric(Finish<=3),0) ) %>%  # w/p/s \n  group_by(EventId) %>%\n    mutate(big_field=(n() >= 5)) %>%\n    mutate(mu0=WN_pool / sum(WN_pool)) %>%\n    mutate(eta0=inv_smax(mu0)) %>%\n  ungroup() %>%\n  dplyr::filter(big_field) %>%\n  dplyr::filter(!is.na(eta0)) %>%\n  mutate(fac_age=cut(Age,c(0,3,5,7,Inf),include.lowest=TRUE)) %>%\n  mutate(bettype=factor(case_when(mu0 < 0.025 ~ \'LONGSHOT\',\n                                  mu0 > 0.50  ~ \'SURESHOT\',\n                                  TRUE        ~ \'VANILLA\'))) \n\n# Henery Model with ...\nbigmod <- hensm(Outcome ~ eta0:bettype + fac_age,data=df,group=EventId,weights=weights,ngamma=3)\nprint(bigmod)\n## --------------------------------------------\n## Maximum Likelihood estimation\n## BFGS maximization, 57 iterations\n## Return code 0: successful convergence \n## Log-Likelihood: -21720 \n## 8  free parameters\n## Estimates:\n##                      Estimate Std. error t value Pr(> t)    \n## fac_age(3,5]           0.2024     0.0697    2.90  0.0037 ** \n## fac_age(5,7]           0.1704     0.0794    2.14  0.0320 *  \n## fac_age(7,Inf]         0.1531     0.0848    1.80  0.0711 .  \n## eta0:bettypeLONGSHOT   1.5191     0.0641   23.70  <2e-16 ***\n## eta0:bettypeSURESHOT   1.1096     0.0367   30.19  <2e-16 ***\n## eta0:bettypeVANILLA    1.1107     0.0231   48.06  <2e-16 ***\n## gamma2                 0.7053     0.0215   32.84  <2e-16 ***\n## gamma3                 0.5274     0.0191   27.63  <2e-16 ***\n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## --------------------------------------------\n\nNote that the gamma coefficients are fit here as\n0.71, 0.53.\nValues around 0.8 or smaller are typical.\n(In a future release it would probably make sense to\nmake the gammas relative to the value 1,\nwhich would make it easier to reject the Harville\nmodel from the print summary.)\nDiving\nThe package is bundled with a dataset of 100 years of Olympic Men\'s\nPlatform Diving Records, originally sourced by Randi Griffin\nand delivered on\nkaggle.\nHere we convert the medal records into finishing places of 1, 2, 3 and 4 (no\nmedal), add weights for the fitting,\nmake a factor variable for age, factor the NOC (country) of the athlete.\nBecause Platform Diving is a subjective competition, based on scores from\njudges, we investigate whether there is a \'home field advantage\'\nby creating a Boolean variable indicating whether the athlete is representing\nthe host nation.\nWe then fit a Henery model to the data. Note that the gamma terms come\nout very close to one, indicating the Harville model would be sufficient.\nThe home field advantage does not appear significant in this analysis.\nlibrary(forcats)\ndata(diving)\nfitdat <- diving %>%\n  mutate(Finish=case_when(grepl(\'Gold\',Medal)   ~ 1,\n                          grepl(\'Silver\',Medal) ~ 2,\n                          grepl(\'Bronze\',Medal) ~ 3,\n                          TRUE ~ 4)) %>%\n  mutate(weight=ifelse(Finish <= 3,1,0)) %>%\n  mutate(cut_age=cut(coalesce(Age,22.0),c(12,19.5,21.5,22.5,25.5,99),include.lowest=TRUE)) %>%\n  mutate(country=forcats::fct_relevel(forcats::fct_lump(factor(NOC),n=5),\'Other\')) %>%\n  mutate(home_advantage=NOC==HOST_NOC)\n\nhensm(Finish ~ cut_age + country + home_advantage,data=fitdat,weights=weight,group=EventId,ngamma=3)\n## --------------------------------------------\n## Maximum Likelihood estimation\n## BFGS maximization, 43 iterations\n## Return code 0: successful convergence \n## Log-Likelihood: -214 \n## 12  free parameters\n## Estimates:\n##                    Estimate Std. error t value Pr(> t)    \n## cut_age(19.5,21.5]   0.0303     0.4185    0.07 0.94227    \n## cut_age(21.5,22.5]  -0.7276     0.5249   -1.39 0.16565    \n## cut_age(22.5,25.5]   0.0950     0.3790    0.25 0.80199    \n## cut_age(25.5,99]    -0.1838     0.4111   -0.45 0.65474    \n## countryGBR          -0.6729     0.8039   -0.84 0.40258    \n## countryGER           1.0776     0.4960    2.17 0.02981 *  \n## countryMEX           0.7159     0.4744    1.51 0.13126    \n## countrySWE           0.6207     0.5530    1.12 0.26172    \n## countryUSA           2.3201     0.4579    5.07 4.1e-07 ***\n## home_advantageTRUE   0.5791     0.4112    1.41 0.15904    \n## gamma2               1.0054     0.2853    3.52 0.00042 ***\n## gamma3               0.9674     0.2963    3.26 0.00109 ** \n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## --------------------------------------------\n\nConfirming Inference\nThe regression coefficients are fit by Maximum Likelihood Estimation,\nand the maxLik package does the \'heavy lifting\' of estimating\nthe variance-covariance of the coefficients.\nHowever, it is not clear a priori that these error estimates\nare accurate, since the individual outcomes in a given event\nare not independent: it could be that the machinery for\ncomputing the Fisher Information matrix is inaccurate.\nIn fact, the help from maxLik warns that this is on the user.\nThis is especially concerning when the logistic fits are\nused in situations where not all finishes are observed,\nand one uses zero weights to avoid affecting the fits.\nHere we will try to establish empirically that\nthe inference is actually correct.\nAgainst Logistic Regression\nAs the Harville model generalizes logistic regression,\nwe should be able to run a logistic regression problem\nthrough the harsm function. Here we do that,\ntesting data with exactly two participants per event.\nThe key observation is that the usual logistic regression\nshould be equivalent to the Harville form, but with\nindependent variables equal to the difference in\nindependent variables for the two participants.\nWe find that the coefficients and variance-covariance\nmatrix are nearly identical.\nWe believe the differences are due to convergence\ncriteria in the MLE fit.\nlibrary(dplyr)\nnevent <- 10000\nset.seed(1234)\nadf <- data_frame(eventnum=floor(seq(1,nevent + 0.7,by=0.5))) %>%\n  mutate(x=rnorm(n()),\n         program_num=rep(c(1,2),nevent),\n         intercept=as.numeric(program_num==1),\n         eta=1.5 * x + 0.3 * intercept,\n         place=ohenery::rsm(eta,g=eventnum))\n\n# Harville model\nmodh <- harsm(place ~ intercept + x,data=adf,group=eventnum)\n\n# the collapsed data.frame for glm\nddf <- adf %>%\n  arrange(eventnum,program_num) %>%\n  group_by(eventnum) %>%\n    summarize(resu=as.numeric(first(place)==1),\n              delx=first(x) - last(x),\n              deli=first(intercept) - last(intercept)) %>%\n  ungroup()\n\n# glm logistic fit\nmodg <- glm(resu ~ delx + 1,data=ddf,family=binomial(link=\'logit\'))\n\nall.equal(as.numeric(coef(modh)),as.numeric(coef(modg)),tolerance=1e-4)\n## [1] TRUE\n\nall.equal(as.numeric(vcov(modh)),as.numeric(vcov(modg)),tolerance=1e-4)\n## [1] TRUE\n\nprint(coef(modh))\n## intercept         x \n##      0.34      1.49\n\nprint(coef(modg))\n## (Intercept)        delx \n##        0.34        1.49\n\nprint(vcov(modh))\n##           intercept       x\n## intercept   0.00071 0.00012\n## x           0.00012 0.00091\n\nprint(vcov(modg))\n##             (Intercept)    delx\n## (Intercept)     0.00071 0.00012\n## delx            0.00012 0.00091\n\nWeighted Observations\nWe now examine whether inference is correct in the case when weights\nare used to denote that some finishes are not observed.\nThe first release of this package defaulted to using \'normalized weights\'\nin the Harville and Henery fits. I believe this leads to an\ninflated Fisher Information that then causes the estimated variance-covariance\nmatrix to be too small. I am changing the default in version 0.2.0\nto fix this issue.\nHere I generate some data, and fit to the Harville model,\nusing weights to signify that we have only observed\nthree of the 8 finishing places in each event.\nI compute the the estimate minus the true value,\ndivided by the estimated standard error. I repeat this\nprocess many times.\nAsymptotically in sample size, this computed value should be a standard normal.\nI check by taking the standard deviation, and Q-Q plotting\nagainst standard normal. The results are consistent with correct inference.\nlibrary(dplyr)\nnevent <- 5000\n# 3 of 8 observed values.\nwlim <- 3\nnfield <- 8\nbeta <- 2\nlibrary(future.apply)\nplan(multiprocess,workers=7)\nset.seed(1234)\nzvals <- future_replicate(500,{ \n  adf <- data_frame(eventnum=sort(rep(seq_len(nevent),nfield))) %>%\n\t\tmutate(x=rnorm(n()),\n\t\t\t\t\t program_num=rep(seq_len(nfield),nevent),\n\t\t\t\t\t eta=beta * x,\n\t\t\t\t\t place=ohenery::rsm(eta,g=eventnum),\n\t\t\t\t\t wts=as.numeric(place <= wlim))\n\n\tmodo <- ohenery::harsm(place ~ x,data=adf,group=eventnum,weights=wts) \n\tas.numeric(coef(modo) - beta) / as.numeric(sqrt(vcov(modo)))\n})\nplan(sequential)\n\n# this should be near 1\nsd(zvals) \n## [1] 0.98\n\n# QQ plot it\nlibrary(ggplot2)\nph <- data_frame(zvals=zvals) %>%\n\tggplot(aes(sample=zvals)) +\n\t\tstat_qq(alpha=0.4) +\n\t\tgeom_abline(linetype=2,color=\'red\') + \n\t\tlabs(title=\'putative z-values from Harville fits\')\nprint(ph)\n\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Feb 16, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jul 28, 2020', 'R', 'Updated Jan 28, 2020', 'Updated Jan 29, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['5.WifiIndoorLocation\nIoT (Internet of Things) Analytics - Detecting Wifi Indoor Locationing: Multivariate classification and regression analysis by using PCA (Principle Component Analysis),  Near Zero Variance, RFE (Recursive Feature Elimination) in R\n'], 'url_profile': 'https://github.com/sunin22', 'info_list': ['R', 'Updated Feb 16, 2020', 'R', 'Updated Jan 29, 2020', 'Updated Feb 1, 2020', 'R', 'Updated Jan 27, 2020', 'R', 'Updated Oct 6, 2020', '1', 'Jupyter Notebook', 'Updated Apr 24, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Jul 28, 2020', 'R', 'Updated Jan 28, 2020', 'Updated Jan 29, 2020']}"
"{'location': 'Delhi, India', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['TitanicDataset\nImport Libraries and read data\n\nImport all the libraries as per requirement of the task.\nRead the given dataset in whichever format it is provided, be it csv, xlsx, json or any other format.\n\nDATA PREPROCESSING\n\nFirst we must get insights on relevant or irrelevant features and then they must be taken care of.\nTaking care of string data, such as ""Sex"" column which contains values as ""male"" or ""female"", to be able to process data. It must be changed to Integer values, for example: 1 for male, 0 for female. This could be done using dummy values.\n\nFEATURE ENGINEERING\n\nFinding dependent and Independent features.\nSplit the dataset into test and train data.\n\nFIT MODEL\n\nNow, we are all set to fit the dataset to train our model.\n\nPREDICTIONS\n\nFor predictions, use testing dataset and generate predictions and other performance evaluation metrics based on these predictions.\n\n'], 'url_profile': 'https://github.com/NamitChawla', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated Sep 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'R', 'Updated Aug 3, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Sep 2, 2020', 'R', 'Updated Jul 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['ormPlot\n\n\n\nPlotting ordinal regression models from R package rms\nThe goal of ormPlot is to to extend the plotting capabilities of rms package.\nIn particular it aims to provide convenient ways of getting ggplot2 plots\nfrom orm and lrm models of the rms package.\nIt provides:\n\nprediction plots of orm models for each dependent variable level together\nwith coefficient intervals.\nforest plots of orm/lrm model summaries\ndata about schoolchildren body measurements and their family details like\nsocioeconomic status and number of siblings\n\nInstallation\nYou can install the CRAN release of ormPlot from CRAN with:\ninstall.packages(""ormPlot"")\nTo install the latest version do:\ninstall.packages(""remotes"")\nremotes::install_github(""rix133/ormPlot"")\nExamples\nvignette(""ormPlot"")\nhelp(""ormPlot"")\nSee the vigentte and/or help files:\nvignette(""ormPlot"")\nhelp(""ormPlot"")\nTo get you started:\n#load the libraries\nlibrary(rms)\nlibrary(ormPlot)\n\n#make the datadist\ndd<-datadist(educ_data)\noptions(datadist=""dd"")\n\n#create the model\ncran_model <- orm(educ_3 ~ YOBc +Rural + sex + height_rzs + n_siblings  + cran_rzs, data = educ_data)\n\n#show simply the summary plot\nforestplot(summary(cran_model))\n\n#show the predictions\nplot(cran_model, cran_rzs, plot_cols = Rural, plot_rows = sex)\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated Sep 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'R', 'Updated Aug 3, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Sep 2, 2020', 'R', 'Updated Jul 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\nHTLR: Bayesian Logistic Regression with Heavy-tailed Priors\n\n\n\n\nHTLR performs classification and feature selection by fitting Bayesian\npolychotomous (multiclass, multinomial) logistic regression models based\non heavy-tailed priors with small degree freedom. This package is\nsuitable for classification with high-dimensional features, such as gene\nexpression profiles. Heavy-tailed priors can impose stronger shrinkage\n(compared to Guassian and Laplace priors) to the coefficients associated\nwith a large number of useless features, but still allow coefficients of\na small number of useful features to stand out with little punishment.\nHeavy-tailed priors can also automatically make selection within a large\nnumber of correlated features. The posterior of coefficients and\nhyperparameters is sampled with resitricted Gibbs sampling for\nleveraging high-dimensionality and Hamiltonian Monte Carlo for handling\nhigh-correlations among coefficients.\nInstallation\nCRAN version (recommended):\ninstall.packages(""HTLR"")\nDevelopment version on GitHub:\n# install.packages(""devtools"")\ndevtools::install_github(""longhaiSK/HTLR"")\nReference\nLonghai Li and Weixin Yao (2018). Fully Bayesian Logistic Regression\nwith Hyper-Lasso Priors for High-dimensional Feature Selection.  2018,\n88:14, 2827-2851, the published\nversion,\nor arXiv version.\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated Sep 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'R', 'Updated Aug 3, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Sep 2, 2020', 'R', 'Updated Jul 18, 2020']}","{'location': 'Rochester, NY', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': [' ** Open Project Report for Details \nlink for dataset: https://www.kaggle.com/wendykan/lending-club-loan-data\nIntroduction:\nFor this project, we worked on deciding the core algorithm to analyze the lending loan risk by classifying available loan data to categorize it as good loans and bad loans. We have considered 5 statistical machine learning models to get the analysis along with their accuracy and efficiency towards the available Lending Loan Club dataset. We worked on Decision trees Classification, KNN Classification, Logistic Regression, and Random Forest Classification. After observing the overall performance of these 4 models, for this instance, we have selected Random Forest Classification as the core algorithm for the system. Further, we worked on the 5th algorithm SVM, which is not covered in the class and tried to fine-tune it to get better outcome. We then compared all algorithms with each other and selected Random Forest Classifier as the core algorithm.\nThe Goal:\nInvestment in loan lending business is financially risky without a proper system to analyze the possibility of the existing loans being a good loan or bad loans. The investors should check the historic as well as current statistics of the borrower and deduce the result to invest more money towards improving bad loans or maintaining good loans. For this herculean task, we are proposing this model based on the historic and currently available data to find out the maximum possibility of existing loans becoming a good loan or a bad loan for investors.\n'], 'url_profile': 'https://github.com/kunjan-mhaske', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated Sep 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'R', 'Updated Aug 3, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Sep 2, 2020', 'R', 'Updated Jul 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['mcp: Regression with Multiple Change Points\n\n\n\n\nmcp does regression with one or Multiple Change Points (MCP) between Generalized and hierarchical Linear Segments using Bayesian inference. mcp aims to provide maximum flexibility for analyses with a priori knowledge about the number of change points and the form of the segments in between.\nChange points are also called switch points, break points, broken line regression, broken stick regression, bilinear regression, piecewise linear regression, local linear regression, segmented regression, and (performance) discontinuity models. mcp aims to be be useful for all of them. See how mcp compares to other R packages.\nUnder the hood, mcp takes a formula-representation of linear segments and turns it into JAGS code. mcp leverages the power of tidybayes, bayesplot, coda, and loo to make change point analysis easy and powerful.\nInstall\n\n\nInstall the latest version of JAGS. Linux users can fetch binaries here.\n\n\nInstall from CRAN:\ninstall.packages(""mcp"")\nor install the development version from GitHub:\nif (!requireNamespace(""remotes"")) install.packages(""remotes"")\nremotes::install_github(""lindeloev/mcp"")\n\n\nAt a glance\nHere are some example mcp models. mcp takes a list of formulas - one for each segment. The change point(s) are the x at which data changes from being better predicted by one formula to the next. The first formula is just response ~ predictors and the most common formula for segment 2+ would be  ~ predictors (more details here).\n\nScroll down to see brief introductions to each of these, or browse the website articles for more thorough worked examples and discussions.\nBrief worked example\nFit a model\nThe following model infers the two change points between three segments.\nlibrary(mcp)\n\n# Define the model\nmodel = list(\n  response ~ 1,  # plateau (int_1)\n  ~ 0 + time,    # joined slope (time_2) at cp_1\n  ~ 1 + time     # disjoined slope (int_3, time_3) at cp_2\n)\n\n# Fit it. The `ex_demo` dataset is included in mcp\nfit = mcp(model, data = ex_demo)\nPlot and summary\nThe default plot includes data, fitted lines drawn randomly from the posterior, and change point(s) posterior density for each chain:\nplot(fit)\n\nUse summary() to summarise the posterior distribution as well as sampling diagnostics. They were simulated with mcp so the summary include the ""true"" values in the column sim and the column match show whether this true value is within the interval:\nsummary(fit)\nFamily: gaussian(link = \'identity\')\nIterations: 9000 from 3 chains.\nSegments:\n  1: response ~ 1\n  2: response ~ 1 ~ 0 + time\n  3: response ~ 1 ~ 1 + time\n\nPopulation-level parameters:\n    name match  sim  mean lower  upper Rhat n.eff\n    cp_1    OK 30.0 30.27 23.19 38.760    1   384\n    cp_2    OK 70.0 69.78 69.27 70.238    1  5792\n   int_1    OK 10.0 10.26  8.82 11.768    1  1480\n   int_3    OK  0.0  0.44 -2.49  3.428    1   810\n sigma_1    OK  4.0  4.01  3.43  4.591    1  3852\n  time_2    OK  0.5  0.53  0.40  0.662    1   437\n  time_3    OK -0.2 -0.22 -0.38 -0.035    1   834\nrhat is the Gelman-Rubin convergence diagnostic, eff is the effective sample size. You may also want to do a posterior predictive check using pp_check(fit).\nplot_pars(fit) can be used to inspect the posteriors and convergence of all parameters. See the documentation of plot_pars() for many other plotting options. Here, we plot just the (population-level) change points. They often have ""strange"" posterior distributions, highlighting the need for a computational approach:\nplot_pars(fit, regex_pars = ""cp_"")\n\nUse fitted(fit) and predict(fit) to get fits and predictions for in-sample and out-of-sample data.\nTests and model comparison\nWe can test (joint) probabilities in the model using hypothesis() (see more here). For example, what is the evidence (given priors) that the first change point is later than 25 against it being less than 25?\nhypothesis(fit, ""cp_1 > 25"")\n     hypothesis mean lower upper     p   BF\n1 cp_1 - 25 > 0 5.27 -1.81 13.76 0.917 11.1\nFor model comparisons, we can fit a null model and compare the predictive performance of the two models using (approximate) leave-one-out cross-validation (see more here). Our null model omits the first plateau and change point, essentially testing the credence of that change point:\n# Define the model\nmodel_null = list(\n  response ~ 1 + time,  # intercept (int_1) and slope (time_1)\n  ~ 1 + time            # disjoined slope (int_2, time_1)\n)\n\n# Fit it\nfit_null = mcp(model_null, ex_demo)\nLeveraging the power of loo::loo, we see that the two-change-points model is preferred (it is on top), but the elpd_diff / se_diff ratio ratio indicate that this preference is not very strong.\nfit$loo = loo(fit)\nfit_null$loo = loo(fit_null)\n\nloo::loo_compare(fit$loo, fit_null$loo)\n       elpd_diff se_diff\nmodel1  0.0       0.0\nmodel2 -7.6       4.6\n\nHighlights from in-depth guides\nThe articles on the mcp website go in-depth with the functionality of mcp. Here is an executive summary, to give you a quick sense of what mcp can do.\nAbout mcp formulas and models:\n\nParameter names are int_i (intercepts), cp_i (change points), x_i (slopes), phi_i (autocorrelation), and sigma_* (variance).\nThe change point model is basically an ifelse model.\nUse rel() to specify that parameters are relative to those corresponding in the previous segments.\nGenerate data using fit$simulate().\n\nUsing priors:\n\nSee priors in fit$prior.\nSet priors using mcp(..., prior = list(cp_1 = ""dnorm(0, 1)"", cp_1 = ""dunif(0, 45)"").\nThe default prior for change points is fast for estimation but is mathematically ""messy"". The Dirichlet prior (cp_i = ""dirichlet(1)"") is slow but beautiful.\nFix parameters to specific values using cp_1 = 45.\nShare parameters between segments using slope_1 = ""slope_2"".\nTruncate priors using T(lower, upper), e.g., int_1 = ""dnorm(0, 1) T(0, )"". mcp applies this automatically to change point priors to enforce order restriction. This is true for varying change points too.\nDo prior predictive checks using mcp(model, data, sample = ""prior"").\n\nVarying change points:\n\nSimulate varying change points using fit$simulate().\nGet posteriors using ranef(fit).\nPlot using plot(fit, facet_by = ""my_group"") and plot_pars(fit, pars = ""varying"", type = ""dens_overlay"", ncol = 3).\nThe default priors restrict varying change points to lie between the two adjacent change points.\n\nSupported families and link functions:\n\nmcp currently supports specific combinations of families (gaussian(), binomial(), bernoulli(), and poisson()) and link functions (identity, logit, probit, and log).\nUse informative priors to avoid issues when using non-default priors.\nUse binomial(link = ""logit"") for binomial change points in mcp. Also relevant for bernoulli(link = ""logit"").\nUse poisson(link = ""log"") for Poisson change points in mcp.\nGet results on the parameter scale rather than the observed scale using plot(fit, scale = ""linear"") or predict(fit, scale = ""linear"").\n\nModel comparison and hypothesis testing:\n\nDo Leave-One-Out Cross-Validation using loo(fit) and loo::loo_compare(fit1$loo, fit2$loo).\nCompute Savage-Dickey density rations using hypothesis(fit, ""cp_1 = 40"").\nLeverage directional and conditional tests to assess interval hypotheses (hypothesis(fit, ""cp_1 > 30 & cp_1 < 50"")), combined other hypotheses (hypothesis(fit, ""cp_1 > 30 & int_1 > int_2"")), etc.\n\nModeling variance and autoregression:\n\n~ sigma(1) models an intercept change in variance. ~ sigma(0 + x) models increasing/decreasing variance.\n~ ar(N) models Nth order autoregression on residuals. ~ar(N, 0 + x) models increasing/decreasing autocorrelation.\nYou can model anything for sigma() and ar(). For example, ~ x + sigma(1 + x + I(x^2)) models polynomial change in variance with x on top of a slope on the mean.\nSimulate effects and change points on sigma() and ar() using fit$simulate()\n\nGet fitted and predicted values and intervals:\n\nfitted(fit) and predict(fit) take many arguments to predict in-sample and out-of-sample values and intervals.\nForecasting with prior knowledge about future change points.\n\nTips, tricks, and debugging\n\nSpeed up fitting using mcp(..., cores = 3) / options(mcp_cores = 3), and/or fewer iterations, mcp(..., adapt = 500).\nHelp convergence along using mcp(..., inits = list(cp_1 = 20, int_2 = -3)).\nMost errors will be caused by circularly defined priors.\n\nSome examples\nmcp aims to support a wide variety of models. Here are some example models for inspiration.\nMeans\nFind the single change point between two plateaus (see how this data was simulated with mcp).\nmodel = list(\n    y ~ 1,  # plateau (int_1)\n    ~ 1     # plateau (int_2)\n)\nfit = mcp(model, ex_plateaus, par_x = ""x"")\nplot(fit)\n\nVarying change points\nHere, we find the single change point between two joined slopes. While the slopes are shared by all participants, the change point varies by id. Read more about varying change points in mcp.\nmodel = list(\n  y ~ 1 + x,          # intercept + slope\n  1 + (1|id) ~ 0 + x  # joined slope, varying by id\n)\nfit = mcp(model, ex_varying)\nplot(fit, facet_by = ""id"")\n\nSummarise the varying change points using ranef() or plot them using plot_pars(fit, ""varying""). Again, this data was simulated so the columns match and sim are added to show simulation values and whether they are inside the interval. Set the width wider for a more lenient criterion.\nranef(fit, width = 0.98)\n           name match   sim  mean   lower   upper Rhat n.eff\n cp_1_id[Benny]    OK -17.5 -18.1 -21.970 -14.877    1   895\n  cp_1_id[Bill]    OK -10.5  -7.6 -10.658  -4.451    1   420\n  cp_1_id[Cath]    OK  -3.5  -2.8  -5.634   0.027    1   888\n  cp_1_id[Erin]    OK   3.5   3.1   0.041   5.952    1  3622\n  cp_1_id[John]    OK  10.5  11.3   7.577  14.989    1  2321\n  cp_1_id[Rose]    OK  17.5  14.1  10.485  18.079    1  5150\nGeneralized linear models\nmcp supports Generalized Linear Modeling. See extended examples using binomial() and poisson(). These data were simulated with mcp here.\nHere is a binomial change point model with three segments. We plot the 95% HDI too:\nmodel = list(\n  y | trials(N) ~ 1,  # constant rate\n  ~ 0 + x,            # joined changing rate\n  ~ 1 + x             # disjoined changing rate\n)\nfit = mcp(model, ex_binomial, family = binomial())\nplot(fit, q_fit = TRUE)\n\nUse plot(fit, rate = FALSE) if you want the points and fit lines on the original scale of y rather than divided by N.\nTime series\nmcp allows for flexible time series analysis with autoregressive residuals of arbitrary order. Below, we model a change from a plateau with strong positive AR(2) residuals to a slope with medium AR(1) residuals. These data were simulated with mcp here and the generating values are in the sim column. You can also do regression on the AR coefficients themselves using e.g., ar(1, 1 + x). Read more here.\nmodel = list(\n  price ~ 1 + ar(2),\n  ~ 0 + time + ar(1)\n)\nfit = mcp(model, ex_ar)\nsummary(fit)\nThe AR(N) parameters on intercepts are named ar[order]_[segment]. All parameters, including the change point, are well recovered:\nPopulation-level parameters:\n    name match   sim    mean     lower   upper Rhat n.eff\n   ar1_1    OK   0.7   0.741  5.86e-01   0.892 1.01   713\n   ar1_2    OK  -0.4  -0.478 -6.88e-01  -0.255 1.00  2151\n   ar2_1    OK   0.2   0.145 -6.56e-04   0.284 1.01   798\n    cp_1       120.0 117.313  1.14e+02 118.963 1.05   241\n   int_1        20.0  17.558  1.51e+01  19.831 1.02   293\n sigma_1    OK   5.0   4.829  4.39e+00   5.334 1.00  3750\n  time_2    OK   0.5   0.517  4.85e-01   0.553 1.00   661\nThe fit plot shows the inferred autocorrelated nature:\nplot(fit_ar)\n\nVariance change and prediction intervals\nYou can model variance by adding a sigma() term to the formula. The inside sigma() can take everything that the formulas outside do. Read more in the article on variance. The example below models two change points. The first is variance-only: variance abruptly increases and then declines linearly with x. The second change point is the stop of the variance-decline and the onset of a slope on the mean.\nEffects on variance is best visualized using prediction intervals. See more in the documentation for plot.mcpfit().\nmodel = list(\n  y ~ 1,\n  ~ 0 + sigma(1 + x),\n  ~ 0 + x\n)\nfit = mcp(model, ex_variance, cores = 3, adapt = 5000, iter = 5000)\nplot(fit, q_predict = TRUE)\n\nQuadratic and other exponentiations\nWrite exponents as I(x^N). E.g., quadratic I(x^2), cubic I(x^3), or some other power function I(x^1.5). The example below detects the onset of linear + quadratic growth. This is often called the BLQ model (Broken Line Quadratic) in nutrition research.\nmodel = list(\n  y ~ 1,\n  ~ 0 + x + I(x^2)\n)\nfit = mcp(model, ex_quadratic)\nplot(fit)\n\nTrigonometric and others\nYou can use sin(x), cos(x), and tan(x) to do trigonometry. This can be useful for seasonal trends and other periodic data. You can also do exp(x), abs(x), log(x), and sqrt(x), but beware that the two latter will currently fail in segment 2+. Raise an issue if you need this.\nmodel = list(\n  y ~ 1 + sin(x),\n  ~ 1 + cos(x) + x\n)\n\nfit = mcp(model, ex_trig)\nplot(fit)\n\nUsing rel() and priors\nRead more about formula options and priors.\nHere we find the two change points between three segments. The slope and intercept of segment 2 are parameterized relative to segment 1, i.e., modeling the change in intercept and slope since segment 1. So too with the second change point (cp_2) which is now the distance from cp_1.\nSome of the default priors are overwritten. The first intercept (int_1) is forced to be 10, the slopes are in segment 1 and 3 is shared. It is easy to see these effects in the ex_rel_prior dataset because they violate it somewhat. The first change point has to be at x = 20 or later.\nmodel = list(\n  y ~ 1 + x,\n  ~ rel(1) + rel(x),\n  rel(1) ~ 0 + x\n)\n\nprior = list(\n  int_1 = 10,  # Constant, not estimated\n  x_3 = ""x_1"",  # shared slope in segment 1 and 3\n  int_2 = ""dnorm(0, 20)"",\n  cp_1 = ""dunif(20, 50)""  # has to occur in this interval\n)\nfit = mcp(model, ex_rel_prior, prior, iter = 10000)\nplot(fit)\n\nComparing the summary to the fitted lines in the plot, we can see that int_2 and x_2 are relative values. We also see that the ""wrong"" priors made it harder to recover the parameters used to simulate this data (match and sim columns):\nsummary(fit)\nPopulation-level parameters:\n    name match   sim  mean  lower upper Rhat n.eff\n    cp_1    OK  25.0 23.15  20.00 25.81 1.00   297\n    cp_2        40.0 51.85  47.06 56.36 1.02   428\n   int_1        25.0 10.00  10.00 10.00  NaN     0\n   int_2    OK -10.0 -6.86 -21.57 11.89 1.03   190\n sigma_1         7.0  9.70   8.32 11.18 1.00  7516\n     x_1         1.0  1.58   1.24  1.91 1.07   120\n     x_2    OK  -3.0 -3.28  -3.61 -2.96 1.04   293\n     x_3         0.5  1.58   1.24  1.91 1.07   120\nDo much more with the MCMC samples\nDon\'t be constrained by these simple mcp functions. fit$samples is a regular mcmc.list object and all methods apply. You can work with the MCMC samples just as you would with brms, rstanarm, jags, or other samplers using the always excellent tidybayes:\nlibrary(tidybayes)\n\n# Extract all parameters:\ntidy_draws(fit$samples) %>%\n  # tidybayes stuff here\n\n# Extract some parameters:\nfit$pars$model  # check out which parameters are inferred.\nspread_draws(fit$samples, cp_1, cp_2, int_1, year_1) %>%\n # tidybayes stuff here\nIt may be convenient to use fitted(fit, summary = FALSE) or predict(fit, summary = FALSE) which return draws in tidybayes format, extended with additional columns for fits and predictions. For example:\nhead(fitted(fit, summary = FALSE))\nCitation\nThis preprint formally introduces mcp. Find citation info at the link, call citation(""mcp"") or copy-paste this into your reference manager:\n  @Article{,\n    title = {mcp: An R Package for Regression With Multiple Change Points},\n    author = {Jonas Kristoffer Lindel√∏v},\n    journal = {OSF Preprints},\n    year = {2020},\n    doi = {10.31219/osf.io/fzqxv},\n    encoding = {UTF-8},\n  }\n\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated Sep 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'R', 'Updated Aug 3, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Sep 2, 2020', 'R', 'Updated Jul 18, 2020']}","{'location': 'DKI Jakarta, Indonesia', 'stats_list': [], 'contributions': '6,006 contributions\n        in the last year', 'description': ['Arga Ghulam Ahmad - 1606821601- Computer Science 2016 - PMPL B - Practice Repo\nApplication Information\nThis application has deployed at http://pmpl-arga.herokuapp.com/\nExercises Stories\nExercise 3 Story\nHow to Implement Test Isolation on Functional Test\nPada exercise 2, Functional test menguji aplikasi yang sudah dideploy di heroku.\nDengan demikian django akan meninggalkan item ToDo baru pada aplikasi heroku setiap functional test dijalankan.\nTentunya, hal itu mengganggu hasil test berikutnya. Oleh karena itu dibutuhkan suatu solusi agar antar aktivitas testing tidak saling mengganggu.\nSalah satu solusi yang dapat diterapkan untuk mengatasi masalah tersebut adalah dengan Test Isolation.\nDengan Test Isolation, Django akan menjalankan proses testing dengan membuat database pengujian baru (terpisah dari yang asli).\nDengan demikian, proses testing dapat dijalankan dengan aman,\nLalu bagaimana cara menerapkan Test Isolation pada exercise 3?\n\nKarena Saya menggunakan selenium dalam melakukan functional test, Saya memanfaatkan LiveTestServerCase\n\nLiveTestServerCase merupakan fitur yang disediakan django yang membantu melakukan functional test. Fitur ini secara otomatis meluncurkan server Django langsung di latar belakang, membuat database uji (seperti yang dilakukan oleh unit test), melakukan proses testing, dan mematikannya pada saat teardown.\nLiveTestServerCase memungkinkan penggunaan klien uji otomatis (misalnya Selenium) untuk melakukan serangkaian uji fungsional di dalam browser dan mensimulasikan tindakan pengguna nyata.\nKeuntungan yang diperoleh dengan menggunakan LiveTestServerCase adalah Saya tidak perlu menulis manual url aplikasi yang diuji.\nLiveTestServerCase akan memberikan url dari server django yang telah diluncurukan di latar belakang.\nURL tersebut dapat diakses melalu variabel self.live_server_url selama proses tes berlangsung.\n\n\n\nDifferences Between Old Design (Exercise 2) with New Design (Exercise 3) to Achieve Test Isolation\nUse LiveServerTestCase instead of unittest.TestCase\nOld Design (Exercise 2):\nimport unittest\n\nclass NewVisitorTest(unittest.TestCase):\nNew Design (Exercise 3):\n# Change the class that use by NewVisitorTest for functional testing\nfrom django.test import LiveServerTestCase\n\nclass NewVisitorTest(LiveServerTestCase):\nDelete the code that take the HEROKU_APP_HOST from environment variables\nOld Design (Exercise 2):\nimport environ\n\nroot = environ.Path(__file__)\nenv = environ.Env(DEBUG=(bool, False), )\nenviron.Env.read_env(\'.env\')\n\nHEROKU_APP_HOST = env(""HEROKU_APP_HOST"")\nprint(""HEROKU_APP_HOST is"", HEROKU_APP_HOST)\nNew Design (Exercise 3):\n# Delete the old design code\nTest the app with new database generated using LiveServerTestCase instead of deployed one\nOld Design (Exercise 2):\nself.browser.get(HEROKU_APP_HOST)\nNew Design (Exercise 3):\nself.browser.get(self.live_server_url)\n\nExercise 4 Story\nCorrelation between code changes in this exercise with chapter 7\nRegression Test\nPada exercise 4, Saya belajar bagaimana bekerja secara incremental dan membuat tes regresi.\nBekerja secara incremental mmerupakan cara bekerja yang dianjurkan oleh Test Driven Development.\nBekerja secara incremental adalah memecah pekerjaan menjadi pekerjaan-pekerjaan kecil yang dapat dikerjakan dengan mudah.\nSelain bekerja secara incremental, Sata juga belajar bagaimana membuat tes regresi.\nTes regresi merupakan salah satu jenis tes yang mengonfirmasi bahwa perubahan kode tidak mempengaruhi fitur yang sudah ada.\nTes regresi menjalankan semua atau sebagian kode yang sudah ada untuk memastikan fungsionalitas yang ada berfungsi dengan baik.\nPengujian ini dilakukan untuk memastikan bahwa perubahan kode baru seharusnya tidak menyebabkan efek samping pada fungsi yang sudah ada.\nPengujian ini memastikan bahwa kode yang lama masih bekerja dengan baik setelah ada penambahan kode/perubahan kode.\nPada exercise ini, Kali ini, Saya mengembangkan fitur TODO dengan desain arsitektur baru.\nDimana, setiap pengunjung dapat membuat TODO yang berbeda dengan pengunjung yang lainnya.\nUntuk dapat menerapkan desain baru tersebut.\nSaya membagi pekerjaan tersebut menjadi pekerjaan-pekerjaan kecil yaitu menyesuaikan model sehingga item dikaitkan dengan daftar yang berbeda, menambahkan URL unik untuk setiap daftar, menambahkan URL untuk membuat daftar baru melalui POST, dan menambahkan URL untuk menambahkan item baru ke daftar yang ada melalui POST.\nPrettification\nSebelum exercise 4, tampilan aplikasi TODO list ini masuh jelek. Pada exercise 4, Saya mulai menata kembali style aplikasi ini, mengintergasikan HTML dengan bootstrap,\nmengonfigurasi static files, menggunakan CSS preprocessor, dan menguji style aplikasi menggunakan functional testing.\nDengan fungsional testing, Saya berhasil menguji tata letak dan gaya aplikasi. Namun, telebih dahulu Saya mempretifikasi aplikasi dengan menggunakan CSS.\nSelain CSS saya juga mencoba SCSS. SCSS merupakan tools yang memungkinkan untuk memprogram CSS dan mengkompilenya menjadi CSS. Untuk mengkompile SCSS menjadi CSS,\nSaya menggunakan file watcher yang disediakan oleh PyCharm. Untuk mempermudah mempretifikasi aplikasi ini, Saya mengunduh bootstrap dan menggunakannya sebagai static files.\nStatic files dapat digunakan dengan menjalankan perintah python manage.py collectstatic terlebih dahulu. Kemudian, menggunakannya di file html dengan keyword static.\nSelain mempretifikasi aplikasi, Saya juga merefactor templates yang sebelumnya tidak modular dan berada pada satu file saja. Sekarang, templates sudah bersifat modular dan reusable.\nTerdapat tiga file html pada templates app list yaitu base, home, dan lists. Pada ketiga file html tersebut, Saya menintegrasikan bootstrap. Dengan bootstrap, Saya dapat menggunakan class row\ndan column. Dengan demikian layout aplikasi ini lebih indah untuk dipandang. Selain row dan column. Selain row dan column, Saya juga menggunakan\njumbotron, table, dan input yang disediakan oleh bootstrap. Walaupun Saya menggunakan bootstrap, Saya juga menulis CSS sendiri, yaitu base.css. Walaupun base.css Saya tulis ulang sebagai SCSS.\nbase.scss Saya gunakan untuk mengubah warna belakang jumbotron menjadi hijau dan warna tulisan menjadi putih.\nKesimpulan yang Saya peroleh setelah mengerjakan exercise 4 adalah programmer tidak harus menulis tes untuk menguji desain dan tata letak.\nKarena desain dan tata letak merupakan konstanta, dan tes ini sering mengalami perubahan. Setidaknya, programmer harus menulis tes minimal sehingga\ndesain dan tata letak yang programmer kembangkan berfungsi tanpa menguji apa itu sebenarnya. Usahakan agar programmer dapat dengan bebas membuat perubahan\npada desain dan tata letak, tanpa harus kembali dan menyesuaikan tes setiap saat.\n\nExercise 5 Story\nRelevance between clean code and test driven development\nPada exercise 5 dan exercise-exercise sebelumnya, Saya sudah menerapkan Test Driven Development dan refactoring.\nDalam praktik TDD, kita diwajibkan wajib menuliskan test terlebih dahulu kemudian menulis berkas sumber kode.\nDengan demikian kita tidak bingung dalam mengimplementasikan fitur yang akan dikembangkan.\nTahap-tahap TDD adalah menulis tes yang gagal, menulis minimal working code sehingga test pass, kemudian refactor minimal working code (remove duplication).\nTujuan utama test driven development (TDD) dan refactoring adalah untuk mencegah (membayar, bila ada) technical debt.\nTechnical debt dapat berupa tidak ada/kurangnya tes otomatis, tidak ada deployment otomatis, tidak ada nya dokumentasi, adanya code smell, software defects, desain yang buruk, dan lain lain.\nDengan adanya TDD dan refactoring, jumlah technical debt dapat ditekan atau dihilangkan. Sehingga, kode dapat digolongkan menjadi clean code.\nClean code memiliki beberapa karakteristik yaitu tidak mengandung code smell (bersih, jelas, dan tidak mengandung duplikasi) , high cohesion. lulus semua tes, dokumentasi berupa test, dan lebih mudah dan murah untuk dikembangkan.\nAdvantages applying test organizations\nKendala yang Saya hadapi pada exercise-exercise sebelumnya adalah semakin sulit untuk menulis test (functional atau unit tests) karena semua tes hanya ditulis dalam satu file saja.\nOleh karena itu, Saya perlu menerapkan test organizations. Test organizations (bisa disebut juga mengorganisir test) adalah suatu cara untuk membagi test menjadi file test-test yang lebih kecil.\nSeperti yang dapat dilihat pada source code, Saya memecah functional test menjadi test_home_page.py,  test_layout_and_styling.py,  test_list_item_validation.py, dan test_simple_list_creation.py.\nSelain itu, Saya juga memecah unit test app lists menjadi test_home_page.py,  test_list_and_item_model.py,  test_list_view.py,  test_new_item.py, dan test_new_list.py.\nDengan demikian, Saya dapat dengan mudah menulis test karena setiap test sudah dibagi dan dibuat modular sesuai dengan fungsionalitas yang ingin diuji.\n\nExercise 6 Story\nCreate two mutant from function in feedback todo feature views\nSebelum membuat mutant untuk fungsi pada views fitur feedback todo,\nmari lihat bagaimana Saya mengimpelementasikan fungsionalitas fitur feedback todo.\nDi bawah ini adalah source code views yang bertanggung jawab untuk memberikan data list todo ke frontend.\nlists/views.py\n\ndef view_list(request, list_id):\n    list_ = List.objects.get(id=list_id)\n    return render(request, \'list.html\', {\'list\': list_})\nDi bawah ini adalah source code template html yang menampilkan todo dan memberikan umpan balik berdasarkan jumlah todo.\nlists/templates/list.html\n\n{% block todo_feedback %}\n    <h5>Jumlah To-Do: {{ list.item_set.all|length }}</h5>\n    <h6 id=""todo-feedback"">\n        {% if list.item_set.all|length == 0 %}\n            Yeahh, tidak ada tugas. Main game ahh.\n        {% elif list.item_set.all|length < 5 %}\n            Kerjain ahhh, biar cepat kelar.\n        {% elif list.item_set.all|length >= 5 %}\n            Oh tidakk, kerjaan ku banyak.\n        {% endif %}\n    </h6>\n{% endblock %}\nDapat dilihat source code di atas,\nyang bertanggung jawab untuk memutuskan umpan balik dari jumlah todo adalah\nlists/templates/list.html. Oleh karena itu, Saya membuat dua mutant pada source code html tersebut.\nBerikut source code template html di mana Saya membuat kedua mutant yaitu\nlists/templates/list.html\n\n{% block todo_feedback %}\n    <h5>Jumlah To-Do: {{ list.item_set.all|length }}</h5>\n    <h6 id=""todo-feedback"">\n        {% if list.item_set.all|length >= 0 %}\n            Yeahh, tidak ada tugas. Main game ahh.\n        {% elif list.item_set.all|length == 5 %}\n            Kerjain ahhh, biar cepat kelar.\n        {% elif list.item_set.all|length >= 5 %}\n            Oh tidakk, kerjaan ku banyak.\n        {% endif %}\n    </h6>\n{% endblock %}\nDua mutant yang Saya buat adalah\n    {% if list.item_set.all|length >= 0 %}\ndan\n    {% elif list.item_set.all|length == 5 %}\nKedua mutant diperoleh dengan menerapkan relational operator replacement (ROR).\nMutant pertama diperoleh dengan mengganti operator == dengan >=.\nMutant kedua diperoleh dengan mengganti operator < dengan ==.\nWhether the tests have successfully strongly killed both mutants that have been made?\nSebelum memeriksa apakah kedua mutant yang sudah dibuat berhasil dibunuh secara kuat oleh\ntest case. Mari kita baca definisi dari membunuh secara kuat mutant yang diperoleh dari slide\npendahuluan pengujian perangkat lunak oleh Ammmann.\nMembunuh secara kuat adalah \n\nDiberikan sebuah mutant m dari sebuah program P dan test t, t dikatakan membunuh secara kuat m\njika dan hanya jika output dari t pada p berbeda dengan output t pada m.\n\n\nPada commit 3a3ede8885cdc640de058a1094e665130f91045a dengan commit message\nCreate two mutant for the todo list feedback feature Using relational operator replacemen,\nSaya sudah berhasil membuat dua mutant untuk fitur feedback todo dan Saya juga sudah berhasil\nmembuat test case yang berhasil membunuh secara kuat dua mutant tersebut.\nBerikut test case yang Saya buat untuk membunuh secara kuat kedua mutant tersebut yaitu\nfunctional_tests/test_list_item_validation.py\n    \ndef test_multiple_users_can_start_lists_at_different_urls(self):\n    # Arga starts a new to-do list\n    self.browser.get(self.server_url)\n    inputbox = self.browser.find_element_by_id(\'id_new_item\')\n    inputbox.send_keys(\'Buy peacock feathers\')\n    inputbox.send_keys(Keys.ENTER)\n    self.check_for_row_in_list_table(\'1: Buy peacock feathers\')\n\n    # He notices that Him list has a unique URL\n    edith_list_url = self.browser.current_url\n    self.assertRegex(edith_list_url, \'/lists/.+\')\n\n    # He want to know what the system\'s feedback when there is a to-do\n    todo_feedback = self.browser.find_element_by_id(""todo-feedback"")\n    self.assertEqual(""Kerjain ahhh, biar cepat kelar."", todo_feedback.text)\n\n    # He is invited to enter some to-do items, to check what the system feedback after he input 5 to-do items in\n    # total\n    for i in range(4):\n        inputbox = self.browser.find_element_by_id(\'id_new_item\')\n        inputbox.send_keys(\'Another todo\' + str(i))\n        inputbox.send_keys(Keys.ENTER)\n\n    todo_feedback = self.browser.find_element_by_id(""todo-feedback"")\n    self.assertEqual(""Oh tidakk, kerjaan ku banyak."", todo_feedback.text)\nBagaimana Saya tahu bahwa test case yang dibuat sudah berhasil membunuh secara kuat mutant tersebut?\nDapat dilihat status pipeline commit ini di gitlab adalah failed, di mana functional test-nya gagal.\nDengan functional test fitur feedback todo yang gagal,\ndapat disimpulkan bahwa output test case (t) fitur feedback todo pada program (p)\nberbeda dengan output test case (t) pada kedua mutant (t).\nJadi, Saya sudah berhasil membuat dua mutant dan test case yang membunuh secara kuat kedua mutant tersebut.\nInstall the django_mutpy and check the result\nSebelum dapat menggunakan django mutpy terlebih dahulu Saya menginstall package tersebut menggunakan pip\ndan menambahkan package tersebut sebagai installed apps.\n# Add the django mutpy yo requirements.txt\n\npip install django-mutpy\npip freeze > requirements.txt\n# Application definition\n\nINSTALLED_APPS = [\n    # \'django.contrib.admin\',\n    \'django.contrib.auth\',\n    \'django.contrib.contenttypes\',\n    \'django.contrib.sessions\',\n    \'django.contrib.messages\',\n    \'django.contrib.staticfiles\',\n    \'django_mutpy\',\n    \'lists\'\n]\nKemudian, Saya menjalankan mutation testing untuk pertama kali menggunakan\ndjango mutpy.\npython manage.py muttest lists\nBerikut merupakan hasil penggunaan django mutpy pada app lists untuk pertama kali.\n[*] Mutation score [7.64198 s]: 49.2%\n   - all: 63\n   - killed: 31 (49.2%)\n   - survived: 32 (50.8%)\n   - incompetent: 0 (0.0%)\n   - timeout: 0 (0.0%) \n\nDilihat dari hasil menjalankan mutation test menggunakan mutpy untuk pertama kali,\nSaya berhasil membunuh 31 mutant dengan mutation score yang diperoleh adalah 49.2%.\nBerdasarkan hasil penggunaan django mutpy pada app lists, untuk pertama kali,\ndapat disimpulkan bahwa semua fungsi yang ada pada source code di lists/models.py, lists/urls.py, dan lists/views.py\nsudah berhasil dibunuh oleh test case yang ada pada lists/tests.py.\nFungsi yang survived adalah fungsi yang berada pada source code di folder  lists/migrations,\ndi mana source code tersebut merupakan hasil manage.py makemigrations dari source code yang ada pada\nlists/models.py. Selain itu juga ada source code lain yang survived yaitu lists/apps.py.\nOleh karena itu, Saya membuat unit test untuk menguji lists/apps.py.\nBerikut unit test yang Saya tulis untuk menguji lists/apps.py yaitu\nlists/tests/test_apps.py\n\nfrom django.apps import apps\nfrom django.test import TestCase\nfrom lists.apps import ListsConfig\n\n\nclass ListsConfigTest(TestCase):\n    def test_apps(self):\n        self.assertEqual(ListsConfig.name, \'lists\')\n        self.assertEqual(apps.get_app_config(\'lists\').name, \'lists\')\nSetelah Saya selesai mengimplementasikan unit test untuk menguji lists/apps.py,\nSaya pun menjalankan mutation test menggunakan mutpy untuk kedua kalinya.\nBerikut hasilnya yaitu\n[*] Mutation score [8.09359 s]: 52.4%\n   - all: 63\n   - killed: 33 (52.4%)\n   - survived: 30 (47.6%)\n   - incompetent: 0 (0.0%)\n   - timeout: 0 (0.0%)\n\nDilihat dari hasil menjalankan mutation test menggunakan mutpy untuk kedua kalinya,\nSaya berhasil membunuh 33 mutant dengan mutation score yang diperoleh adalah 52.4%.\nJadi Saya berhasil merefactor unit test untuk memperbaiki kualitas yang Saya miliki\nberdasarkan hasil penggunaan mutation testing tool django mutpy.\n\nExercise 7 Story\nMy Experience on applying spiking and de-spiking technique\nPada exercise 7 Saya mempelajari teknik bagaimana agar dapat belajar suatu application programming interface dengan bebas tetapi tetap menerapkan test driven development. Teknik tersebut dinamakan dengan spiking dan de-spiking.\nTeknik spiking merupakan suatu teknik untuk mengeksplorasi suatu API tanpa menerapkan test driven development (TDD) dan menulis unit test. Sebagai contoh, Saya belajar bagaimana menggunakan API untuk mengirim email smtp menggunakan library Django mail dan otentikasi menggunakan library Django auth tanpa menerapkan test driven development. Teknik spiking tersebut Saya lakukan dengan membuat branch baru (exercise-7-1) dari branch master, implementasi kode pada exercise-7-1 (tahap spiking), kemudian kembali ke branch master saat de-spiking. Sebelum de-spiking, saya membuat fungsional tes dari hasil implementasi teknik spiking agar dapat digunakan pada tahap de-spiking. Pada tahap de-spiking, Saya kembali ke branch master dan membuat branch baru (exercise-7-2), kemudian Saya membuang kode yang dibuat pada tahap spiking dan mengimplementasikan fitur dari awal lagi dengan menerapkan test driven development.\nKeuntungan yang Saya peroleh dengan menggunakan teknik spiking dan de-spiking adalah Saya dapat membuat fungsional test pada teknik spiking yang mana fungsional test tersebut dapat dijadikan acuan pada teknik de-spiking. Dengan demikian, pada proses implementasi di tahap de-spiking Saya dapat fokus menulis unit test dan implementasi kode yang lebih baik, efektif dan efisien daripada kode yang ditulis pada tahap spiking.\n\nExercise 8 Story\nDifferences between manual mocking with mocking using library\nMocking merupakan aktivitas mensimulasikan objek dengan meniru perilaku objek nyata dengan cara yang terkontrol. Salah satu penerapan mocking adalah untuk menguji dependencies eksternal. Pada tutorial ini, Saya menggunakan mocking untuk menguji fungsi layanan pengirim email. Mocking yang Saya praktikan ada dua macam yaitu mocking manual dan mocking menggunakan library. Mocking dapat dilakukan dengan dua cara yaitu mocking secara manual dan mocking menggunakan library.\nKarena bahasa pemrograman python merupakan bahasa pemrograman dinamis. Kita dapat memanfaatkan sifat dinamisnya untuk melakukan mocking secara manual. Yang dimaksud dengan mocking manual adalah mereplace fungsi yang ingin di-mock menggunakan fungsi baru palsu. Fungsi baru palsu ini dibuat dengan membuat variabel mocking di dalam fungsi tersebut. Fungsi tersebut akan berperilaku seperti mutable objek yang berisi daftar variabel, dimana bila ada perubahan input variabel maka isi daftar variable juga berubah.\nNamun, melakukan mocking manual membutuhkan upaya yang cukup banyak. Oleh karena itu, Kita dapat memanfaatkan library mocking python yaitu unittest.mock. Library ini dapat melakukan mocking pemanggilan fungsi atau atribut apapun. Dimana, nilainya dapat dikonfigurasikan secara spesifik. Untuk memocking suatu object kita tinggal menambahkan decorator function yaitu patch. Dimana kita dapat mengotomatisasikan mocking manual dengan menggunakan  decorator function ini.\nWhy mocking can make the implementation that we make tightly coupled\nWalaupun, mocking membantu pengembang perangkat lunak untuk menguji dependencies, mocking dapat membuat implementasi yang kita buat tightly coupled. Terlebih apabila kita menguji detail implementasi dibandingkan perilaku. Sebagai contoh fungsi unit test test_adds_success_message:\ndef test_adds_success_message(self):\n    response = self.client.post(\'/accounts/send_login_email\', data={\n        \'email\': \'edith@example.com\'\n    }, follow=True)\n\n    message = list(response.context[\'messages\'])[0]\n    self.assertEqual(\n        message.message,\n        ""Check your email, we\'ve sent you a link you can use to log in.""\n    )\n    self.assertEqual(message.tags, ""success"")\nApa yang terjadi pada accounts/views.py apabila kita menguji fungsionalitas menambahkan pesan sukses dengan mocking? Kita dapat me-mock modul messages dan mengecek atribut success pada messages dengan parameter request asli, dan pesan yang kita inginkan. Kurang lebih seperti ini nantinya:\nfrom django.contrib import messages\nfrom django.core.mail import send_mail\nfrom django.shortcuts import redirect\nfrom django.urls import reverse\n\nfrom accounts.models import Token\n\ndef send_login_email(request):\n    email = request.POST[\'email\']\n    token = Token.objects.create(email=email)\n    url = request.build_absolute_uri(\n        reverse(\'login\') + \'?token=\' + str(token.uid)\n    )\n    message_body = f\'Use this link to log in:\\n\\n{url}\'\n    send_mail(\n        \'Your login link for Superlists\',\n        message_body,\n        \'noreply@superlists\',\n        [email]\n    )\n    messages.add_message(\n        request,\n        messages.SUCCESS,\n        ""Check your email, we\'ve sent you a link you can use to log in.""\n    )\n    return redirect(\'/\')\nBila diperhatikan, ada sesuatu implementasi yang berbeda dari implementasi accounts/views.py (mocking) yang Saya terapkan yaitu\nmessages.add_message(\n    request,\n    messages.SUCCESS,\n    ""Check your email, we\'ve sent you a link you can use to log in.""\n)\nPadahal implementasi tersebut dapat dicapai dengan source code (non mocking) berikut\nmessages.success(\n    request,\n    ""Check your email, we\'ve sent you a link you can use to log in.""\n)\nApabila kita mengeksekusi unit test. Tes mock akan berhasil, tetapi tidak dengan tes non mock. Tes non mock akan gagal. Meskipun hasil akhirnya sama, implementasi benar, dan unit test non mock rusak.\nInilah akibat apabila kita tidak berhati-hati dalam memutuskan apakah suatu fungsionalitas harus diuji menggunakan mocking atau tidak. Apabila kita salah memutuskan apakah mocking diperlukan untuk menguji suatu fungsionalitas, implementasi yang kita buat tightly coupled dengan unit test yang dibuat. Masalah ini merupakan hal yang harus dihindari. Oleh karena itu, Kita harus bijak dalam memilih apakah suatu fungsionalitas harus diuji menggunakan mocking atau tidak.\n\nExercise 9 Story\nDifferences between functional implementation between sub bab 20.1 with sub bab 18.3\nPada functional test subbab 20.1, Saya menulis sebuah fungsi di file test_my_lists.py yang digunakan untuk men-skip semua proses login pada saat melakukan pengujian fungsional. Fungsi tersebut bernama create_pre_authenticated_session.\nclass MyListsTest(FunctionalTest):\n\n    def create_pre_authenticated_session(self, email):\n        user = User.objects.create(email=email)\n        session = SessionStore()\n        session[SESSION_KEY] = user.pk\n        session[BACKEND_SESSION_KEY] = settings.AUTHENTICATION_BACKENDS[0]\n        session.save()\n\n        # to set a cookie we need to first visit the domain.\n        # 404 pages load the quickest!\n\n        self.browser.get(self.live_server_url + ""/404_no_such_url/"")\n        self.browser.add_cookie(dict(\n            name=settings.SESSION_COOKIE_NAME,\n            value=session.session_key,\n            path=\'/\',\n        ))\nCara kerja fungsi ini adalah membuat ulang session object pada database testing. Session object berisi session key yang merupakan primary key dari objek user. Kemudian, fungsi tersebut menambahkan cookie ke browser. Dengan cookie tersebut, browser akan menyimpan informasi bahwa proses functional test ini merupakan pengguna yang sudah login. Oleh karena itu, fungsi ini memungkinkan proses login functional test dilakukan hanya sekali saja untuk semua test. Berbeda dengan functional test subbab 18.3 yang melakukan proses login pada setiap test.\nWhy sub bab 20.1 login functional test implementation is better than sub bab 18.3 login functional test implementation?\nDengan implementasi subbab 20.1 functional test untuk fitur login yaitu proses login functional test dilakukan hanya sekali saja untuk semua test, diperoleh keuntungan yaitu menghemat waktu yang diperlukan untuk menjalankan proses functional test. Berbanding terbalik dengan, subbab 18.3 yang menghabiskan banyak waktu untuk setiap test untuk proses login saja.\nObserve the results of the deployment for the new implementation. Did the error occur as described in sub bab 21.1?\nError yang dijelaskan pada subbab 21.1 tidak terjadi di hasil deployment Saya yang baru. Kesimpulan ini Saya peroleh dengan melakukan instruksi pada sub bab 21.2. Untuk memeriksa apakah error yang dijelaskan pada subbab 21.1 muncul pada hasil deployment Saya yang baru, Saya memeriksa log aplikasi melalui heroku log pada dashboard heroku. Tepatnya pada pada dashboard heroku aplikasi pmpl-arga. Berikut merupakan isi dari heroku log tersebut.\n2019-11-25T09:43:13.951175+00:00 app[web.1]: 10.5.206.253 - - [25/Nov/2019:09:43:13 +0000] ""GET /static/bootstrap/css/bootstrap.min.css HTTP/1.1"" 304 0 ""http://pmpl-arga.herokuapp.com/"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36""\n2019-11-25T09:43:13.962569+00:00 app[web.1]: 10.35.212.74 - - [25/Nov/2019:09:43:13 +0000] ""GET /static/base.css HTTP/1.1"" 304 0 ""http://pmpl-arga.herokuapp.com/"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36""\n2019-11-25T09:43:13.962585+00:00 heroku[router]: at=info method=GET path=""/static/base.css"" host=pmpl-arga.herokuapp.com request_id=f9e7e6fd-e993-40a2-b41a-e3f401c0a487 fwd=""180.252.203.230"" dyno=web.1 connect=0ms service=3ms status=304 bytes=194 protocol=http\n2019-11-25T09:43:29.613007+00:00 app[api]: Starting process with command `bash` by user argaghulamahmad@gmail.com\n2019-11-25T09:43:35.193265+00:00 heroku[run.9200]: Awaiting client\n2019-11-25T09:43:35.214047+00:00 heroku[run.9200]: Starting process with command `bash`\n2019-11-25T09:43:35.312117+00:00 heroku[run.9200]: State changed from starting to up\n2019-11-25T09:44:19.922892+00:00 heroku[run.9200]: Client connection closed. Sending SIGHUP to all processes\n2019-11-25T09:44:20.466045+00:00 heroku[run.9200]: State changed from up to complete\n2019-11-25T09:44:20.453497+00:00 heroku[run.9200]: Process exited with status 129\n2019-11-25T09:44:59.956663+00:00 app[web.1]: 10.63.60.103 - - [25/Nov/2019:09:44:59 +0000] ""GET / HTTP/1.1"" 200 2290 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36""\n2019-11-25T09:44:59.953904+00:00 heroku[router]: at=info method=GET path=""/"" host=pmpl-arga.herokuapp.com request_id=e3b7befb-e34f-465d-8e95-ac8e9a136a6b fwd=""180.252.203.230"" dyno=web.1 connect=0ms service=6ms status=200 bytes=2662 protocol=http\n2019-11-25T09:45:00.628661+00:00 heroku[router]: at=info method=GET path=""/static/base.css"" host=pmpl-arga.herokuapp.com request_id=ba024881-e8df-42a8-91a7-b4654869bae6 fwd=""180.252.203.230"" dyno=web.1 connect=1ms service=4ms status=304 bytes=194 protocol=http\n2019-11-25T09:45:00.628002+00:00 app[web.1]: 10.155.219.74 - - [25/Nov/2019:09:45:00 +0000] ""GET /static/base.css HTTP/1.1"" 304 0 ""http://pmpl-arga.herokuapp.com/"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36""\n2019-11-25T09:45:00.630022+00:00 app[web.1]: 10.13.136.134 - - [25/Nov/2019:09:45:00 +0000] ""GET /static/bootstrap/css/bootstrap.min.css HTTP/1.1"" 304 0 ""http://pmpl-arga.herokuapp.com/"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36""\n2019-11-25T09:45:00.629295+00:00 heroku[router]: at=info method=GET path=""/static/bootstrap/css/bootstrap.min.css"" host=pmpl-arga.herokuapp.com request_id=5ba609ce-f281-453f-8bb6-0e76f19dc98f fwd=""180.252.203.230"" dyno=web.1 connect=0ms service=4ms status=304 bytes=197 protocol=http\n2019-11-25T09:45:08.986131+00:00 heroku[router]: at=info method=POST path=""/accounts/send_login_email"" host=pmpl-arga.herokuapp.com request_id=d9d6bed7-60fb-42ae-bab8-e50e3f359484 fwd=""180.252.203.230"" dyno=web.1 connect=1ms service=1445ms status=302 bytes=408 protocol=http\n2019-11-25T09:45:08.985770+00:00 app[web.1]: 10.43.184.146 - - [25/Nov/2019:09:45:08 +0000] ""POST /accounts/send_login_email HTTP/1.1"" 302 0 ""http://pmpl-arga.herokuapp.com/"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36""\n2019-11-25T09:45:09.518554+00:00 heroku[router]: at=info method=GET path=""/"" host=pmpl-arga.herokuapp.com request_id=18a3a882-0210-4c4e-9143-29ef43a342c4 fwd=""180.252.203.230"" dyno=web.1 connect=1ms service=10ms status=200 bytes=3051 protocol=http\n2019-11-25T09:45:09.517300+00:00 app[web.1]: 10.43.252.33 - - [25/Nov/2019:09:45:09 +0000] ""GET / HTTP/1.1"" 200 2596 ""http://pmpl-arga.herokuapp.com/"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36""\n2019-11-25T09:45:09.921335+00:00 app[web.1]: 10.155.219.74 - - [25/Nov/2019:09:45:09 +0000] ""GET /static/bootstrap/css/bootstrap.min.css HTTP/1.1"" 304 0 ""http://pmpl-arga.herokuapp.com/"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36""\n2019-11-25T09:45:09.921116+00:00 heroku[router]: at=info method=GET path=""/static/bootstrap/css/bootstrap.min.css"" host=pmpl-arga.herokuapp.com request_id=1ddc5722-64c4-4aff-bffb-0411b972f723 fwd=""180.252.203.230"" dyno=web.1 connect=1ms service=4ms status=304 bytes=197 protocol=http\n2019-11-25T09:45:10.178953+00:00 app[web.1]: 10.5.206.253 - - [25/Nov/2019:09:45:10 +0000] ""GET /static/base.css HTTP/1.1"" 304 0 ""http://pmpl-arga.herokuapp.com/"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36""\n2019-11-25T09:45:10.179202+00:00 heroku[router]: at=info method=GET path=""/static/base.css"" host=pmpl-arga.herokuapp.com request_id=ea1ad116-e63b-47d0-86df-0a86402c8238 fwd=""180.252.203.230"" dyno=web.1 connect=0ms service=4ms status=304 bytes=194 protocol=http\n2019-11-25T09:45:15.727759+00:00 heroku[router]: at=info method=GET path=""/accounts/login?token=ed9e5407-1048-4339-a3bd-73cdb462deb3"" host=pmpl-arga.herokuapp.com request_id=8e335eaf-c0b1-4d04-aed1-542b38d5265f fwd=""180.252.203.230"" dyno=web.1 connect=1ms service=37ms status=302 bytes=529 protocol=http\n2019-11-25T09:45:15.728567+00:00 app[web.1]: 10.61.164.89 - - [25/Nov/2019:09:45:15 +0000] ""GET /accounts/login?token=ed9e5407-1048-4339-a3bd-73cdb462deb3 HTTP/1.1"" 302 0 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36""\n2019-11-25T09:45:16.258426+00:00 heroku[router]: at=info method=GET path=""/"" host=pmpl-arga.herokuapp.com request_id=4169ada5-361c-4770-9576-c9a2b071feae fwd=""180.252.203.230"" dyno=web.1 connect=1ms service=26ms status=200 bytes=2450 protocol=http\n2019-11-25T09:45:16.258485+00:00 app[web.1]: 10.102.191.252 - - [25/Nov/2019:09:45:16 +0000] ""GET / HTTP/1.1"" 200 2078 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36""\n2019-11-25T09:45:16.905148+00:00 heroku[router]: at=info method=GET path=""/static/base.css"" host=pmpl-arga.herokuapp.com request_id=f830041d-1dd4-48a0-bb54-1ecbb5531087 fwd=""180.252.203.230"" dyno=web.1 connect=0ms service=4ms status=304 bytes=194 protocol=http\n2019-11-25T09:45:16.896640+00:00 heroku[router]: at=info method=GET path=""/static/bootstrap/css/bootstrap.min.css"" host=pmpl-arga.herokuapp.com request_id=639742e3-5908-4b8b-b418-527486133ccd fwd=""180.252.203.230"" dyno=web.1 connect=0ms service=5ms status=304 bytes=197 protocol=http\n2019-11-25T09:45:16.897220+00:00 app[web.1]: 10.63.206.231 - - [25/Nov/2019:09:45:16 +0000] ""GET /static/bootstrap/css/bootstrap.min.css HTTP/1.1"" 304 0 ""http://pmpl-arga.herokuapp.com/"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36""\n2019-11-25T09:45:16.906428+00:00 app[web.1]: 10.93.223.38 - - [25/Nov/2019:09:45:16 +0000] ""GET /static/base.css HTTP/1.1"" 304 0 ""http://pmpl-arga.herokuapp.com/"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36""\n\nDiamati dari heroku log hasil deployment Saya yang baru, error yang dijelaskan pada subbab 21.1 tidak muncul pada hasil deployment Saya yang baru.\n'], 'url_profile': 'https://github.com/argaghulamahmad', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated Sep 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'R', 'Updated Aug 3, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Sep 2, 2020', 'R', 'Updated Jul 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['Salary-Predictor\nThis Python code predicts the salary that can be paid to an individual depending upon the work experience using the Linear Regression Algorithm. This model predicts the salary based upon the data set fed to the algorithm.\n'], 'url_profile': 'https://github.com/Ritayan-Dhara', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated Sep 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'R', 'Updated Aug 3, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Sep 2, 2020', 'R', 'Updated Jul 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\n\n\n\nScope\nMultivariate elastic net regression through stacked generalisation\n(extending the R package\nglmnet).\nInstallation\nInstall the current release from\nCRAN:\ninstall.packages(""joinet"")\nor the latest development version from\nGitHub:\n#install.packages(""devtools"")\ndevtools::install_github(""rauschenberger/joinet"")\nReference\nArmin Rauschenberger and Enrico Glaab (2020). ‚ÄúPredicting correlated\noutcomes from molecular data‚Äù. Manuscript in preparation.\n\n\n\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated Sep 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'R', 'Updated Aug 3, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Sep 2, 2020', 'R', 'Updated Jul 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['skedastic\n\nThe purpose of the skedastic package is to make diagnostic methods\nfor detecting heteroskedasticity in linear regression models accessible to R\nusers.\nInstallation\n# Install from CRAN\ninstall.packages(""skedastic"", dependencies = c(""Depends"", ""Imports""))\n\n# Or the development version from GitHub:\ninstall.packages(""devtools"")\ndevtools::install_github(""tjfarrar/skedastic"")\nUsage\nThe purpose of the skedastic package is to make diagnostic methods\nfor detecting heteroskedasticity in linear regression models accessible to R\nusers. Heteroskedasticity (sometimes spelt \'heteroscedasticity\') is a violation\nof one of the assumptions of the classical linear regression model (the\nGauss-Markov Assumptions). This assumption, known as homoskedasticity, holds\nthat the variance of the random error term remains constant across all\nobservations.\n23 distinct functions in the package implement hypothesis testing\nmethods for detecting heteroskedasticity that have been previously published in academic literature. Other functions implement graphical methods for detecting\nheteroskedasticity or perform supporting tasks for the tests such as computing transformations of the Ordinary Least Squares (OLS) residuals that are useful in heteroskedasticity detection, or computing probabilities from the null distribution of a nonparametric test statistic. Certain functions have applications beyond the problem of heteroskedasticity in linear regression. These include pRQF, which computes cumulative probabilities from the distribution of a ratio of quadratic forms in normal random vectors, twosidedpval, which implements three different approaches for calculating two-sided $p$-values from asymmetric null distributions, and dDtrend and pdDtrend, which compute probabilities from Lehmann\'s nonparametric trend statistic.\nMost of the exported functions in the package take a linear model as their primary argument (which can be passed as an lm object). Thus, to use this package a user must first be familiar\nwith how to fit linear regression models using the lm function from package\nstats. Note that the package currently supports only linear regression models\nfit using OLS.\nFor heteroskedasticity tests that are implemented in other R packages on CRAN,\nor in other statistical software such as SAS or SHAZAM, the functions in the\nskedastic package have been checked against them to ensure that they produce\nthe same values of the test statistic and $p$-value. This is true of breusch_pagan, cook_weisberg, glejser,\ngoldfeld_quandt (parametric test only), harvey, and white_lm.\nHere is an example of implementing the Breusch-Pagan Test for heteroskedasticity\non a linear regression model fit to the cars\ndataset, with distance (cars$dist) as the response (dependent) variable and\nspeed (cars$speed) as the explanatory (independent) variable.\nlibrary(skedastic)\nmylm <- lm(dist ~ speed, data = cars)\nbreusch_pagan(mylm)\nTo compute BLUS residuals for the same model:\nmyblusres <- blus(mylm, omit = ""last"")\nmyblusres\nTo create customised residual plots for the same model:\nhetplot(mylm, horzvar = c(""explanatory"", ""log_explanatory""), vertvar = c(""res"", ""res_stud""), vertfun = ""2"", filetype = NA)\nLearn More\nNo vignettes have been created yet for this package. Watch this space.\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated Sep 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'R', 'Updated Aug 3, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Sep 2, 2020', 'R', 'Updated Jul 18, 2020']}","{'location': 'Provo, UT', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Bayesian-Forest-Fire-Analysis\nIn the past couple years, forest Ô¨Åres have seemed more prevalent especially in the West. Experiences like the campÔ¨Åre in Paradise California in 2018 cause drastic economic and ecological damage. Understanding factors that play in forest Ô¨Åres will help mitigate damage caused by the Ô¨Åre, and help prevent future Ô¨Åres. The goal of this analysis is to better understand the factors that play into forest Ô¨Åres and what causes them to be more harmful by utilizing Bayesian techniques. I utilize a hierarchical Bayesian model to model the total area burned in Monteshino National Park in Portugal. The model is basically a combination of linear and logistic regression and I use MCMC to obtain draws from the posterior distribution.\n'], 'url_profile': 'https://github.com/spebert', 'info_list': ['Jupyter Notebook', 'Updated Apr 28, 2020', 'R', 'Updated Dec 21, 2020', 'R', 'Updated Sep 14, 2020', 'HTML', 'Updated Jun 16, 2020', 'R', 'Updated Aug 3, 2020', 'Python', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Nov 25, 2020', 'R', 'Updated Sep 2, 2020', 'R', 'Updated Jul 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['nlraa: R package for Nonlinear Regression for Agricultural\nApplications\n\n\n\nDocumentation\nThis package supports the following publications:\n\n\nArchontoulis, S. V., and F. E. Miguez. 2015. Nonlinear Regression\nModels and Applications in Agricultural\nResearch. Agron. J. 107:786-798. \nhttps://doi.org/10.2134/agronj2012.0506\n\n\nMiguez, F., S. Archontoulis, H. Dokoohaki 2018. Chapter 15:\nNonlinear Regression Models and Applications. In: B. Glaz,\nK. M. Yeater, editors, Applied Statistics in Agricultural,\nBiological, and Environmental Sciences, ASA, CSSA, and SSSA,\nMadison, WI. p. 401-448. \nhttps://doi.org/10.2134/appliedstatistics.2016.0003\n\n\nOddi et. al. (2019). A nonlinear mixed-effects modelling approach\nfor ecological data: Using temporal dynamics of vegetation moisture\nas an example. Ecology and Evolution. \nhttps://doi.org/10.1002/ece3.5543\n\n\nOther publications where we have used nonlinear mixed models:\n\n\nFernando E. Miguez, Mar√≠a Bonita Villamil, Stephen P. Long,\nGerm√°n A. Bollero, Meta-analysis of the effects of management\nfactors on Miscanthus √ó giganteus growth and biomass production,\nAgricultural and Forest Meteorology, Volume 148, Issues 8‚Äì9, 2008,\nPages 1280-1292, ISSN 0168-1923, \nhttps://doi.org/10.1016/j.agrformet.2008.03.010. \nSupporting information: \nhttp://miguezlab.agron.iastate.edu/OldWebsite/Research/Miscanthus/MetaMiscan.tar.gz\n\n\nEichenberger, S., F. Miguez, J. Edwards, and A. Knapp. 2015. Changes\nin Kernel Filling with Selection for Grain Yield in a Maize\nPopulation. Crop Sci. 55:521-526. \nhttps://doi.org/10.2135/cropsci2014.07.0462\n\n\nStein, M., F. Miguez, and J. Edwards. 2016. Effects of Plant Density\non Plant Growth before and after Recurrent Selection in\nMaize. Crop Sci. 56:2882-2894. \nhttps://doi.org/10.2135/cropsci2015.09.0599\n\n\nTalks:\n2008 ACS meeting.\nSlides: \nhttp://miguezlab.agron.iastate.edu/OldWebsite/Research/Talks/ASA_Miguez.pdf\nData and code: \nhttp://miguezlab.agron.iastate.edu/OldWebsite/Research/Talks/Barley.zip\nIdeas for future versions:\n\nInclude more data and analysis code from the above examples\nInclude more SS functions\n\nTo install\n\nlibrary(remotes) \nremotes::install_github(""femiguez/nlraa"") \nlibrary(nlraa)\n\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Jan 29, 2021', '2', 'Jupyter Notebook', 'Updated May 24, 2020', '2', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Austin, TX', 'stats_list': [], 'contributions': '1,016 contributions\n        in the last year', 'description': [""Austin Air Quality Analysis and Prediction\nThe scope of this project is limited to the city of Austin, Texas in USA.\n\nProject Purpose\nThe purpose of this project is to analyze the air pollutants for the city of Austin in Texas and try to establish the correlation between pollutants like Ozone and NO2 with the weather parameters.\nTools & Models Used\nRegression Models | Linear Regression | Lasso | Random Forest | Polynomial Regression | Feature Selection | Hypothesis Testing | Scikit Learn | Python3 | Pandas | Numpy | Plotly | Seaborn | Feature Scaling | API | Requests\nData Sources\nFive years of data has been used for this project from 2015 to 2019. Data has been collected using API from the following two sources:\n\nepa.gov - air pollutants (Ozone, PM2.5, NO2, SO2)\nworldweatheronline.com - weather parameters\n\nFindings & Conclusion\nNitrogen Dioxide (NO2) and Weather Co-relation Study for the City of Austin, Texas\nBased on the nitrogen dioxide and weather data from 2015 to 2019, we can see that the weather conditions does impact nitrogen dioxide level specifically minimum temperature and wind speed.\nFollowing inferences can be made from the regression model used:\n\nA decrease in Min temperature, wind speed, cloud cover, humidity increases the nitrogen dioxide level.\nA decrease in pressure decreases the nitrogen dioxide level.\n\nAlthough based on the correlation matrix, the correlation between weather parameters and nitrogen dioxide doesn't seem to be very strong but based on the Null Hypothesis, we can conclude that weather parameters like Min Temperature, Wind Speed, Pressure and Cloud Cover does have an impact on nitrogen dioxide and we cannot ignore them.\nOzone (O3) and Weather Correlation Study for the City of Austin, Texas\nBased on the ozone and weather data from 2015 to 2019, we can see that the weather conditions does impact ozone level specifically maximum temperature and humidity.\nFollowing inferences can be made from the regression model used:\n\nAn increase in temperature increases the ozone level.\nAn increase in humidity decreases the ozone level.\nAn increase in cloudcover and wind speed also decreases the ozone level.\n\nAlthough based on the correlation matrix, the correlation between weather parameters and ozone doesn't seem to be very strong but based on the Null Hypothesis, we can conclude that weather parameters like Max Temperature, Humidity, Wind Speed and Cloud Cover does have an impact on ozone and we cannot ignore them.\nFrom the official EPA website:\nClimate change can impact air quality and, conversely, air quality can impact climate change.\nChanges in climate can result in impacts to local air quality. Atmospheric warming associated with climate change has the potential to increase ground-level ozone in many regions, which may present challenges for compliance with the ozone standards in the future. The impact of climate change on other air pollutants, such as particulate matter, is less certain, but research is underway to address these uncertainties.\nEmissions of pollutants into the air can result in changes to the climate. Ozone in the atmosphere warms the climate, while different components of particulate matter (PM) can have either warming or cooling effects on the climate. For example, black carbon, a particulate pollutant from combustion, contributes to the warming of the Earth, while particulate sulfates cool the earth's atmosphere.\n""], 'url_profile': 'https://github.com/esharma3', 'info_list': ['R', 'Updated Jan 29, 2021', '2', 'Jupyter Notebook', 'Updated May 24, 2020', '2', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Dortmund, Germany', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Meteoro_Pred_Sundarban\nTable of Contents\n\nAbout the Project\n\nMotivation and Objectives\nBuilt With\n\n\nGetting Started\n\nUbuntu 18.04 installation\nPycharm Installation\n\n\n\nAbout the Project\nThis project is focused on predicting the meteorological status of the largest mangrove forest in the world ""Sundarban"". The major tasks of the project will be, data cleaning, feature selection / extraction, a statistical representation of the data, regression analysis on the data, cross-validation, data visualization, experimental result representation and Conclusion of the result.\nMotivation and Objectives\n\nThis project is focused on predicting the meteorological status of the largest mangrove forest in the world ""Sundarban"". Sundarban is situated on the southern part of Bangladesh and scientists predicts that it will be sunk within 2050 because of the rise of sea level.\nAs part of fighting global warming, the project members have collected data of the years 1949 to 2015 from the Bangladesh Meteorological Department.\nThis project is focused on understanding the meteorological behavior in a statistical way.\nThe main objective of this project is to predict the weather of the Sundarban region in the upcoming years and give some insights for future.\n\nBuilt With\nThe project will be developed based on the following tools:\n\nPython\nPycharm\nUbuntu 18.04\n\nGetting Started\nTo start the project several tools and technoloies are required to install.\nUbuntu 18.04 installation:\n\n\nThe iso image of 64-bit version of Ubuntu 18.04 is available here: http://releases.ubuntu.com/18.04/ubuntu-18.04.3-desktop-amd64.iso\n\n\nUbuntu 18.04 dependencies should be updated using\n\n\nsudo apt-get update\nPycharm Installation:\n\nDetails of Pycharm installation in Ubuntu 18.04 can be found in this video: https://www.youtube.com/watch?v=cVROiVgR_qg&t=364s\nAfter the installation of Pycharm, it is important to check that the Project Interpreter is working correctly. To check that please go to:\n\nFile > Settings > Project:""your project name"" > Project Interpreter\n\nThe defaulf Python3 verson comes with Ubuntu 18.04 is Python 3.6 and it should be shown as interpreter.\n\nPython Libraries Installation:\n'], 'url_profile': 'https://github.com/NafeesKhandker', 'info_list': ['R', 'Updated Jan 29, 2021', '2', 'Jupyter Notebook', 'Updated May 24, 2020', '2', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Ankara, TR', 'stats_list': [], 'contributions': '326 contributions\n        in the last year', 'description': ['\n\n\n\nEarthqauke Damage Prediction - Will Fall or Not\n\n\n\n\n\nüìù Table of Contents\n\nAbout\nDataset\nWork Done\nAdditional Work\nBuilt Using\nAuthors\n\nüßê About \nIn this project, we have used the data of a past earthquake\nwhich includes the informations of structures which effected by earthquake. With using this building informations, we will be trying to predict the damage level of a structure. For this purpose, different Machine Learning algorithms will be used to see the difference between them on our dataset.\nüìÉ Dataset \n2015 Nepal Earthquake: Open Data Portal\n762094 instances and 43 features\nüöÄ Work Done \n\nData cleaning and analysis.\nIdentified the relationships between features and fetched the meaningful ones.\nK-Nearest Neighbor with using scikit-learn and KD-tree\nLogistic Regression\nDecision Tree\nRandom Forest\nLightGBM\nAdaBoost\nXGBoost\nNeural Network\nVoting Classifier\nSVM\n\nüí™ Additional Work \n\nEach week, as we made progress on the project, we wrote a blog post to explain the thing we done. All of them can be seen here :\n\nhttps://medium.com/bbm406f19/week-7-will-fall-or-not-6fb4bf6763ba\n\nAlso, at the end of the project, we created a short video to introduce our work :\n\n\n‚õèÔ∏è Built Using \n\nPandas & Numpy\nMatplotlib & Seaborn\nscikit-learn\nimbalanced-learn\nPyTorch\nCUDA\nPowtoon - For video\n\n‚úçÔ∏è Authors \n\n√ñner ƒ∞nce\nZeki Emre √úNEL\nMehmet Ali B√ñBECƒ∞\n\n'], 'url_profile': 'https://github.com/OnerInce', 'info_list': ['R', 'Updated Jan 29, 2021', '2', 'Jupyter Notebook', 'Updated May 24, 2020', '2', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Salvador', 'stats_list': [], 'contributions': '247 contributions\n        in the last year', 'description': ['ANALYTICS-UTILS\nPackage contain function for data analytics\nInstalling\npip install analytics-utils\nUsage\ndescribe_data\nThis function describe the datas of a dataframe. Returning the max, min, mean, median, quantile, variance, standard deviation, mean, absolute deviation, amplitude, root mean squared, kurtosis, skewness and count for all headers in dataframe\nfunction\nfrom analytics_utils.describe_data import describe_data\n\ndescribe_data(dataframe, headers, lang)\n\n\ndataframe: dataframe for describe\n\n\nheaders: columns of dataframe for describe. Returnered descriptions:\n\nmaximum\nminimum\nmean\nmedian\n[1|3]-quartile\nvariance\nstandard deviation\nmean absolute deviation\namplitude\nroot mean squared\nkurtosis\nskewness\ncount\n\n\n\nlang: output language (default: \'pt\')\n\n\'pt\': portuguese\n\'en\': english\n\n\n\nterminal\n\nHelp message\n\nusage: describe_data.py [-h] -d DATASET [-f FILE_OUT] [-o ORIENT] [-l LANG]\n                        [-pd [PARSE_DATES [PARSE_DATES ...]]]\n                        [-i [INDEX [INDEX ...]]] [-hd [H [H ...]]]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d DATASET, --dataset DATASET\n                        path to input dataset\n  -f FILE_OUT, --file-out FILE_OUT\n                        path to file of output json\n  -o ORIENT, --orient ORIENT\n                        format json output {\'split\', \'records\', \'index\',\n                        \'values\', \'table\', \'columns\'} (default: \'columns\')\n  -l LANG, --lang LANG  language for the output result {\'pt\', \'en\'} (default:\n                        \'pt\')\n  -pd [PARSE_DATES [PARSE_DATES ...]], --parse-dates [PARSE_DATES [PARSE_DATES ...]]\n                        Headers of columns to parse dates. A column named\n                        datetime is created.\n  -i [INDEX [INDEX ...]], --index [INDEX [INDEX ...]]\n                        Headers of columns to set as index.\n  -hd [H [H ...]], --headers [H [H ...]]\n                        an string for the header in the dataset\n\nUsage\n\npython describe_data.py -d dataset.csv -pd date time -i datetime -f out.json\ncorrelate\nThis function returns the correlation between the columns of a dataframe. This is the same corr function in pandas package.\nfunction\nfrom analytics_utils.correlate import correlate\n\ncorrelate(dataframe, method, min_periods)\n\n\ndataframe: correlation dataframe\n\n\nmethod: correlation method (default: {""pearson""}):\n\npearson\nkendall\nspearman\nor callable with input two 1d ndarrays\n\n\n\nmin_periods: Minimum number of observations required per pair of columns to have a valid result. Currently only available for Pearson and Spearman correlation (default: {1}).\n\n\nterminal\n\nHelp message\n\nusage: correlate.py [-h] -d DATASET [-f FILE_OUT] [-o ORIENT] [-m METHOD]\n                    [-p MIN_PERIODS]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d DATASET, --dataset DATASET\n                        path to input dataset\n  -f FILE_OUT, --file-out FILE_OUT\n                        path to file of output json\n  -o ORIENT, --orient ORIENT\n                        format json output {\'split\', \'records\', \'index\',\n                        \'values\', \'table\', \'columns\'} (default: \'columns\')\n  -m METHOD, --method METHOD\n                        method of correlation {‚Äòpearson‚Äô, ‚Äòkendall‚Äô,\n                        ‚Äòspearman‚Äô} (default: \'pearson\')\n  -p MIN_PERIODS, --min-periods MIN_PERIODS\n                        Minimum number of observations required per pair of\n                        columns to have a valid result. Currently only\n                        available for Pearson and Spearman correlation\n                        (default: 1).\n\nUsage\n\npython correlate.py -d dataset.csv -f out.json\ninterpolate\nThis function returns the Series or DataFrame of same shape interpolated at the NaNs. This is a adapted interpolate function of pandas package.\nfunction\nfrom analytics_utils.interpolate import interpolate\n\ninterpolate(dataframe, headers, method, limit)\n\n\ndataframe: dataframe for interpolation\n\n\nheaders: columns of dataframe for interpolating (default: {None}). For default, all are interpolated.\n\n\nmethod: interpolation method. Please note that only method=\'linear\' is supported for DataFrame/Series with a MultiIndex. (default: {""linear""}):\n\nlinear\ntime\nindex\nvalues\nnearest\nzero\nslinear\nquadratic\ncubic\nbarycentric\nkrogh\npolynomial\nspline\npiecewise_polynomial\npchip\n\n\n\nlimit: Maximum number of consecutive NaNs to fill (default: {None}).\n\n\nterminal\n\nHelp message\n\nusage: interpolate.py [-h] -d DATASET [-f FILE_OUT] [-o ORIENT] [-m METHOD]\n                      [-l LIMIT] [-pd [PARSE_DATES [PARSE_DATES ...]]]\n                      [-i [INDEX [INDEX ...]]] [-hd [H [H ...]]]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d DATASET, --dataset DATASET\n                        path to input dataset\n  -f FILE_OUT, --file-out FILE_OUT\n                        path to file of output json\n  -o ORIENT, --orient ORIENT\n                        format json output {\'split\', \'records\', \'index\',\n                        \'values\', \'table\', \'columns\'} (default: \'columns\')\n  -m METHOD, --method METHOD\n                        method of interpolation. Please note that only\n                        method=\'linear\' is supported for DataFrame/Series with\n                        a MultiIndex. {‚Äòlinear‚Äô, ‚Äòtime‚Äô, ‚Äòindex‚Äô, ‚Äòvalues‚Äô,\n                        ‚Äònearest‚Äô, ‚Äòzero‚Äô, ‚Äòslinear‚Äô, ‚Äòquadratic‚Äô, ‚Äòcubic‚Äô,\n                        ‚Äòbarycentric‚Äô, ‚Äòkrogh‚Äô, ‚Äòpolynomial‚Äô, ‚Äòspline‚Äô\n                        ‚Äòpiecewise_polynomial‚Äô, ‚Äòpchip‚Äô} (default: \'linear\')\n  -l LIMIT, --limit LIMIT\n                        Maximum number of consecutive NaNs to fill (default:\n                        None)\n  -pd [PARSE_DATES [PARSE_DATES ...]], --parse-dates [PARSE_DATES [PARSE_DATES ...]]\n                        Headers of columns to parse dates. A column named\n                        datetime is created.\n  -i [INDEX [INDEX ...]], --index [INDEX [INDEX ...]]\n                        Headers of columns to set as index.\n  -hd [H [H ...]], --headers [H [H ...]]\n                        an string for the header in the dataset\n\nUsage\n\npython analytics-utils/interpolate.py -d dataset.csv -f out.json\nrolling window\nThis function Provide rolling window calculations. This is a adapted rolling function of pandas package.\nfunction\nfrom analytics_utils.roll import roll\n\nroll(dataframe, window, roll_type, headers)\n\n\ndataframe: dataframe for apply rolling\n\n\nwindow: Size of the moving window. This is the number of observations used for calculating the statistic. Each window will be a fixed size.\n\n\nroll_type: rolling method (default: {""mean""}):\n\nmean\nvar (variance)\nstd (standard deviation)\n\n\n\nterminal\n\nHelp message\n\nusage: roll.py [-h] -d DATASET [-f FILE_OUT] [-o ORIENT] -w WINDOW\n               [-t ROLL_TYPE] [-pd [PARSE_DATES [PARSE_DATES ...]]]\n               [-i [INDEX [INDEX ...]]] [-hd [HEADERS [HEADERS ...]]]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d DATASET, --dataset DATASET\n                        path to input dataset\n  -f FILE_OUT, --file-out FILE_OUT\n                        path to file of output json\n  -o ORIENT, --orient ORIENT\n                        format json output {\'split\', \'records\', \'index\',\n                        \'values\', \'table\', \'columns\'} (default: \'columns\')\n  -w WINDOW, --window WINDOW\n                        Size of the moving window. This is the number of\n                        observations used for calculating the statistic. Each\n                        window will be a fixed size.\n  -t ROLL_TYPE, --roll_type ROLL_TYPE\n                        {‚Äòmean‚Äô, ‚Äòvar‚Äô, \'std\'} (default: {""mean""}).\n  -pd [PARSE_DATES [PARSE_DATES ...]], --parse-dates [PARSE_DATES [PARSE_DATES ...]]\n                        Headers of columns to parse dates. A column named\n                        datetime is created.\n  -i [INDEX [INDEX ...]], --index [INDEX [INDEX ...]]\n                        Headers of columns to set as index.\n  -hd [HEADERS [HEADERS ...]], --headers [HEADERS [HEADERS ...]]\n                        an string for the header in the dataset\n\nUsage\n\npython analytics-utils/roll.py -w 12 -d dataset.csv -f out.json\nexponential weighted moving\nThis function provide exponential weighted functions. This is a adapted ewm function of pandas package.\nfunction\nfrom analytics_utils.ewm import ewm\n\newm(dataframe, com, span, halflife, alpha, ignore_na, ewm_type, headers)\n\n\ndataframe: dataframe for apply ewm\n\n\ncom: specify decay in terms of center of mass, Œ±=1/(1+com), for com‚â•0 (default: {None}).\n\n\nspan: specify decay in terms of span, Œ±=2/(span+1), for span‚â•1 (default: {None}).\n\n\nhalflife: specify decay in terms of half-life, Œ±=1‚àíexp(log(0.5)/halflife),for halflife>0 (default: {None}).\n\n\nalpha: specify smoothing factor Œ± directly, 0<Œ±‚â§1 (default: {None}).\n\n\nignore_na: ignore missing values when calculating weights; specify True to reproduce pre-0.15.0 behavior (default: {False}).\n\n\newm_type: ewm method (default: {""mean""}):\n\nmean\nvar (variance)\nstd (standard deviation)\n\n\n\nheaders: columns of dataframe for apply ewm (default: {None}).\n\n\nterminal\n\nHelp message\n\nusage: ewm.py [-h] -d DATASET [-f FILE_OUT] [-o ORIENT] [-c COM] [-s SPAN]\n              [-hl HALFLIFE] [-a ALPHA] [-ina IGNORE_NA] [-t EWM_TYPE]\n              [-pd [PARSE_DATES [PARSE_DATES ...]]] [-i [INDEX [INDEX ...]]]\n              [-hd [HEADERS [HEADERS ...]]]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d DATASET, --dataset DATASET\n                        path to input dataset\n  -f FILE_OUT, --file-out FILE_OUT\n                        path to file of output json\n  -o ORIENT, --orient ORIENT\n                        format json output {\'split\', \'records\', \'index\',\n                        \'values\', \'table\', \'columns\'} (default: \'columns\')\n  -c COM, --com COM     Specify decay in terms of center of mass, Œ±=1/(1+com),\n                        for com‚â•0 (default: None).\n  -s SPAN, --span SPAN  Specify decay in terms of span, Œ±=2/(span+1), for\n                        span‚â•1 (default: None).\n  -hl HALFLIFE, --halflife HALFLIFE\n                        Specify decay in terms of half-life,\n                        Œ±=1‚àíexp(log(0.5)/halflife) , for halflife>0 (default:\n                        None).\n  -a ALPHA, --alpha ALPHA\n                        Specify smoothing factor Œ± directly, 0<Œ±‚â§1 (default:\n                        None).\n  -ina IGNORE_NA, --ignore-na IGNORE_NA\n                        Ignore missing values when calculating weights;\n                        specify True to reproduce (default: False).\n  -t EWM_TYPE, --ewm-type EWM_TYPE\n                        {‚Äòmean‚Äô, ‚Äòvar‚Äô, \'std\'} (default: {""mean""}).\n  -pd [PARSE_DATES [PARSE_DATES ...]], --parse-dates [PARSE_DATES [PARSE_DATES ...]]\n                        Headers of columns to parse dates. A column named\n                        datetime is created.\n  -i [INDEX [INDEX ...]], --index [INDEX [INDEX ...]]\n                        Headers of columns to set as index.\n  -hd [HEADERS [HEADERS ...]], --headers [HEADERS [HEADERS ...]]\n                        an string for the header in the dataset\n\nUsage\n\npython analytics-utils/ewm.py -hl 12 -d dataset.csv -f out.json\nseasonal decompose\nSeasonal decomposition using moving averages. This is a adapted seasonal_decompose function of statsmodels package.\nfunction\nfrom analytics_utils.decompose import decompose\n\ndecompose(dataframe, model, filt, freq, two_sided, extrapolate_trend, lang, headers)\n\n\ndataframe: dataframe for apply decompose\n\n\nmodel: Type of seasonal component. Abbreviations are accepted (default: {\'additive\'}).\n\nadditive\nmultiplicative\n\n\n\nfilt: The filter coefficients for filtering out the seasonal component. The concrete moving average method used in filtering is determined by two_sided (default: {None}).\n\n\nfreq: Frequency of the series. Must be used if x is not a pandas object. Overrides default periodicity of x if x is a pandas object with a timeseries index (default: {None}).\n\n\ntwo_sided: The moving average method used in filtering. If True (default), a centered moving average is computed using the filt. If False, the filter coefficients are for past values only (default: {True}).\n\n\nextrapolate_trend: If set to > 0, the trend resulting from the convolution is linear least-squares extrapolated on both ends (or the single one if two_sided is False) considering this many (+1) closest points. If set to \'freq\', use freq closest points. Setting this parameter results in no NaN values in trend or resid components (default: {0}).\n\n\nheaders: columns of dataframe for apply ewm (default: {None}).\n\n\nterminal\n\nHelp message\n\nusage: decompose.py [-h] -d DATASET [-f FILE_OUT] [-o ORIENT] [-m MODEL]\n                    [-ft [FILT [FILT ...]]] [-fq FREQ] [-t TWO_SIDED]\n                    [-e EXTRAPOLATE_TREND] [-l LANG]\n                    [-pd [PARSE_DATES [PARSE_DATES ...]]]\n                    [-i [INDEX [INDEX ...]]] [-hd [HEADERS [HEADERS ...]]]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d DATASET, --dataset DATASET\n                        path to input dataset\n  -f FILE_OUT, --file-out FILE_OUT\n                        path to file of output json\n  -o ORIENT, --orient ORIENT\n                        format json output {\'split\', \'records\', \'index\',\n                        \'values\', \'table\', \'columns\'} (default: \'columns\')\n  -m MODEL, --model MODEL\n                        Type of seasonal component. Abbreviations are accepted\n                        (default: \'additive\').\n  -ft [FILT [FILT ...]], --filt [FILT [FILT ...]]\n                        The filter coefficients for filtering out the seasonal\n                        component. The concrete moving average method used in\n                        filtering is determined by two_sided (default: None).\n  -fq FREQ, --freq FREQ\n                        Frequency of the series. Must be used if x is not a\n                        pandas object. Overrides default periodicity of x if x\n                        is a pandas object with a timeseries index (default:\n                        None).\n  -t TWO_SIDED, --two-sided TWO_SIDED\n                        The moving average method used in filtering. If True\n                        (default), a centered moving average is computed using\n                        the filt. If False, the filter coefficients are for\n                        past values only (default: True).\n  -e EXTRAPOLATE_TREND, --extrapolate-trend EXTRAPOLATE_TREND\n                        If set to > 0, the trend resulting from the\n                        convolution is linear least-squares extrapolated on\n                        both ends (or the single one if two_sided is False)\n                        considering this many (+1) closest points. If set to\n                        ‚Äòfreq‚Äô, use freq closest points. Setting this\n                        parameter results in no NaN values in trend or resid\n                        components (default: 0).\n  -l LANG, --lang LANG  language for the output result {\'pt\', \'en\'} (default:\n                        \'pt\')\n  -pd [PARSE_DATES [PARSE_DATES ...]], --parse-dates [PARSE_DATES [PARSE_DATES ...]]\n                        Headers of columns to parse dates. A column named\n                        datetime is created.\n  -i [INDEX [INDEX ...]], --index [INDEX [INDEX ...]]\n                        Headers of columns to set as index.\n  -hd [HEADERS [HEADERS ...]], --headers [HEADERS [HEADERS ...]]\n                        an string for the header in the dataset\n\nUsage\n\npython analytics-utils/decompose.py -pd date time -i datetime -fq 12 -d dataset.csv -f out.json\n'], 'url_profile': 'https://github.com/patricksferraz', 'info_list': ['R', 'Updated Jan 29, 2021', '2', 'Jupyter Notebook', 'Updated May 24, 2020', '2', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lebc97', 'info_list': ['R', 'Updated Jan 29, 2021', '2', 'Jupyter Notebook', 'Updated May 24, 2020', '2', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Jan 29, 2021', '2', 'Jupyter Notebook', 'Updated May 24, 2020', '2', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Mandi, India', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['The-Best-Classifier\nThis repository aims at implementing K Nearest Neighbor, Decision Tree, Support Vector Machine and Logistic Regression algorithms. For each of the algorithms the metrices F1-score, Jaccard Index and Log-Loss are calculated to determine the best classifier among the 4 algorithms\n'], 'url_profile': 'https://github.com/whitewolf1905', 'info_list': ['R', 'Updated Jan 29, 2021', '2', 'Jupyter Notebook', 'Updated May 24, 2020', '2', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\ndeeplearning\nCreate and train deep neural network of ReLU type with SGD and batch normalization\nAbout\nThe deeplearning package is an R package that implements deep neural networks in R. It employes Rectifier Linear Unit functions as its building blocks and trains a neural network with stochastic gradient descent method with batch normalization to speed up the training and promote regularization. Neural networks of such kind of architecture and training methods are state of the art and even achieved suplassing human-level performance in ImageNet competition. The deeplearning package is inspired by another R package darch which implements layerwise Restricted Boltzmann Machine pretraining and dropout and uses its class DArch as the default class.\nInstalltion\nInstall deeplearning from CRAN\ninstall.packages(""deeplearning"")\n\nOr install it from github\ndevtools::install_github(""rz1988/deeplearning"")\n\nUse deeplearning\nUsing the deeplearning package is designed to be easy and fun. It only takes two steps to run your first neural network.\nIn step one, the user will create a new neural network. You will need to specify the strucutre of the neural network which are the number of layers and neurons in the network and the type of activation functions. The default activation is rectifier linear unit function for the hidden layers but you can also use other types of activation such as sigmoidal function or write your own activation function.\nIn step two, the user will train the neural network with a training input and a traing target. There are a number of other training parameters. For how to choose these training parameters please refer to https://github.com/rz1988/deeplearning.\nExamples\nTrain a neural networ for regression\ninput <- matrix(runif(1000), 500, 2)\ninput_valid <- matrix(runif(100), 50, 2)\ntarget <- rowSums(input + input^2)\ntarget_valid <- rowSums(input_valid + input_valid^2)\n\n\n# create a new deep neural network for classificaiton\ndnn_regression <- new_dnn(\n                          c(2, 50, 50, 20, 1),  # The layer structure of the deep neural network.\n                                                # The first element is the number of input variables.\n                                                # The last element is the number of output variables.\n                          hidden_layer_default = rectified_linear_unit_function, \n                          # for hidden layers, use rectified_linear_unit_function\n                          output_layer_default = linearUnitDerivative # for regression, use linearUnitDerivative function\n                          )\n\ndnn_regression <- train_dnn(\n                     dnn_regression,\n\n                     # training data\n                     input, # input variable for training\n                     target, # target variable for training\n                     input_valid, # input variable for validation\n                     target_valid, # target variable for validation\n\n                     # training parameters\n                     learn_rate_weight = exp(-8) * 10, # learning rate for weights, higher if use dropout\n                     learn_rate_bias = exp(-8) * 10, # learning rate for biases, hihger if use dropout\n                     learn_rate_gamma = exp(-8) * 10, # learning rate for the gamma factor used\n                     batch_size = 10, # number of observations in a batch during training. Higher for faster training. Lower for faster convergence\n                     batch_normalization = T, # logical value, T to use batch normalization\n                     dropout_input = 0.2, # dropout ratio in input.\n                     dropout_hidden = 0.5, # dropout ratio in hidden layers\n                     momentum_initial = 0.6, # initial momentum in Stochastic Gradient Descent training\n                     momentum_final = 0.9, # final momentum in Stochastic Gradient Descent training\n                     momentum_switch = 100, # after which the momentum is switched from initial to final momentum\n                     num_epochs = 300, # number of iterations in training\n\n                     # Error function\n                     error_function = meanSquareErr, # error function to minimize during training. For regression, use meanSquareErr\n                     report_classification_error = F # whether to print classification error during training\n)\n\n# the prediciton by dnn_regression\npred <- predict(dnn_regression)\n\n# calculate the r-squared of the prediciton\nrsq(dnn_regression)\n\n# calcualte the r-squared of the prediciton in validation\nrsq(dnn_regression, input = input_valid, target = target_valid)\n\nTrain a neural network for classification\ninput <- matrix(runif(1000), 500, 2)\ninput_valid <- matrix(runif(100), 50, 2)\ntarget <- (cos(rowSums(input + input^2)) > 0.5) * 1\ntarget_valid <- (cos(rowSums(input_valid + input_valid^2)) > 0.5) * 1\n\n# create a new deep neural network for classificaiton\ndnn_classification <- new_dnn(\n  c(2, 50, 50, 20, 1),  # The layer structure of the deep neural network.\n                        # The first element is the number of input variables.\n                        # The last element is the number of output variables.\n  hidden_layer_default = rectified_linear_unit_function, # for hidden layers, use rectified_linear_unit_function\n  output_layer_default = sigmoidUnitDerivative # for classification, use sigmoidUnitDerivative function\n)\n\ndnn_classification <- train_dnn(\n  dnn_classification,\n\n  # training data\n  input, # input variable for training\n  target, # target variable for training\n  input_valid, # input variable for validation\n  target_valid, # target variable for validation\n\n  # training parameters\n  learn_rate_weight = exp(-8) * 10, # learning rate for weights, higher if use dropout\n  learn_rate_bias = exp(-8) * 10, # learning rate for biases, hihger if use dropout\n  learn_rate_gamma = exp(-8) * 10, # learning rate for the gamma factor used\n  batch_size = 10, # number of observations in a batch during training. Higher for faster training. Lower for faster convergence\n  batch_normalization = T, # logical value, T to use batch normalization\n  dropout_input = 0.2, # dropout ratio in input.\n  dropout_hidden = 0.5, # dropout ratio in hidden layers\n  momentum_initial = 0.6, # initial momentum in Stochastic Gradient Descent training\n  momentum_final = 0.9, # final momentum in Stochastic Gradient Descent training\n  momentum_switch = 100, # after which the momentum is switched from initial to final momentum\n  num_epochs = 100, # number of iterations in training\n\n  # Error function\n  error_function = crossEntropyErr, # error function to minimize during training. For regression, use crossEntropyErr\n  report_classification_error = T # whether to print classification error during training\n)\n\n# the prediciton by dnn_regression\npred <- predict(dnn_classification)\n\nhist(pred)\n\n# calculate the r-squared of the prediciton\nAR(dnn_classification)\n\n# calcualte the r-squared of the prediciton in validation\nAR(dnn_classification, input = input_valid, target = target_valid)\n\n# print the layer weights\n# this function can print heatmap, histogram, or a surface\nprint_weight(dnn_regression, 1, type = ""heatmap"")\n\nprint_weight(dnn_regression, 2, type = ""surface"")\n\nprint_weight(dnn_regression, 3, type = ""histogram"")\n\nReferences\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, 2013, Dropout: A Simple Way to Prevent Neural Networks from Overfitting, Journal of Machine Learning Research 15 (2014) 1929-1958\nSergey Ioffe, Christian Szegedy, 2015, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 2015, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, arXiv\nX. Glorot, A. Bordes, and Y. Bengio, 2011,Deep sparse rectifier networks. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, pages 315‚Äì323\nDrees, Martin (2013). ""Implementierung und Analyse von tiefen Architekturen in R"". German. Master\'s thesis. Fachhochschule Dortmund.\nRueckert, Johannes (2015). ""Extending the Darch library for deep architectures"". Project thesis. Fachhochschule Dortmund. URL: saviola.de\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Jan 29, 2021', '2', 'Jupyter Notebook', 'Updated May 24, 2020', '2', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Pune,India', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Pranil98', 'info_list': ['R', 'Updated Jan 29, 2021', '2', 'Jupyter Notebook', 'Updated May 24, 2020', '2', 'Python', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 1, 2020', 'Python', 'MIT license', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lebc97', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Sep 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Feb 3, 2020', '2', 'TypeScript', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Breast-Cancer-Prediction\nThis is my second Machine Learning Project where I have tried to predict whether a breast tumor is benign or malignant. I have used both Logistic Regression and SVM on the data and got almost similar accuracy in both cases. I have used my knowledge of Normalization and using the optimum values of C and Gamma as well.\n'], 'url_profile': 'https://github.com/bastobika', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Sep 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Feb 3, 2020', '2', 'TypeScript', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '225 contributions\n        in the last year', 'description': ['Classification-Algorithms\nIn this project,  we will build a model to predict whether a loan case will be paid off or not. Different classification algorithms such as: k-Nearest Neighbour, Decision Tree, Support Vector Machine and Logistic Regression are applied on the data and accuracy of each classifier is reported using the following metrics: Jaccard index, F1-score and Log Loss.\n'], 'url_profile': 'https://github.com/guneetdeol', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Sep 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Feb 3, 2020', '2', 'TypeScript', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Used-car-price-prediction-using-ML\nThis dataset contains the data of used cars in germany. Here we are going to work on this dataset of used cars listed from Ebay and this data was scraped from Kaggle. The main objetive of this dataset is to train a linear regression model in order to predict the price of a used car.\n'], 'url_profile': 'https://github.com/himanshukhasdev', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Sep 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Feb 3, 2020', '2', 'TypeScript', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021']}","{'location': 'Detroit, Michigan', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Machine Learning Basics Nanodegree - Udacity\nThe goal of the Machine Learning Basics Nanodegree from Udacity is to learn techniques such as data transformation and algorithms that can find patterns in data and apply machine learning algorithms to tasks of their own design. Learning outcomes of this Nanodegree are as follows:\n\nUse Python and SQL to access and analysze data from several different data sources.\nBuild predictive models using a variety of unsupervised and supervised machine learning techniques.\nPerform feature engineering to improve the performance of machine learning models.\nOptimize, tune, and improve algorithms according to specific metrics like accuracy and speed.\nCompare the performaces of learned models using suitable metrics.\n\nThis is the repository of my exercises and projects for Machine Learning Basics Nanodegree from Udacity.\nCourse 1: Supervised Learning\n\nLesson 1: Regression\nLesson 2: Perceptron Algorithm\nLesosn 3: Decision Trees\nLesson 4: Naive Bayes\nLesson 5: Support Vector Machines\nLesson 6: Ensemble of Learners\nLesson 7: Evaluation Matrices\nLesson 8: Training and Tuning Models\n\nProject 1: Find Donors for CharityML\nCharityML is a fictious charity organization located in the heart of Silicon Vallet. To expand their potential donor base, CharityML has decided to send letters to residents of California, but to only those most likely to donate to the charity.\nProject Goal:  is to evaluate and optimize several supervised learning algorithms to determine which algorithm will provide the highest donation yield while under some marketing constraints.\nKey Skills: will include supervised learning, model evaluation and comparison, and tuning models according to constraints.\nCourse 2: Deep Learning\n\nLesson 1: Introduction to Neural Networks\nLesson 2: Implementing Gradient Descent\nLesson 3: Training Neural Networks\nLesson 4: Deep Learning with PyTorch\n\nProject 2: Build an Image Classifier\nImplement an image classifier application using deep neural network. This application will train a deep learning model on a dataset of images.\nKet Skills: include PyTorch and neural networks, and model validation and evaluation.\nCourse 3: Unsupervised Learning\n\nLesson 1: Clustering\nLesson 2: Hierarchicaal and Density-Based Clustering\nLesson 3: Gaussian Mixture Models\nLesson 4: Dimensionality Reduction\n\nProject 3: Create Customer Segments\nIn this project, you will apply unsepervised learning techologies on product spending data collected for customers of a wholesale distributor in Lisbo, Portugal to identigy customer segments hidden in the data. You will first explore and pre-process the data by scaling each product category and then identifying (and removing) unwanted outliers. With the cleaned customer spending data, you will apply PCA transformations to the data and implement clustering algorithms to segment the transformed customer data. Finally, you will compare the segmentation found with an additional labeling and consider ways in which this information could assist the wholesale distributor with furure service changes.\nKey Skills: include data cleaning, dimensionality reduction with PCA, and unsupervised clustering.\n'], 'url_profile': 'https://github.com/ygsingh', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Sep 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Feb 3, 2020', '2', 'TypeScript', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Modern-Low-Footprint-Cyber-Attack-Detection\nBuilt a Network Attack detector, a predictive model capable of distinguishing between bad and good connections. Tried different models like Logistic Regression, KNN, SVM and Neural Network as a ‚ÄòBinary‚Äô and ‚ÄòMulti-Class‚Äô Classification problem. Found top 10 important features related to target using ‚ÄúCorrelation Analysis‚Äù and used ‚ÄúDownSampling‚Äù to make dataset more balanced.\n'], 'url_profile': 'https://github.com/JeetShah10', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Sep 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Feb 3, 2020', '2', 'TypeScript', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Sep 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Feb 3, 2020', '2', 'TypeScript', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021']}","{'location': 'Armenia, Yerevan', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['I built an ml5 SharePoint Framework (SPFx) extension for Classification and Regression Analysis.\nml5.js aims to make machine learning approachable for a broad audience of artists, creative coders, and students. The library provides access to machine learning algorithms and models in the browser, building on top of TensorFlow.js with no other external dependencies.\nTo get started.\n       Clone the repository\n\n       git clone https://github.com/Ashot72/ml5-spfx-extension.git\n       cd ml5-spfx-extension\n\n       # installs dependencies\n       npm install\n\n       # creates release package which should be deployed in SharePoint app catalog\n        npm run deploy-prod\n\nGo to ml5 SharePoint Extension (SPFx) Video page\nGo to ml5 SharePoint Extension (SPFx) description page\n'], 'url_profile': 'https://github.com/Ashot72', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Sep 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Feb 3, 2020', '2', 'TypeScript', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021']}","{'location': 'Lagos, Nigeria.', 'stats_list': [], 'contributions': '189 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Way4ward17', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Sep 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Feb 3, 2020', '2', 'TypeScript', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021']}","{'location': 'Moscow', 'stats_list': [], 'contributions': '541 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/khaykingleb', 'info_list': ['R', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Sep 19, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 30, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'R', 'Updated Feb 3, 2020', '2', 'TypeScript', 'Updated Jan 31, 2020', '1', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Feb 24, 2021']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 3, 2020', '2', 'HTML', 'MIT license', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'Ireland', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/popeoba', 'info_list': ['R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 3, 2020', '2', 'HTML', 'MIT license', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/namratapoojary', 'info_list': ['R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 3, 2020', '2', 'HTML', 'MIT license', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'Brussels', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['Relevance Vector Machine\n\nsupport vector machine (SVM) implementation for regression and classifier\nrelevance vector machine (RVM) implemetation for regression and classifier\n\n'], 'url_profile': 'https://github.com/ujohn33', 'info_list': ['R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 3, 2020', '2', 'HTML', 'MIT license', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['FightTheVirus\n-- frontend: HTML, CSS, JavaScript, BootStrap\n-- backend: Flask\n-- machine learning: Python, NumPy, Pandas, Scikit-learn\n-- model: logistic regression\nInspiration\nCoronavirus, our new enemy, has continued to infect us and threaten our lives. According to Baidu data, 1.06 people in China got infected by the Coronavirus every minute (Feb. 1. 2020), and this number has continued to rise. In Canada, there had been 4 cases confirmed which disturb the lives of many civilians. If there is no action to prevent it, humanity is at stake. Now, it is time for us to stand up against it and use the power of machine learning.\nWhat it does\nFightTheVirus is available on both web and IOS platform. The main mission is to utilize machine learning to predict the probability of a person having Coronavirus based on attributes such as gender, age, fever, cough, headache, breathing, travel history in Wuhan and China, and sputum. We trained the data of 150 confirmed and suspected cases of patients and by the model of Logistic Regression, we are able to compute the probability. Also, on our app, we have tips section that included all the information that people need to know about Coronavirus. Lastly, we integrated machine learning again to display projected data -- the number of confirmed cases, suspected cases, deaths and recoveries to remind everyone of our situation in order to prepare for it.\nHow we built it\nWe built the app both on the website and mobile. For the web app, we implemented the Python machine learning library scikit-learn for the training of the dataset. The model is Logistic Regression which uses the logistic function to model a binary dependent variable and output the probability from 0 to 1 in the form of a sigmoid/softmax function. Numpy and Pandas were also used in preprocessing data. To integrate the machine learning backend with our web front end, we used flask to connect everything together. The front end website was designed with HTML, CSS, Bootstrap and JavaScript which blended well with flask and Python. For the mobile app, we made an IOS prototype that has all of the functions on the website.\nTraining\nWe only have limited amount of data (150 patients) and here is one of our training results:\n              precision   recall   f1-score    support\nNegative         1.00      0.24      0.39        25\nPositive         0.17      1.00      0.30         4\naccuracy                             0.34        29\nmacro avg        0.59      0.62      0.34        29\nweighted avg     0.89      0.34      0.37        29\n\nChallenges we ran into\nSince our team is only made of two members and we are both relatively new to machine learning, it was somewhat difficult for us to start. Without any backend experience, we learned flask in a short period amount of time and successfully implemented it in the fusion of frontend and backend. The machine learning part was also tricky to start but with tons of failures, we still manage to finish our model in processing patient attributes and outputting probabilities.\nWhat\'s next for FightTheVirus\nWe were somehow pitied by the limited amount of Coronavirus data online in order for a better machine learning training result. If we have more time, we would request data from official databases to further optimize the prediction. Our app was also a prototype and we can develop the fully operative IOS + Android App in the future.\nAcknowledgement\nThe basic model of FightTheVirus project is based on ""Flask and Data Science Workshop"" by Nordstrom.\n'], 'url_profile': 'https://github.com/kevinxyc1', 'info_list': ['R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 3, 2020', '2', 'HTML', 'MIT license', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'Tampa, Florida', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sriranganath21', 'info_list': ['R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 3, 2020', '2', 'HTML', 'MIT license', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['California Housing Price Prediction\n#The project aims at building a model of housing prices to predict median house values in California using the provided dataset. This model should learn from the data and be able to predict the median housing price in any district, given all the other metrics.  #Districts or block groups are the smallest geographical units for which the US Census Bureau #publishes sample data (a block group typically has a population of 600 to 3,000 people). There are 20,640 districts in the project dataset.  #Domain: Finance and Housing  #Analysis Tasks to be performed:  #1. Build a model of housing prices to predict median house values in California using the provided dataset.  #2. Train the model to learn from the data to predict the median housing price in any district, given all the other metrics.  #3. Predict housing prices based on median_income and plot the regression chart for it.\n'], 'url_profile': 'https://github.com/reenathomas18', 'info_list': ['R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 3, 2020', '2', 'HTML', 'MIT license', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'Hanoi', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ddm198', 'info_list': ['R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 3, 2020', '2', 'HTML', 'MIT license', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': [""Classify-Song-Genres-from-Audio-Data\nUsing a dataset comprised of songs of two music genres (Hip-Hop and Rock), you will train a classifier to distinguish between the two genres based only on track information derived from Echonest (now part of Spotify). You will first make use of pandas and seaborn packages in Python for subsetting the data, aggregating information, and creating plots when exploring the data for obvious trends or factors you should be aware of when doing machine learning. Next, you will use the scikit-learn package to predict whether you can correctly classify a song's genre based on features such as danceability, energy, acousticness, tempo, etc. You will go over implementations of common algorithms such as PCA, logistic regression, decision trees, and so forth.  This project lets you apply what you learned in Supervised Learning with scikit-learn, plus data preprocessing, dimensionality reduction, and machine learning using the scikit-learn package.\n""], 'url_profile': 'https://github.com/rohanchutke', 'info_list': ['R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 3, 2020', '2', 'HTML', 'MIT license', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Reducing-Traffic-Mortality-in-the-USA\nWhile the rate of fatal road accidents has been decreasing steadily since the 80s, the past ten years have seen a stagnation in this reduction. Coupled with the increase in number of miles driven in the nation, the total number of traffic related-fatalities has now reached a ten year high and is rapidly increasing.  By looking at the demographics of traÔ¨Éc accident victims for each US state, we find that there is a lot of variation between states. Now we want to understand if there are patterns in this variation in order to derive suggestions for a policy action plan. In particular, instead of implementing a costly nation-wide plan we want to focus on groups of states with similar profiles. How can we find such groups in a statistically sound way and communicate the result effectively?\nThis project lets you apply skills from:\nIntroduction to Shell for Data Science, including how to navigate the file system and view files pandas Foundations, including reading, exploring, filtering, and grouping data Manipulating DataFrames with pandas, including how to reshape data into the long format and how to perform multiple aggregations Merging DataFrames with pandas, including how two merge two DataFrames Unsupervised Learning in Python, including KMeans clustering, dimensionally reduction through PCA, and visualizations using matplotlib Supervised Learning with scikit-learn, including multivariate regression Intermediate Python for Data Science, including visualizations using matplotlib Data Visualization With Seaborn, including statistical visualizations using seaborn\n'], 'url_profile': 'https://github.com/rohanchutke', 'info_list': ['R', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Feb 3, 2020', '2', 'HTML', 'MIT license', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Feb 2, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 28, 2020']}"
