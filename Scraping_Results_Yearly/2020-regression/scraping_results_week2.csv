"{'location': 'jaipur', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Linear_regression_projects\nProjects of Linear Regression\n'], 'url_profile': 'https://github.com/saurab-inurture', 'info_list': ['9', 'Jupyter Notebook', 'Updated Feb 18, 2021', '10', 'Python', 'Updated Jan 8, 2020', '3', 'Julia', 'MIT license', 'Updated Aug 27, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 3, 2020', '143', 'Jupyter Notebook', 'MIT license', 'Updated Oct 26, 2020', '9', 'R', 'Updated Feb 20, 2021', '4', 'Rust', 'MIT license', 'Updated Feb 21, 2021', '4', 'Python', 'Updated Sep 12, 2020', '73', 'Updated May 12, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['RLPA\nRegression and Learning with Pixel-wise Attention for Retinal Fundus Glaucoma Segmentation and Detection\nPaper\nThis work is one challenge workshop presented on MICCAI 2018. Our solution is in the top rank. The details can be found from https://refuge.grand-challenge.org/\n. The paper can be found from https://arxiv.org/abs/2001.01815\nData\nDownload data from https://drive.google.com/open?id=1d7L5BFP_QGAPYaQyK_fGgeM3WF8B5E86\nNetwork\nUNet and Deeplab+3 are the basis of our solutions.  We rebuilded them to fit the data in the challenge.\n'], 'url_profile': 'https://github.com/cswin', 'info_list': ['9', 'Jupyter Notebook', 'Updated Feb 18, 2021', '10', 'Python', 'Updated Jan 8, 2020', '3', 'Julia', 'MIT license', 'Updated Aug 27, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 3, 2020', '143', 'Jupyter Notebook', 'MIT license', 'Updated Oct 26, 2020', '9', 'R', 'Updated Feb 20, 2021', '4', 'Rust', 'MIT license', 'Updated Feb 21, 2021', '4', 'Python', 'Updated Sep 12, 2020', '73', 'Updated May 12, 2020']}","{'location': 'Stockholm', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['Splines2.jl package for regression splines\nA Julia package for regression splines. The package currently includes B-splines, natural B-splines, M-splines and I-splines.\nNews\nVersion 0.2.0:\n\nMainly bug fixes.\nA change of behaviour for Splines2.is_ and Splines2.is: intercept=true will include a columns of ones, while the default intercept=false will keep all of the spline terms, but exclude the column of ones. This behaviour is different to the splines2 package in R, which will give all of the spline terms for intercept=TRUE and drop the first spline term for intercept=FALSE.\n\nInstallation\nThe package is not currently registered. Installation from GitHub:\nusing Pkg; Pkg.add(PackageSpec(url = ""https://github.com/mclements/Splines2.jl""))\nUsage\nExported functions include Splines2.bs, Splines2.ns, Splines2.ms and Splines2.is, which provide evaluating spline bases for B-splines, natural B-splines, M-splines and I-splines, respectively. These functions take an ::Array{<:Real,1} argument and some design information and return the given spline basis.\nDocumentation for Splines2.bs\nbs(x :: Array{T,1}; <keyword arguments>) where T<:Real\nCalculate a basis for B-splines.\nThe keyword arguments include one of:\n\ndf, possibly in combination with intercept\nboundary_knots and interior_knots\nknots\n\nArguments\n\nboundary_knots :: Union{Tuple{T,T},Nothing} = nothing: boundary knots\ninterior_knots :: Union{Array{T,1},Nothing} = nothing: interior knots\norder :: Int32 = 4: order of the spline\nintercept :: Bool = false: bool for whether to include an intercept\ndf :: Int32 = order - 1 + Int32(intercept): degrees of freedom\nknots :: Union{Array{T,1}, Nothing} = nothing: full set of knots (excluding repeats)\ncentre :: Union{T,Nothing} = nothing): value to centre the splines\nders :: Int32 = 0: derivatives of the splines\n\nDocumentation for Splines2.bs_\nbs_(x :: Array{T,1}; <keyword arguments>) where T<:Real\nCalculate a basis for B-splines and return a function with signature\n(x:: Array{T,1}; ders :: Int32 = 0) for evaluation of ders\nderivative for the splines at x.\nThe keyword arguments include one of:\n\ndf, possibly in combination with intercept\nboundary_knots and interior_knots\nknots\n\nArguments\n\nboundary_knots :: Union{Tuple{T,T},Nothing} = nothing: boundary knots\ninterior_knots :: Union{Array{T,1},Nothing} = nothing: interior knots\norder :: Int32 = 4: order of the spline\nintercept :: Bool = false: bool for whether to include an intercept\ndf :: Int32 = order - 1 + Int32(intercept): degrees of freedom\nknots :: Union{Array{T,1}, Nothing} = nothing: full set of knots (excluding repeats)\ncentre :: Union{T,Nothing} = nothing): value to centre the splines\n\nThe documentation for the other bases are similar, except that the I-splines do not include the centre argument.\nExamples\nSome short examples are given below.\njulia> using Splines2\njulia> x = collect(0.0:0.1:1.0);\njulia> bs(x, df=3)\n\n11×3 Array{Float64,2}:\n 0.0    0.0    0.0  \n 0.243  0.027  0.001\n 0.384  0.096  0.008\n 0.441  0.189  0.027\n 0.432  0.288  0.064\n 0.375  0.375  0.125\n 0.288  0.432  0.216\n 0.189  0.441  0.343\n 0.096  0.384  0.512\n 0.027  0.243  0.729\n 0.0    0.0    1.0\n \njulia> ns(x, boundary_knots=(0.0,1.0), interior_knots=[0.2])\n\t\n11×2 Array{Float64,2}:\n 0.0        0.0      \n 0.196457  -0.106365 \n 0.363908  -0.179949 \n 0.479393  -0.194802 \n 0.544119  -0.152288 \n 0.565337  -0.0606039\n 0.550299   0.072056 \n 0.506256   0.237496 \n 0.44046    0.427522 \n 0.360161   0.633938 \n 0.272611   0.84855\n \n julia> ms(x, knots=[0.0,0.4,1.0], centre=0.4)\n\n11×4 Array{Float64,2}:\n -1.44      -1.92       -0.64      0.0      \n  0.6075    -1.665      -0.63      0.0      \n  1.14      -1.08       -0.56      0.0      \n  0.7425    -0.435      -0.37      0.0      \n  0.0        0.0         0.0       0.0      \n -0.606667   0.0244444   0.563704  0.0308642\n -1.01333   -0.284444    1.14963   0.246914 \n -1.26      -0.78        1.54      0.833333 \n -1.38667   -1.31556     1.51704   1.97531  \n -1.43333   -1.74444     0.862963  3.85802  \n -1.44      -1.92       -0.64      6.66667\n \n julia> is(x, df=4)\n\n11×4 Array{Float64,2}:\n 0.0     0.0     0.0     0.0   \n 0.3439  0.0523  0.0037  0.0001\n 0.5904  0.1808  0.0272  0.0016\n 0.7599  0.3483  0.0837  0.0081\n 0.8704  0.5248  0.1792  0.0256\n 0.9375  0.6875  0.3125  0.0625\n 0.9744  0.8208  0.4752  0.1296\n 0.9919  0.9163  0.6517  0.2401\n 0.9984  0.9728  0.8192  0.4096\n 0.9999  0.9963  0.9477  0.6561\n 1.0     1.0     1.0     1.0   \nWe also provide functions that return a function for evaluating spline bases with a function signature (x::Array{T<:Real,1}; ders::Int32 = 0). These are useful for ""safe"" predictions in regression modelling. As an example:\njulia> using Splines2, GLM, Random\njulia> Random.seed!(12345);\njulia> x = collect(range(0.0, length=301, stop=2.0*pi));\njulia> y = sin.(x)+randn(length(x)); \njulia> ns1 = Splines2.ns_(x,df=5,intercept=true); # this is a function\njulia> X = ns1(x);\njulia> fit1 = lm(X,y)\n\nLinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}}:\n\nCoefficients:\n────────────────────────────────────────────────────────────────────\n     Estimate  Std. Error    t value  Pr(>|t|)  Lower 95%  Upper 95%\n────────────────────────────────────────────────────────────────────\nx1   1.23751     0.269035   4.59981     <1e-5    0.708047   1.76698 \nx2   0.12448     0.249256   0.499407    0.6179  -0.366058   0.615018\nx3  -1.89278     0.256808  -7.37043     <1e-11  -2.39819   -1.38738 \nx4   0.187169    0.22469    0.833012    0.4055  -0.255023   0.629361\nx5  -0.240554    0.254986  -0.943404    0.3462  -0.742369   0.26126 \n────────────────────────────────────────────────────────────────────\n\njulia> newx = collect(0.0:0.5:3.5);\njulia> predict(fit1, ns1(newx)) # safe predictions\n\n8-element Array{Float64,1}:\n  0.2982757838333453 \n  0.6021897830602807 \n  0.8365641389496451 \n  0.9318592081638681 \n  0.8310124040845238 \n  0.5536590079608558 \n  0.14855743047881534\n -0.3299373638222967 \nUsing Splines2 with @formula\nWe provide code below for using the Splines2 package with @formula. Note that these do not provide ""safe"" predictions.\nusing StatsModels\nns(x,df) = Splines2.ns(x,df=df,intercept=true) # assumes intercept\nconst NSPLINE_CONTEXT = Any\nstruct NSplineTerm{T,D} <: AbstractTerm\n    term::T\n    df::D\nend\nBase.show(io::IO, p::NSplineTerm) = print(io, ""ns($(p.term), $(p.df))"")\nfunction StatsModels.apply_schema(t::FunctionTerm{typeof(ns)},\n                                  sch::StatsModels.Schema,\n                                  Mod::Type{<:NSPLINE_CONTEXT})\n    apply_schema(NSplineTerm(t.args_parsed...), sch, Mod)\nend\nfunction StatsModels.apply_schema(t::NSplineTerm,\n                                  sch::StatsModels.Schema,\n                                  Mod::Type{<:NSPLINE_CONTEXT})\n    term = apply_schema(t.term, sch, Mod)\n    isa(term, ContinuousTerm) ||\n        throw(ArgumentError(""NSplineTerm only works with continuous terms (got $term)""))\n    isa(t.df, ConstantTerm) ||\n        throw(ArgumentError(""NSplineTerm df must be a number (got $(t.df))""))\n    NSplineTerm(term, t.df.n)\nend\nfunction StatsModels.modelcols(p::NSplineTerm, d::NamedTuple)\n    col = modelcols(p.term, d)\n    Splines2.ns(col, df=p.df,intercept=true)\nend\nStatsModels.terms(p::NSplineTerm) = terms(p.term)\nStatsModels.termvars(p::NSplineTerm) = StatsModels.termvars(p.term)\nStatsModels.width(p::NSplineTerm) = 1\nStatsModels.coefnames(p::NSplineTerm) = ""ns("" .* coefnames(p.term) .* "","" .* string.(1:p.df) .* "")""\nTo show that this is not safe:\njulia> using DataFrames\njulia> d = DataFrames.DataFrame(x=x,y=y);\njulia> fit2 = lm(@formula(y~ns(x,5)+0),d) # equivalent to fit1 with nicer labels\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}}\n\ny ~ 0 + ns(x, 5)\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n          Estimate  Std. Error    t value  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\nns(x,1)   1.23751     0.269035   4.59981     <1e-5    0.708047   1.76698 \nns(x,2)   0.12448     0.249256   0.499407    0.6179  -0.366058   0.615018\nns(x,3)  -1.89278     0.256808  -7.37043     <1e-11  -2.39819   -1.38738 \nns(x,4)   0.187169    0.22469    0.833012    0.4055  -0.255023   0.629361\nns(x,5)  -0.240554    0.254986  -0.943404    0.3462  -0.742369   0.26126 \n─────────────────────────────────────────────────────────────────────────\n\njulia> predict(fit2, DataFrames.DataFrame(x=newx)) # unsafe predictions!\n\n8-element Array{Union{Missing, Float64},1}:\n  0.29827578383334535\n  0.7976143687096604 \n  0.8964195213823501 \n  0.40991870738161984\n -0.4167421148184624 \n -1.0400611367418444 \n -0.7710405835443831 \n  0.1787886772299305\nFor further details, see the discussion here.\n'], 'url_profile': 'https://github.com/mclements', 'info_list': ['9', 'Jupyter Notebook', 'Updated Feb 18, 2021', '10', 'Python', 'Updated Jan 8, 2020', '3', 'Julia', 'MIT license', 'Updated Aug 27, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 3, 2020', '143', 'Jupyter Notebook', 'MIT license', 'Updated Oct 26, 2020', '9', 'R', 'Updated Feb 20, 2021', '4', 'Rust', 'MIT license', 'Updated Feb 21, 2021', '4', 'Python', 'Updated Sep 12, 2020', '73', 'Updated May 12, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '445 contributions\n        in the last year', 'description': ['Vadoadra-House-Price-Prediction\nThis is a simple Linear Regression implementation machine learning model and deployment of the same using flask. Data-set of Vadodara House Price is created by us.\nHow to run this App\n\nJust run python app.py in command prompt for Windows or terminal in Linux or MacOS.\nThen in browser just type localhost:5000 on url and the app will run and you are ready to predict prices.\n\n'], 'url_profile': 'https://github.com/v1zh3d', 'info_list': ['9', 'Jupyter Notebook', 'Updated Feb 18, 2021', '10', 'Python', 'Updated Jan 8, 2020', '3', 'Julia', 'MIT license', 'Updated Aug 27, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 3, 2020', '143', 'Jupyter Notebook', 'MIT license', 'Updated Oct 26, 2020', '9', 'R', 'Updated Feb 20, 2021', '4', 'Rust', 'MIT license', 'Updated Feb 21, 2021', '4', 'Python', 'Updated Sep 12, 2020', '73', 'Updated May 12, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '1,702 contributions\n        in the last year', 'description': ['Logistic-Regression-From-Scratch-on-IRIS-Dataset\nFamous IRIS Dataset Classification Using Logistic_Regression\n\nClassfication Of (IRIS VERSICOLOR IRIS SETOSA IRIS VIRGINICA)\nAuthor\nCodePerfectPlus\n'], 'url_profile': 'https://github.com/codePerfectPlus', 'info_list': ['9', 'Jupyter Notebook', 'Updated Feb 18, 2021', '10', 'Python', 'Updated Jan 8, 2020', '3', 'Julia', 'MIT license', 'Updated Aug 27, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 3, 2020', '143', 'Jupyter Notebook', 'MIT license', 'Updated Oct 26, 2020', '9', 'R', 'Updated Feb 20, 2021', '4', 'Rust', 'MIT license', 'Updated Feb 21, 2021', '4', 'Python', 'Updated Sep 12, 2020', '73', 'Updated May 12, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '8,835 contributions\n        in the last year', 'description': ['\n#100DaysofMLCode\nTable of Contents\n1. Data Pre-processing\n\nImporting Libraries\nImporting Data sets\nHandling the missing data values\nEncoding categorical data\nSplit Data into Train data and Test data\nFeature Scaling\n\n2. Regression\n\nSimple Linear Regression\nMulti Linear Regression\nPolynomial Regression\nSupport Vector Regression\nDecision Tree Regression\nRandom Forest Regression\n\n3. Classification\n\nLogistic Regression\nK Nearest Neighbors Classification\nSupport Vector Machine\nKernel SVM\nNaive Bayes\nDecision Tree Classification\nRandom Forest Classification\n\n4. Clustering\n\nK-Means Clustering\nHierarchical Clustering\n\n5. Association Rule\n\nApriori\nEclat\n\n6. Reinforcement Learning\n\nUpper Confidence Bounds\nThompson Sampling\n\n7. Natural Language Processing \n\nAWS Comprehend\n\n8. Deep Learning\n\nArtificial Neural Networks (ANN)\n2. Convolutional Neural Networks (CNN)\n\n9. Dimensionality Reduction\n\nPrincipal Component Analysis\nLinear Discriminant Analysis\nKernel PCA\n\n10. Model Selection\n\nGrid Search\nK-fold Cross Validation\nXGBoost\n\n11. Data Visualization\n\nMatplotlib library in Python\nTableau\nPower BI\nGrafana\n\nLog of my Day-to-Day Activities\nTrack my daily activities here\nHow to Contribute\nThis is an open project and contribution in all forms are welcomed.\nPlease follow these Contribution Guidelines\nCode of Conduct\nAdhere to the GitHub specified community code.\nLicense\nCheck the official MIT License here.\n'], 'url_profile': 'https://github.com/NishkarshRaj', 'info_list': ['9', 'Jupyter Notebook', 'Updated Feb 18, 2021', '10', 'Python', 'Updated Jan 8, 2020', '3', 'Julia', 'MIT license', 'Updated Aug 27, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 3, 2020', '143', 'Jupyter Notebook', 'MIT license', 'Updated Oct 26, 2020', '9', 'R', 'Updated Feb 20, 2021', '4', 'Rust', 'MIT license', 'Updated Feb 21, 2021', '4', 'Python', 'Updated Sep 12, 2020', '73', 'Updated May 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '414 contributions\n        in the last year', 'description': ['glueformula: string interpolation to build regression formulas\nThe main function is gl. It is a variant of glue to simplify\nbuilding regression formulas given vectors of variable names.\nlibrary(glueformula)\n\n# Example: build a formula for ivreg\n# with gf\n\ncontr = c(""x1"",""x2"",""x3"") # exogenous control variables\ninstr = c(contr,""z1"",""z2"") # instruments\n\ngf(q ~ p + {contr} | {instr})\n\nAnother helper function is varnames_snippet. It just uses the colnames of a data frame to create a code snippet with a vector of all variable names. Here is an example:\ndf = dplyr::tibble(x=1,y=5,z=4,`a b`=4)\nvarnames_snippet(df)\n\nThis prints (and on Windows copies to your clipboard) the following code snippet:\nc(""x"", ""y"", ""z"", ""`a b`"")\nYou can then manually select the used variables in your regression by deleting the unused variables. With many variables, this can be much quicker than writing down all used variables.\nInstallation\nThe easiest way to install glueformula is from my Github powered package repository by running:\ninstall.packages(""glueformula"",repos = c(""https://skranz-repo.github.io/drat/"",getOption(""repos"")))\nAlteneratively, you can run the following code to install the source package from Github:\nif (!require(glue)) install.packages(""glue"")\nif (!require(remotes)) install.packages(""remotes"")\nremotes::install_github(""skranz/glueformula"")\n\nIf the Github installation fails because unimportant warnings are converted to errors run before the installation:\nSys.setenv(R_REMOTES_NO_ERRORS_FROM_WARNINGS=""true"")\nSee https://remotes.r-lib.org/#environment-variables\n'], 'url_profile': 'https://github.com/skranz', 'info_list': ['9', 'Jupyter Notebook', 'Updated Feb 18, 2021', '10', 'Python', 'Updated Jan 8, 2020', '3', 'Julia', 'MIT license', 'Updated Aug 27, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 3, 2020', '143', 'Jupyter Notebook', 'MIT license', 'Updated Oct 26, 2020', '9', 'R', 'Updated Feb 20, 2021', '4', 'Rust', 'MIT license', 'Updated Feb 21, 2021', '4', 'Python', 'Updated Sep 12, 2020', '73', 'Updated May 12, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '276 contributions\n        in the last year', 'description': ['ndarray-glm\nRust library for solving linear, logistic, and generalized linear models through\niteratively reweighted least squares, using the\nndarray-linalg module.\n\n\n\nStatus\nThis package is in early alpha and the interface is likely to undergo many\nchanges. Functionality may change from one release to the next.\nThe regression algorithm uses iteratively re-weighted least squares (IRLS) with\na step-halving procedure applied when the next iteration of guesses does not\nincrease the likelihood.\nMuch of the logic is done at the type/trait level to avoid compiling code a user does\nnot need and to allow general implementations that the compiler can optimize in trivial\ncases.\nPrerequisites\nThe recommended approach is to use a system BLAS implementation. For instance, to install\nOpenBLAS on Debian/Ubuntu:\nsudo apt update && sudo apt install -y libopenblas-dev\n\nThen use this crate with the openblas-system feature.\nTo use an alternative backend or to build a static BLAS implementation, refer to the\nndarray-linalg\ndocumentation. Use\nthis crate with the appropriate feature flag and it will be forwarded to\nndarray-linalg.\nExample\nTo use in your crate, add the following to the Cargo.toml:\nndarray = { version = ""0.14"", features = [""blas""]}\nndarray-glm = { version = ""0.0.8"", features = [""openblas-system""] }\n\nAn example for linear regression is shown below.\nuse ndarray_glm::{array, Linear, ModelBuilder, standardize};\n\n// define some test data\nlet data_y = array![0.3, 1.3, 0.7];\nlet data_x = array![[0.1, 0.2], [-0.4, 0.1], [0.2, 0.4]];\n// The design matrix can optionally be standardized, where the mean of each independent\n// variable is subtracted and each is then divided by the standard deviation of that variable.\nlet data_x = standardize(data_x);\n// The interface takes `ArrayView`s to allow for efficient passing of slices.\nlet model = ModelBuilder::<Linear>::data(data_y.view(), data_x.view()).build()?;\n// L2 (ridge) regularization can be applied with l2_reg().\nlet fit = model.fit_options().l2_reg(1e-5).fit()?;\n// Currently the result is a simple array of the MLE estimators, including the intercept term.\nprintln!(""Fit result: {}"", fit.result);\nFor logistic regression, the y array data must be boolean, and for Poisson\nregression it must be an unsigned integer.\nCustom non-canonical link functions can be defined by the user, although the\ninterface is not particularly ergonomic. See tests/custom_link.rs for examples.\nFeatures\n\n Linear regression\n Logistic regression\n Generalized linear model IRLS\n Linear offsets\n Generic over floating point type\n Non-float domain types\n L2 (ridge) Regularization\n L1 (lasso) Regularization\n\nAn experimental smoothed version with an epsilon tolerance is WIP\n\n\n Other exponential family distributions\n\n Poisson\n Binomial (nightly only)\n Exponential\n Gamma\n Inverse Gaussian\n\n\n Option for data standardization/normalization\n Weighted and correlated regressions\n Non-canonical link functions\n Goodness-of-fit tests\n\nReference\nThese notes on generalized linear models\nsummarize many of the relevant concepts and provide some additional references.\n'], 'url_profile': 'https://github.com/felix-clark', 'info_list': ['9', 'Jupyter Notebook', 'Updated Feb 18, 2021', '10', 'Python', 'Updated Jan 8, 2020', '3', 'Julia', 'MIT license', 'Updated Aug 27, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 3, 2020', '143', 'Jupyter Notebook', 'MIT license', 'Updated Oct 26, 2020', '9', 'R', 'Updated Feb 20, 2021', '4', 'Rust', 'MIT license', 'Updated Feb 21, 2021', '4', 'Python', 'Updated Sep 12, 2020', '73', 'Updated May 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': [""use backpropagation to solve linear regression in 3D space\nlinear_regression_closed_form.py use linear algebra method to find the best fit model in one shot.\ngradient_descent_visualization.py only one plot is drawn, easy for interactivity.\n3plots_1costFunction.py three plots from different angle plus one cost function plot are drawn.it takes computational power to process.\n\n1, use numpy and sklearn to extract Boston housing prices data. two attributes are chosen .\n2, implement gradient descent using python\nhttps://towardsdatascience.com/implementation-of-multi-variate-linear-regression-in-python-using-gradient-descent-optimization-b02f386425b9\n3, draw a plane in matplotlib . https://stackoverflow.com/questions/36013063/what-is-the-purpose-of-meshgrid-in-python-numpy the key of drawing a plane is to use meshgrid.\n3, use the history of theta(the parameters for this regression model ) and animate 'historical plane' using matplotlib.animation function.\nhttps://brushingupscience.com/2016/06/21/matplotlib-animations-the-easy-way/\nreddit discussion\nhttps://www.reddit.com/r/learnmachinelearning/comments/enleb8/gradient_descent_visualisation_in_linear/\n""], 'url_profile': 'https://github.com/zhanggiene', 'info_list': ['9', 'Jupyter Notebook', 'Updated Feb 18, 2021', '10', 'Python', 'Updated Jan 8, 2020', '3', 'Julia', 'MIT license', 'Updated Aug 27, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 3, 2020', '143', 'Jupyter Notebook', 'MIT license', 'Updated Oct 26, 2020', '9', 'R', 'Updated Feb 20, 2021', '4', 'Rust', 'MIT license', 'Updated Feb 21, 2021', '4', 'Python', 'Updated Sep 12, 2020', '73', 'Updated May 12, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': [""Towards a complete 3D morphable model of the human head\nStylianos Ploumpis 1,2, Evangelos Ververas 1,2, Eimear O'Sullivan 1,2, Stylianos Moschoglou 1,2, Haoyang Wang 1, Nick Pears 3, William A. P. Smith 3, Baris Gecer 1,2, & Stefanos Zafeiriou 1,2\n\n1 Imperial College London, UK\n\n2 FaceSoft.io\n\n3 University of York, UK\n\n (3D Graphic visuals by Vasilis Triantafyllou)\n\n[CVPR 2019] Extension in [TPAMI 2020]\n\n\n\n\nAbstract\nThree-dimensional Morphable Models (3DMMs) are powerful statistical tools for representing the 3D shapes and textures of an object class. Here we present the most complete 3DMM of the human head to date that includes face, cranium, ears, eyes, teeth and tongue. To achieve this, we propose two methods for combining existing 3DMMs of different overlapping head parts: i. use a regressor to complete missing parts of one model using the other, ii. use the Gaussian Process framework to blend covariance matrices from multiple models. Thus we build a new combined face-and-head shape model that blends the variability and facial detail of an existing face model (the LSFM) with the full head modelling capability of an existing head model (the LYHM). Then we construct and fuse a highly-detailed ear model to extend the variation of the ear shape. Eye and eye region models are incorporated into the head model, along with basic models of the teeth, tongue and inner mouth cavity. The new model achieves state-of-the-art performance. We use our model to reconstruct full head representations from single, unconstrained images allowing us to parameterize craniofacial shape and texture, along with the ear shape, eye gaze and eye color.\nApproach\n\nDetailed overview of our regression modeling pipeline. 1) The left part illustrates the matrix formulation from the original LYHM head model; 2) the central part demonstrates how we utilize the MeIn3D database to produce highly-detailed head shapes; 3) the final part on the right depicts the registration framework along with the per-vertex template weights and the statistical modeling.\n\nA graphical representation of the non-rigid registration of all mean meshes along with our head template St and the calculation of the local covariance matrix Ki,j based on the locations of the ith and jth points.\n\nEmbeded Ear and Eye 3DMM\n\n\n\nHead Reconstruction Results from single images\n\n\nCitation\nIf you find this work is useful for your research, please cite our papers:\n@inproceedings{ploumpis2019combining,\n  title={Combining 3D Morphable Models: A Large scale Face-and-Head Model},\n  author={Ploumpis, Stylianos and Wang, Haoyang and Pears, Nick and Smith, William AP and Zafeiriou, Stefanos},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={10934--10943},\n  year={2019}\n}\n\n@article{ploumpis2020towards,\n  title={Towards a complete 3D morphable model of the human head},\n  author={Ploumpis, Stylianos and Ververas, Evangelos and O'Sullivan, Eimear and Moschoglou, Stylianos and Wang, Haoyang and Pears, Nick     and Smith, William and Gecer, Baris and Zafeiriou, Stefanos P},\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  year={2020},\n  publisher={IEEE}\n}\n\n\n\nPublic release models\nThe head models have been freely available for research and education purposes. To obtain access to the models, you need to complete and sign the user agreement form (can be found in this repository, user_agreement.pdf). This agreement should be completed by a full-time academic staff member (not a student) or a craniofacial clinician. The form should be signed, and emailed to Stylianos Ploumpis (s.ploumpis@imperial.ac.uk), Nick Pears (nick.pears@york.ac.uk). We will verify your request and contact you on how to download the model package. Note that the agreement requires that:\n\nThe models are used for non-commercial research and education purposes only.\nYou agree not copy, sell, trade, or exploit the model for any commercial purposes.\nIn any published research using the models, you cite the following papers:\n\nCombining 3D Morphable Models: A Large scale Face-and-Head Model, S.Ploumpis, H.Wang, N.Pears, W. AP Smith, S. Zafeiriou, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (CVPR) 2019\nTowards a complete 3D morphable model of the human head, S.Ploumpis, E.Ververas, E.Sullivan, S.Moschoglou, H.Wang, N.Pears, W.Smith and S.Zafeiriou, Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2020\n\nOral CVPR 19 presentation\n\n""], 'url_profile': 'https://github.com/steliosploumpis', 'info_list': ['9', 'Jupyter Notebook', 'Updated Feb 18, 2021', '10', 'Python', 'Updated Jan 8, 2020', '3', 'Julia', 'MIT license', 'Updated Aug 27, 2020', '5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'MIT license', 'Updated Jun 3, 2020', '143', 'Jupyter Notebook', 'MIT license', 'Updated Oct 26, 2020', '9', 'R', 'Updated Feb 20, 2021', '4', 'Rust', 'MIT license', 'Updated Feb 21, 2021', '4', 'Python', 'Updated Sep 12, 2020', '73', 'Updated May 12, 2020']}"
"{'location': 'Indore', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['Tarang-Jain\n'], 'url_profile': 'https://github.com/waves025', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 25, 2020', '4', 'R', 'Updated Apr 28, 2020', 'Python', 'MPL-2.0 license', 'Updated Jul 28, 2020', '2', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'Melbourne, Australia', 'stats_list': [], 'contributions': '291 contributions\n        in the last year', 'description': ['\nRRRR\n\n\n\n\nThe R package RRRR provides methods for estimating online Robust\nReduced-Rank Regression.\nTo cite package ‘RRRR’ in publications use:\n\nYangzhuoran Fin Yang and Ziping Zhao (2020). RRRR: Online Robust\nReduced-Rank Regression Estimation. R package version 1.1.0.\nhttps://pkg.yangzhuoranyang.com/RRRR/.\n\nInstallation\nYou can install the stable version on R\nCRAN.\ninstall.packages(""RRRR"")\nYou can install the development version from\nGithub with:\n# install.packages(""devtools"")\ndevtools::install_github(""FinYang/RRRR"")\nUsage\nThe R package RRRR provides the following estimation methods.\n\nReduced-Rank Regression using Gaussian MLE: RRR\nRobust Reduced-Rank Regression using Cauchy distribution and\nMajorisation-Minimisation: RRRR\nOnline Robust Reduced-Rank Regression: ORRRR\n\nSMM: Stochastic Majorisation-Minimisation\nSAA: Sample Average Approximation\n\n\nOnline update of the above model (except RRR): update.RRRR\n\nSee the vignette for a more detailed illustration.\nlibrary(RRRR)\nset.seed(2222)\ndata <- RRR_sim()\nres <- ORRRR(y=data$y, x=data$x, z=data$z)\nres\n#> Online Robust Reduced-Rank Regression\n#> ------\n#> Stochastic Majorisation-Minimisation\n#> ------------\n#> Specifications:\n#>            N            P            R            r initial_size        addon \n#>         1000            3            1            1          100           10 \n#> \n#> Coefficients:\n#>          mu         A         B         D    Sigma1    Sigma2    Sigma3\n#> 1  0.078343 -0.167661  1.553252  0.204748  0.656940 -0.044872  0.050316\n#> 2  0.139471  0.442293  0.919832  1.138335 -0.044872  0.657402 -0.063890\n#> 3  0.106746  0.801818 -0.693768  1.955019  0.050316 -0.063890  0.698777\nplot(res)\n\nnewdata <- RRR_sim(A = data$spec$A,\n                   B = data$spec$B,\n                   D = data$spec$D)\nres2 <- update(res, newy=newdata$y, newx=newdata$x, newz=newdata$z)\nres2\n#> Online Robust Reduced-Rank Regression\n#> ------\n#> Stochastic Majorisation-Minimisation\n#> ------------\n#> Specifications:\n#>            N            P            R            r initial_size        addon \n#>         2000            3            1            1         1010           10 \n#> \n#> Coefficients:\n#>          mu         A         B         D    Sigma1    Sigma2    Sigma3\n#> 1  0.073939  0.159814 -1.520309  0.208943  0.675436 -0.021789  0.040888\n#> 2  0.142791 -0.450992 -0.962698  1.117024 -0.021789  0.679136 -0.024140\n#> 3  0.107647 -0.817590  0.670435  1.957084  0.040888 -0.024140  0.703949\nplot(res2)\n\nLicense\nThis package is free and open source software, licensed under GPL-3.\n'], 'url_profile': 'https://github.com/FinYang', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 25, 2020', '4', 'R', 'Updated Apr 28, 2020', 'Python', 'MPL-2.0 license', 'Updated Jul 28, 2020', '2', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['SSC442\nThis repo will be used for hosting and tracking commits to files across the entire semester of SSC442. It will hold indpendent projects related to SSC442 on top of the current SSC442 class repo files that will be updated weekly.\n'], 'url_profile': 'https://github.com/lathamri', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 25, 2020', '4', 'R', 'Updated Apr 28, 2020', 'Python', 'MPL-2.0 license', 'Updated Jul 28, 2020', '2', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fenix Nightly Performance Regression Monitoring System\nScripts to automatically test the startup performance of Fenix\'s\nnightly variant of Fenix or manually test the startup performance of\nFennec or Fenix.\nUsage\nTo use the system automatically, run:\n\ntest.sh to run the test & generate logs\ndo_times.sh to take the logs and extract timing information from them\nplot.sh to plot the timing information\n\nTo use the system manually, run:\n\nmanual_test.sh to run the test & generate logs\nmanual_do_times.sh to take the logs and extract timing information from them\nplot.sh to plot the timing information\n\nInstallation\nPre-run Customization:\ncommon_devices.sh:\n\nfpm_adb and ADB: Change to the location of adb on the system and remove the serial parameter. e.g.\n\n    -fpm_adb=""/opt/fnprms/Android/Sdk/platform-tools/adb""\n    -export ADB=""${fpm_adb} -s ${fpm_dev_serial}""\n    +fpm_adb="".../Library/Android/sdk/platform-tools/adb""\n    +export ADB=""${fpm_adb}""\n\ncommon.sh:\n\n``fpm_log_dir`: Change the location to store test output logs (default is only writable on root)\n\nmanual_test.sh:\n\nrun_test...: lower integer run count if you don\'t want the test to take as long (though this may impact accuracy)\n\nRunning manually\nA typical invocation to run start to homescreen tests on Fenix\'s fennecNightly variant:\n./manual_test.sh <fennec-nightly-apk> hanoob fennec-nightly && ./manual_do_times.sh fennec-nightly && cat <logs>/hanoob-results.csv\n\nLICENSE\nThis Source Code Form is subject to the terms of the Mozilla Public\nLicense, v. 2.0. If a copy of the MPL was not distributed with this\nfile, You can obtain one at http://mozilla.org/MPL/2.0/\n\n'], 'url_profile': 'https://github.com/mozilla-mobile', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 25, 2020', '4', 'R', 'Updated Apr 28, 2020', 'Python', 'MPL-2.0 license', 'Updated Jul 28, 2020', '2', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Heteroscedastic multicollinear regression, version 1.0\nThe software implemented in R presents a model regression analysis of\na complex dataset under both heteroscedasticity and multicollinearity.\nWhile standard tools are used here as well, the main novelty\nis the usage of regularized regression quantiles, including tools\nfor graphical output and variable selection. This very detailed analysis\nwas tested over a real economic dataset. The analysis without this\nsoftware would require a tedious work to perform variable selection\nfrom the regularized regression quantiles.\nFeel free to use or modify the code.\nRequirements\nYou need to install these packages of R software: glmnet, quantreg, Qtools.\nUsage\n\nWe recommend to perform the analysis in this order, starting with traditional statistical tools and proceeding to regression quantiles and\nfinally regularized regression quantiles: StandardAnalysis.R, Quantiles.R, RegularizedQuantiles.R.\n\nAuthors\n\nJan Kalina, The Czech Academy of Sciences, Institute of Computer Science\nEva Litavcová, University of Prešov\nPetra Vašaničová, University of Prešov\n\nContact\nDo not hesitate to contact us (kalina@cs.cas.cz) or write an Issue.\nHow to cite\nPlease consider citing the following:\nKalina J, Vašaničová P, Litavcová E (2019): Regression quantiles under heteroscedasticity and multicollinearity: Analysis of travel and tourism\ncompetitiveness. Ekonomický časopis 67 (1), 69-85.\nAcknowledgement\nThis work was supported by the Czech Science Foundation grant GA19-05704S.\n'], 'url_profile': 'https://github.com/jankalinaUI', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 25, 2020', '4', 'R', 'Updated Apr 28, 2020', 'Python', 'MPL-2.0 license', 'Updated Jul 28, 2020', '2', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['regression\n'], 'url_profile': 'https://github.com/kunparekh', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 25, 2020', '4', 'R', 'Updated Apr 28, 2020', 'Python', 'MPL-2.0 license', 'Updated Jul 28, 2020', '2', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nik6dhurye', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 25, 2020', '4', 'R', 'Updated Apr 28, 2020', 'Python', 'MPL-2.0 license', 'Updated Jul 28, 2020', '2', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'Reading, UK', 'stats_list': [], 'contributions': '186 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bHodges97', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 25, 2020', '4', 'R', 'Updated Apr 28, 2020', 'Python', 'MPL-2.0 license', 'Updated Jul 28, 2020', '2', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ArtHerasymov', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 25, 2020', '4', 'R', 'Updated Apr 28, 2020', 'Python', 'MPL-2.0 license', 'Updated Jul 28, 2020', '2', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Regression\nThese assignments were undertaken as a part of CS F320 Foundation of Data Science.\nTeam:\n\nShreeya Nelekar\nPrateek Agarwal\nShriya Choudhary\nSurabhi Toshniwal\n\n'], 'url_profile': 'https://github.com/Shreeya1699', 'info_list': ['2', 'Jupyter Notebook', 'Updated Feb 18, 2020', '2', 'R', 'Updated Jul 25, 2020', '4', 'R', 'Updated Apr 28, 2020', 'Python', 'MPL-2.0 license', 'Updated Jul 28, 2020', '2', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': [""regression\n\n\n\n\n\n\nWeb app for\nregression analysis\nof provided data files\n\nNOTE: This project is no longer developed nor maintained.\nChangelog »\n\nInstallation\nInstall with pip:\n$ pip install regression\n\nUsage\nTo open config.cfg file and change configuration\nvalues\nrun:\n$ regression config\nDeploy web app to a WSGI server or run locally on Flask's built-in server:\n$ regression run\n\n\nFiles\n\nProvide your data files in one of the following ways:\n\n\nusing upload form, which will save the file in the app root directory,\nby copying them into into the app subdirectory\n(run $ regression path to find the copy path),\nby adding links to files stored online as URL dict of config file:\n\n$ regression config\n...\nURL = {'name': 'download_link'} # 'name' supports HTML format\n\nSupported file formats and extensions:\n\n\nDelimited text files (CSV)\n\nindex,features,...,outcome  # First row is a header\nitem 1,X11,X12,...,X1p,y1   # Follow order: index, X, y\nitem 2,X21,X22,...,X2p,y2   # Features can be given as\n...                         # numerical or categorical\nitem n,Xn1,Xn2,...,Xnp,yn   # Use ';' when decimal sep is ','\n\nExcel files (XLS, XLSX)\n\n\n\nindex\nfeature 1\n...\nfeature p\noutcome\n\n\n\nitem 1\nX11\n...\nX1p\ny1\n\nitem 2\nX21\n...\nX2p\ny2\n\n...\n...\n...\n...\n...\n\nitem n\nXn1\n...\nXnp\nyn\n\n\n\n""], 'url_profile': 'https://github.com/makr3la', 'info_list': ['Python', 'MIT license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Updated Jan 12, 2020', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 7, 2020', '2', 'MATLAB', 'MIT license', 'Updated Jan 9, 2020', '3', 'MATLAB', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['ML_Regression\nLinear regression, ridge regression and lasso regression\nData-set can be downloaded at https://www.kaggle.com/c/web-traffic-time-series-forecasting/data\n'], 'url_profile': 'https://github.com/Umerqazi786', 'info_list': ['Python', 'MIT license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Updated Jan 12, 2020', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 7, 2020', '2', 'MATLAB', 'MIT license', 'Updated Jan 9, 2020', '3', 'MATLAB', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['linear_regression_tutorial\n'], 'url_profile': 'https://github.com/PyExtreme', 'info_list': ['Python', 'MIT license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Updated Jan 12, 2020', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 7, 2020', '2', 'MATLAB', 'MIT license', 'Updated Jan 9, 2020', '3', 'MATLAB', 'Updated Jan 9, 2020']}","{'location': 'S.Korea', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Linear regression (선형 회귀) / Logistic regression (로지스틱 회귀)\n회귀분석이나 분산분석은 종속변수가  정규분포되어 있는 연속형 변수 이다.\n하지만 많은 경우에 있어서 종속변수가 정규분포되어 있다는 가정을 할 수 없는 경우도 있으며 범주형 변수가 종속변수인 경우도 있다. 다음과 같은 경우에 일반화 선형모형을 사용한다.\n\n종속변수가 범주형변수인 경우 : 이항변수(0 또는 1, 합격/불합격, 사망.생존 등)인 경우도 있으며 다항변수(poor,good,excellent / 공화당,민주당,무소속)인 경우 정규분포 하지 않는다.\n종속변수가 count인 경우 이들 값은 매우 제한적이며 음수가 되지 않고 평균과 분산이 밀접하게 관련되어 있고 정규분포 하지 않는다.\n\n일반화 선형 모형은 종속변수가 정규분포하지 않는 경우를 포함하는 선형모형의 확장이며 glm()함수를 사용한다.\n\n\nLinear regression (선형 회귀)\n수치 예측을 위한 선형 회귀 사용\n입력값의 선형 가산 함수인 숫자형 결괏값을 예측하는 모델을 만든다. 이것은 매우 효과적인 근사치로실제는 비선형인 경우에도 유용하다.\n결과 모델은 각각의 입력변수가 결과에 상대적으로 미치는 효과의 예측을 제공한다.  숫자 값을 예측하고자 할 때 가장 최선의 모델이다.\n\n\nLogistic regression (로지스틱 회귀)\n종속변수가 이항변수인 경우로 의학연구에서 많이 사용된다. (fmaily = binomial)  != family = gaussian (정규분포)\n확률과 비율을 예측하는데 적합한 모델로 항상 0과 1 사이의 값으로 예측한다.\n로지스틱 회귀는 분류화 시키는 것 이외에  범주의 확률을 예측하고자 할 때 유용하다.(어떤 대상이 주어진 범주에 포함될 확률) 또한 다른 입력변수과 결괏값에 상대적으로 미치는 영향을 알고자 할 때도 유용하다.  의학 연구에 많이 사용된다.\nex. 신용카드의 사기 확률 예측\nex. 구매 금액을 100달러 증가할 때 사기 거래율이 증가하는지 이전과 동등한지 파악 가능\n\n\nT-test\nT-test ? : 모집단의 표준편차가 알려지지 않았을 때, 정규분포의 모집단에서 모은 샘플(표본)의 평균값에 대한 가설검정정 방법\n\'why we call it like this? ; 1908年 Wiliiam Sealy Gosset이 개발한 방법인데, 그의 가명이 studen \'t\' 였다.\n목적 : 두개의 집단이 같은지 다른지 비교하기 위해 사용한다.\n집단이란, 샘플(표본)을 일반적으로 얘기한다. 모집단(population)이라는 개념도 존재한다. 무슨 차이가?\n\n모집단 :   대한민국 국민 전체\n관측치 = N\n표본(샘플) : 게 중 몇명\n\'관측치 = n\n대부분 논문은 표본(샘플)에 대한 내용으로, n, x,bar, s, s-square로 표기한다.  \n\nex. A대학 178.5  , B대학 179.9cm\n: A대학과 B대학의 남학생 푠균키 차이인 1.4cm 우연히 발생했을 확률은 얼마나 될까?   1.4cm는 과연 큰 것인가 작은 것인가? 우리는 1.4cm가 얼마나 큰지 혹은 작은지 알 수 없기에, 그 기준을 정할 비교 대상이 필요하다. 누구를 가지고와서 비교해야 할까요?\n\n표준편차(분산) 이 그 역할을 한다.\n; 우리가 가진 자료가 {1,2,3,4,5}이면, 평균은 3, 분산은 2.5, 표준편차는 1.58이다.\n; 우리가 가진 데이터는 평균값 3을 중심을으로 평균적으로 1.58만큼 퍼져 있다는 의미가 됩니다.\n\nZ-test, T-test?\n\nz-test : 모집단을 분석\nt-test : 샘플(표본)을 분석\n\n정규분포\n\n종모양 (bell shape)\n평균값 뮤를 중심으로 좌우가 대칭이다.\n정규분포의 양 끝은 영원히 0에 닿지 않는다.\n\n표준정규분포\n\n평균이 0이고, 표준편차가 ""1""인 정규분포\n무한대 가지의 정규분포 곡선을 적분하는 번거로움을 덜기 위해\n; z-test :  z-score(z값)고 표준정규분포표를 이용하는 테스트 = z-transformation ; (= standardization 표준화)\n\n결론\n\n두집단의 평균값의 차이가 표준편차보다 현전히 작으면, 우리는 이 차이가 우연히 발생했다고 결론을 내릴 것 입니다.\n반대로, 두집단의 평균값의 차이가 표준편차보다 현저히 크면, 우리는 이 차차이가 우연히 발생하지 않았다고 결론을 내릴 것 입니다.\n통계학이란 분산*(표준편차)의 마법이라고 하였다. 즉, t-test는 평균균값의 차이와 표준편차의 비율이 얼마나 큰지/작은지 보고 결정하는 과정\n\n'], 'url_profile': 'https://github.com/seunghuilee91', 'info_list': ['Python', 'MIT license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Updated Jan 12, 2020', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 7, 2020', '2', 'MATLAB', 'MIT license', 'Updated Jan 9, 2020', '3', 'MATLAB', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sdanish94', 'info_list': ['Python', 'MIT license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Updated Jan 12, 2020', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 7, 2020', '2', 'MATLAB', 'MIT license', 'Updated Jan 9, 2020', '3', 'MATLAB', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Linear-Regression-in-Python\nTable of Conten\n➢Regression\n➢ What Is Regression?\n➢ Linear Regression\n➢ Problem Formulation\n➢ Regression Performance\n➢ Simple Linear Regression\n➢ Multiple Linear Regression\n➢ Polynomial Regression\n➢ Underfitting and Overfitting\n➢ Implementing Linear Regression in Python\n➢ Python Packages for Linear Regression\n➢ Simple Linear Regression With scikit-learn\n➢ Multiple Linear Regression With scikit-learn\n➢ Polynomial Regression With scikit-learn\n➢ Advanced Linear Regression With statsmodels\n➢ Beyond Linear Regression ➢ Conclusion.\n'], 'url_profile': 'https://github.com/VIKAS96-AI', 'info_list': ['Python', 'MIT license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Updated Jan 12, 2020', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 7, 2020', '2', 'MATLAB', 'MIT license', 'Updated Jan 9, 2020', '3', 'MATLAB', 'Updated Jan 9, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Regression-Model\nHow to choose the best fit regression model.\nThe dataset was gotten from http://www-eio.upc.edu/~pau/cms/rdata/datasets.html\n'], 'url_profile': 'https://github.com/frex1', 'info_list': ['Python', 'MIT license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Updated Jan 12, 2020', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 7, 2020', '2', 'MATLAB', 'MIT license', 'Updated Jan 9, 2020', '3', 'MATLAB', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['sentiment-analysis-using-machine-learning\nSentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations\nText Classifier — The basic building blocks\nSentiment Analysis\nSentiment Analysis is the most common text classification tool that analyses an incoming message and tells whether the underlying sentiment is positive, negative our neutral.\nPython is used as programming.\nWe have used Bag of Words method to map text into vector representations.Logistic Regression and Suppoer Vector Classifier(LinearSVC) is used as model.\nWhat we do is :\n1)We clean the given data.Symbols are not useful for analysis.Also we remove stopwords like ""is,are,am,have etc.."" because they are not the decideing words of positive or negative sentiment in the text.\n2)Represent every text using vector.From training data, we create a large matrix(Sparse matrix) whose column represent every words in those data,and rows are the actual text in the form of 1 and 0. Words of text corrosponds to column is set to 1 and all other columns\nare 0.\n3)Feed this data to logistic and svc model ,so it can train itself about which words appear in the positive sentense and negative sentence.It will assign weight to every word if this word is used for positive/negative feedback.\n4)Now,find another texts for which we want to find sentiment,clean that data,vectorise it and give it to model.\nIt will tell us if it is negative or positive.\n'], 'url_profile': 'https://github.com/umang25011', 'info_list': ['Python', 'MIT license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Updated Jan 12, 2020', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 7, 2020', '2', 'MATLAB', 'MIT license', 'Updated Jan 9, 2020', '3', 'MATLAB', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['SPQR\nPenalized Kernel Quantile Regression for Varying Coefficient Models\nOverview\nSPQR is a novel and efficient penalized kernel smoothing algorithm that consistently identifies the partially linear structure in the varying coefficient quantile regression model.\nEfficient ADMM algorithm is developed for implementation.\nMain functions\n\n\n[main_admm.m]: Perform the proposed ADMM algorithm to obtain SPQR estimate.\n\n\n[simulation_example.m]: Perform simulation analysis when n=400, p=7, and tau=0.25.\n\n\nSupplementary functions\n\n\n[keru.m]:  Epanechnikov kernel function used for constructing kernels.\n\n\n[scad.m]: Scad derivation function used to obtain weight parameters in SPQR.\n\n\n[rq.m]: Solve conventional quantile regression problem using the dual problem. Created by Roger Koenker (http://www.econ.uiuc.edu/~roger/research/rq/rq.html).\n\n\nAuthor\n\nEun Ryung Lee and Seyoung Park\nDepartment of Statistics, Sungkyunkwan University\n\nContact\n\nishspsy@skku.edu\n\nLicense\nThis project is licensed under the MIT License.\n'], 'url_profile': 'https://github.com/ishspsy', 'info_list': ['Python', 'MIT license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Updated Jan 12, 2020', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 7, 2020', '2', 'MATLAB', 'MIT license', 'Updated Jan 9, 2020', '3', 'MATLAB', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chenzhe207', 'info_list': ['Python', 'MIT license', 'Updated Jan 1, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 26, 2020', 'Updated Jan 12, 2020', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 7, 2020', '2', 'MATLAB', 'MIT license', 'Updated Jan 9, 2020', '3', 'MATLAB', 'Updated Jan 9, 2020']}"
"{'location': 'Moscow', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/z106a', 'info_list': ['JavaScript', 'Updated Jan 9, 2020', '3', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '820 contributions\n        in the last year', 'description': ['BostonHousePrediction\nPredicts the price of house in the boston dataset using a multiple linear regression and polynomial regression.\nFrom the model evaluation a polynomial regression with a degree of 2 is the best model for the dataset\n'], 'url_profile': 'https://github.com/Babatunde13', 'info_list': ['JavaScript', 'Updated Jan 9, 2020', '3', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'Kraków', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Carcow real estate prices analysis \nAnalysis of housing prices in Cracow, downloaded from Otodom.pl service\n-Geoenrchiment (adding explanatory variables from map) and linear regression of housing prices notebook link\n\n-Geocoding of housing offers base on their description notebook link.\n\n\n'], 'url_profile': 'https://github.com/marcinszwagrzyk', 'info_list': ['JavaScript', 'Updated Jan 9, 2020', '3', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['LR\nlogistic regression\n'], 'url_profile': 'https://github.com/lkw87', 'info_list': ['JavaScript', 'Updated Jan 9, 2020', '3', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic Regression Case Study\n'], 'url_profile': 'https://github.com/ashrafaj', 'info_list': ['JavaScript', 'Updated Jan 9, 2020', '3', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'Kathmandu Nepal', 'stats_list': [], 'contributions': '170 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MadhavSapkota', 'info_list': ['JavaScript', 'Updated Jan 9, 2020', '3', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'Toulouse, France', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Algorithme-LinReg\nLinear Regression\n'], 'url_profile': 'https://github.com/Cedric-M', 'info_list': ['JavaScript', 'Updated Jan 9, 2020', '3', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/matbun', 'info_list': ['JavaScript', 'Updated Jan 9, 2020', '3', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'Iran', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['linear regression\nthis code estimates coefficients of linear regression function by OSL and MLE methods and draw them.\nUsage:\nInstall project requirements:\n pip install -r requirements.txt\nRun:\npython3 linearRegression.py\n'], 'url_profile': 'https://github.com/khashayarghamati', 'info_list': ['JavaScript', 'Updated Jan 9, 2020', '3', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anuragsriram', 'info_list': ['JavaScript', 'Updated Jan 9, 2020', '3', 'Jupyter Notebook', 'Updated Mar 30, 2020', '3', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'R', 'Updated Jan 8, 2020', 'TypeScript', 'Updated Jan 9, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '312 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/radheysm', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'R', 'Updated Jan 8, 2020', 'TypeScript', 'Updated Jan 9, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'St. Cloud, MN', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['LinReg\nLinear Regression Template\n'], 'url_profile': 'https://github.com/MuzzyB', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'R', 'Updated Jan 8, 2020', 'TypeScript', 'Updated Jan 9, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['linear regression\nProgram written in Python to perform a gradient descent on a linear function,\nand print the best fit line on a graph, for educational purpose\nTrain\nIf you want to train with the providen dataset juste run make train,\nelse if you want to train with your own dataset, just run python3.7 srcs/train.py [dataset]\nPredict\nTo make a prediction you need to run python3.7 srcs/predict.py [input]\nDependencies\nmatplotlib\nLicense\nThis project is licensed under the GNU General Public License 3.\n'], 'url_profile': 'https://github.com/ldemesla', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'R', 'Updated Jan 8, 2020', 'TypeScript', 'Updated Jan 9, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'san diego, ca', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mikasd', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'R', 'Updated Jan 8, 2020', 'TypeScript', 'Updated Jan 9, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dicksang', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'R', 'Updated Jan 8, 2020', 'TypeScript', 'Updated Jan 9, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Capstone\nCapstone project and Grand hackathon on classification and regression problems\n'], 'url_profile': 'https://github.com/Saiharishc', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'R', 'Updated Jan 8, 2020', 'TypeScript', 'Updated Jan 9, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Angular\nThis project was generated with Angular CLI version 8.0.1.\nDevelopment server\nRun ng serve for a dev server. Navigate to http://localhost:4200/. The app will automatically reload if you change any of the source files.\nCode scaffolding\nRun ng generate component component-name to generate a new component. You can also use ng generate directive|pipe|service|class|guard|interface|enum|module.\nBuild\nRun ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build.\nRunning unit tests\nRun ng test to execute the unit tests via Karma.\nRunning end-to-end tests\nRun ng e2e to execute the end-to-end tests via Protractor.\nFurther help\nTo get more help on the Angular CLI use ng help or go check out the Angular CLI README.\n'], 'url_profile': 'https://github.com/TomaszCz', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'R', 'Updated Jan 8, 2020', 'TypeScript', 'Updated Jan 9, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'St. Cloud, MN', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['TSReg\nTime Series - Regression Methods\n'], 'url_profile': 'https://github.com/MuzzyB', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'R', 'Updated Jan 8, 2020', 'TypeScript', 'Updated Jan 9, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jul 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mehmetfatihdata', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'R', 'Updated Jan 8, 2020', 'TypeScript', 'Updated Jan 9, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jul 24, 2020']}"
"{'location': 'Vancouver BC', 'stats_list': [], 'contributions': '279 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/domgolonka', 'info_list': ['1', 'Go', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 6, 2020', '2', 'R', 'Updated Jan 10, 2020', '2', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""House-Prices-Prediction\nAdvanced Regression Tchniques\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nAcknowledgments\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.\n""], 'url_profile': 'https://github.com/pradhakrishna4', 'info_list': ['1', 'Go', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 6, 2020', '2', 'R', 'Updated Jan 10, 2020', '2', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['In this project we will be working with a fake advertising data set, indicating whether or not a particular internet user clicked on an\nAdvertisement.We will try to create a model that will predict whether or not they will click on an ad based off the features of that user.\n'], 'url_profile': 'https://github.com/AkashJadhav-git', 'info_list': ['1', 'Go', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 6, 2020', '2', 'R', 'Updated Jan 10, 2020', '2', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'Noida, India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sadique-Automation', 'info_list': ['1', 'Go', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 6, 2020', '2', 'R', 'Updated Jan 10, 2020', '2', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['Health-Insurance-Regression-Analysis\nSee full report below:\n\n\n\n\n\n\nHello\n\n\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/trujivan', 'info_list': ['1', 'Go', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 6, 2020', '2', 'R', 'Updated Jan 10, 2020', '2', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['#include<stdio.h>\n#include<stdlib.h>\n#include<math.h>\nint randomgen (int a, int b)\n{\nreturn rand () % (b);\n}\nvoid main()\n{\nfloat x1[70],x2[70],target[70],y_predicted[50],M1,M2,bias,lr=0.03;\nfor(int i=0;i<70;i++)\n{\nx1[i]=randomgen(1,50);\nx2[i]=randomgen(1,50);\n}\nprintf(""Enter M1 : "");\nscanf(""%f"",&M1);\nprintf(""Enter M2 : "");\nscanf(""%f"",&M2);\nprintf(""Enter bias : "");\nscanf(""%f"",&bias);\nfor(int i=0;i<70;i++)\n{\nif((M1x1[i]+M2x2[i])>=bias)\ntarget[i]=1;\nelse\ntarget[i]=0;\n}\nfloat cost=0,summ1,summ2,sumbai,m1=1,m2=2,bai=3;\nint prassy=1500;\nwhile(prassy--)\n{\ncost=0;\nsumm1=0;summ2=0;sumbai=0;\nfor(int i=0;i<50;i++)\n{\ny_predicted[i]=1/(1+exp(-((m1x1[i])+(m2x2[i])+bai)));\n}\nfor(int i=0;i<50;i++)\n{\ncost=cost+((-target[i]*log(y_predicted[i]))-(1-target[i])log(1-y_predicted[i]));\n}\nfor(int i=0;i<50;i++)\n{\nsumm1+=(y_predicted[i]-target[i])x1[i];\nsumm2+=(y_predicted[i]-target[i])x2[i];\nsumbai+=(y_predicted[i]-target[i]);\n}\nm1=m1-(lrsumm1)/100;\nm2=m2-(lrsumm2)/100;\nbai=bai-(lrsumbai)/100;\nprintf("" cost : %f \\n m1 : %f \\n m2 : %f \\nbias : %f      \\n"" ,cost,m1,m2,bias);\n}\nfor(int i=0;i<50;i++)\nprintf(""%f        %f\\n"" ,target[i],y_predicted[i]);\nprintf(""\\nPredictions***\\noriginal     |       predicted\\n"");\nfor(int i=51;i<70;i++)\n{\nprintf(""%f     |     %f\\n"",target[i],1/(1+exp(-((m1x1[i])+(m2x2[i])+bai))));\n}\nprintf(""\\n*********** 100 percent working model***********************"");\n}\n'], 'url_profile': 'https://github.com/prasannanivas', 'info_list': ['1', 'Go', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 6, 2020', '2', 'R', 'Updated Jan 10, 2020', '2', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Linear_Regression_without_sklearn\nThe linear regression model that I was taught in the training. The model is made with simple functions without the use of any ML package.\n'], 'url_profile': 'https://github.com/diyamahendru', 'info_list': ['1', 'Go', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 6, 2020', '2', 'R', 'Updated Jan 10, 2020', '2', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Paper Reimplementation in Best Subset Selection\nBertsimas, Dimitris, Angela King, and Rahul Mazumder. ""Best subset selection via a modern optimization lens."" The annals of statistics 44.2 (2016): 813-852.\n\nImplementation Details:\n\n\nUsing Projected Gradient Descent methods to find a heuristic solution.\n\n\nUsing three methods to find a upper bound of L1 and Linf norm for β and Xβ\n\nCoherence and Restricted Eigenvalue\n(there is a typo in the paper 2.12(c), should be sqrt(n)*norm(y) not sqrt(k)*norm(y))\nConvex Optimization\nheuristic-solution-based Bound\n\n\n\n\nHow to run it?\n\n\nStep 1: Install Gurobi License and gurobipy into Anaconda\nObtain Individual Academic Licenses of Gurobi on the website. Here.\nFrom an Anaconda terminal issue the following command to add the Gurobi channel to your default search list:\n  conda config --add channels http://conda.anaconda.org/gurobi\n\nNow issue the following command to install the Gurobi package:\n  conda install gurobi\n\nYou can remove the Gurobi package at any time by issuing the command:\n  conda remove gurobi\n\n\n\nStep 2:\n\n\nRun for simple dataset:\nn, p = 100, 20\nX = rng.random(size=(n, p))\ncoef = np.zeros(p)\ncoef[[0, 2, 4]] = 1\ncoef[[1, 3, 5]] = 2\nintercept = 3\nerror = rng.randn(n)\ny = np.dot(X, coef) + intercept + error\ny = y.reshape(n, )\nmip_model = L0Regression(verbose=False).fit(X, y, k=6)\nmip_error = sum((mip_model.coef_ - coef)**2)\nmip_error += (mip_model.intercept_ - intercept)**2\nprint(""best subset selection:"")\nprint(""coef:"", mip_model.coef_, ""intercept:"", mip_model.intercept_, ""error:"", mip_error, sep=""\\n"")\nprint()\nprint(""LASSO:"")\nlasso_model = LassoCV(cv=10).fit(X, y)\nlasso_error = sum((lasso_model.coef_ - coef)**2)\nlasso_error += (lasso_model.intercept_ - intercept)**2\nprint(""coef:"", lasso_model.coef_, ""intercept:"", lasso_model.intercept_, ""error:"", lasso_error, sep=""\\n"")\n\nOutput:\nUsing license file /Users/zhoutang/gurobi.lic\nAcademic license - for non-commercial use only\nChanged value of parameter TimeLimit to 60.0\n   Prev: inf  Min: 0.0  Max: inf  Default: inf\nChanged value of parameter MipGap to 1e-05\n   Prev: 0.0001  Min: 0.0  Max: inf  Default: 0.0001\ncumulative coherence larger than 1, bound 1 fails\nbest subset selection:\ncoef:\n[1.00157844 2.23547144 1.33017972 2.18760993 0.9282457  2.16010474\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.        ]\nintercept:\n2.4966815408189174\nerror:\n0.48377709899904553\n\nLASSO:\ncoef:\n[ 0.63580476  1.80683851  0.94027029  1.8444559   0.55423598  1.73261153\n  0.          0.         -0.         -0.          0.         -0.\n  0.          0.31048407 -0.         -0.          0.          0.\n -0.          0.55432892]\nintercept:\n3.221779335838728\nerror:\n0.9207802752097756\n\n\n\n\n\n'], 'url_profile': 'https://github.com/zhoutang776', 'info_list': ['1', 'Go', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 6, 2020', '2', 'R', 'Updated Jan 10, 2020', '2', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'Pittsburgh', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sakchy', 'info_list': ['1', 'Go', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 6, 2020', '2', 'R', 'Updated Jan 10, 2020', '2', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'No.15 North 3rd Ring Rd East, Chaoyang District, Beijing, China.', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Adversarial_regression\nGenerative Adversarial Networks for Non-Linear Regression: Theory and Assessment\nMaster Thesis: Yoann Boget\nSupervisor at SLAC (Stanford University: Dr. Micheal Kagan\nSupervisor at University of Neuchâtel: Dr. Clément Chevalier\nhttps://arxiv.org/abs/1910.09106\nGrade: 5.5/6 (Very good)\nAbstract\nAdversarial Regression is a proposition to perform high dimensional non-linear regression with uncertainty estimation. It uses Conditional Generative Adversarial Network to obtain an estimate of the full predictive distribution for a new observation.\nGenerative Adversarial Networks (GAN) are implicit generative models which produce samples from a distribution approximating the distribution of the data. The conditional version of it can be expressed as follow:\n\nwhere D and G are real-valuated functions, x and y respectively the explained and explanatory variables, and z a noise vector from a known distribution, typically the standard normal. An approximated solution can be found by training simultaneously two neural networks to model D and G. After training, we have that G(z, y) approximate p(x, y). By fixing y, we have G(z|y) approximating p(x|y). By sampling z, we can therefore obtain samples following approximately p(x|y), which is the predictive distribution of x for a new observation y.\nWe ran experiments to test various loss functions, data distributions, sample sizes, and dimensions of the noise vector. Even if we observed differences, no set of hyperparameters consistently outperformed  the others. The quality of CGAN for regression relies on fine-tuned hyperparameters depending on the task rather than on fixed values. From a broader perspective, the results show that adversarial regressions are promising methods to perform uncertainty estimation for high dimensional non-linear regression.\n'], 'url_profile': 'https://github.com/zhongsheng-chen', 'info_list': ['1', 'Go', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 6, 2020', '2', 'R', 'Updated Jan 10, 2020', '2', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 12, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 7, 2020']}"
"{'location': 'Pune', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['linear-regreesion-\nML algorithm simple linear regression\n'], 'url_profile': 'https://github.com/Avinash237', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'C++', 'MIT license', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 6, 2020']}","{'location': 'Trichy', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Introduction-to-Linear-Regression\nSalary Prediction using Linear Regression which can predict the average salary per annum for an employee with parameters\nsuch as test_score, experience, and interview_score as independent variable to predict target variable as Salary\n'], 'url_profile': 'https://github.com/RATHEESHWARAA', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'C++', 'MIT license', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['linearRegression\nProjects based on linear regression.\n'], 'url_profile': 'https://github.com/Rohit-Nikam', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'C++', 'MIT license', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['The dataset is downloaded from here.\n'], 'url_profile': 'https://github.com/dandycheng', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'C++', 'MIT license', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 6, 2020']}","{'location': 'Lahore, Pakistan', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['PricePredictionOfHouse\nLinear Regression With Multiple Variables\nSuppose you are selling your house and you want to know what a good market price would be. One way to do this is to ﬁrst collect information on recent houses sold and make a model of housing prices. The ﬁle ex1data2.txt contains a training set of housing prices in a city. The ﬁrst column is the size of the house (in square feet), the second column is the number of bedrooms and the third column is the price of the house.\n'], 'url_profile': 'https://github.com/ayeshatahreem', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'C++', 'MIT license', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': ['LinearRegression\nLinear Regression Algorithm in C++\n'], 'url_profile': 'https://github.com/Lemon-cmd', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'C++', 'MIT license', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 6, 2020']}","{'location': 'Lahore, Pakistan', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Outlet Prediction Of Bakery\nLinear regression with one variable\nChief Executive Officer of a bakery is considering diﬀerent cities for opening a new outlet. The chain already has outlets in various cities and you have data for proﬁts and populations from the cities. By using this data, CEO selects which city to expand to next. The ﬁle ex1data1.txt contains the dataset for linear regression problem. The ﬁrst column is the population of a city and the second column is the proﬁt of an outlet in that city. A negative value for proﬁt indicates a loss.\n'], 'url_profile': 'https://github.com/ayeshatahreem', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'C++', 'MIT license', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ahmed25121996', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'C++', 'MIT license', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['SimpleLinearRegression\nExercises on Simple Linear Regression\n'], 'url_profile': 'https://github.com/shariff94', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'C++', 'MIT license', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Keras Refresher\nBe sure to check out Keras' Documentation before you start out!\nLinear Regression\n\nBuild a Keras Model for linear regression (check: https://keras.io/activations/). Use Boston Housing Dataset to train and test your model.\nBuild the simplest model for logistic regression with Keras and compare your model performance with from sklearn.linear_model import LogisticRegression\n\nLogistic Regression\n\nBuild a Keras Model for logistic regression. Use diabetes.csv to train and test your model.\nBuild the simplest model for linear regression with Keras and compare your model performance with from sklearn.linear_model import LinearRegression\n\nSpecial Thanks\n\nebonnecab: awesome, easy-to-read project files\n\nFurther Reading\n\ndf.loc() and df.iloc()\n\n""], 'url_profile': 'https://github.com/noltron000-coursework', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'C++', 'MIT license', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['MultiLinearRegression\nMulti Linear Regression using R\n'], 'url_profile': 'https://github.com/shariff94', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'JavaScript', 'Updated Oct 30, 2020', '2', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 9, 2020', 'Updated Jan 17, 2021']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': [""Linear Regression From Scratch\nAn implementation of Linear Regression from scratch in python 📈. Includes the hypothesis function, partial differentiation of variables, parameter update rule and gradient descent.\n\nGetting Started\n\n\nDownload the project on your computer.\ngit clone https://github.com/codePerfectPlus/LinearRegression_from_Scratch\nor download the ZIP file\n\n\nGo to the directory with the file: cd LinearRegression\n\n\nDownload the required packages: pip install -r requirements.txt\n\n\nRun the project: python3 LinearRegression.py\n\n\nHow It Works\nWhat Is Linear Regression?\nLinear regression is a method for approximating a linear relationship between two variables. While that may sound\ncomplicated, all it really means is that it takes some input variable, like the age of a house, and finds out how\nit's related to another variable, for example, the price it sells at.\n\nWe use it when the data has a linear relationship, which means that when you plot the points on a graph, the\ndata lies approximately in the shape of a straight line.\nLinear Regression involves a couple of steps:\n\nRandomly initialising parameters for the hypothesis function\nComputing the mean squared error\nCalculating the partial derivatives\nUpdating the parameters based on the derivatives and the learning rate\nRepeating from 2 until the error is minimised\n\nThe Hypothesis Function\nThe linear equation is the standard form that represents a straight line on a graph, where m represents the gradient,\nand b represents the y-intercept.\n\nThe Hypothesis Function is the exact same function in the notation of Linear Regression.\n\nThe two variables we can change – m and b – are represented as parameters theta1 and theta0\nIn the beginning, we randomly initialize our parameters, which means we give theta1 and theta0 random values\nto begin with. This will output a random line, maybe something like this:\n\nThe Error Function\nClearly the line drawn in the graph above is wrong. But how wrong is it? That's what the error function is for - it\ncalculates the total error of your graph.\nWe'll be using an error function called the Mean Squared Error function, or MSE, represented by the letter J.\n\nNow while that may look complicated, what it's doing is actually quite simple. The function J takes our parameters\ntheta0, and theta1 as an input. Then, from every point from 1 to m, where m is the number of points, it calculates\nthe distance between the x value and the value predicted by our function and squares it. Squaring it ensures that\nthe error is always positive, and that the error is greater the further away the predicted value is from the actual\nvalue.\nNow that we have a value for how wrong our function is, we need to adjust the function to reduce this error.\nCalculating Derivatives\nIf we graph our parameters against the error (i.e graphing the cost function), we'll find that it forms something similar to the graph below. Our goal is to find the lowest point of that graph, where the error is at its lowest. This is called minimising the cost function.\nTo do this, we need to consider what happens at the bottom of the graph - the gradient is zero. So to minimize the cost function, we need to get the gradient to zero.\nThe gradient is given by the derivative of the function, and the partial derivatives of the functions are\n\n\nUpdating The Parameters Based On The Learning Rate\nNow we need to update our parameters to reduce the gradient. To do this, we use the gradient update rule\n\n\nAlpha is what we call the Learning rate, which is a small number that allows the parameters to be updated by a small amount. As mentioned above, we are trying to update the gradient such that it's closer to zero (the bottom). The Learning rate helps guide the network to the lowest point on the curve by small amounts.\nMinimising the Cost Function\nNow we repeat these steps - checking the error, calculating the derivatives, and updating the weights, until the error is as low as possible. This is called minimising the cost funtion.\nOnce your error is minimised, your line should now be the best possible fit to approximate the data!\n\n\nOther Reference Github Repository\nCode_perfect\nBasic EDA On Kaggle Dataset\nPython Codes\nBasic Start OF Python Codes\nLogistic Regression\nLogistic Regression ON IRIS DataSet\nRecommender_System\nRecommender_System For Movie DataSet\nBasic Machine learning Models\nBasic Regression And Classfication Machine learning Models\nIndia Defense Analysis\nIndia Defense Analysis 1963 to 2018 - Kaggle\nK-NN Alorithm\nK-NN Algorithm From Scratch For Random Points\n""], 'url_profile': 'https://github.com/DrakeEntity', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'JavaScript', 'Updated Oct 30, 2020', '2', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 9, 2020', 'Updated Jan 17, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DixitAman10', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'JavaScript', 'Updated Oct 30, 2020', '2', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 9, 2020', 'Updated Jan 17, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/Citcon', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'JavaScript', 'Updated Oct 30, 2020', '2', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 9, 2020', 'Updated Jan 17, 2021']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['House-Price-Prediction-ML\nThe objective is to build a regression model to predict the price of houses.\nPre-processed the dataset by analysing the missing values in the data.\nUsed Regressor algorithms like Support Vector Regressor and Random Forest.\nThere are 1460 samples of house prices in the dataset.\n'], 'url_profile': 'https://github.com/Kunal1198', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'JavaScript', 'Updated Oct 30, 2020', '2', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 9, 2020', 'Updated Jan 17, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['RegressionProject\nThis project was done for ISyE 6414: Regression Analysis.\nThis project used the European Social Survey to create an Ordinal Logistic Regression model to find explanatory socio-economic variables for happiness.\n'], 'url_profile': 'https://github.com/JesseHaulk', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'JavaScript', 'Updated Oct 30, 2020', '2', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 9, 2020', 'Updated Jan 17, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Machine-learning\nML and deep learning knowledge\n'], 'url_profile': 'https://github.com/divyanshu9210', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'JavaScript', 'Updated Oct 30, 2020', '2', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 9, 2020', 'Updated Jan 17, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Regression with R\nInsurance Forecasting\nThis project was conducted with two other teammates and it aims to analyze a real life data set and use statistical methods to create a predictive model with given data. A data set from Kaggle on insurance forecasting was chosen, which provided certain patient information and their respective insurance costs. R programming language was used to clean, organize and analyze the data to create an accurate model to predict insurance costs based on patient information.\nThe data set consisted of 1338 data points, with one response variable (charges) and six predictive variables (age, sex, bmi, children, smoker and region). The variables were a mix of numerical and categorical.\nTo create an accurate predictive model, firstly some descriptive statistics were drawn to better understand the dataset. Secondly various variable selection methods such as forward, backward and stepwise regression along with LASSO regression was performed. Interaction terms were introduced and Box Cox variable transformation was used to transform the model to better fit the data. Finally cross validation was used to pick the best predictive model. R² adj and root mean square error were used as measures to select the best predictive model.\nThe code I have posted was written in collaboration with the teammate. I do not take full credit for the work\n'], 'url_profile': 'https://github.com/chamodib', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'JavaScript', 'Updated Oct 30, 2020', '2', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 9, 2020', 'Updated Jan 17, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '864 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/haututu', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'JavaScript', 'Updated Oct 30, 2020', '2', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 9, 2020', 'Updated Jan 17, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['RegressionCalculator\nLanguage: Python\nPurpose: Calculates normalized jerk, a measure of arm movement smoothness, which is used to assess stroke patient rehabilitation progress. Takes raw arm joint position data, performs nonlinear regression on data by iterating through different equation types (i.e. cubic, quadratic, etc) to find the line of best fit with which to caclulate normalized jerk. Plots regression equation with original data plot for regression accuracy assessment.\n2subject2.png shows an example regression generation!\n'], 'url_profile': 'https://github.com/tangteresa', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'JavaScript', 'Updated Oct 30, 2020', '2', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Feb 8, 2020', 'R', 'Updated Feb 9, 2020', 'Updated Jan 17, 2021']}"
"{'location': 'Tunisia', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['PySpark-ML---LogisticRegression\n'], 'url_profile': 'https://github.com/saibioussama', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2021', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'LGPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020']}","{'location': 'Minneapolis', 'stats_list': [], 'contributions': '603 contributions\n        in the last year', 'description': [""Logistic Regression & PCA\n\nBuild Logistic Regression classification model using numpy, scipy libraries and compare with sci-kit's Logistic Regression\nPerform PCA using Mixture of Gaussian models\n\nLogistic Regression\nBuild a 2 class Logistic Regression classification model without using sci-kit library functions. Develop code with a set of parameters (w,w0) where w ∈ Rd, w0 ∈ R. Assuming the two classes are {0,1}, and the data x ∈ Rd, the posterior probability of class C1 is given by\nP(1|x)= exp(wTx+w0) / 1+exp(wTx+w0) \nand P(0|x) = 1 − P(1|x)\nPCA\nCompute a PCA projection Z ∈ Rd×n, d ≤ D of the original data X ∈ RD×n so that α% of the variance is preserved in the projected space. Develop code only using numpy and scipy libraries.\nThe feature covariance matrix of the data can be computed as:\nΣ=1/n Σ(xj−μˆ)(xj−μˆ)T\nwhere μˆ = n1 \n\U0010fc09nj=1 \nxj is the mean of the data points\nDatasets\n\nBoston Housing Dataset\nDigits dataset\n\n""], 'url_profile': 'https://github.com/Roopana', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2021', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'LGPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020']}","{'location': 'Taiwan', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Kaggle_House_Price_Advanced_Regression\nKaggle House Price - Advanced Regression處理流程。過程中使用多種嘗試，僅上傳最後使用的資料清潔方式、模型選擇及參數調整。最終得分0.12659，1867/5397名（top35%）\n'], 'url_profile': 'https://github.com/EricTsengTW', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2021', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'LGPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Fast And Accurate Ranking Regression\nThe code in this repository implements the algorithms and experiments in the following paper:\n\nI. Yildiz, J. Dy, D. Erdogmus, J. Kalpathy-Cramer, S. Ostmo, J. P. Campbell, M. F. Chiang, S. Ioannidis, “Fast and Accurate Ranking Regression”, AISTATS, Italy, 2020.\n\nWe implement seven inference algorithms to estimate Plackett-Luce scores from choice observations. Four are feature methods, i.e., algorithms that regress Plackett-Luce scores from features:\n\nPLADMM and PLADMM-log that we propose (implemented in admm_lin.py and admm_log.py, respectively),\nSequential least-squares quadratic programming (SLSQP) that parametrizes scores as linear functions of features as in PLADMM (implemented in mle_lin.py), and\nNewton on \\beta that parametrizes scores as logistic functions of features as in PLADMM-log and solves the resulting convex problem via Newton's method (implemented in mle_exp.py).\n\nThe remaining three are featureless methods, i.e., algorithms that learn the Plackett-Luce scores from the choice observations alone (implemented in only_scores.py):\n\nIterative Luce Spectral Ranking (ILSR) by Maystre and Grossglauser (2015),\nMinorization-Maximization (MM) by Hunter (2004), and\nNewton on $\\bm \\theta$ that reparametrizes scores via exponentials and solves the resulting convex problem via Newton's method.\n\nWe evaluate all algorithms on synthetic and real-life datasets, prepared by the methods in preprocessing.py.  We perform 10-fold cross validation (CV) for each dataset. For synthetic datasets, we also repeat experiments over 5 random generations. We partition each dataset into training and test sets in two ways. In observation CV, implemented by the create_partitions method, we partition the dataset w.r.t. observations, using 90% of the observations for training and the remaining 10% for testing. In sample CV, implemented by the create_partitions_wrt_sample method, we partition samples using 90% of the samples for training and the remaining 10% for testing. We construct synthetic datasets via save_synthetic_data; details of the process are provided in the supplement of the paper.\nWe run each algorithm until convergence via run_methods.py. We measure the elapsed time, including time spent in initialization, in seconds and the number of iterations. We measure the prediction performance by Top-1 accuracy and Kendall-Tau correlation on the test set. For synthetic datasets, we also measure the quality of convergence by the norm of the difference between estimated and true Plackett-Luce scores. We report averages and standard deviations over folds via the metric_and_CI method.\nutils.py contains fundamental methods reused by all algorithms. init_params initializes the Plackett-Luce scores for all seven inference algorithms described above. Particularly, init_beta_b_convex_QP initializes the parameter vector for PLADMM, as described in Section 4 of the paper, while init_exp_beta initializes the parameter vector for PLADMM-log, as described in the supplement. Given the transition matrix of a Markov-Chain, statdist finds the stationary distribution. objective evaluates the log-likelihood under the Plackett-Luce model for a given set of scores and choice observations. check_global_balance_eqn tests if the given pair of transition matrix and scores satisfy the global balance equations. Given a pair of score estimates and rankings, top1_test_accuracy and kendall_tau_test evaluate the corresponding average metrics.\nCiting This Paper\nPlease cite the following paper if you intend to use this code for your research.\n\nI. Yildiz, J. Dy, D. Erdogmus, J. Kalpathy-Cramer, S. Ostmo, J. P. Campbell, M. F. Chiang, S. Ioannidis, “Fast and Accurate Ranking Regression”, AISTATS, Italy, 2020.\n\nAcknowledgements\nOur work is supported by NIH (R01EY019474), NSF (SCH-1622542 at MGH; SCH-1622536 at Northeastern; SCH-1622679 at OHSU), and by unrestricted departmental funding from Research to Prevent Blindness (OHSU).\n""], 'url_profile': 'https://github.com/neu-spiral', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2021', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'LGPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Machine_Learning_Algorithms\nLinear Regression, Multi Variable Linear Regression, Feature Selection, Polynomial Regression, Gradient Descent, Dataset creation and Normalization\nTo run this code............................\nStep 1\nInitialize git in a directory ... $git init\nStep 2\nClone the project ... $git clone https://github.com/Shaykat/Machine_Learning_Algorithms.git\nStep 3\nCreate a virtual environment and activate it.\nStep 4\nNow inside the virtual environment install python 3.7 and other required packages included in the requirements.txt file.\nStep 5\nIf the IDE you are using is pycharm, then you can just assign the file you want to run in the configuration section and select the virtual environment you created or you can create virtual environment from pycharm also.\n\n'], 'url_profile': 'https://github.com/Shaykat', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2021', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'LGPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Classification-models\nLogistic Regression and LDA (Linear Discriminative Analysis)\nIn this project, we investigated two classification models, logistic regression with gradient descent (LR) and linear discriminative analysis (LDA), and how they predict the binary outcome on two specific datasets, which are wine quality prediction set and breast cancer diagnosis set. We applied the heatmap correlation analysis on the different datasets and create a new subset of features, which slightly increased 2% of accuracy in the final test. We also showed that for both datasets with different feature data types, LDA model predicts more precisely and efficiently on both datasets, while LR model predicts the categorical dataset well when choosing a large learning rate. Meanwhile, we compared the different running time of the LDA model and LR model.  We also analysed the prediction result in depth by interpreting confusion matrix and discussed findings which would be studied in future research.\n'], 'url_profile': 'https://github.com/Zhenghua-404', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2021', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'LGPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020']}","{'location': 'Chennai , India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression\nPrediction Of Movie Box Office Revenue using Linear Regression.\n'], 'url_profile': 'https://github.com/PojaaR', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2021', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'LGPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sivasoundar14', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2021', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'LGPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '742 contributions\n        in the last year', 'description': ['EDA-and-Linear-Regression-on-Boston-Housing-in-R\nExploratory Data Analysis and Multiple Linear Regression on Boston Housing Dataset\n'], 'url_profile': 'https://github.com/kritikseth', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2021', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'LGPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['LinearRegression\nContains the code for Linear Regression\n'], 'url_profile': 'https://github.com/raybg', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2021', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', '2', 'Python', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'LGPL-3.0 license', 'Updated Apr 30, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020']}"
"{'location': 'San Francisco', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['linear_regression_implementation\nPython implementation from scratch - Linear Regression\nThis is a python implementation of Linear (simple and L2/Ridge) and Logistic Regression using the AdaGrad optimization algorithm. In the file linreg.py, I have defined 3 classes for the same:\n\nLinearRegression621: simple linear regression. It has methods fit() and predict(), just like scikit-learn.\nRidgeRegression621: linear regression with L2 regularisation. It has methods predict() and fit().\nLogisticRegression621: logistic regression. It has methods fit(), predict() and predict_proba().\n\nThe AdaGrad optimization algorithm is encapsulated inside minimize() function, which is called by above classes and returns the parameter combination that minimizes loss.\ntest_class.py and test_regr.py are tests designed to validate my implementation.\nThanks to Prof. Terence Parr for his guidance and support in this school project.\n'], 'url_profile': 'https://github.com/ShishirKumar93', 'info_list': ['Python', 'Updated May 21, 2020', '1', 'Python', 'Updated May 13, 2020', 'Python', 'Updated Oct 31, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'MATLAB', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'Paris, France', 'stats_list': [], 'contributions': '230 contributions\n        in the last year', 'description': [""Working with the Sklearn Breast Cancer dataset\nThe purpose of this repository is myself practice of Logistic Regression. I am a student that is learning, let me know if you find any errors, the original code is from examples and exercises found in books, tutorials and other sources all mentioned in this file. I am just practicing what I have learned, the proper authors and creators of the algorithms/code are the ones mentioned in the file.\nI will be using Streamlit to develop an App where I will be sharing what I've learned. To run the app, first clone the repository, install the project dependencies, then run the app with streamlit.\nBefore installing the requirements, it's recommended to create a virtual environment.\npip install -r requirements.txt\nstreamlit run logistic_regression.py\n\n\nResources that I used to learn about this fun topic:\n\nTutorial Python Engineer :https://www.youtube.com/watch?v=JDU3AzH3WKg\nBook: Data Science from Scratch, Joel Grus\nhttps://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#\nhttps://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset\nhttps://www.youtube.com/watch?v=JDU3AzH3WKg&t=807s\nhttps://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\nhttps://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\nMaximum likelihood https://www.youtube.com/watch?v=XepXtl9YKwc\nCoefficient https://www.youtube.com/watch?v=vN5cNN2-HWE\n\n""], 'url_profile': 'https://github.com/parismollo', 'info_list': ['Python', 'Updated May 21, 2020', '1', 'Python', 'Updated May 13, 2020', 'Python', 'Updated Oct 31, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'MATLAB', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Classifications (w/ Python & R)\nThis repository consists of various classification techniques such as Logistic regression, Decision Tree, K Nearest Neighbors, Naive Bayes, Random Forest Classification, Support Vector Machine (SVM) and Kernel-SVM.\nProject name: # Classifications (w/ Python & R)\nDescription\n\xa0\xa0\xa0\xa0Classification in machine learning is a predictive modeling preoblem where a class lable is predicted for a given example of given data. For e.g. classify if e-mail is spam or ham, to predict if the person is diabetic and more.\n\xa0\xa0\xa0\xa0In this repository, we are focusing on building complex statistical Classification models to predict and visualize the prediction results on categorical dependent variables. Based on the nature of problem and dataset, we can determine the right classification techniques for better accuracy.\nInstallation\n\xa0\xa0\xa0\xa0In order to be able to apply these regression techniques to predict outcome on your dataset you need to have following tools and libraries:\n\nPython 2.x or Python 3.x\nPandas\nNumPy\nScikit-Learn\nR (For R implementation)\n\nUsage\n\xa0\xa0\xa0\xa0Classification methods are very common term and technique in the field of data science. You can use these classification methods to predict the categorical dependent variables such as status of email (ham or spam), result of student in exam (Pass or Fail), etc.\nContributing\n\xa0\xa0\xa0You can implement these different models on the same dataset to determine the most accurate model.\n'], 'url_profile': 'https://github.com/peak27', 'info_list': ['Python', 'Updated May 21, 2020', '1', 'Python', 'Updated May 13, 2020', 'Python', 'Updated Oct 31, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'MATLAB', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['R-for-Logistic-Regression\nA R script for Logistic Regression\nPlease follow the EXANPLE file to build your own data.\n'], 'url_profile': 'https://github.com/Wanyi-Huang', 'info_list': ['Python', 'Updated May 21, 2020', '1', 'Python', 'Updated May 13, 2020', 'Python', 'Updated Oct 31, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'MATLAB', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AyushJadhav', 'info_list': ['Python', 'Updated May 21, 2020', '1', 'Python', 'Updated May 13, 2020', 'Python', 'Updated Oct 31, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'MATLAB', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'PUNE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['cab-fare-predtiocn\nedwisor project on a regression problem\n'], 'url_profile': 'https://github.com/SURAJGITA', 'info_list': ['Python', 'Updated May 21, 2020', '1', 'Python', 'Updated May 13, 2020', 'Python', 'Updated Oct 31, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'MATLAB', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Smart Buys: NBA\n'], 'url_profile': 'https://github.com/samad-m', 'info_list': ['Python', 'Updated May 21, 2020', '1', 'Python', 'Updated May 13, 2020', 'Python', 'Updated Oct 31, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'MATLAB', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DS-Sagar', 'info_list': ['Python', 'Updated May 21, 2020', '1', 'Python', 'Updated May 13, 2020', 'Python', 'Updated Oct 31, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'MATLAB', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'Kingston, Ontario', 'stats_list': [], 'contributions': '125 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rasamps', 'info_list': ['Python', 'Updated May 21, 2020', '1', 'Python', 'Updated May 13, 2020', 'Python', 'Updated Oct 31, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'MATLAB', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Logistic-Regression\nImplementation of Logistic Regression from Scratch\n'], 'url_profile': 'https://github.com/NightmareNight-em', 'info_list': ['Python', 'Updated May 21, 2020', '1', 'Python', 'Updated May 13, 2020', 'Python', 'Updated Oct 31, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'MATLAB', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Linear Regression in One Variable\n\nImplementation of linear regression in one variable\n\nExecution\n$ gcc one_variable.c\n$ ./a.out\n'], 'url_profile': 'https://github.com/kindlerprince', 'info_list': ['C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'JavaScript', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Sep 26, 2020', 'Python', 'Updated Jan 6, 2020', '1', 'MATLAB', 'Updated Jul 24, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Logistic-Regression\nContains the code for Logistic Regression\n'], 'url_profile': 'https://github.com/raybg', 'info_list': ['C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'JavaScript', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Sep 26, 2020', 'Python', 'Updated Jan 6, 2020', '1', 'MATLAB', 'Updated Jul 24, 2020']}","{'location': 'Sweden, Västerås', 'stats_list': [], 'contributions': '6,054 contributions\n        in the last year', 'description': ['\nui-diff\nHow to use it\n1. Create a new project on ui-diff\napp.ui-diff.com/new-project\n2. Install dependency with yarn or npm\nyarn add ui-diff\n\n3. Supply configuration files (in project root)\nThese can be found at ui-diff.com/documentation/getting-started\n4. Run screenshot tests\nyarn screenshots\n\nCommand line options can also be found at ui-diff.com/documentation/getting-started\nBuilt using\n\n📸 puppeteer\n🌐 ora\n⚙️ minimist\n⚡️ axios\n\nContributors\n\n\n\n\n\n\n\n\nalbingroen\n\n\n\n'], 'url_profile': 'https://github.com/albingroen', 'info_list': ['C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'JavaScript', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Sep 26, 2020', 'Python', 'Updated Jan 6, 2020', '1', 'MATLAB', 'Updated Jul 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['coursera_ML_Python_project\nKNN, Decision tree, SVM, Logistic regression\n'], 'url_profile': 'https://github.com/ywan168', 'info_list': ['C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'JavaScript', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Sep 26, 2020', 'Python', 'Updated Jan 6, 2020', '1', 'MATLAB', 'Updated Jul 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['linear-regression\nThis is a project to predict house price from a dataset.\n\nThese codes are clear and simple for those who want to see a standard linear regression project.\n\nof cource a small project :)\nGetting Started\nJust download it and run it !remember you need to install data analysis packages like numpy , pandas , seaborn , skylearn and matplotlib  to run these codes.\nInstall Packages\nTo install packages you need to run below commands in your pyCharm terminal :\n\npip install seaborn\n\npip install numpy\n\npip install matplotlib\n\npip install pandas\n\npip install scikit-learn\n'], 'url_profile': 'https://github.com/homayounsr', 'info_list': ['C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'JavaScript', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Sep 26, 2020', 'Python', 'Updated Jan 6, 2020', '1', 'MATLAB', 'Updated Jul 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': [""Progression\n\nDataset acquired from Kaggle search\nExploratory data analysis has been performed\nLinear Assumptions have been tested\nModel is built and ridge regularization has been performed\nThe findings have been presented\n\nIntroduction\nLike many people, iPhone 3Gs was the first smartphone I've ever purchased. It was indeed revolutionary, but it was never my go-to device for web surfing; the screen was small and functions were much limited compared to a PC or Mac. Fast-forwarding 11 years, my surfing habit did not change drastically despite a miraculous increase in the computing power on mobile devices. That's why I was perplexed when came across the following chart:\n\nThe chart above alleges that worldwide internet traffic is comprised of more mobile devices than conventional PC/Mac. My immediate reaction was 'that can't be right.' Then, I became more curious about exploring what people do on their smartphone; the web traffic's only one component of smartphone usage. One question led to another, and naturally, I arrived at data sciency question: what Apps do they use, and would it be possible to assemble a model that predict the number of installs given features?\nLet's find out.\nDatasets\nGoogle Play Store dataset from Kaggle\n\ngoogleplaystore.csv\ngoogleplaystore_user_reviews.csv\n\nMetrics to consider: googleplaystore_user_reviews.csv\nMost metrics in this model is intuitive. The reviews dataset, however, needs a little guidance.\nThe dataset is composed of '100 most relevant reviews from the play store,' according to its author. Along with the actual review, preprocessed NLP measures are included. The two metrics being utilized for the sake of my search are sentiment polarity and sentiment subjectivity. The following are the definitions borrowed from Alice Zhao's lecture on Natural Language Processing.\n\nSentiment Polarity: how positive, or negative, a word is. -1 is very negative. +1 is very positive.\nSentiment Subjectivity: how subjective, or opinionated a word is. 0 is a fact and 1 is very much opinion.\n\nFurthermore, 'delta' has been added as an engineered feature, which measures the days since the last update.\nMethodology\nAfter cleaning and modifying features, the assumptions of linear regression were checked using the following:\n\nPair plot: Linearity\nQQ plot: Normality\nScatter plot: Homoscedasticity\nHeatmap: Multi-collinearity\n\nOnce done, regression models were built using two libraries (sklearn and statsmodel) and three regularizations were applied (Ridge, Lasso, and Elastic Net).\nFindings\nThe initial model built with engineered featured performed well, at R-squared value of .826 and adjusted R-squared of .824. Also, the difference in root mean squared error between train and test was minuscule and regularizations performed seemed to validate the accuracy of the model.\nHowever, where this model falls short is in its very features. One of the features that was weighted heavily according to its coefficient was the Box-cox transformation of the number of reviews. Of course, there will be more reviews when the number of installs is high! ARRRGGH.\nAlthough this project was a very fruitful experience, I was a little disappointed in my failure to foresee this fatal flaw.\nSources\n\nAlice Zhao - Natural Language Processing in Python https://github.com/adashofdata/nlp-in-python-tutorial\nLavanya Gupta Google Play Store Apps web scraped data of 10k Play Store apps https://www.kaggle.com/lava18/google-play-store-apps\nMobile vs. Desktop Usage https://www.broadbandsearch.net/blog/mobile-desktop-internet-usage-statistics\n\n""], 'url_profile': 'https://github.com/Daveedlee', 'info_list': ['C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'JavaScript', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Sep 26, 2020', 'Python', 'Updated Jan 6, 2020', '1', 'MATLAB', 'Updated Jul 24, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': [""Caffe2 Examples\nCaffe2 is an open-source deep learning framework. It's focus is on efficiency and works with constrained environments such as on mobile devices.\n\nBlobs and operations\nOperators and Net\nSimple Linear Regression (on one feature of Boston Dataset)\nClassification using Caffe2\nImage pre-processing pipeline\nPrediction using pretrained models\n\nPrerequisites\nInstall Caffe2 with Python bindings.\nDownload\nTo download a model locally, run\npython -m caffe2.python.models.download squeezenet\nwhich will create a folder squeezenet/ containing both an init_net.pb and predict_net.pb.\nInstall\nTo install a model, run\npython -m caffe2.python.models.download -i squeezenet\nwhich will allow later imports of the model directly in Python:\nfrom caffe2.python.models import squeezenet\nprint(squeezenet.init_net.name)\nprint(squeezenet.predict_net.name)\n`\n""], 'url_profile': 'https://github.com/asharifara', 'info_list': ['C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'JavaScript', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Sep 26, 2020', 'Python', 'Updated Jan 6, 2020', '1', 'MATLAB', 'Updated Jul 24, 2020']}","{'location': 'Adelaide', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['House-price-prediction\nCOMP SCI 7209 - Big Data Analysis and Project\n\n\nDATASET\n\n\nREPORT\n\n\n'], 'url_profile': 'https://github.com/AbhishekD15', 'info_list': ['C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'JavaScript', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Sep 26, 2020', 'Python', 'Updated Jan 6, 2020', '1', 'MATLAB', 'Updated Jul 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['BigDataSchoolGrades\nThis project was my degree class Big Data coursework. The program was written in python 3 which import an excel spreadsheet dataset consisting of student data. The data was pre-processed to check for missing values, and converted to allow the data to be analysed using regression techniques. The program produced a heatmap and histograms.\n'], 'url_profile': 'https://github.com/AllanMcCaffery', 'info_list': ['C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'JavaScript', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Sep 26, 2020', 'Python', 'Updated Jan 6, 2020', '1', 'MATLAB', 'Updated Jul 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['FFR\nFunction-on-Function Regression\nMatlab files for\n\'Function-on-Function Regression for the Identification of Epigenetic Regions Exhibiting Windows of Susceptibility to Environmental Exposures\' by Zemplenyi, Meyer, Cardenas, et al. (pre-print available on arXiv: https://arxiv.org/abs/1912.07359.\nThis code base builds upon the WFMM package created at MD Anderson, available here: https://biostatistics.mdanderson.org/SoftwareDownload/SingleSoftware/Index/70.\nFiles:\n\nrun_FFR.m: script to initiate a function-on-function regression analysis. You can use either the simulated exposure (""simX_N400_T90.csv"") and response (""simY_N400_S100.csv"") data provided or specify the exposure, response, and covariate data for your custom analysis.\ngenerate_X.m: script used to generate the simulated exposure data contained in ""simX_N400_T90.csv.""\ngenerate_Y.m: script used to generate the simulated response data contained in ""simY_N400_S100.csv.""\nmake_sim_heatmaps.m: function called by script ""run_FFR.m"" that creates three heatmaps (estimated beta surface, Bayesian false discovery rate, and simutaneous band scores) using the ""results.mat"" object from the FFR analysis.\nThe following outputs are sent to the ""Results"" folder after the script has finished:\n\n""FFR_output.mat"" raw results object\nthree heatmaps (estimated beta surface, Bayesian False Discovery Rate inferential procedure, and Simultaneous Band Score inferential procedure)\n\n\n\n'], 'url_profile': 'https://github.com/MorrisStatLab', 'info_list': ['C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'JavaScript', 'Updated Jul 3, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'Jupyter Notebook', 'Updated Sep 26, 2020', 'Python', 'Updated Jan 6, 2020', '1', 'MATLAB', 'Updated Jul 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Big_data_Clustering_ClusterCodes\nIn the repository most files are explanatory notes of the homework\nthe codes are in py form\n'], 'url_profile': 'https://github.com/rgfrgf', 'info_list': ['Python', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Peer-graded Assignment: Regression Models Course Project\nYou work for Motor Trend, a magazine about the automobile industry. Looking at a data set of a collection of cars, they are interested in exploring the relationship between a set of variables and miles per gallon (MPG) (outcome). They are particularly interested in the following two questions:\n\n“Is an automatic or manual transmission better for MPG”\n""Quantify the MPG difference between automatic and manual transmissions""\n\nPeer Grading\n\nThe criteria that your classmates will use to evaluate and grade your work are shown below.\nEach criteria is binary: (1 point = criteria met acceptably; 0 points = criteria not met acceptably)\nYour Course Project score will be the sum of the points and will count as 40% of your final grade in the course.\n\nCriteria\n\nDid the student interpret the coefficients correctly?\nDid the student do some exploratory data analyses?\nDid the student fit multiple models and detail their strategy for model selection?\nDid the student answer the questions of interest or detail why the question(s) is (are) not answerable?\nDid the student do a residual plot and some diagnostics?\nDid the student quantify the uncertainty in their conclusions and/or perform an inference correctly?\nWas the report brief (about 2 pages long) for the main body of the report and no longer than 5 with supporting appendix of figures?\nDid the report include an executive summary?\nWas the report done in Rmd (knitr)?\n\nQuestion\nTake the \'mtcars\' data set and write up an analysis to answer their question using regression models and exploratory data analyses.\nYour report must be:\n\nWritten as a PDF printout of a compiled (using knitr) R markdown document.\nBrief. Roughly the equivalent of 2 pages or less for the main text. Supporting figures in an appendix can be included up to 5 total pages including the 2 for the main report. The appendix can only include figures.\nInclude a first paragraph executive summary.\n\n'], 'url_profile': 'https://github.com/thawatchai-p', 'info_list': ['Python', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'Greater Toronto Area', 'stats_list': [], 'contributions': '413 contributions\n        in the last year', 'description': ['Movie Revenue Predictor\nUsing Linear Regression\n\n'], 'url_profile': 'https://github.com/Nayalash', 'info_list': ['Python', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'hyderabad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shyamssr', 'info_list': ['Python', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': [""Linear-Regression-Generator\nLinear Regression Generator from CSV files\nRun program with 'Python3 leastsquares.py'\n""], 'url_profile': 'https://github.com/blankdean', 'info_list': ['Python', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/neelkandlikar', 'info_list': ['Python', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'West Bengal, India', 'stats_list': [], 'contributions': '99 contributions\n        in the last year', 'description': ['House_Pricing\nPredict house pricing using regression techniques\nDownload and execute in Google Colab. Works Best !!!\n'], 'url_profile': 'https://github.com/samiranberahaldia', 'info_list': ['Python', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Linear_Regression\nPython Implementation of Linear Regression Algorithm\n'], 'url_profile': 'https://github.com/them0e', 'info_list': ['Python', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Projet Python: Année 5\nDataset: Incident event log\n\n\nI. Modélisation\n\nLa partie modélisation se trouve dans le fichier Final Project.ipynb\nUne modélisation avec des résultats différents est également présente: Option 2.ipynb\n\n\n\nII. API Django\n\nLe modèle est sur git LFS (fichier volumineux) et doit être téléchargé manuellement (api/predictors/models/models.p)\nConfiguration de l\'environnement: Ouvrir un terminal dans DjangoAPI, et lancer les commandes suivantes:\n\npython3 -m venv env\nsource env/bin/activate\n\n\nSi les paquets suivants ne sont pas installés, lancer la commande:\n\npip install django djangorestframework sklearn numpy pandas\n\n\napi/api/urls.py contient les URL endpoints\napi/predictor/apps.py contient les variables extraites du modèle\napi/predictors/views.py contient les réponses aux requêtes\napi/predictor/models/models.p contient les modèles résultant de notre modélisation (I.)\nEndpoint attendu: regressor/\nParamètres attendus:\n\ncaller_id\nlocation\nresolved_by\nopened_by\nopened_at (format YYYY/MM/DD HH:MM)\n\n\nFormat de la réponse: {""Random Forest Regressor:"": prediction, ""Decision Tree:"": prediction, ""Valeur probable:"": prediction, ""Finira le:"" prediction}\n\n\n\nIII. Présentation\n\nLa présentation PowerPoint est enregistrée sous le nom ""Projet python for data analysis.pptx""\n\n\n\n'], 'url_profile': 'https://github.com/shenzou', 'info_list': ['Python', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'Bangalore,India', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['House Prices- advanced regression\nThis is a jupyter notebook solution for house price prediction problem in kaggle.\nHere, we have trained the model using both ridge regression and lasso regression. Final model chosen is lasso gression because model is less complex compared to ridge model.\nReferences : https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/Shari18', 'info_list': ['Python', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}"
"{'location': 'South Africa', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Polynomial-Regression\nSalary over years worked using Polynomial regression\nHere I was tasked to use Polynomial Regression on any chosen dataset and display my results using a visual representation of the findings. I opted to display how a employee's salary goes up over the amount of years worked.\n""], 'url_profile': 'https://github.com/Lordwan', 'info_list': ['PowerShell', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'JavaScript', 'Updated Jan 7, 2020', '1', 'MATLAB', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['RegressionDiscontinuity_Demo\nRegression discontinuity methods can be useful for establishing causal relationships. While there are a number of great packages for performing these analyses in Stata, there are limited tools available for performing regression discontinuity studies in Python. This script demonstrates how to use packages in Python, most notably Seaborn for visualization, to perform a Regression Discontinuity study. Data not provided as it was proprietary.\n'], 'url_profile': 'https://github.com/pjcizek', 'info_list': ['PowerShell', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'JavaScript', 'Updated Jan 7, 2020', '1', 'MATLAB', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'Kahramanmaraş/Turkey', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['SimpleChatBot\nSimple Chatbot with python. Uses Regression Algorithm\nYou can use it with your purpose by just changing the ""intents"" json file and train.\nI use Conda for the environment but you just need to install a couple of libs and it will be fine.\nEEA\n'], 'url_profile': 'https://github.com/EnsarErayAkkaya', 'info_list': ['PowerShell', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'JavaScript', 'Updated Jan 7, 2020', '1', 'MATLAB', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'jaipur', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['CSV_RegressionMochaTesting\n'], 'url_profile': 'https://github.com/rk23597', 'info_list': ['PowerShell', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'JavaScript', 'Updated Jan 7, 2020', '1', 'MATLAB', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '842 contributions\n        in the last year', 'description': ['Numerical Methods\nThis repository is used to store code from course Computational Techniques, which I took at my university. Here you can find some of interpolation, approximation methods or numerical integration.\nUsage\nI tested this code using GNU Octave and MATLAB. It is your choice, which program you are going to use.\n'], 'url_profile': 'https://github.com/Mregussek', 'info_list': ['PowerShell', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'JavaScript', 'Updated Jan 7, 2020', '1', 'MATLAB', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '205 contributions\n        in the last year', 'description': ['Dataset used was Examination results of a Medical school which is confifential\nCode can be used for other datasets too.\n'], 'url_profile': 'https://github.com/vidhlakh', 'info_list': ['PowerShell', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'JavaScript', 'Updated Jan 7, 2020', '1', 'MATLAB', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Convolutional-Neural-Networks-for-Financial-Text-Regression\n创新工场金融项目：Financial volatility forecasting from annual reports\n复现自acl paper Convolutional-Neural-Networks-for-Financial-Text-Regression.\nData\nThe dataset from Tsai et al. (2016) includes Extended 10-K Corpus available on the U.S. Security Exchange Commission (SEC) Electronic Data Gathering, Analysis and Retrieval (EDGAR) website2. Following previous works (Kogan et al., 2009; Wang et al., 2013; Tsai and Wang, 2014; Tsai et al., 2016), section 7, Management’s Discussion and Analysis (MD&A) is used instead of the complete 10-K report.\nThe dataset also includes a volatility value for each report of 12 months after the report is published. The volatility value in the dataset is the natural logarithm of stock return volatility and used as the prediction target.\nModel\n\nCNN-baseline\nCNN-STC\nCNN-NTC\nCNN-STC-multichannel\nCNN-NTC-multichannel\n\nFinal hyperparameters are selected as mini-batch size 10, fixed text length 20000, convolution layer kernels 3, 4 and 5 with 100 output features, probability of dropout layer 0.5, and learning rate 0.001.\nResults\n\n\n\nModel\n2008\n2009\n2010\n2011\n2012\n2013\nAvg\n\n\n\n\nCNN-simple (baseline)\n0.3716\n0.4708\n0.1471\n0.1312\n0.2412\n0.2871\n0.2748\n\n\nCNN-STC\n0.5358\n0.3575\n0.3001\n0.1215\n0.2164\n0.1497\n0.2801\n\n\nCNN-NTC-multichannel\n0.5077\n0.4353\n0.1892\n0.1605\n0.2116\n0.1268\n0.2718\n\n\nCNN-STC-multichannel\n0.4121\n0.4040\n0.2428\n0.1574\n0.2082\n0.1676\n0.2653\n\n\nCNN-NTC\n0.4672\n0.3169\n0.2156\n0.1154\n0.1944\n0.1238\n0.2388\n\n\n\nPerformance of different models(from paper), measured by Mean Square Error (MSE). Lower is better and boldface shows the  best result among presented models for the corresponding column.\n'], 'url_profile': 'https://github.com/HouchangX-AI', 'info_list': ['PowerShell', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'JavaScript', 'Updated Jan 7, 2020', '1', 'MATLAB', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'Mauritius', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['Using Advanced Regression Techniques to Predict House Prices\nRepo for the House Prices: Advanced Regression Techniques Kaggle competition - https://www.kaggle.com/c/house-prices-advanced-regression-techniques\nRefer to https://github.com/kavish-p/kaggle-house-prices/blob/master/house-prices.ipynb for Jupyter Notebook documenting the data science approach used for this competition.\nMy submission was ranked in the top 14% at the time of submission. (703/5280)\n'], 'url_profile': 'https://github.com/kavish-p', 'info_list': ['PowerShell', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'JavaScript', 'Updated Jan 7, 2020', '1', 'MATLAB', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Regressions (w/ Python & R)\nThis repository consists ofvarious regression techniques such as Simple Linear Regression, Multiple Linear Regression, Polynomial Linear Regression, Decision Tree Regression, Random Forest regression, and Support Vector Regression.\nProject name: Regressions (w/ Python & R)\nDescription\n\xa0\xa0\xa0\xa0Regression is the preocess of predicting the values of each individuals from dependent variables based on theie information available on independent variables. Also regressions can determine the effects of some exploratory variable on the dependent variable.\n\xa0\xa0\xa0\xa0In this repository, we are focusing on building complex statistical regression models to predict and visualize the prediction results only on continous dependent variables. Mostly, based on the nature of datapoints (Scatterplot) and the correlation between dependent and independent variables, we can decide which model is appropriate for the given dataset and what variables are important for the model building process.\nInstallation\n\xa0\xa0\xa0\xa0In order to be able to apply these regression techniques to predict outcome on your dataset you need to have following tools and libraries:\n\nPython 2.x or Python 3.x\nPandas\nNumPy\nScikit-Learn\nR (For R implementation)\n\nUsage\n\xa0\xa0\xa0\xa0Regression methodes are very common term and technique in the field of data science. You can use these regression methods to predict the continous dependent variables such as salary of individuals, total sales of the year, age of a person and more.\nContributing\n\xa0\xa0\xa0You can implement multiple models on the same dataset to determine the most accurate model.\n'], 'url_profile': 'https://github.com/peak27', 'info_list': ['PowerShell', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'JavaScript', 'Updated Jan 7, 2020', '1', 'MATLAB', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Roumanov', 'info_list': ['PowerShell', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'JavaScript', 'Updated Jan 7, 2020', '1', 'MATLAB', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Oct 31, 2020', 'Python', 'Updated Jan 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""academic_earhart_bones\nLogistic Regression Analysis - The Amelia Earhart Case and Biological Gender Determination from Bone Length\nThis project was inspired by a CNN article about the Amelia Earhart case. Human bones were found on an island and somebody claimed they were Amelia Earhart's. There were conflicting scientific conclusions. There were few bones to work with.\nhttps://edition.cnn.com/2018/03/08/health/amelia-earhart-bones-island-intl/index.html\nThe goal of this project was to see if bone length can be used to accurately determine biological gender.\n""], 'url_profile': 'https://github.com/Timothy-L-Baron', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 9, 2020', '1', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'Stockholm', 'stats_list': [], 'contributions': '199 contributions\n        in the last year', 'description': ['Gaussian-processes\nDemonstration of Gaussian processes for a simple regression problem\nA Gaussian process with RBF kernel:\n\nAnd with linear kernel:\n\nThe simple regression problem is a sinusoidal function with noise added:\n\nPredictive posterior without taking the noise into account:\n\nTaking the noise into account:\n\n'], 'url_profile': 'https://github.com/MaxHolmberg96', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 9, 2020', '1', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['logistic_regression\nThis project performs classification using machine learning for a two-class problem. The classification features used are pre-computed from the digital images of a fine needle aspirate (FNA) of a breast mass. We have classified the suspected FNA cells as Benign (class 0) or Malignant (class 1) using logistic regression as the classifier using the Wisconsin Diagnostic Breast Cancer (wdbc. dataset) as our dataset.\n'], 'url_profile': 'https://github.com/rspai', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 9, 2020', '1', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Linear Regression in Multiple Variable\n\nImplementation of linear regression in Multiple Variable\nThe equation solve using Gaussian Elimination\n\nExecution\n$ gcc multi_variable.c\n$ ./a.out\n'], 'url_profile': 'https://github.com/kindlerprince', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 9, 2020', '1', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'Chicago, Illinois', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Predicting-stock-prices-using-Support-Vectore-Regression\nMain_all.m is the main file of matlab code. All datasets are taken from Yahoo Finance.\n'], 'url_profile': 'https://github.com/taniyarajani', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 9, 2020', '1', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Hunga-Bunga\nBrute Force all scikit-learn models and all scikit-learn parameters with fit predict.\n\nLets brute force all sklearn models with all of sklearn parameters!  Ahhh Hunga Bunga!!\nfrom hunga_bunga import HungaBungaClassifier, HungaBungaRegressor\nAnd then simply:\n\n\n\n\nWhat?\nYes.\nNo! Really! What?\nMany believe that\n\nmost of the work of supervised (non-deep) Machine Learning lies in feature engineering, whereas the model-selection process is just running through all the models or just take xgboost.\n\nSo here is an automation for that.\nHOW IT WORKS\nRuns through all sklearn models (both classification and regression), with all possible hyperparameters, and rank using cross-validation.\nMODELS\nRuns all the model available on sklearn for supervised learning here. The categories are:\n\nGeneralized Linear Models\nKernel Ridge\nSupport Vector Machines\nNearest Neighbors\nGaussian Processes\nNaive Bayes\nTrees\nNeural Networks\nEnsemble methods\n\nNote: Some models were dropped out (nearly none of them..) and some crash or cause exceptions from time to time. It takes REALLY long to test this out so clearing exceptions took me a while.\nInstallation\npip install hunga-bunga\nDependencies\n\n- Python (>= 2.7)\n- NumPy (>= 1.11.0)\n- SciPy (>= 0.17.0)\n- joblib (>= 0.11)\n- scikit-learn (>=0.20.0)\n- tabulate (>=0.8.2)\n- tqdm (>=4.28.1)\n\n\nOption I (Recommended): brain = False\nAs any other sklearn model\nclf = HungaBungaClassifier()\nclf.fit(x, y)\nclf.predict(x)\nAnd import from here\nfrom hunga_bunga import HungaBungaClassifier, HungaBungaRegressor\nOption II: brain = True\nAs any other sklearn model\nclf = HungaBungaClassifier(brain=True)\nclf.fit(x, y)\n\nThe output looks this:\n\n\n\nModel\naccuracy\nTime/clf (s)\n\n\n\n\nSGDClassifier\n0.967\n0.001\n\n\nLogisticRegression\n0.940\n0.001\n\n\nPerceptron\n0.900\n0.001\n\n\nPassiveAggressiveClassifier\n0.967\n0.001\n\n\nMLPClassifier\n0.827\n0.018\n\n\nKMeans\n0.580\n0.010\n\n\nKNeighborsClassifier\n0.960\n0.000\n\n\nNearestCentroid\n0.933\n0.000\n\n\nRadiusNeighborsClassifier\n0.927\n0.000\n\n\nSVC\n0.960\n0.000\n\n\nNuSVC\n0.980\n0.001\n\n\nLinearSVC\n0.940\n0.005\n\n\nRandomForestClassifier\n0.980\n0.015\n\n\nDecisionTreeClassifier\n0.960\n0.000\n\n\nExtraTreesClassifier\n0.993\n0.002\n\n\n\nThe winner is: ExtraTreesClassifier with score 0.993.\n'], 'url_profile': 'https://github.com/vpenumar', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 9, 2020', '1', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['microsoftproject\nThis project is about the logistic regression for microsoft example.\n'], 'url_profile': 'https://github.com/Ravitejagrt25', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 9, 2020', '1', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'Rajkot , Gujarat', 'stats_list': [], 'contributions': '966 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nipun214', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 9, 2020', '1', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/minamaher58', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 9, 2020', '1', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tytea', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 9, 2020', '1', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}"
"{'location': 'Boston, MA', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rakesh-choudhury', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated May 16, 2020', 'CSS', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'HTML', 'MIT license', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Project Overview\nThe ""spam"" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography…\nOur collection of spam e-mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word \'george\' and the area code \'650\' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.\n'], 'url_profile': 'https://github.com/vaibhavukarande', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated May 16, 2020', 'CSS', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'HTML', 'MIT license', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 7, 2020']}","{'location': 'St. Louis, MO', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['MA425 Housing Data Report (Fall 2017)\nStatistical Report from Housing Data Project in MA425 Applied Regression Analysis\nThe purpose of this group project was to create a model to accurately predict the sales price of a house based on its amenities.\nThe variables can be viewed in QuestionAndVariables.pdf and the data can be viewed in HousingData.txt.\nThe statistical report can be viewed in MA425StatisticalReport.docx.\n'], 'url_profile': 'https://github.com/britaniprescott', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated May 16, 2020', 'CSS', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'HTML', 'MIT license', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression-Model\nQuick Start Guide to using Linear Regression Models in Python\nIntroduction\nA linear regression model might be the simplest of all models to understand. It tries to explain a relationship between\nindependent variables and a dependent variable. We will do this by using the least squared method. This is a statistical   procedure to find the best fit for a set of data points by minimizing the sum of the offsets or residuals of points from the   plotted curve. Least squares regression is used to predict the behavior of dependent variables.\nPrerequisites\n\nPython 3.7 or newer\nScikit-Learn module for Python\nPandas module for Python\nNumpy module for Python\nMatplotlib module for Python\nSeaborn module for Python\n\nWalkthough\nStart by importing all modules you will need at the beginning of your script. These include: Pandas, Scikit-Learn,\nMatplotlib, and seaborn.\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nNext import the csv file and take a closer look. Below we are importing the csv file as an object ""df"".\nOnce again we print the head of the data along with the info to get an idea of what the data looks like.\ndf = pd.read_csv(r""D:\\Datasets\\USA_Housing.csv"")\nprint(df.head())\nprint(df.info())\n\nMake a pairplot using seaborn to compare each variable to one other. this will show us if there is any correlation between\nfeatures. To see if the data is evenly distributed, we can make a distribution plot of the price. This data appars to be evenly\ndistributed.\nsns.pairplot(df)\nplt.show()\n\nsns.distplot(df[\'Price\'])\nplt.show()\n\nThe x and y variables can now be established. The X variables are going to be the features you would like to use to\npredict y. Y is going to be the dependant variable. In this case, the y variable is ""Price"".\nX= df[[\'Avg. Area Income\', \'Avg. Area House Age\', \'Avg. Area Number of Rooms\',\n       \'Avg. Area Number of Bedrooms\', \'Area Population\']]\n\ny=df[\'Price\']\n\nNow that the variables have been established, it is time to split the data into two parts. The training set\nis what our model is going to look at and learn from. The model will then try to apply what it has learned to the test data.\nThe ""test_size"" argument, is set to 0.4 in this example. That means that 60% of the data will be used to train the model\nand the remaining 40% will be used to test how accurate the model is. The ""random_state"" parameter sets the random\nnumber generator. This makes it so we are able to replicate results.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101) \n\nThe model can now be instantiated. Call the linear regression model and assign it to lm. Next the model needs to be fitted\nor trained as shown below. We can also check the coefficient of each feature below to see which features play the biggest\nrole in price change of houses. This data is synthetically generated so it might not look realistic, but it is a worth while\nexercise. Next the perdictions need to be generated using the .predict() method.\nm = LinearRegression() #creating an instanciation of the linear regression model\nlm.fit(X_train,y_train) #this is attempting to teach the algo based on the training data set that we pulled\n\ncdf = pd.DataFrame(lm.coef_,X.columns,columns=[\'Coeff\']) # as follows -- the data we want to show (the coef), for what?(each of the categories in the independant variable),name the column (\'Coeff\')\nprint(cdf)\n\npredictions = lm.predict(X_test) #setting the prediction variable equal to the x test\n\n\nFor a visual of the correlation between the features and result, a scatter plot can be made. The linear pattern\nof the data points is a good sign and means that the features do have correlation to the result.\nLastly, it is time to analyize the accuracy of the model. This will be done using Mean Absolute Error, Mean Squared Error,\nand Root Mean Squared.\nprint(""Mean Absolute Error"")\nprint(metrics.mean_absolute_error(y_test,predictions))\nprint(""Mean Squared Error"")\nprint(metrics.mean_squared_error(y_test,predictions))\nprint(""Root Mean Squared Error"")\nprint(np.sqrt(metrics.mean_squared_error(y_test,predictions)))\n\n\n'], 'url_profile': 'https://github.com/Drew-code', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated May 16, 2020', 'CSS', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'HTML', 'MIT license', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 7, 2020']}","{'location': 'Japan', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Planetary-image-inpainting\nPlanetary image inpainting by learning modality specific regression models.\nIntroduction:\nSophisticated imaging instruments on-board spacecraft orbiting different planets and planetary bodies in this solar system, enable humans to discover and visualize the unknown. However, these planetary surface images suffer from some missing pixel regions, which could not be captured by the spacecraft onboard cameras because of some technical limitations. In this work, we try to inpaint these missing pixels of the planetary images using modality-specific regression models that were trained with clusters of different images with similar histogram distribution on the experimental dataset. Filling in missing data via image inpainting enables downstream scientific analysis such as the analysis of morphological features on the planetary surface - e.g., craters and their sizes, central peaks, interior structure, etc|in comparison with other planetary bodies. Here, we use the grayscale version of Mars orbital images captured by the HiRISE (High-Resolution Imaging Science Experiment) camera on the Mars Reconnaissance Orbiter (MRO) for the experimental purpose. The results show that our method can fill in the missing pixels existing on the Mars surface image with good visual and perceptual quality and improved PSNR values. Detailed description of the system can be found in our paper.\nDataset:\n\nPlease download the dataset from here: Mars orbital image (HiRISE)labeled data set\n\n1) Training\nTo train the model, run:\npython main.py --cluster_num ""0"" \n'], 'url_profile': 'https://github.com/hiyaroy12', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated May 16, 2020', 'CSS', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'HTML', 'MIT license', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['MRT Sakay: Predicting Hourly Passenger Data in the MRT-3 (Philippines)\nPredicting the number of passengers in the crowded MRT-3 is an important first step in planning future improvements and in managing foot traffic in the train stations.\nIn this study, we used Logistic Regression (L1 and L2), Random Forest Regression, and Gradient Boosted Regression to predict the hourly passenger count in an MRT train station.\nHistorical passenger data for 2016 and 2017 was collected from the Department of Transportation (DoTr) official website and weather data is from wundergound.com.\nThe Gradient Boosted Model with learning rate of 0.2 and max depth of 20 produced the best result with an average r-squared accuracy of 0.9436 using 10-fold cross validation.\nTo create the environment and run the application, start an anaconda prompt in the cloned directory and run the following:\nconda env create --file environment.yml\nconda activate daw-individual-2-delnorte\npython application.py\nThen open this link in your browser: http://127.0.0.1:8082/\n\nA deployed application is also available in the link below until January 31, 2020:\nhttp://individual2-dev.ap-northeast-1.elasticbeanstalk.com/\n'], 'url_profile': 'https://github.com/nortz8', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated May 16, 2020', 'CSS', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'HTML', 'MIT license', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 7, 2020']}","{'location': 'Fremont,California', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Toyota-Corolla-Used-Car-Price-Prediction\nAnalysis of Toyota Corolla Used Car price Prediction  By Regression.\n'], 'url_profile': 'https://github.com/priyaroopa', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated May 16, 2020', 'CSS', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'HTML', 'MIT license', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 7, 2020']}","{'location': 'Germany', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['KaggleHousePrices\nThis notebook (fully rendered view) is my first attempt at the Kaggle challenge House Price: Advanced Regression Techniques. It combines Lasso, XGB and a feedforward Neural Network and achieves a public leaderboard score of approx. 0.1203.\n'], 'url_profile': 'https://github.com/larpal', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated May 16, 2020', 'CSS', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'HTML', 'MIT license', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 7, 2020']}","{'location': 'Towson, MD', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['LA Crime Predictor\nTable of Contents\n\nDescription\nDeployment\nDataset\nLicensing\n\nDescription \nJavascript Web Application for crime forcasting using linear regression\nDeployment \nYou can visit the web applciation at LA Crime Predictor\nDataset \nThe dataset contains about 1.31 million records: arrest incidents that occurred in the City of Los Angeles dating back to 2010. This data is transcribed from original arrest reports that are typed on paper and therefore there may be some inaccuracies within the data.  Address fields are only provided to the nearest hundred block in order to maintain privacy.\nThe dataset is available at data.lacity.org.\nLicensing \nStandard MIT License.\n'], 'url_profile': 'https://github.com/emichris', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated May 16, 2020', 'CSS', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'HTML', 'MIT license', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['linear-regression-curve-fitting\nthis is the implementation for linear regression using matlab/octave\n'], 'url_profile': 'https://github.com/yonasyifter', 'info_list': ['Jupyter Notebook', 'Updated Apr 26, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated May 16, 2020', 'CSS', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'HTML', 'MIT license', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['nbdev template\nUse this template to more easily create your nbdev project.\n'], 'url_profile': 'https://github.com/AbhishekKaps', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 12, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': [""Here I've added examples of tree models and linear regression\nSGB - give better result of RMSE than Gradient Boosting\n""], 'url_profile': 'https://github.com/lewandowskimariusz', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 12, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'Los Angeles, California', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': [""NBAmvp\nPredict MVP of the National Basketball Association using linear regression\nUsing statistical data from over 630+ NBA MVP candidates, this model predicted the 2020 MVP would be Luka Doncic of the Dallas Mavericks.\nThis model takes into account the points, rebounds, assists, steals, and blocks per game, combined with the the player's contribution in team wins.\nThis model also factors the increased MVP odds of younger candidates (ex. 2011 Derrick Rose) and historic statistical seasons (ex. 2017 Russell Westbrook).\nUsed stats from basketball-reference.com\n""], 'url_profile': 'https://github.com/jnakama', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 12, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '356 contributions\n        in the last year', 'description': ['Weather Prediction\nPredicting temperature\nData\nAPI Powered by Dark Sky\nThe Forecast Request returns the current weather conditions, a minute-by-minute forecast for the next hour (where available), an hour-by-hour forecast for the next 48 hours, and a day-by-day forecast for the next week.\nDocumentation: https://darksky.net/dev/docs#forecast-request\nForecast Request: https://api.darksky.net/forecast/[key]/[latitude],[longitude]?<parameters>\nThe API is used to obtain the current weather conditions along with the day-by-day forecast for the next week.\nSample Request:\nhttps://api.darksky.net/forecast/[key]/22.5726,88.3639?exclude=[currently,minutely,hourly,alerts,flags]&units=auto\n\nSample Response:\n{ \n    ""latitude"":22.5726,\n    ""longitude"":88.3639,\n    ""timezone"":""Asia/Kolkata"",\n    ""daily"":{ \n        ""summary"":""No precipitation throughout the week."",\n        ""icon"":""clear-day"",\n        ""data"":[\n            { \n                ""time"":1579113000,\n                ""summary"":""Clear throughout the day."",\n                ""icon"":""clear-day"",\n                ""sunriseTime"":1579135800,\n                ""sunsetTime"":1579175040,\n                ""moonPhase"":0.72,\n                ""precipIntensity"":0.0031,\n                ""precipIntensityMax"":0.0074,\n                ""precipIntensityMaxTime"":1579174800,\n                ""precipProbability"":0.04,\n                ""precipType"":""rain"",\n                ""temperatureHigh"":27.59,\n                ""temperatureHighTime"":1579163700,\n                ""temperatureLow"":15.31,\n                ""temperatureLowTime"":1579218000,\n                ""apparentTemperatureHigh"":27.31,\n                ""apparentTemperatureHighTime"":1579163700,\n                ""apparentTemperatureLow"":15.58,\n                ""apparentTemperatureLowTime"":1579218000,\n                ""dewPoint"":11.8,\n                ""humidity"":0.58,\n                ""pressure"":1014.8,\n                ""windSpeed"":2.21,\n                ""windGust"":5.92,\n                ""windGustTime"":1579142700,\n                ""windBearing"":6,\n                ""cloudCover"":0,\n                ""uvIndex"":7,\n                ""uvIndexTime"":1579155300,\n                ""visibility"":16.093,\n                ""ozone"":246.8,\n                ""temperatureMin"":15.27,\n                ""temperatureMinTime"":1579131780,\n                ""temperatureMax"":27.59,\n                ""temperatureMaxTime"":1579163700,\n                ""apparentTemperatureMin"":15.54,\n                ""apparentTemperatureMinTime"":1579131780,\n                ""apparentTemperatureMax"":27.31,\n                ""apparentTemperatureMaxTime"":1579163700\n            }\n        ]\n    },\n    ""offset"":5.5\n}\n\n\nDependencies\n\nDatetime\nRequests\nPandas\nMatplotlib\nNumpy\nScikit-learn\n\npip install -r requirements.txt\n\nData Collection\n\nSpecify API Key, Latitude, Longitude, and parameters in the Forecast API.\nSpecify the features that are needed to be parsed from the responses returned from the API.\n\nThe features are the keys present in the daily -> data portion of the JSON response.\n\n\nUse the features to define a namedtuple that will be used to organize the data.\n\nData is collected and saved in data/year-mon-date_hour_min.csv using the script in weather_data_collection.ipynb\n\nIntuition\nMachine Learning experiments often have a few characteristics that are oxymoronic or self-contradictory, as well as highly influential self-explanatory variables and patterns that arise out of having almost a naive or at least very open and minimal presuppositions about the data.\nTherefore, it is required to have the knowledge-based intuition to look for potentially useful features and patterns as well as unforeseen idiosyncrasies in an unbiased manner for a successful analytics project.\nIn this regards, we have selected a number of features of the weather data, but many of these might be uninformative in predicting weather depending on the model being used. So we need to rigorously investigate the data\n\nFeature Engineering\n\nDevelop an algorithm to find out the N previous days measurement of a feature\n\nFor each day (row) and for a given feature (column), find the value for that feature N days prior.\nFor each value of N, make a new column for that feature representing the Nth prior day\'s measurement.\n\n\nUse the algorithm to find out the 3 previous days measurement of each feature.\n\nFeatures obtained:\n\ndate\nsummary\ncloudCover\ntemperatureHigh\ntemperatureLow\nhumidity\nprecipProbability\nprecipType\ndewPoint\npressure\nwindSpeed\nwindGust\nvisibility\nuvIndex\nozone\ncloudCover_-1\ncloudCover_-2\ncloudCover_-3\ntemperatureHigh_-1\ntemperatureHigh_-2\ntemperatureHigh_-3\ntemperatureLow_-1\ntemperatureLow_-2\ntemperatureLow_-3\nhumidity_-1\nhumidity_-2\nhumidity_-3\nprecipProbability_-1\nprecipProbability_-2\nprecipProbability_-3\nprecipType_-1\nprecipType_-2\nprecipType_-3\ndewPoint_-1\ndewPoint_-2\ndewPoint_-3\npressure_-1\npressure_-2\npressure_-3\nwindSpeed_-1\nwindSpeed_-2\nwindSpeed_-3\nwindGust_-1\nwindGust_-2\nwindGust_-3\nvisibility_-1\nvisibility_-2\nvisibility_-3\nuvIndex_-1\nuvIndex_-2\nuvIndex_-3\nozone_-1\nozone_-2\nozone_-3\n\n\nData Preprocessing\nIdentify unnecessary data, missing values, consistency of data types, outliers and handle them.\n\n\nDrop unnecessary columns.\n\n\nConvert categorical columns to numeric, if any.\n\n\nCheck features\' statistics to find outliers.\n\nFind the inter quartile range (IQR)\n\nDifference between third and first quartiles\n\n\nCheck for the outliers\n\nThose which are 3 IQRs below first quartile\nThose which are 3 IQRs above third quartile\n\n\nGet the features which contain outliers.\n\n\n\n10 features have outliers\n\ncloudCover\ncloudCover_-1\ncloudCover_-2\ncloudCover_-3\nprecipProbability\nprecipProbability_-1\nprecipProbability_-2\nprecipProbability_-3\nuvIndex\nuvIndex_-1\n\n\n\nAnalyzing and handling the outliers\n\nDistribution of cloudCover: It si natural to have clear skies on some days.\nDistribution of precipProbability: Since the dry days (ie, no precipitation) are much more frequent, it is sensible to see outliers here.\nSimilar is the situation for uvIndex where some days have higher measurement and others have lower.\n\n\n\nCheck for missing data\n\nThe features contain relatively few missing (null / NaN) values, only the ones introduced due to the N prior days values.\n\n\n\nHandle missing data\n\nFill missing values with the mean column values\n\n\n\n\nData Analysis\n\nFind the relationship between the dependent variable and each independent variable.\n\nPearson Correlation Coefficient: A measurement of the amount of linear correlation between equal length arrays which outputs a value ranging -1 to 1.\n\n0 to 1 represent increasingly positive correlation, i.e. strong linear relationship.\n-1 to 0 represent inverse, or negative, correlation, i.e. weak linear relationship.\n\n\n\n\n\n0.8 - 1.0\tVery Strong\n0.6 - 0.8\tStrong\n0.4 - 0.6\tModerate\n0.2 - 0.4\tWeak\n0.0 - 0.2\tVery Weak\n\nKeep relevant features\n\n\nThe features that have correlation values less than the absolute value of 0.6 are not to be used as predictors.\n\n\nThe columns with the min and max temperature of the day are not useful for the model as the target \'meanTemp\' is simply the mean value of these twofeatures.\n\n\'temperatureLow\' and \'temperatureLow are removed.\n\n\n\nSplit data into Training and Test sets\n\nTest size = 20%\n\nCreate and Train Model\n\nLinear Regression\n\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nEvaluate the Model\n\nExplained Variance: 95.63 %\n\nThe model is able to explain 95.63% of the variance observed in the outcome variable.\n\n\nMean Absolute Error: 0.47 units\n\nOn average the predicted value is about 0.47 units off.\n\n\nMedian Absolute Error: 0.47 units\n\nHalf of the time the predicted value is off by about 0.47 units.\n\n\n\n'], 'url_profile': 'https://github.com/likarajo', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 12, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['#include<stdio.h>\n#include<stdlib.h>\n#include<math.h>\nint\nrandomgen (int a, int b)\n{\nreturn rand () % (b);\n}\nvoid\nmain ()\n{\nfloat summ1, summ2, sumbai;\nint x1[50] = { 0 };\nint x2[50] = { 0 };\nfloat cost;\nfloat lr = 0.05;\nfloat y_predict[50] = { 0 };\nfloat target[50] = { 0 };\nint bias;\nfloat m1 = 1, m2 = 2, bai = 1;\nfor (int i = 0; i < 50; i++)\n{\nx1[i] = randomgen (1, 10);\nx2[i] = randomgen (1, 10);\n}\nint M1, M2;\nprintf (""\\nEnter M1 : "");\nscanf (""%d"", &M1);\nprintf (""\\nEnter M2 : "");\nscanf (""%d"", &M2);\nprintf (""\\nEnter bias : "");\nscanf (""%d"", &bias);\nfor (int i = 0; i < 50; i++)\n{\ntarget[i] = M1 * x1[i] + M2 * x2[i] + bias;\n}\nint prassy = 1500;\nfor (int i = 0; i < 50; i++)\n{\nprintf (""%d         %d      %d       %f\\n"", x1[i], x2[i], bias,\ntarget[i]);\n}\nwhile (prassy--)\n{\ncost = 0;\nsumm1 = 0;\nsumm2 = 0;\nsumbai = 0;\nfor (int i = 0; i < 50; i++)\n{\ny_predict[i] = (m1 * x1[i]) + (m2 * x2[i]) + bai;\n}\nfor (int i = 0; i < 50; i++)\n{\ncost +=\n(y_predict[i] - target[i]) * (y_predict[i] - target[i]) / 100;\n}\nfor (int i = 0; i < 50; i++)\n{\nsumm1 += (y_predict[i] - target[i]) * x1[i];\nsumm2 += (y_predict[i] - target[i]) * x2[i];\nsumbai += ((y_predict[i] - target[i]));\n}\nm1 = m1 - (lr * summ1) / 100;\nm2 = m2 - (lr * summ2) / 100;\nbai = bai - (lr * sumbai) / 100;\nprintf (""\\n Cost : %f \\n m1 : %f\\n m2 : %f\\nBias : %f"", cost, m1, m2,bai);\n}\nprintf(""\\nDid you enter %f , %f , %f"",m1,m2,bai);\n}\n'], 'url_profile': 'https://github.com/prasannanivas', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 12, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GeneSubra', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 12, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'San Diego, CA', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': [""Introduction to TensorFlow\nBasic TensorFlow linear regression notes\nThis is a simple notebook to confirm my understanding on how TensorFlow works. I will do simple example with a univariate linear regression with numerical derivations.\nHypothesis function, h(x)\ny = mx\nStarting parameter / Initial hypothesis\nm = 0 \nTarget Parameter / Goal\nm = 1\nDataset\n(-3,3), (-2,-2), (-1, -1), (0,0), (1,1), (2,2), (3,3)\nLoss function\nSquare loss function\nOptimizer\nGradient descent with $\\alpha = 0.0001 $\nWe will also stop after one epoch and compare our calculated weight update with TensorFlow's to confirm our expectation.\nNote: This is using TensorFlow 1.x.\n""], 'url_profile': 'https://github.com/ohui', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 12, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lenmade', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 12, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['AutomatingMETS\nThis is to automate METS QA Regression cases\n'], 'url_profile': 'https://github.com/anoopgupta85', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 12, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'France', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Sentence Boundary Detection\nThis repository attempts to detect sentence boundaries (which is a harder problem than it appears at first sight).\nIt also attempts to apply its algorithm to segment html (surrounding sentences with <span class=""sentence"">[...]</span>\nDon\'t use this repository in production as it\'s not battle tested yet. Some functions seem brittle and error recovery is absent. There are more mature tool out there like nltk punkt.\nIt has also only been tested on the english language.\nThe HTML feature will respect existing inline tags (e.g span, strong ...). If a sentence partially overlaps with such tags, they will be closed and reopened. But this behavior will mess with blocking tags such as p, div et...\nIn the future :\n\nSentence detection will occur only inside HTML sections that contains not a single blocking tag\nAn opening blocking tag such as <p> will be interpreted as linebreak.\n\nBe warned that there are no setting files. If you need a different behavior (set of features, tweaks on the HTML output), you have to edit the source code itself.\nUsage\nInstallation\nUse python 3.7 or later. Check it with :\n$ python3 --version\n\nExplanation : f-strings are used and there are places where dictionaries are expected to be ordered (which is almost the case for 3.6, but without strong warranties).\nInstall the dependencies with your favorite virtualenv manager.\nUnless you have better options, I advocate for using using venv as follows:\n$ python3 -m venv .venv --without-pip\n$ source .venv/bin/activate # or .\\venv\\Scripts\\activate on windows\n$ wget https://bootstrap.pypa.io/get-pip.py  # or manually download it on windows\n$ python3 get-pip.py\n$ python3 -m pip install -r requirements.txt\n\nAlternatively, you can use pipenv (taking as granted that it\'s already installed)\n$ pipenv --python=3.7 install\n$ pipenv shell\n\nTrain the Model\nIt\'s required to train the model before using it.\nFirst download the resources (thanks to Read & Al. 2012 for exposing clean datasets in the open, ref at the end).\nGiven your dependencies installed and your virtualenv activated,\n$ python cli.py download-corpus\n\nTrain the logistic regression on brown or wsj corpus.\n$ python cli.py train brown --modelname myfirstmodel\n\nUse the Model\nUse your model to segment a text file.\nOutput of txt files will be represented as one sentence per line.\nOutput for html files will contains amended html with <span class=""sentence""> markups\n$ python cli.py segment ./examples/boll.txt --modelname myfirstmodel > boll_output.txt\n$ python cli.py segment ./examples/boll.html --modelname myfirstmodel --ishtml > boll_output.html\n\n(Or without stdout redirection (the ""> file"" part) if you prefer to see the result in your shell)\nFor developpers\nRun the tests\nFew tests have been put in place.\nRun it with\n$ pytest\n\nTo avoid long running tests (which are running on the whole wsj/brown corpus), you can use the dedicated marker to filter these tests out\n$ pytest -m ""not long""\n\nStyle\nblack and flake8 has been used (following this tutorial)\nFor Data Science People\nApproach\nI tried my luck with a logistic regression on common patterns that can be observed both at the start and the end of a sentence and also other features.\nI picked the logistic regression for convenience (I know this kind of model). I chose to not dig too much into the state-of-the-art approaches for the sake of the ""challenge"" to come to a solution myself.\nPatterns of sentences termination can be found in the features.py file.\nThey have been also handpicked and hardcoded after an explorative phase. I did not take time to assess their importance. But I started with only these features, which granted around 90% + success rate.\nI also added handpicked hardcoded abbreviations (advice from the wikipedia page) and also added a numerical feature (number of words).\nBoth improved the model on both corpus.\nResults\nOn termination candidates (which are linebreaks, punctuation and special symbols (?!;:.), and also ."".\nThe Logistic regression touts a 98%+ classification success rate on both brown and wsj dataset.\nBut the model is not refined at all and work remains. Specifically:\n\nOver-learning may be a reality, the common cross-validation step (intra or inter-corpus) has not been done.\nCoefficients in the logistic regression have not been inspected at all. Some features could be excluded.\nSome rare (yet with a frequency that has to be determined) termination points were not considered by our model as valid candidates, hence they were not counted as missed as they should (this is a common pitfall, as pointed by the article cited at the end of the readme).\n\nNotes\nMessy notes\nYou can find some mess reflecting some of my ideas (in order to keep track of the thoughts during the exercice) in the exploration subfolder (a journal of ideas and an exploratory jupyter notebook).\nFinal notes\nThis repository has been set up for a recruitement purpose. I don\'t think I\'ll invest time to properly maintain it. If you fork it and do nice things, shout me an email so that I can publicize your fork on this Readme.\nTODOS And Ideas\n\nTreat gracefully erroneous input (like unfindable files)\nEncapsulate dictionnary iterators into classes.\nInspect and improve the model, use crossvalidation, inspect awesome fails (i.e couples of sentences where the model gives high confidence and yet fails miserably)\nNormalisation step on user inputs of unicode characters (cf article at the end of the readme)\nhtml special cars must be taken care of (presence of symbols such as ;)\nPlot the ROC curve !\nTest more thoroughly html and text input, possibly with a generation mechanism.\nRefactor/Comment hard-to-reason-about functions\nSeparate dev requirements (black, pytest, jupyter) from user requirements\nDokerize the application.\nAs the ""number of words"" is an important feature, it may be useful to classify with a multi pass approach to conservatively accept candidates as sentences on a first pass, then progressively accept more and more sentences.\nConsider that False positive may be more damaging than False negative and adapt the classifier. Maybe setting the coefficient for the feature ""number of words"" to a fixed level, a bit more impactful than its trained value.\nUse beautiful soup instead of naive markup detection using regexes.\nConsider false posivite patterns such as the ellipis (...)\nTrain on the more datasets (such as the other datasets exposed by Read. & al)\n\nCredits\nDatasets were taken from http://svn.delph-in.net/odc/ put in place by these authors :\nJonathon Read, Rebecca Dridan, Stephan Oepen, and Lars JØorgen Solberg.\n2012a. Sentence boundary detection: A long solved problem? In Proceedings of COLING 2012: Posters, pages 985–994, Mumbai, India.\n\n'], 'url_profile': 'https://github.com/brumar', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Feb 14, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 12, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}"
"{'location': 'Victoria, BC', 'stats_list': [], 'contributions': '1,869 contributions\n        in the last year', 'description': ['linear-regression\nLearning linear regression algorithm to predict student grade\n'], 'url_profile': 'https://github.com/sumesh-aot', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aW3st', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Regression Test Case Selection based on Machine Learning\nThis note book consist of machine learning approach to select regression test cases\n'], 'url_profile': 'https://github.com/KushBhatnagar', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'Punjab, India', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Car_Purchase_Amount_Prediction_Using_ANNs\nANN prediction regression model for predicting Car purchase amounts\n'], 'url_profile': 'https://github.com/SagarPalyal', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ariannaesmith', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harrisonmckenny', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'Melbourne, Victoria', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Churn-Analysis\nLogistic Regression on Telecom data to predict the churn likelihood.\n'], 'url_profile': 'https://github.com/dipayanbanerjee', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Empirical-Study-of-Sparse-Penalized-Regression-\nImplemented pADMM and scdADMM algorithms in R for solving sparse penalized regression problem.\nPerformed large size simulation in comparing the performance of high-dimensional quantile regression models including Picasso, GLMNET and ILAMM.\n'], 'url_profile': 'https://github.com/zhangbo1997', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'UK, USA, TH', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['961733-EnergyDataAnalytics\nPA0 - A Toy Dataset with A Simple Hold-out Sampling\n'], 'url_profile': 'https://github.com/preenet', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'Beijing', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': [""Semantic Graph Convolutional Networks for 3D Human Pose Regression (CVPR 2019)\nThis repository holds the Pytorch implementation of Semantic Graph Convolutional Networks for 3D Human Pose Regression by Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia and Dimitris N. Metaxas. If you find our code useful in your research, please consider citing:\n@inproceedings{zhaoCVPR19semantic,\n  author    = {Zhao, Long and Peng, Xi and Tian, Yu and Kapadia, Mubbasir and Metaxas, Dimitris N.},\n  title     = {Semantic Graph Convolutional Networks for 3D Human Pose Regression},\n  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  pages     = {3425--3435},\n  year      = {2019}\n}\n\n\nIntroduction\nWe propose Semantic Graph Convolutional Networks (SemGCN), a novel graph convolutional network architecture that operates on regression tasks with graph-structured data. The code of training and evaluating our approach for 3D human pose estimation on the Human3.6M Dataset is provided in this repository.\nIn this repository, 3D human poses are predicted according to Configuration #1 in our paper: we only leverage 2D joints of the human pose as inputs. We utilize the method described in Pavllo et al. [2] to normalize 2D and 3D poses in the dataset, which is different from the original implementation in our paper. To be specific, 2D poses are scaled according to the image resolution and normalized to [-1, 1]; 3D poses are aligned with respect to the root joint . Please refer to the corresponding part in Pavllo et al. [2] for more details. We predict 16 joints (as the skeleton in Martinez et al. [1] without the 'Neck/Nose' joint). We also provide the results of Martinez et al. [1] in the same setting for comparison.\nResults on Human3.6M\nUnder Protocol 1 (mean per-joint position error) and Protocol 2 (mean per-joint position error after rigid alignment).\n\n\n\nMethod\n2D Detections\n# of Epochs\n# of Parameters\nMPJPE (P1)\nP-MPJPE (P2)\n\n\n\n\nMartinez et al. [1]\nGround truth\n200\n4.29M\n44.40 mm\n35.25 mm\n\n\nSemGCN\nGround truth\n50\n0.27M\n42.14 mm\n33.53 mm\n\n\nSemGCN (w/ Non-local)\nGround truth\n30\n0.43M\n40.78 mm\n31.46 mm\n\n\nMartinez et al. [1]\nSH (fine-tuned)\n200\n4.29M\n63.48 mm\n48.15 mm\n\n\nSemGCN (w/ Non-local)\nSH (fine-tuned)\n100\n0.43M\n61.24 mm\n47.71 mm\n\n\n\nResults using two different 2D detections (Ground truth and Stacked Hourglass detections fine-tuned on Human3.6M) are reported.\nReferences\n[1] Martinez et al. A simple yet effective baseline for 3d human pose estimation. ICCV 2017.\n[2] Pavllo et al. 3D human pose estimation in video with temporal convolutions and semi-supervised training. CVPR 2019.\nQuick start\nThis repository is build upon Python v2.7 and Pytorch v1.1.0 on Ubuntu 16.04. NVIDIA GPUs are needed to train and test. See requirements.txt for other dependencies. We recommend installing Python v2.7 from Anaconda, and installing Pytorch (>= 1.1.0) following guide on the official instructions according to your specific CUDA version. Then you can install dependencies with the following commands.\ngit clone git@github.com:garyzhao/SemGCN.git\ncd SemGCN\npip install -r requirements.txt\n\nDataset setup\nYou can find the instructions for setting up the Human3.6M and results of 2D detections in data/README.md. The code for data preparation is borrowed from VideoPose3D.\nEvaluating our pretrained models\nThe pretrained models can be downloaded from Google Drive. Put checkpoint in the project root directory.\nTo evaluate Martinez et al. [1], run:\npython main_linear.py --evaluate checkpoint/pretrained/ckpt_linear.pth.tar\npython main_linear.py --evaluate checkpoint/pretrained/ckpt_linear_sh.pth.tar --keypoints sh_ft_h36m\n\nTo evaluate SemGCN without non-local blocks, run:\npython main_gcn.py --evaluate checkpoint/pretrained/ckpt_semgcn.pth.tar\n\nTo evaluate SemGCN with non-local blocks, run:\npython main_gcn.py --non_local --evaluate checkpoint/pretrained/ckpt_semgcn_nonlocal.pth.tar\npython main_gcn.py --non_local --evaluate checkpoint/pretrained/ckpt_semgcn_nonlocal_sh.pth.tar --keypoints sh_ft_h36m\n\nNote that the error is calculated in an action-wise manner.\nTraining from scratch\nIf you want to reproduce the results of our pretrained models, run the following commands.\nFor Martinez et al. [1]:\npython main_linear.py\n\nFor SemGCN without non-local blocks:\npython main_gcn.py --epochs 50\n\nBy default the application runs in training mode. This will train a new model for 50 epochs without non-local blocks, using ground truth 2D detections. You may change the value of num_layers (4 by default) and hid_dim (128 by default) if you want to try different network settings. Please refer to main_gcn.py for more details.\nFor SemGCN with non-local blocks:\npython main_gcn.py --non_local --epochs 30\n\nThis will train a new model with non-local blocks for 30 epochs, using ground truth 2D detections.\nFor training and evaluating models using 2D detections generated by Stacked Hourglass, add --keypoints sh_ft_h36m to the commands:\npython main_gcn.py --non_local --epochs 100 --keypoints sh_ft_h36m\npython main_gcn.py --non_local --evaluate ${CHECKPOINT_PATH} --keypoints sh_ft_h36m\n\nVisualization\nYou can generate visualizations of the model predictions by running:\npython viz.py --architecture gcn --non_local --evaluate checkpoint/pretrained/ckpt_semgcn_nonlocal.pth.tar --viz_subject S11 --viz_action Walking --viz_camera 0 --viz_output output.gif --viz_size 3 --viz_downsample 2 --viz_limit 60\n\nThe script can also export MP4 videos, and supports a variety of parameters (e.g. downsampling/FPS, size, bitrate). See viz.py for more details.\nAcknowledgement\nPart of our code is borrowed from the following repositories.\n\n3d-pose-baseline\n3d_pose_baseline_pytorch\nVideoPose3D\n\nWe thank to the authors for releasing their codes. Please also consider citing their works.\n""], 'url_profile': 'https://github.com/rujiewu', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', '1', 'R', 'Updated Dec 2, 2020', 'R', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Python', 'Updated Jan 8, 2020']}"
"{'location': 'Nairobi Kenya', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Machine-Leaning-Naive-Bayes-and-Logistic-Regression\nCreating a logistic regression and Naive Bayes Model\n'], 'url_profile': 'https://github.com/owuor-stack', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'C#', 'MIT license', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 8, 2020', 'R', 'MIT license', 'Updated Dec 9, 2020', 'Python', 'Updated Jan 9, 2020']}","{'location': 'Hove, UK', 'stats_list': [], 'contributions': '766 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/idg10', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'C#', 'MIT license', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 8, 2020', 'R', 'MIT license', 'Updated Dec 9, 2020', 'Python', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': [""NNregressionNFL2019\nSimple NN regression example using TensorFlow 2.0 and Sequential()\nThis is an example ANN regression using TensorFlow and the Sequential() model.\nThe import stuff loads mystic, but that package is not used (yet).\nThere are a few steps to running the net:\n\n\nclone the repository, set path to repo location:\npath='C:/Users/Computer/Documents/NFL' ... actually path='your path to repo here'\n\n\nLines 89-82 set\ntrain_size = 0.67  # what fraction of the data for training\nepochs = 100  # iterations of estimate\nrandomSeed= int(7)\n\n\nThe data set, NFL2019.csv is a collection of game stats for the 32 teams of the NFL.  The idea is to take a subset of 'train_size' to train the network, and evaluate on the remaing data.  Change the train_size (between 0 and 1) to train/test different proportions.\nEpochs is the number of steps in the forward/backward estimation process. Too few and you underfit the model; too many and you overfit.  Try it!\nrandomSeed is set to 7 to basicallyt replicate results using diffeent 'tuning' values\nThe output is the modeled wins/losses for each team, along with the actual wins/losses.  Use different random seeds to see how poor this model is-- not enough data.  Also this is not a prediction model.  Train data were used to fit the model; the test data were used to evaluate the fit.   If one really wanted to see how well the model works, train/test on 2018 data and then predict results for 2019 season.   I don't recommend that one uses this model for anything else but learning to use Sequential() from TensorFlow.\n""], 'url_profile': 'https://github.com/jamesbrownlow', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'C#', 'MIT license', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 8, 2020', 'R', 'MIT license', 'Updated Dec 9, 2020', 'Python', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Kaggle-House-Price-Regression\nA Kaggle competition ""House Prices: Advanced Regression Techniques"".\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview\nHIT(SZ) 2019 ML class project.\nfinal score : 0.11475\nblog about it : https://blog.csdn.net/dc199706/article/details/103460040 (in Chinese)\n'], 'url_profile': 'https://github.com/DDDCai', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'C#', 'MIT license', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 8, 2020', 'R', 'MIT license', 'Updated Dec 9, 2020', 'Python', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Logistic-Regression\nClassified type of cancer using logistic regression with gradient descent\n'], 'url_profile': 'https://github.com/HaritaRamesh', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'C#', 'MIT license', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 8, 2020', 'R', 'MIT license', 'Updated Dec 9, 2020', 'Python', 'Updated Jan 9, 2020']}","{'location': 'Stockholm, Sweden', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Lasso Regression\nRegularized regression of a forest fire data set supplied by Cortez and Morais (2007)\nAuthors: Patrik Mirzai and Huixin Zhong\nThis project aims at predicting the burned area of wildfires using Lasso regression. Moreover, a comparison to multiple regression and regression trees is also carried out. A summary of the Lasso procedure is given below. See the attached source code ""Lasso implementation of wildfires data set.R"" for full details on the project.\nCode used for the Lasso implementation\n#Upload packages\nlibrary(readxl)  #For latex output (optional)\nlibrary(glmnet)  #For Lasso regression\n\ndf = read.table(\'forestfires.csv\', sep="","", header = T)\ndf$area = log(df$area+1)  #Transform the variables\n\nset.seed(2)\n\nLet\'s divide the data into a train- and test set\n#Index for our train data\ntrain_index = sample(1:nrow(df), size = nrow(df)*0.7, replace = F)\n\n#Select train and test set\ntrain_data = df[train_index,]\ntest_data = df[-train_index,]\n\n#Model matrix for train data\nx = model.matrix(area~., train_data)[,-1]\ny = train_data$area\nLet\'s plot the coefficients against the L1 norm\n#Plotting the coefficients against different values of lambda\nfit = glmnet(x, y)\nplot(fit)\n\nNow let\'s choose the tuning parameter lambda through cross-validation\n#Create a sequence of our tuninig parameter used in the cross validation\nlambda_seq = 10^seq(2, -2, by = -.1)\n\n#Train model with different tuning parameters\nset.seed(2)\ncv_output = cv.glmnet(x, y, alpha = 1, lambda = lambda_seq, type.measure=""mse"")\n\n#Cross validation plot\nplot(cv_output)\nbest_lam = cv_output$lambda.min\n\n#Fit lasso model again with the best lambda\nbest_lasso = glmnet(x, y, alpha = 1, lambda = best_lam)\n\ncoef(best_lasso) #Get coefficients\nThe plot displays the mean squared error using 10-fold cross validation\n\nFinally, let\'s compute the mean squared error of the test data\n#Predicting\nx_test = model.matrix(area~., test_data)[,-1]\npred = predict(best_lasso, x_test)\n\nactual_test = test_data$area\nmse = mean((actual_test - pred)^2)  #mse is 2.049039\nReferences\nCortez, P. and Morais, A. (2007), ‘A Data Mining Approach to Predict Forest Fires using\nMeteorological Data’, New Trends in Artificial Intelligence, Proceedings of the 13th EPIA\n2007 - Portuguese Conference on Artificial Intelligence pp. 512–523.\nFriedman, J., Hastie, T., Höfling, H. and Tibshirani, R. (2007), ‘Pathwise Coordinate Optimization’, The Annals of Applied Statistics 1(2), 302–332.\nFriedman, J., Hastie, T. and Tibshirani, R. (2009), The Elements of Statistical Learning,\nsecond edn, New York: Springer Verlag\nFriedman, J., Hastie, T. and Tibshirani, R. (2010), ‘Regularized Paths for Generalized Linear Models Via Coordinate Descent’, Journal of Statistical Software 33, 1–22.\nHastie, T., Tibshirani, R. and Wainwright, M. (2015), Statistical Learning with Sparsity:\nThe Lasso and Generalizations, first edn, Chapman & Hall/CRC.\nTibshirani, R. (1996), ‘Regression Shrinkage and Selection via the Lasso’, Journal of the\nRoyal Statistical Society. Series B (Methodological) 58(1), 267–288.\n'], 'url_profile': 'https://github.com/mirzaipatrik', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'C#', 'MIT license', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 8, 2020', 'R', 'MIT license', 'Updated Dec 9, 2020', 'Python', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': [""MultipleLinearReg-HousePrices\nKaggle dataset on housing data for Ames Iowa\nPurpose\nCreate a multiple linear regression model for home price data for a rural town Iowa.\nGoal\nCreating a linear model will enable future price prediction on houses, given the same features. Being able to predict the house sale price and the current asking price, I'll be able to see whether the house is overprice, or underpriced, compared to the prediction.\nProcess\n\nData cleaning; replacing/removing NaNs, and columns that have too many NaNs to be considered viable features.\nSeperating catagorical and continuous features\nFeature Scaling numerical features to standardize the features and eliminate outliers\nTest for intercorrelation amongst features using created function\nTry using Linear Regression inspite of large amount of features\n\nOverfitting, and getting poor results for R^2 on the testing data\n\n\nNow going to use Lasso regression that'll zero out unneeded terms\n\nThe test set scored an R^2 of approximately .7 and the train scored about .87\n\nLasso regression scored an R^2 around .902 with an adjusted R^2 around .75, which is permissable\nOptimizing alpha based on adjusted alpha will achive a more generalized model and remove unneeded terms\nBest alpha ~ 1.34 (But this bound to change with adjustments to code)\n\n\n\nNew progress\nTheory: The amount that people are willing to spend is going to be affected by the interest rate that they obtain to borrow money for the purchase. Thus, the additional mortgage rate data, will have a significant effect on predicting sale price.\nBEFORE adding avg monthly mortgage rates:\n\nrmse: 24016.52425600875\nAdjusted R2: 0.765931653457979\nR2: 0.9019800668694578\n\nAFTER:\n\nrmse: 22242.281922013546\nAdjusted R2: 0.7594046798964573\nR2: 0.8998751696847417\n\nBy introducing an additional piece of data to the model I was able to improve the RSME by ~$1,800, without jepordizing the integrity of the model\nTheory: The concept of supply and demand. If there are more houses/living situations availible, the price will decrease because of an increase supply and a stationary demand.\nBEFORE adding newly build housing data:\n\nrmse: 22242.281922013546\nAdjusted R2: 0.7594046798964573\nR2: 0.8998751696847417\n\nAFTER:\n\nrmse: 23717.64270306713\nAdjusted R2: 0.7319021333025424\nR2: 0.8880982817262786\n\nThe additional housing permit data did not improve the accuracy of the model, and the error in evaluation increased.\n""], 'url_profile': 'https://github.com/wzog99', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'C#', 'MIT license', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 8, 2020', 'R', 'MIT license', 'Updated Dec 9, 2020', 'Python', 'Updated Jan 9, 2020']}","{'location': 'New York, New York ', 'stats_list': [], 'contributions': '599 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/melanieshi0120', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'C#', 'MIT license', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 8, 2020', 'R', 'MIT license', 'Updated Dec 9, 2020', 'Python', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '735 contributions\n        in the last year', 'description': ['   \nR Tools for Panel Data and Optimization\nThis is a work-in-progress website consisting of files for doing Panel Data Statistics/Econometrics Analysis. Materials gathered from various projects in which R code is used. Files are from Fan\'s REconTools repository.\nFrom Fan\'s other repositories: For dynamic borrowing and savings problems, see MEconTools and Dynamic Asset Repository; For code examples, see also  Python Example Code (bookdown site), R Example Code (bookdown site), Matlab Example Code and Stata Example Code; For intro econ with Matlab, see Intro Mathematics for Economists, and for intro stat with R, see Intro Statistics for Undergraduates (bookdown site). See here for all of Fan\'s public repositories.\nPlease contact FanWangEcon for issues or problems.\n   \nInstallation\nThis package contains tools used by various projects that use R. To run code from various projects, need to install this package first.\n# To Install the Programs in the R folder of the REconTools Repository\ndevtools::install_github(""fanwangecon/REconTools"")\n\nTable of Content for Selected Functions\nClick on the Functions tab to see all functions. Click on the Guide Articles tab to see vignettes for a subset of functions. Vignettes for some functions are provided in the R Example Code (R4Econ) Example repository and linked to functions under function references.\n1. Arithmetics\n1.1 Tabulate and Counting\n\nBy Groups, Count Unique Individuals: r | ref | vignette\n\nBy Groups, Count Unique Individuals and non-NA observations of other Variables.\ntidy: group_by + mutate_if + mutate + n_distinct + slice(1L)\n\n\n\n1.2 Averaging\n\nAll Variables Summary Stats: r | ref | vignette\n\nAll Variables: N + NAcount + Mean + SD + Percentiles.\ntidy: summarise_if(is.numeric) + gather + separate + spread  + select\n\n\nBy Groups One Variable All Statistics: r | ref | vignette\n\nOne Variable: mean + median + sd + IQR + mad + min + max + first + last + n.distinct\nThe above statistics categorized by variable factors jointly\ntidy: map(ar_fl, !partial(quantile, probs = .x)) + set_names(nm = ar_st) + group_by + summarize_at(, funs()) + rename(!!var := !!sym(var)) + mutate(!!var := paste0(var,’str’,!!!syms(vars))) + gather + unite + spread(varcates, value)\n\n\n\n2. Panel\n2.1 Wider or Longer\n\nLong Panel Duplicate One Variable to Wide: r | ref | vignette\n\nlong panel var X, average X by within i t subgroups, expand avgX_{i,tgroup} to wide, merge to long panel\ntidy: group_by + summarise + spread + left_join\n\n\nLong Roster to Wide Day and ID Roster with Cumsum: r | ref | vignette\n\nlong panel record date of attendance for each id, expand to wide panel of cumulative attendance by dates\ntidy: pivot_wider + rename_at + mutate_at + list(~replace_na) + list(~cumsum)\n\n\n\n2.2 Statistcs by Dates\n\nCumulative Stat Last Observation Across Individuals: r | ref | vignette\n\nSumming latest at current date SAT score from N individuals where dataframe contains all scores, dataframe by date, moving statistics with date.\ntidy: for (ctr) + data[1:ctr,] + groupby + slice(n()) + summarize\n\n\n\n3. Distributions\n\nDiscrete Random Normal Variable: r | ref | vignette\n\nDiscretized normal random variable with Trapezoidal rule.\n\n\nCompute Gini for a non-negative Vector: r | ref | vignette\n\nSingle line gini inequality formula.\n\n\n\n4. Optimization\n\nConcurrent Bisection Different Parameters: r | ref | vignette\n\nA strictly monotonic linear or nonlinear function with one root\nA dataframe where each column is a different parameter for the function, and each row parameter values\nFind roots for this function concurrently over all rows at the row specific parameters\ntidy: case_when + pmap\n\n\n\n\nPlease contact   for issues or problems.\n\n\n\n\n\n'], 'url_profile': 'https://github.com/FanWangEcon', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'C#', 'MIT license', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 8, 2020', 'R', 'MIT license', 'Updated Dec 9, 2020', 'Python', 'Updated Jan 9, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/jadhavakash', 'info_list': ['Jupyter Notebook', 'Updated Jan 11, 2020', 'C#', 'MIT license', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 8, 2020', 'R', 'MIT license', 'Updated Dec 9, 2020', 'Python', 'Updated Jan 9, 2020']}"
"{'location': 'Tempe, AZ, United States', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression-Analysis\nWorked on ‘red vinho verde wine dataset’ to develop Multiple regression model using the backward elimination process.\nSoftware Used - JMP SAS\n'], 'url_profile': 'https://github.com/vishalk1995', 'info_list': ['Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nwornson', 'info_list': ['Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['PolynomialRegression\nComparison between how well fitted the points get in a nonlinear relationship, using a linear regression and a polynomial regression.\nTo do this it was used the relation between fuel consumption and horsepower.\nThe dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. It was used in the 1983 American Statistical Association Exposition. (c) Date: July 7, 1993\nDataset\n""The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes."" (Quinlan, 1993)\n\nNumber of Instances: 398\nNumber of Attributes: 9 including the class attribute\n\n\nAttribute Information:\n\n\nmpg: continuous\ncylinders: multi-valued discrete\ndisplacement: continuous\nhorsepower: continuous\nweight: continuous\nacceleration: continuous\nmodel year: multi-valued discrete\norigin: multi-valued discrete\ncar name: string (unique for each instance)\n\nAnalysis\nDetails and graphs are shown in the ipython file\nAcknowledgements\nDataset: UCI Machine Learning Repository Data link : https://archive.ics.uci.edu/ml/datasets/auto+mpg\n'], 'url_profile': 'https://github.com/stdevelopr', 'info_list': ['Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '210 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/xaviave', 'info_list': ['Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HaoyuHou', 'info_list': ['Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['LogisticRegression\n'], 'url_profile': 'https://github.com/prashanth-repositories', 'info_list': ['Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harshv19', 'info_list': ['Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'Warangal,Telangana', 'stats_list': [], 'contributions': '300 contributions\n        in the last year', 'description': [""Advanced-Regression\nPredicting house prices using Regression techniques but this time with around 80 properties of  houses !\nThis notebook is my submission to  Kaggle's  House Prices Prediction with Advanced Regression techniques .\nModel used : XGBoost, RandomForestRegressor\nXGBoost RMSLE score on training data = 0.013483440455372187\n\nRandomForestRegressor best score = 0.848\n\n""], 'url_profile': 'https://github.com/rainmaker29', 'info_list': ['Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'mirpur dhaka ', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['-linear-regression\n'], 'url_profile': 'https://github.com/shakhawat-khan', 'info_list': ['Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['logistic regression in python\nimplemented logistic regression from scratch on breast cancer dataset.\n'], 'url_profile': 'https://github.com/rheacarmel', 'info_list': ['Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 31, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/oliveradk', 'info_list': ['Python', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bibek2018', 'info_list': ['Python', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Bristol, UK', 'stats_list': [], 'contributions': '273 contributions\n        in the last year', 'description': ['Base R Implementation of Logistic Regression with Regularization\nThis is a very simple package containing base R code for Logistic Regression. It implements binary logistic regression where\nthe optimization is performed for two cost functions:\n\nNegative Log-Likelihood, corresponding to doing MLE.\nNegative Log-Likelihood plus a (ridge) regularization term. This is equivalent to do Maximum-A-Posteriori with an isotropic\nGaussian prior. Notice that in this specific implementation regularization is done also on the intercept.\n\nIn both cases, 2 implementation methods are implemented in base R:\n\nGradient Ascient (using a learning rate)\nNewton\'s Method (again using a learning rate)\n\nA third optimization method is possible via a call to the optim function in R. The method is customizable, although it is\n""BFGS"" by default.\n'], 'url_profile': 'https://github.com/MauroCE', 'info_list': ['Python', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Logistic Regression\nThis repository loads train and test samples for digit 7 and digit 8 from mnist dataset. It then performs classification on the data using Logistic Regression.\nOnce you clone the repository, download the dataset from https://drive.google.com/open?id=1L1SVmXSy_nnsdlxq7B2jRj9tPo387gke and put it inside the data folder.\nAn important step in logistic regression is calculating Log-Likelihood. Gradient ascent is used here for logistic regression. Gradient ascent is same as gradient descent with the only difference that a function is maximized instead of minimizing it.\nDuring training phase, weights are calculated. Later, weights calculated in training phase are used for prediction during testing phase. Used parameters for logistic regression:\n\nLearning rate = 0.001\nNumber of iterations: 400\n\n'], 'url_profile': 'https://github.com/kanchanchy', 'info_list': ['Python', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Richardson Texas', 'stats_list': [], 'contributions': '266 contributions\n        in the last year', 'description': ['Logistic-Regression\nPredicting customer Ad Clicks\n•\tUtilized Logistic Regression Algorithm to predict consumer activity on a company website achieving precision of 91%\n•\tPerformed Exploratory Data Analysis using Python visualization libraries to get insights on the data\nTitanic Dataset\n•\tUtilized Logistic Regression Algorithm to predict consumer activity on a company website achieving precision of 81%\n•\tPerformed Exploratory Data Analysis using Python visualization libraries to get insights on the data\n'], 'url_profile': 'https://github.com/SSR-ds', 'info_list': ['Python', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'South Africa', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Polynomial-Regression\n'], 'url_profile': 'https://github.com/BushBaBy1989', 'info_list': ['Python', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['hockey-regression\n'], 'url_profile': 'https://github.com/sydroth', 'info_list': ['Python', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/laibanasir', 'info_list': ['Python', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['Regression_Test\n'], 'url_profile': 'https://github.com/Shane-Bowen', 'info_list': ['Python', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Polynomial-Regression\nPolynomial regression is a form of regression analysis in which the relationship between the independent variable x and dependent variable y is modelled as an nth degree polynomial of x. That is, if your dataset holds the characteristic of being curved when plotted in the graph, then you should go with a polynomial regression model instead of Simple Linear or Multiple Linear regression.\n'], 'url_profile': 'https://github.com/Avinash237', 'info_list': ['Python', 'Updated Jan 8, 2020', 'Updated Jan 9, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 8, 2020', 'MIT license', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}"
"{'location': 'India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jthakur10', 'info_list': ['Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['deploy-mlm-flask-heroku\n'], 'url_profile': 'https://github.com/ikhwanudin', 'info_list': ['Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '266 contributions\n        in the last year', 'description': ['Crime_regression\nThe purpose of this report is to understand the key determinants of crime in order to generate effective, actionable policy recommendations for political candidates running for election in the state of North Carolina (modified data provided). To achieve this goal, we will examine a cross-sectional dataset of crime rate in various counties of North Carolina in the year 1987. We provide three carefully interpreted linear regression models to explore the predictive power of the independent variables in this dataset with respect to our dependent variable crime rate, focusing on how potential relationships may be utilized for better control and detection of crime. This work will focus on addressing the overarching research question: What are the major deterrents and motivators of crime, and how does the existence of factors from both categories influence overall crime rate?\nSpecifically, we will look to three subquestions to help narrow down our focus:\n\nHow does fear of arrest and convictions deter crime across North Carolina?\nHow does wage for all types of employees influence overall crime rate?\nWhat variables are strong predictors of crime, and at the same time are robust across the entire state, irrespective of location and population density?\n\nWe will provide rational for variable and model selection using both background information and automated methods. The development, evaluation, and interpretation of our models were divided in three stages, which can be found in sections Model 1, Model 2 and Model 3. Finally, the policy recommendations derived from this process are presented in the Policy Recommendations and Concluding Remarks section.\n'], 'url_profile': 'https://github.com/siduojiang', 'info_list': ['Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Polynomial-Regression\nPolynomial regression is a form of regression analysis in which the relationship between the independent variable x and dependent variable y is modelled as an nth degree polynomial of x. That is, if your dataset holds the characteristic of being curved when plotted in the graph, then you should go with a polynomial regression model instead of Simple Linear or Multiple Linear regression.\n'], 'url_profile': 'https://github.com/Avinash237', 'info_list': ['Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Aizeren', 'info_list': ['Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Logistic_Regression\n'], 'url_profile': 'https://github.com/sunil-kumar-g', 'info_list': ['Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shravan399', 'info_list': ['Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dungr9', 'info_list': ['Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nileshjoshic', 'info_list': ['Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Israel', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/drorrosen', 'info_list': ['Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', '1', 'Jupyter Notebook', 'Updated Jan 7, 2020']}"
"{'location': 'New Delhi', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Linear_Regression\n'], 'url_profile': 'https://github.com/sunil-kumar-g', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 9, 2020']}","{'location': 'Russia', 'stats_list': [], 'contributions': '431 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tanya525625', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 9, 2020']}","{'location': 'Gothenburg', 'stats_list': [], 'contributions': '355 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/vnadhan', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear_Regression\n'], 'url_profile': 'https://github.com/Akash-coder1', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['upload Correlation and Regression files\n\nupdate information\n\nupdate information\n\n\n'], 'url_profile': 'https://github.com/JimmyCWU', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 9, 2020']}","{'location': 'Nanded,Maharashtra', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['Logistic_regression\nLogistic regression is a classification algorithm used to predict binary value\n'], 'url_profile': 'https://github.com/swapniladnak2510', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 9, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Using Regression Analysis to Determine Marketing ROI\nA clothing retailer is considering an ad campaign for a Times Square takeover campaign. This is a big investment, but it's a competitive business, and the winners have to make big bets to build consumer perception that drives demand. It's our job to make sure those are safe bets as well.\nOur client in this branded lifestyle space has made similar investments in the past and we have some really good data on hand to analyze, data that will help us to see what sort of response we have achieved using different channels, like broadcast media, out-of-home billboard, and more. Our job is to leverage that data to clarify the impact on those investments.\n""], 'url_profile': 'https://github.com/NituSidhu', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shwaldemar', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 9, 2020']}","{'location': 'Gothenburg', 'stats_list': [], 'contributions': '355 contributions\n        in the last year', 'description': ['Multivariate-Regression\n'], 'url_profile': 'https://github.com/vnadhan', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 9, 2020']}","{'location': 'Buffalo Grove, Illinois', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mkbansal0588', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Feb 26, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Updated Jan 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Regression-Project\n'], 'url_profile': 'https://github.com/ankitapatidar', 'info_list': ['Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'R', 'Updated Jan 11, 2020']}","{'location': 'Saudi Arabia', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': [""Logistic-Regression\nA Logistic Regression to predict the survival status using Haberman's Survival Dataset\nFunctions\n\nComputing the cost and gradient\nLearn Theta value parameters\n\n""], 'url_profile': 'https://github.com/abeeraleisa', 'info_list': ['Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'R', 'Updated Jan 11, 2020']}","{'location': 'Khlong Luang, Pathum Thani, Thailand', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/developeravsk', 'info_list': ['Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'R', 'Updated Jan 11, 2020']}","{'location': 'Nagpur, Maharashtra, India', 'stats_list': [], 'contributions': '620 contributions\n        in the last year', 'description': ['Logistic-regression\nBank Marketing Data\n'], 'url_profile': 'https://github.com/vishalnarnaware', 'info_list': ['Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'R', 'Updated Jan 11, 2020']}","{'location': 'Khlong Luang, Pathum Thani, Thailand', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/developeravsk', 'info_list': ['Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'R', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rajatjain97', 'info_list': ['Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'R', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['로지스틱 회귀분석\n개요\n지리정보와 기상정보를 이용하여 침수 여부 예측\n침수 여부라는 0과 1의 클래스를 활용하기 위해 logistic regression기법 활용\nscikit-learn의 패키지를 활용하는 것이 아닌 코딩을 통해 기법 구축\nlogistic function\n로지스틱 회귀는 결과값에 특정한 함수 로지스틱을 적용한다.\n\nlogistic function을 결과값에 적용하여 [0,1]의 값을 갖게 하여, 해당 데이터가 feature에 대한 y = True가 나올 conditional probability로 표현된다.\nlogsitic function은 딥러닝의 sigmoid함수라고도 불린다.\nlikelihood\n로지스틱은 least square방식이 아니라 maximum liklihood로써 적합을 한다.\n그러므로 로지스틱은 회귀분석과 잘리 정규성가정, 등분산성가정이 필요없다.\n하지만, error들이 서로 독립적이어야하고 다중공선성은 없어야한다.\nlikleihood는 단순하게 해당 target을 맞출수 있을 가장 그럴듯한 계수이다.\n고정된 계수에서 특정 target에 대해 로지스틱 모형이 추정할 확률은 다음과 같다.\n\n이를 단순히 게수에 대해 표현하면, 계수에 대한 likelihood가 되어 해당 likelihood를 maximize하는 계수를 구할 수 있다.\n또한 언더피팅방지, 계산상의 이유로 log를 사용하여 log_likelihood라고 표현하기도 한다.\n\n머신러닝 기법의 기본가정과 같은 데이터가 서로 독립이라고 가정한다면, 위에서 구한 likelihood를 모든 target들에 대해서 cumproduct를 해서 전체 데이터에 대한 likelihood와, 그 전체 데이터의 likeihood를 maximize하는 계수를 구할 수 있을 것이다.\n즉, 모든 데이터 target에 대해 구한 최종적인 목적함수 log likelihood는 다음과 같다.\n\n해당 최종식을 gradient descent를 통해 최적화를 할 것이다. 그러기 위해선 각 계수에 대해 gradient를 구해야한다.\n\n즉, 특정 데이터 x에 대해 계수는 gradient는\n\n가 된다.\n참고\n\nhttp://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/\nhttp://www.columbia.edu/~so33/SusDev/Lecture_10.pdf\nhttps://onlinecourses.science.psu.edu/stat414/node/191/\nhttps://godongyoung.github.io/\nhttp://gnujoow.github.io/ml/2016/01/29/ML3-Logistic-Regression/\n\n'], 'url_profile': 'https://github.com/byeongyong-bae', 'info_list': ['Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'R', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['regression_metrics\n'], 'url_profile': 'https://github.com/TarunWuyyuru26', 'info_list': ['Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'R', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Linear-Regression\nApplication of Statistical OLS (Ordinary Least Squares) model and comparison with ML Linear Regression model in terms of R2 score\n'], 'url_profile': 'https://github.com/saurabhw608', 'info_list': ['Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'R', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/iammeghana7', 'info_list': ['Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'BSD-3-Clause license', 'Updated Oct 23, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Feb 11, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'R', 'Updated Jan 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['LinearRegression\n'], 'url_profile': 'https://github.com/SreeKaranam', 'info_list': ['R', 'Updated Jan 11, 2020', 'R', 'CC0-1.0 license', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'MIT license', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '249 contributions\n        in the last year', 'description': ['applied_regression\nThese are the Rbookdown files needed to build the class notes for MATH/STAT 4870 Applied Regression at Saint Louis University.\nThe book is hosted here.\nI welcome contributions, big or small! I will treat all contributors respectfully and expect the same from contributors.\n\nIf you want to fix a small typo, then please submit a pull request.\nIf you want to make a larger change, please open an issue first and discuss the change that you are thinking about.\n\n'], 'url_profile': 'https://github.com/speegled', 'info_list': ['R', 'Updated Jan 11, 2020', 'R', 'CC0-1.0 license', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'MIT license', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['BIA_Regression\nUsing advanced regression techniques to achieve lowest RMSE on Kaggle Housing prices dataset\n'], 'url_profile': 'https://github.com/seanwdc', 'info_list': ['R', 'Updated Jan 11, 2020', 'R', 'CC0-1.0 license', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'MIT license', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '515 contributions\n        in the last year', 'description': ['regression_examples\nnotebook shows linear and logistic regression\n'], 'url_profile': 'https://github.com/carsen-stringer', 'info_list': ['R', 'Updated Jan 11, 2020', 'R', 'CC0-1.0 license', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'MIT license', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BSayon', 'info_list': ['R', 'Updated Jan 11, 2020', 'R', 'CC0-1.0 license', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'MIT license', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'BSD, Greater Jakarta, Indonesia', 'stats_list': [], 'contributions': '148 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mnrclab', 'info_list': ['R', 'Updated Jan 11, 2020', 'R', 'CC0-1.0 license', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'MIT license', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'Toronto Canada', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ammarmuqeet', 'info_list': ['R', 'Updated Jan 11, 2020', 'R', 'CC0-1.0 license', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'MIT license', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'Taiwan', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression_student_Grades\nOverview:\nStudent grades are predicted via a simple regression model.\n11 variables are chosen from the dataset as input variables: ""G1"", ""G2"", ""studytime"",""health"", ""famrel"", ""goout"",""Walc"",""freetime"", ""Dalc"",""failures"", ""absences""\nPredicted variable: G3 = Final grade\nDataset:\nhttps://archive.ics.uci.edu/ml/datasets/Student+Performance\n'], 'url_profile': 'https://github.com/paragparashar03', 'info_list': ['R', 'Updated Jan 11, 2020', 'R', 'CC0-1.0 license', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'MIT license', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'DELHI NCR', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Linear Regression From Scratch\nAn implementation of Linear Regression from scratch in python 📈. Includes the hypothesis function, partial differentiation of variables, parameter update rule and gradient descent.\n\nGetting Started\n\n\nDownload the project on your computer.\ngit clone https://github.com/sarvasvkulpati/LinearRegression\nor download the ZIP file\n\n\nGo to the directory with the file: cd LinearRegression\n\n\nDownload the required packages: pip install -r requirements.txt\n\n\nRun the project: python3 LinearRegression.py\n\n\nHow It Works\nWhat Is Linear Regression?\nLinear regression is a method for approximating a linear relationship between two variables. While that may sound\ncomplicated, all it really means is that it takes some input variable, like the age of a house, and finds out how\nit's related to another variable, for example, the price it sells at.\n\nWe use it when the data has a linear relationship, which means that when you plot the points on a graph, the\ndata lies approximately in the shape of a straight line.\nLinear Regression involves a couple of steps:\n\nRandomly initialising parameters for the hypothesis function\nComputing the mean squared error\nCalculating the partial derivatives\nUpdating the parameters based on the derivatives and the learning rate\nRepeating from 2 until the error is minimised\n\nThe Hypothesis Function\nThe linear equation is the standard form that represents a straight line on a graph, where m represents the gradient,\nand b represents the y-intercept.\n\nThe Hypothesis Function is the exact same function in the notation of Linear Regression.\n\nThe two variables we can change – m and b – are represented as parameters theta1 and theta0\nIn the beginning, we randomly initialize our parameters, which means we give theta1 and theta0 random values\nto begin with. This will output a random line, maybe something like this:\n\nThe Error Function\nClearly the line drawn in the graph above is wrong. But how wrong is it? That's what the error function is for - it\ncalculates the total error of your graph.\nWe'll be using an error function called the Mean Squared Error function, or MSE, represented by the letter J.\n\nNow while that may look complicated, what it's doing is actually quite simple. The function J takes our parameters\ntheta0, and theta1 as an input. Then, from every point from 1 to m, where m is the number of points, it calculates\nthe distance between the x value and the value predicted by our function and squares it. Squaring it ensures that\nthe error is always positive, and that the error is greater the further away the predicted value is from the actual\nvalue.\nNow that we have a value for how wrong our function is, we need to adjust the function to reduce this error.\nCalculating Derivatives\nIf we graph our parameters against the error (i.e graphing the cost function), we'll find that it forms something similar to the graph below. Our goal is to find the lowest point of that graph, where the error is at its lowest. This is called minimising the cost function.\nTo do this, we need to consider what happens at the bottom of the graph - the gradient is zero. So to minimize the cost function, we need to get the gradient to zero.\nThe gradient is given by the derivative of the function, and the partial derivatives of the functions are\n\n\nUpdating The Parameters Based On The Learning Rate\nNow we need to update our parameters to reduce the gradient. To do this, we use the gradient update rule\n\n\nAlpha is what we call the Learning rate, which is a small number that allows the parameters to be updated by a small amount. As mentioned above, we are trying to update the gradient such that it's closer to zero (the bottom). The Learning rate helps guide the network to the lowest point on the curve by small amounts.\nMinimising the Cost Function\nNow we repeat these steps - checking the error, calculating the derivatives, and updating the weights, until the error is as low as possible. This is called minimising the cost funtion.\nOnce your error is minimised, your line should now be the best possible fit to approximate the data!\n""], 'url_profile': 'https://github.com/Vishal1Mishra', 'info_list': ['R', 'Updated Jan 11, 2020', 'R', 'CC0-1.0 license', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'MIT license', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Multi-linear-refression\n'], 'url_profile': 'https://github.com/Avinash237', 'info_list': ['R', 'Updated Jan 11, 2020', 'R', 'CC0-1.0 license', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jul 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'MIT license', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020']}"
"{'location': 'Pune', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Decision-Tree-Regression\nThis Regression is based on the decision tree structure. A decision tree is a form of a tree or hierarchical structure that breaks down a dataset into smaller and smaller subsets. At the same time, an associated decision tree is incrementally developed. The tree contains decision nodes and leaf nodes. The decision nodes(e.g. Outlook) are those nodes represent the value of the input variable(x). It has two or more than two branches(e.g. Sunny, Overcast, Rainy). The leaf nodes(e.g. Hours Played) contains the decision or the output variable(y). The decision node that corresponds to the best predictor becomes the topmost node and called the root node.\n'], 'url_profile': 'https://github.com/Avinash237', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated May 15, 2020', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['Quantile Regression Example\nSee https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html\nor https://statweb.stanford.edu/~owen/courses/305a/lec18.pdf (note the point about ""quantile crossing"").\n\n'], 'url_profile': 'https://github.com/atorch', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated May 15, 2020', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Metalearning for robust regression, version 1.0\nThe code in R performs an optimal method selection (known as meta-learning) over a database of available datasets. Here, various linear regression estimators are compared including several robust ones. The comutationally intensive\nmethod predicts the most suitable method (estimator) for a new dataset.\nFeel free to use or modify the code.\nRequirements\nYou need to install these package of R software: MASS, robustbase, moments, lmtest, glmnet, class, e1071.\nUsage\n\nThe metalearning analysis should be performed in the following order: ReadingData.R, Primary.R, Features.R, Secondary.R. Alternative approaches\nto the methods of Secondary.R are included in files RandomForest.R and Regression.R. Alternative validation (i.e. autovalidation instead of cross\nvalidation) is implemented in Autovalidation.R.\n\nAuthors\n\nJan Kalina, The Czech Academy of Sciences, Institute of Computer Science\nPetra Vidnerová, The Czech Academy of Sciences, Institute of Computer Science\nBarbora Peštová, The Czech Academy of Sciences, Institute of Computer Science\n\nContact\nDo not hesitate to contact us (kalina@cs.cas.cz) or write an Issue.\nHow to cite\nPlease consider citing the following:\nKalina J, Vidnerová P (2020): A metalearning study for robust nonlinear regression. Proceedings of the 21st EANN (Engineering Applications of Neural Networks) 2020 Conference, Springer, Cham, pp. 499-510.\nAcknowledgement\nThis work was supported by the Czech Science Foundation grants GA19-05704S and GA18-23827S.\n'], 'url_profile': 'https://github.com/jankalinaUI', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated May 15, 2020', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Random-Forest-Regression\nThe basic idea behind Random Forest is that it combines multiple decision trees to determine the final output. That is it builds multiple decision trees and merge their predictions together to get a more accurate and stable prediction.\n'], 'url_profile': 'https://github.com/Avinash237', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated May 15, 2020', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Multiple_linear_regression\n'], 'url_profile': 'https://github.com/Akash-coder1', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated May 15, 2020', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'Kyiv', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['ft_linear_regression\n'], 'url_profile': 'https://github.com/osloboda', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated May 15, 2020', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Riya1608', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated May 15, 2020', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Regressionbug', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated May 15, 2020', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'Oslo, Norway', 'stats_list': [], 'contributions': '618 contributions\n        in the last year', 'description': ['Comparing Visualizations using a Classification and Regression Approach\nThis repository contains the code used to compare layer-wise classification visualizations using a classification and regression approach. The purpose of this experiment is to see if state-of-the-art explainability methods work as well for regression tasks as they do for regression. As an initial study, we use the MNIST dataset to train a classifier, for which we compare the two approaches.\n'], 'url_profile': 'https://github.com/Stevenah', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated May 15, 2020', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'Urbana-Champaign, IL', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': [""stock-compare-regression\nUsing the least squares regression between Apple and Microsoft stock prices since last year, I analyzed the behavior of these stocks and attempted to display traces of co-dependence. Using a linear regression methodology, I found that changes in Microsoft's price affect Apple's price more than the other way around.\n""], 'url_profile': 'https://github.com/kausarp2', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated May 15, 2020', 'R', 'MIT license', 'Updated Aug 17, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Feb 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020']}"
"{'location': 'Sidoarjo', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['simple-regression-calculator\nKalkulator yang membantu perhitungan analisa regresi sederhana.\nDibuat oleh Tri Mulyo Atmojo menggunakan javascript dom\nhttps://github.com/trimulyoatmojo/simple-regression-calculator\n'], 'url_profile': 'https://github.com/trimulyoatmojo', 'info_list': ['JavaScript', 'Updated Nov 29, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Taipei City, Taiwan', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['Boston_Housing_Price_Regression\nThe simple example code for boston housing price regression with keras.\n'], 'url_profile': 'https://github.com/kctoayo88', 'info_list': ['JavaScript', 'Updated Nov 29, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['ft_linear_regression\n'], 'url_profile': 'https://github.com/mbriffau', 'info_list': ['JavaScript', 'Updated Nov 29, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Calcutta, India', 'stats_list': [], 'contributions': '1,419 contributions\n        in the last year', 'description': [""Implementation of a Logistic regression neural network\nIn this project we've implemented a neural network of logistic regression using numpy only.\n""], 'url_profile': 'https://github.com/theroyakash', 'info_list': ['JavaScript', 'Updated Nov 29, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SangeethaSA', 'info_list': ['JavaScript', 'Updated Nov 29, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': [""Automated Machine Learning Regression Analysis and Optimization\nTired of always coding regression models from scratch for different datasets? Yeah...me too.\nIn this page, I demonstrate a Python Jupyter notebook shell that uses Object-Oriented programming techniques to automate analysis and performance optimization for various Machine Learning regression algorithms. It can be run on any ‘scrubbed’ dataset with categorical or numeric features, and it provides detailed regression metric comparisons and visualizations for each tested algorithm. The algorithms tested here were linear/polynomial regression, random forests, gradient-boosting, and kNN, but it's pretty simple to implement new algorithms with the same coding structure.\nThis was all demonstrated using a dataset for King's County housing prices. My exploratory data analysis (EDA) and scrubbing on this data can be found here\nEnjoy!\n""], 'url_profile': 'https://github.com/omshapira', 'info_list': ['JavaScript', 'Updated Nov 29, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Fremont, CA', 'stats_list': [], 'contributions': '250 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/joeyhuaa', 'info_list': ['JavaScript', 'Updated Nov 29, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Khlong Luang, Pathum Thani, Thailand', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/developeravsk', 'info_list': ['JavaScript', 'Updated Nov 29, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Austin, TX', 'stats_list': [], 'contributions': '144 contributions\n        in the last year', 'description': ['Logistic-Regression-Model\nImplementation of logistic regression algorithm on a data set with 2 features\n'], 'url_profile': 'https://github.com/sindhusha-parimi', 'info_list': ['JavaScript', 'Updated Nov 29, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Support-Vector-Regression\nSupport Vector Regression is quite different than other Regression models. It uses Support Vector Machine, a classification algorithm) algorithm to predict a continuous variable. While other linear regression models try to minimize the error between the predicted and the actual value, Support Vector Regression tries to fit the best line within a predefined or threshold error value. What does in this sense, it tries to classify all the prediction lines in two types, ones that pass through the error boundary( space separated by two parallel lines) and ones that Those lines which do not pass the error boundary are not considered as the difference between the predicted value and the actual value has exceeded the error threshold,  The lines that pass, are considered for a potential support vector to predict the value of an unknown. The following illustration will help you to grab this concept.\n'], 'url_profile': 'https://github.com/Avinash237', 'info_list': ['JavaScript', 'Updated Nov 29, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 9, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'R', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}"
"{'location': 'Calcutta, India', 'stats_list': [], 'contributions': '1,419 contributions\n        in the last year', 'description': [""Implementation of a Logistic regression neural network\nIn this project we've implemented a neural network of logistic regression using numpy only.\n""], 'url_profile': 'https://github.com/theroyakash', 'info_list': ['Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 16, 2020', 'Updated Jan 9, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SangeethaSA', 'info_list': ['Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 16, 2020', 'Updated Jan 9, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'Germany', 'stats_list': [], 'contributions': '130 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Handballfreak', 'info_list': ['Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 16, 2020', 'Updated Jan 9, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HayetBD', 'info_list': ['Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 16, 2020', 'Updated Jan 9, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Vaishnavi1310', 'info_list': ['Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 16, 2020', 'Updated Jan 9, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Simple Linear Regression\nSimple linear regression is a statistical method for obtaining a formula to predict values of one variable from another where there is a causal relationship between the two variables.\n'], 'url_profile': 'https://github.com/skolambkar98', 'info_list': ['Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 16, 2020', 'Updated Jan 9, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'Cincinnati, Ohio', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Linear-Regression-using-R\nAlumni donations are an important source of revenue for colleges and universities. If administrators could determine the factors that influence increases in the percentage of alumni donation, they might be able to implement policies that could lead to increased revenues. Research shows that students who are more satisfied with their contact with teachers are more likely to graduate. As a result, one might suspect that smaller class sizes and lower student-faculty ratios might lead to a higher percentage of satisfied graduates, which in turn might lead to increases in the percentage of alumni donations. Similarly, to find various other factors that can affect the alumni donation rate, we have taken the dataset of 48 national universities (America’s Best Colleges, Year 2000 Edition) and have done exploratory data analysis and fit linear regression models to understand implemented various linear regression model to find best model which can answer this question.\n'], 'url_profile': 'https://github.com/singal-harsh', 'info_list': ['Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 16, 2020', 'Updated Jan 9, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression_flatPrice\nPredict the Flat Resale Prices in Singapore\nInvesting in Flats is the biggest investment for most households in cities like Singapore which is very small but heavily urbanised. Therefore, being able to accurately value the flat prices will not only facilitate market transactions by providing valuable guidance for all market participants (be it home owners, home buyers, landlords, tenants or banks that underwrite mortgages), but also provide useful insights for policy makers and government authorities in understanding the current state of the economy.\nTraining DataSets:\nTo build the model and tune the model & visualization\ni. train1.csv\nii. train2.csv\niii. train3.csv\niv. building_to_mrt_distance.csv (distance in meters) - Can be used for both train and validation\nImp notes: Singapore Flats have 99-year lease.\n'], 'url_profile': 'https://github.com/VENKATCHGIT', 'info_list': ['Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 16, 2020', 'Updated Jan 9, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['R_Logistic_Regression\n#TASKS: Make logistic regression; Split data into training and testing to validate; Make a confusion matrix; Calculate the accuracy.\n#PROJECT DESCRIPTION: This is a Logistic regression. I split data into training and testing sets (70% training, 30% testing). I wrote a code to calculate the confusion matrix and calculate the accuracy.\n'], 'url_profile': 'https://github.com/arash26m', 'info_list': ['Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 16, 2020', 'Updated Jan 9, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'R', 'Updated Feb 23, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['41100 Applied Regression\nDependencies:\n\nR Programming Language Version 3.6.x\n\nQuickstart\n\nInstall R on OSX (via Homebrew). Assumes you have installed dependencies like xcode-select:\n\nbrew doctor && brew update\nbrew install r \n\n\nInstall R Studio\n\nResources\n\nR Styleguide\nHands-On Programming with R\nYaRrr! The Pirate’s Guide to R\n\n'], 'url_profile': 'https://github.com/stephen-puiszis', 'info_list': ['Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 16, 2020', 'Updated Jan 9, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'R', 'Updated Feb 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nelsonbunnys', 'info_list': ['Updated Jan 9, 2020', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['mtcars_regression_analysis\nmtcars dataset contains a collection of data compiled for 32 cars analyzed among 11 parameters. The following two questions are taken up for analysis in this work.\n\nIs an automatic or manual transmission better for efficent fuel consumption \nQuantifying the MPG difference between automatic and manual transmissions\n\nFirst, exploratory data analysis is performed to analyse and understand the data. Then, regression models are used to determine the relationship and correlation among the measured parameters in the dataset. Regression analysis is done by including and excluding different parameters contained in the dataset.\nThe main objective is to infer how mpg is affected by manual and automatic transmission. An analysis is also performed to measure the influence of other parameters present in the dataset using regression analysis.\n'], 'url_profile': 'https://github.com/kalai4390', 'info_list': ['Updated Jan 9, 2020', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'hyderabad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hanuaiml', 'info_list': ['Updated Jan 9, 2020', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShreyaSolanki', 'info_list': ['Updated Jan 9, 2020', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Yakal', 'info_list': ['Updated Jan 9, 2020', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bhaviniparekh', 'info_list': ['Updated Jan 9, 2020', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'Winnipeg, Manitoba', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['tensorflow-linear-regression-exercise\nLinear Regression with TensorFlow exercise.\n'], 'url_profile': 'https://github.com/khanh-t', 'info_list': ['Updated Jan 9, 2020', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression-Project\nThe Goal of this project is to analyse the results from an experiment that has been done to study\nthe performance of a given network. the theoretical model we have is: � T(S) = L + S/C.\n\nT� is the time required for sending a message.\nS is the size of the message.\nC is the capacity of the network/ Links.\nL is the latency.\nIn this model we are considering here is neglecting a lot of details.\nWith this work we want to find two parameters the Latency and the Capacity, to do that they used\nsome random samples choosing different sizes and did some pings and at each time they\ncalculate the time needed to do the transmission.\nWe have two different datasets we need to analyse and do the linear regression.\n\nYou will find here a rmd file and the datasets i used that will help you to repruduce the analysis.\n'], 'url_profile': 'https://github.com/SADKIAbdel', 'info_list': ['Updated Jan 9, 2020', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': [""Regression_Insurance_Analysis\nAuthors: Finn Tan and Sara Zylfo\nWe are a healthcare consultancy company hired by an European Life Insurance firm who is looking to expand into the US market. As such, we have been tasked to provide some statisical insights into the main drivers of life expectancy which will be a crucial input in driving our client's pricing strategies.\nCoupled with the above, we were also required to suggest 5 US states that would be best to break into, which were ultimately conducted by finding a subset of US states with high life expectancy but with low life insurance coverage.\nIn order to accomplish this, we have used the 2019 County Health Rankings National Data which provides a comprehensive numerical data on counties':\n- Health Outcome\n- Health Behviours\n- Clinical Care\n- Social & Economic Factors\n- Physical Environment\n- Demographics\n\nFor better readability, we have split our findings into several parts including:\n- 'index - Data Cleaning.ipynb'\n- 'index - EDA & Visualization.ipynb' \n- 'index - Regression Modelling, Model Evaluation & Conclusion.ipynb'\n- 'functions.py' - include customized functions\n\nProcess\nGiven the sheer size of the features involved and for better interpretation for our final user ('Life Insurer'), it is crucial to narrow down the number of variables. There are several ways to do this but the ones we are about to list below are definitely not exhaustive.\n- Baseline Method: Using all available variables\n- Naive Selection: using top features that are correlated to Life Expectancy\n- Filter Method: dropping low variance features followed by removing highly correlated features\n- Stepwise Selection: Adding features with p-values below certain threshold and dropping those\n- Recursive Feature Elimination: sklearn's function of greedily choosing\n- Lasso: use GridSearch to find the best penalizing parameter 'alpha' for the Lasso algo. We will then select features that have not been shrinked to 0\n\nOnce we get all the features selected by each method above, we pass those into Statsmodel's OLS function. Subsequently, we will select our most prefferred model by comparing their R2 scores, AIC (model complexity) and also consider the number of features included, which is the primary consideration here.\nPost model selection, we will then check if the chosen model satisfies the assumptions of a regression; no multicollinearity between selected features, homosceasticity and normality of errors. In the end, we shall evaluate the model if it fits the purpose for our final user.\nMethod Evaluations\n\nBaseline\nFor the baseline model, we used all 60 features which managed to generate train R2 scores in the 72 - 75% region. However the large number of features coupled with the high non-signficant (p-values) is not very appealing, especially for our end user.\nNaive Selection\nThe Naive Selection method uses features with high correlation (set at 0.65) with the 'life expectancy' variable. Whilst the technique might be useful for reducing the number of features, one of the pitfalls of this is that the method might ignore other useful features.\nFilter Method\nCoupled with the above, the previously contemplated models might include various highly correlated features, violating the assumptions of a regression model. As such, we tried using a middle approach by:\n- dropping features with no or low variance. These features typically do not add much predictive value in a model\n- dropping features which are highly correlated\n\nThis method however still results in a high number of features / non-significant features.\nRecursive Feature Elimination\nUsing sklearn's RFECV function returns the best number of features to use according to the R2 scoring criteria. With this we obtained:\n\nThis looks slightly better than the filter method however the number of features still remains an issue here.\nStepwise ['Excerpt from the Learn.co']\n'In stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.'\nWith this model, we achieved a fairly reasonable R2 and a manageable number of features.\nLasso\nUsing GridSearch CV to identify the best alpha hyperparameter / penalizer. We then took the non-zero coefficients from the model and fitted that into an OLS model which resulted in outcomes similar to the above stepwise model.\nModel Selection\nLooking purely by R2 scores, model complexity and optimum number of features, we have decided to go for the Stepwise model. However we will still need to check if the model fits the regression assumptions. First let check for multicollinearity using the VIF metric.\n\nSince no features are more than 5, multicollinearity is not a big concern here. Next we checked for homoscedasticity by plotting the model's residuals.\n\nThis looks fine with no concerning trends. No we look at the normality of the errors using the QQ plot before and after removing outliers:\nBefore:\n\nAfter:\n\nThis looks better now. Finally we can explore the coefficients and finally deploy our chosen & updated model on the test data.\n\nUsing the coefficients above, we ended up with a:\nTest R2 score of 68%\nSummary map:\n\nConclusion, Limitations and Future Work\nTo conclude, it is important to emphasize that whilst we have chosen the model above, there are many other methods out there which may result in a better model. That said, the above model should at least provide our end user with the crucial indicators for estimating life expectancy.\nOur main factors for increasing life expectancy are boiled down to:\nMedian Household Income\nMammography Screening\nCollege Education\nOur main factors for decreasing life expectancy:\nAdult Smoking\nPhysical Inactivity\nAir Pollution\nDiabetes\nWe identified states with low current coverage of insurance, but who's populations satisfy the feature detailed above to increase life expectancy and those were:\nColorado\nNevado\nIdaho\nIowa\nConneticut.\nFor future work, we will look to explore:\n- investigate performance of forward / backward selection, Ridge regression, interactions and\npolynomials\n- user other models apart from OLS, ie RF, CART, etc\n- investigate indirect correlations between features\nA further population we would like to explore are millenials.\n44% of millennials overestimate the cost of life insurance by up to 5 times.\n89% of millennial prioritise benefits over pay rises.\n83% of millennials would change their job for better benefits, of which family benefits is in their top three requirements.\nConsider selling group insurance to companies who retain a high % of millennial talent to expose the demographic to life insurance.\n""], 'url_profile': 'https://github.com/sarazylfo', 'info_list': ['Updated Jan 9, 2020', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['LogisticRegression-Project\nLogisticRegression model\n'], 'url_profile': 'https://github.com/AbhishekSen05', 'info_list': ['Updated Jan 9, 2020', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShreyaSolanki', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Yakal', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bhaviniparekh', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Winnipeg, Manitoba', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['tensorflow-linear-regression-exercise\nLinear Regression with TensorFlow exercise.\n'], 'url_profile': 'https://github.com/khanh-t', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gerojordan', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Visual regression boilerplate project\n\nJava\nTestNG\nSelenium Webdriver\naShot\n\nHow to start using this project\n1 - Installation\n\nInstall java\nInstall maven\nInstall browsers\n\nchrome\nfirefox\n\n\n\n2 - Explore DriverWrapper class constructor method\nDriverWrapper creates WebDriver and has some helpers methods. \nWebDriver is created in DriverWrapper class constructor.\nYou may add new browsers here.\n3 - Set project settings in TestConfig class\nProject settings are located in TestConfig class. \nYou can pass command line arguments when starting tests.  \nFor example mvn test -Dbrowser=chrome, where -Dbrowser=chrome = TestConfig.browser \nCommand line parameters are extracted in TestConfig.initConfig() method.\n4 - Set URL for site you\'re going to test\nBasePage is a base class for all page objects. All page object classes must extend BasePage class. \nYou can set URL for test site in BasePage.setBaseUrl() method. \nIt\'s not necessary to use BasePage or App. It\'s just an example.\n5 - Explore TestExample class and create your own tests\n\nAll test classes must extend BaseTest class.\n\n6 - Add new tests in testng.xml file\nScreenshots comparison\nBefore start you should set allowableDiffSize. \nIt is allowable difference size (number of different pixels) between two screenshots. \nTest will not fail if difference between 2 screenshots is less than allowableDiffSize.\n\nThe variable is located in TestConfig class.\nBy default allowableDiffSize == 10.\n\nCompare page screenshots\n\ncomparePageScreenshots(""index_page"");\n\nCompare page screenshots and ignore specified elements(only css selectors are allowed)\n\nignore one element - comparePageScreenshots(""index_page_ignored_element"", ""div.macbook"");\nignore many elements - comparePageScreenshots(""index_page_ignored_elements"", new String[]{""section.panel.features.dark"", ""div.macbook""});\n\nCompare element screenshots\n\nuse css selector - compareElementScreenshots(""index_page_element"", ""a.full.forge"");\nor pass WebElement object - compareElementScreenshots(""index_page_element"", driver.findElement(By.cssSelector(""a.full.forge"")));\n\n\nProject structure\napp:\nPage object pattern example.\n\nApp - main application / site class. It contains all page objects.\nBasePage - all page objects must extend this class.\nIndexPage - page object for index page.\n\ntools:\n\nTestConfig - project settings(browser, env, etc..), paths to screenshots and other settings are located here.\nDriverWrapper - wrapper for Selenium WebDriver with some helpers methods.\nScreenshoter - creates and saves actual and expected screenshots.\nDiffImageGenerator - generates diff images. Difference is highlighted with red color.\nGifGenerator - gif images generator. Generates gif file from actual, expected and diff image.\nReportGenerator - report generator.\n\ntests\n\nBaseTestInit - is started before all tests, removes old screenshots and cleans logs\nBaseTestFinish - is started after all tests, generates report.\nBaseTestListener - writes errors to errors.log.\nBaseTest - base test class. All test classes must extend it.\nTextExample - class with tests examples.\n\n\nTest execution\nYou can run tests using next command:\nmvn test - start all tests with default parameters\nTest parameters\n\n-Dbrowser=chrome - browser. By default 4 browsers are available - chrome, firefox, ch(chrome headless), fh(firefox headless).\n-Denv=live - environment. dev, stage, live or your own values.\n-Ddimension=360x1000 - width and height of browser window\n-Dclean=1 - removes expected screenshots. 0 - false, 1 - true. if true actual screenshots will be saved as expected ones.\n-Dtest=TestSuite - run only specified test suite\n-Dtest=TestSuite#testMethod - run only test method from specified test suite\n\nAll parameters are optional. Default values are set in TestConfig class.\n\nScreenshots\nScreenshots are stored in screenshots folder in next sub-folders\n\nactual - actual screenshots\nexpected -  expected screenshots\ndiff - screenshots with difference between actual and expected screenshots\ngifs - each gif is created from actual, expected and diff screenshot\n\nActual, diff and gif screenshots are removed at tests execution start.\n\nReport\n\nReport is generated after all tests finish executing.\nReport file is located here - report/REPORT.html\nFor now ReportGenerator supports only 3 resolutions\n\nmobile - 360_x_any_height\ntablet - 768_x_any_height\ndesktop - 1920_x_any_height\n\n\n\n\n\nBash script\nThere is an example bash script - test_3_resolutions.sh. \nYou can use it to test your site in 3 resolutions at once (mobile, tablet, desktop).\n'], 'url_profile': 'https://github.com/Rainfox98', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Germany', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Context-free-Importance-Regression\n'], 'url_profile': 'https://github.com/MarkusZopf', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '207 contributions\n        in the last year', 'description': ['Tutorial-for-Polynomial-Regression-\n'], 'url_profile': 'https://github.com/CREVIOS', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Riyadh, Saudi Arabia', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': [""Logistic-regression-using-MATLAB\nUsing logistic regression our model predicts the survival of patients who had undergone surgery for breast cancer using Haberman's Survival Datases by computing the cost and gradient and learning Theta value parameters.\n""], 'url_profile': 'https://github.com/elaugh9', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['SimpleLinearRegression\n'], 'url_profile': 'https://github.com/a-n-u-p', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Python', 'Updated Sep 1, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'MIT license', 'Updated Jan 9, 2020', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '200 contributions\n        in the last year', 'description': ['ML\nlinear regression\nEconomic and unemployment data were recorded.\nThere are 16 rows of data.  The data include:\n col 1:  the index;\n col 2: the percentage price deflation;\n col 3:  the GNP in millions of dollars;\n col 4:  the number of unemployed in thousands;\n col 5: the number of people employed by the military;\n col 6:  the number of people over 14;\n col 7:  the year\n col 8: prediction required- the number of people employed.\n\n1   83.0  234289  2356  1590  107608  1947  60323\n2   88.5  259426  2326  1456  108632  1948  61122\n3   88.2  258054  3682  1616  109773  1949  60171\n4   89.5  284599  3351  1650  110929  1950  61187\n5   96.2  328975  2099  3099  112075  1951  63221\n6   98.1  346999  1932  3594  113270  1952  63639\n7   99.0  365385  1870  3547  115094  1953  64989\n8  100.0  363112  3578  3350  116219  1954  63761\n9  101.2  397469  2904  3048  117388  1955  66019\n10  104.6  419180  2822  2857  118734  1956  67857\n11  108.4  442769  2936  2798  120445  1957  68169\n12  110.8  444546  4681  2637  121950  1958  66513\n13  112.6  482704  3813  2552  123366  1959  68655\n14  114.2  502601  3931  2514  125368  1960  69564\n15  115.7  518173  4806  2572  127852  1961  69331\n16  116.9  554894  4007  2827  130081  1962  70561\n'], 'url_profile': 'https://github.com/deepanshukumarpali', 'info_list': ['MATLAB', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 12, 2020', '1', 'R', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Updated Jan 11, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': [""Geographically-Weighted-Regression\nSpatial Diagnostics similar to Geoda outputs including Moran's I, spatial lag, and spatial error model residual visualization as a first step before applying geographically weighted regression (GWR)\n\n""], 'url_profile': 'https://github.com/RawanAloula', 'info_list': ['MATLAB', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 12, 2020', '1', 'R', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Updated Jan 11, 2020']}","{'location': 'Erode', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['PYSPARK-LINEAR-REGRESSION\nPYSPARK-LINEAR-REGRESSION :\nWE ARE GOING TO PREDICT CUSTOMER WHO SPEND LOTS OF TIME AND PUSCHASED HIGH AMOUNT OF PRODUCT.\nTHIS HELPS BUSINESS TO CONCENTRATE ON OUR HAPPY CUSTOMERS\nLINEAR REGRESSION.\n'], 'url_profile': 'https://github.com/dr-rubeshanand', 'info_list': ['MATLAB', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 12, 2020', '1', 'R', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Updated Jan 11, 2020']}","{'location': 'Indore, Madhya Pradesh, India', 'stats_list': [], 'contributions': '481 contributions\n        in the last year', 'description': ['Simple_Linear_Regression\nI took a data Set of peoples, In this Data Set we have two types of values one is for Experience of people and another is salaries based on their Experience.\nIn the data set we have total 30 Entries in which i got selecet 20 Entries for our training model to train a model, and they capable of predict the another values and remaining 10 entries for testing the mode\n'], 'url_profile': 'https://github.com/pradhyumvyas', 'info_list': ['MATLAB', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 12, 2020', '1', 'R', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Surprise-Housing-Advanced-Regression\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.\nThe company is looking at prospective properties to buy to enter the market.\nYou are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\nThe company wants to know:\nWhich variables are significant in predicting the price of a house, and\nHow well those variables describe the price of a house.\nAlso, determine the optimal value of lambda for ridge and lasso regression.\nBusiness Goal\nYou are required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for the management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/saurabdongre', 'info_list': ['MATLAB', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 12, 2020', '1', 'R', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Updated Jan 11, 2020']}","{'location': 'Mountain View, California', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/peter-sushko', 'info_list': ['MATLAB', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 12, 2020', '1', 'R', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': [""LinearRegressionBasketball\nfirst ML project...linear regression to analyze NBA players' stats over a season\n""], 'url_profile': 'https://github.com/sathvikb007', 'info_list': ['MATLAB', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 12, 2020', '1', 'R', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NonLinear Regression\nThis repository implements a non-linear regression using zero mean noiseless Gaussian Processes to estimate the function values f(x) = sin(0.5 * x) for x values ranging from -4 to 4. The repository performs below specific operations:\n\nIt plots 10 prior functions in the interval -4 to 4.\n\n\n\nIt plots the mean estimate for the non-linear regression and the error curves above and below indicating confidence in the estimate.\n\n\n\nIt plots 10 posterior functions sampling the conditional distribution GP(f|D).\n\n\n'], 'url_profile': 'https://github.com/kanchanchy', 'info_list': ['MATLAB', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 12, 2020', '1', 'R', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Updated Jan 11, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '132 contributions\n        in the last year', 'description': [""Logistic-Regression-Python\nLogistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.\nLogistic Regression Assumptions\n\nBinary logistic regression requires the dependent variable to be binary.\nFor a binary regression, the factor level 1 of the dependent variable should represent the desired outcome.\nOnly the meaningful variables should be included.\nThe independent variables should be independent of each other. That is, the model should have little or no multicollinearity.\nThe independent variables are linearly related to the log odds.\nLogistic regression requires quite large sample sizes.\n\nData:\n\n\niris dataset,This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently       to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly           separable from the other 2; the latter are NOT linearly separable from each other.\nPredicted attribute: class of iris plant.\nAttribute Information:\n\nsepal length in cm\nsepal width in cm\npetal length in cm\npetal width in cm\nclass:\n-- Iris Setosa\n-- Iris Versicolour\n-- Iris Virginica\n\n\n\nLet's build logistic regression model and predict the class of iris plant.\n""], 'url_profile': 'https://github.com/dhamvi01', 'info_list': ['MATLAB', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 12, 2020', '1', 'R', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Updated Jan 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Tonyopt57', 'info_list': ['MATLAB', 'Updated Jan 8, 2020', 'R', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 12, 2020', '1', 'R', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 27, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Updated Jan 11, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sidharthrao', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 12, 2020', '1', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 15, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['cab_prediction_regression\n'], 'url_profile': 'https://github.com/tsushant1996', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 12, 2020', '1', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 15, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TejasSutar01', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 12, 2020', '1', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 15, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['ft_linear_regression\nAn introduction to machine learning\n'], 'url_profile': 'https://github.com/apsaint', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 12, 2020', '1', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 15, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'Sweden', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['This is a pytorch implementation for joint state estimation from RGB, depth and RGB-Depth image using 3D-resnet architecture presented in the paper Hara et al, ""Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?"", 2018. In this paper, they show that 3D CNN architectures can be trained from scratch using big video datasets for action recognition. In this code, we convert this architectures to regression architecture to estimate joint states of robotic manipulator directly from video images.\nThe network model implementations are copied from the repository 3D-ResNets-Pytorch and are altered. Please check the paper and the repository for more detailed explanations about the method, results and implementation details. Also, in the same repository, they provide the pretrained weights for several architectures which can be used to fine tune this network.\nTo run the code:\nIn ""data"" folder, there is a small example dataset to demonstrate what kind of data the code uses and how to create a dataset.\nSteps to run the code\n\nCreate .csv files that contains information of train, validation and test splits of the dataset:\n\npython createCSV.py --data_dir data/ --perc_train 50 --perc_test 25 --per_val 25.\nIn the ""data/"" folder, where the dataset exits, this code will create {train, val, test}.csv files and info_csv.xml that contains some information related to the dataset such as mean and standard derivation to normalize data later.\n2. Then to run the training you can run by setting some parameters:\npython main.py --root_path \'/data/\' \\\n               --n_classes [number of classes] \\\n               --is_train \\\n               --is_rgb \\\n               --is_scale \\\n               --n_epochs 100000 \\\n               --batch_size 64\nand for test\npython main.py --root_path \'/data/\' \\\n               --n_classes [number of classes] \\\n               --is_test \\\n               --is_rgb \\\n               --is_scale \\\n               --n_epochs 100000 \\\n               --batch_size 64               \nYou can check for more parameter options in opts.py file and in the repository.\nCheck data/results folder for saved checkpoints.\nThere is also a Dockerfile that enables one to run everything without installing the necessary tools and libraries.\n'], 'url_profile': 'https://github.com/puren', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 12, 2020', '1', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 15, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'Saudi Arabia', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['Multivariate-linear-regression\nA multivariate linear regression to predict the mean January high temperature of Texas counties given their latitude, longitude, and elevation.\nDataset\ndataset.data1 has four columns:\n• The latitude.\n• The longitude.\n• The elevation.\n• The mean January high temperature.\nFuctions\nmulti_regression.m is the main script that performs multivariate linear regression\nby loading the dataset and making calls to functions in other files.\n\nFeatures normalization\nComputing the cost\nGradient descent\nPrediction\n\n'], 'url_profile': 'https://github.com/abeeraleisa', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 12, 2020', '1', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 15, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'Xiamen, China', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['LNC建模实例\n对于LNC数据，通过岭回归和Lasso回归以及弹性网络分别建模\n对于LNC数据，通过随机森林分别建模\n'], 'url_profile': 'https://github.com/StarSky2019', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 12, 2020', '1', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 15, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'Beijing', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Deep Stereo Matching\nReproduce the work of Kendall A, et al. Meantime, I conduct some extra experiments for a further research.\nRequirements\n\nPython 3.6\nPyTorch 0.4.0\n\nDemos\nLeft Image\n\n\n\nGroundtruth Disparity\n\n\n\nPrediction Disparity\n\n\n\nQuantitative Results\nResults on the Monkaa part of the SceneFlow datasets.\n\n\n\nInputs Resolution\nOutputs Resolution\nFull resolution method\nAverage Endpoint Error (EPE)\n\n\n\n\n540x960\n540x960\n2D Deconv + F + 0.5F + 0.25F\n2.29\n\n\n540x960\n540x960\n2D Deconv\n2.45\n\n\n540x960\n540x960\nBilinear\n5.69\n\n\n\nNo more experiments was conducted in the whole SceneFlow datasets duo to some reasons.\nReferences\n\nKendall A, Martirosyan H, Dasgupta S, et al. End-to-End Learning of Geometry and Context for Deep Stereo Regression[C]. international conference on computer vision, 2017: 66-75.\n\n'], 'url_profile': 'https://github.com/TaooCAI', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 12, 2020', '1', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 15, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['CorseraRegressionModeling\n'], 'url_profile': 'https://github.com/nickorka', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 12, 2020', '1', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 15, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'Bengaluru, India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Advanced-logistics-regression\nProblem Statement - Part I\nThis assignment contains two parts. Part-1 is a programming assignment (to be submitted in a Jupyter notebook) whereas part-2 includes subjective questions (to be submitted in a PDF file).\nAssignment Part-I\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual value and flip them at a higher price. For the same purpose, the company has collected a data set from house sales in Australia. The data is provided in the csv file below.\nThe company is looking at prospective properties to buy to enter the market.\nYou are required to build a regression model using regularization, so as to predict the actual value of the prospective properties and decide whether to invest in them or not.\nThe company wants to know:\nWhich variables are significant in predicting the price of a house\nHow well those variables describe the price of a house\nAlso, determine the optimal value of lambda for ridge and lasso regression.\nBusiness Goal >>\nYou are required to model the price of houses with the available independent variables. It will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high rewards. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\nProblem Statement - Part II\nAssignment Part-II\nThe following questions are the second part of the graded assignment. Please submit the answers in one PDF file. For writing normal text, please use MS Word (or similar software which can convert documents to PDF). For writing equations and drawing figures, you can write/draw them on a blank sheet of paper using a pen, click images and upload them in the same word document.\nThe final submission will be one PDF file. A sample PDF to illustrate the submission format is provided below.\nNote: Avoid copying and pasting from anywhere and type the answers in your own words - your solution files will be tested using automatic plagiarism checkers and will attract a heavy penalty if plagiarism is detected.\nPlease limit your answers to less than 500 words per question.\nQuestion-1:\nRahul built a logistic regression model having a training accuracy of 97% while the test accuracy was 48%. What could be the reason for the seeming gulf between test and train accuracy and how can this problem be solved.\nQuestion-2:\nList at least 4 differences in detail between L1 and L2 regularization in regression.\nQuestion-3:\nConsider two linear models\nL1: y = 39.76x + 32.648628\nAnd\nL2: y = 43.2x + 19.8\nGiven the fact that both the models perform equally well on the test dataset, which one would you prefer and why?\nQuestion-4:\nHow can you make sure that a model is robust and generalisable? What are the implications of the same for the accuracy of the model and why?\nQuestion-5:\nAs you have determined the optimal value of lambda for ridge and lasso regression during the assignment, which one would you choose to apply and why?\n'], 'url_profile': 'https://github.com/krunalchalakh', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 12, 2020', '1', 'Python', 'Updated May 4, 2020', 'Python', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 7, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 15, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 10, 2020', 'HTML', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}"
"{'location': 'Melbourne, Australia', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['JHU Regression\nsee deliverable:\nWeek4_submission.pdf\n'], 'url_profile': 'https://github.com/rphardy', 'info_list': ['Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Africa', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['image_classification\nUsing Logistic regression for image classification with a dataset from sklearn\n'], 'url_profile': 'https://github.com/okeks', 'info_list': ['Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rananjaydebroy1999', 'info_list': ['Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Bengaluru, India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Predictive-Analysis\nProblem Statement>>\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\nBusiness Goal>>\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\nData Preparation>>\nThere is a variable named CarName which is comprised of two parts - the first word is the name of 'car company' and the second is the 'car model'. For example, chevrolet impala has 'chevrolet' as the car company name and 'impala' as the car model name. You need to consider only company name as the independent variable for model building.\nModel Evaluation:\nWhen you're done with model building and residual analysis, and have made predictions on the test set, just make sure you use the following two lines of code to calculate the R-squared score on the test set.\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_pred)\nwhere y_test is the test data set for the target variable, and y_pred is the variable containing the predicted values of the target variable on the test set.\nPlease don't forget to perform this step as the R-squared score on the test set holds some marks. The variable names inside the 'r2_score' function can be different based on the variable names you have chosen.\n""], 'url_profile': 'https://github.com/krunalchalakh', 'info_list': ['Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/leonardloh', 'info_list': ['Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rj1346', 'info_list': ['Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Washington D.C.', 'stats_list': [], 'contributions': '1,014 contributions\n        in the last year', 'description': [""New York City Airbnb Open Data - Practice\n\nAttribution: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, by Aurelien Geron (O'Reilly). Copyright 2019 Kiwisoft S.A.S, 978-1-492-03264-9\nMachine Learning Practice. Implimenting the project following the Chapter-2 project on O'REILLY's Hands-On Machine Learning. \nGoal: Predict the unit price for a Airbnb post, given all the other metrics. Predict NYC Airbnb Rental Prices\nApproach:\n\nSupervised Learning task, because given labeled traning examples (each instance comes with expected output, i.e. unit's price).\nRegression task, since we need to predict a value.\nMultiple regression problem since the system will use multiple features to make a prediction.\nUnivariate regression problem since we are only trying to predict a single value for each unit.\nThere is no continuous flow of data, no need to adjust to changing data, and the data is small enough to fit in memmory: Batch Learning\n\nPossible Performance Measure: Root Mean Square Error (RMSE), and Mean Absolute Error (MAE).\nData: New York City Airbnb Open Data | Kaggle\nProject Author: Maksim Ekin Eren\n""], 'url_profile': 'https://github.com/MaksimEkin', 'info_list': ['Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Stockholm', 'stats_list': [], 'contributions': '199 contributions\n        in the last year', 'description': ['Bayesian-sequential-learning\nBayesian sequential learning for a simple linear regression problem with some visualization\n\n\n'], 'url_profile': 'https://github.com/MaxHolmberg96', 'info_list': ['Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Credit-Fraud-Detection\nImbalanced credit card data analysis and comparative study using Naive Bayes, XGBoost and Logistic Regression.\nOne can download the dataset from https://www.kaggle.com/mlg-ulb/creditcardfraud/download\n'], 'url_profile': 'https://github.com/muniah', 'info_list': ['Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Priyanshu21101997', 'info_list': ['Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}"
"{'location': 'chengdu,china', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['TTR\nCode for the paper ""Low rank tensor train coefficent array estimation for tensor-on-tensor regression""\nRequirements\nThe algorithms have been implemented in MATLAB and make extensive use of:\nMATLAB Tensor Toolbox 2.6 (http://www.sandia.gov/~tgkolda/TensorToolbox/)\nTT-Toolbox (TT=Tensor Train) Version 2.2 (https://github.com/oseledets/TT-Toolbox)\nContents\nThe main algorithms are in /tools.\nThe demo folder contains files for reproducing the synthetic experiments from the paper.\n'], 'url_profile': 'https://github.com/liujiani0216', 'info_list': ['MATLAB', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'MIT license', 'Updated Jan 10, 2020', '1', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '539 contributions\n        in the last year', 'description': [""Life Expectancy in the US\nExecutive Summary\nWe are taking the position of a consultancy company hired by the government to study the relationship between life expectancy in the US versus various factors related to health and lifestyle. The study's objective is to support the government in formulating a healthcare policy based on the life expectancy model that we build.\nLatest data indicates that there are large differences in life expectancy (over 20 years) between some counties: our model will be particularly useful in addressing healthcare issues in vulnerable counties to bring them at par with the rest of the country.\nKey files\n\nLink here : Presentation in Google Slides format\nLife_Expectancy_USA.pdf : Presentation in PDF format\nLife_Expectancy_USA.ipynb : Jupyter notebook file with Python code + commentaries\nLife_Analysis.py : local Python functions source file for Jupyter notebook\nanalytic_data2019.csv : Raw data source file in CSV format\n\nMethodology\n\nData Import\nData Cleansing\n2.1 Clean-up columns and data\n2.2 Remove outliers\nData Exploration\n3.1 Overview of all data via plots\n3.2 Overview of target (Life expectancy)\n3.3 Split and transform training and test data\nFeature Selection (Part 1): Evaluate predictors\n4.1 Baseline model : calculate k-fold cv with all predictors\n4.2 Baseline model : investigate regularization using Lasso\n4.3 Evaluate predictors (Step 1) : P-value of baseline predictors vs. target\n4.4 Evaluate predictors (Step 2) : Correlation of predictors vs. target\n4.5 Evaluate predictors (Step 3) : Multicollinearity between predictors\n4.6 Model 1 : using Top predictors\n4.7 Evaluate predictors (Step 4) : Interaction between top predictors\n4.8 Evaluate predictors (Step 5): Polynomial terms\n4.9 Add top interaction terms and top polynomial terms into data frame\nFeature Selection (Part 2) : Finalize predictors\n5.1 Model 2 : use Top predictors + interactions + polynomial terms\n5.2 Determine strongest predictor terms (based on correlation)\n5.3 Determine strongest predictor terms (based on standardized coefficient)\n5.4 Evaluate linear regression model assumptions via residual analysis\nFinal Model\n6.1 Prepare final training and test data\n6.2 Final model : run with training and test data\n\nKey findings\nThe first baseline model using all available predictors in the dataset gives a very high accuracy, but similar results would not be achieved in production due to overfitting of sample data. We employed several techniques to reduce the predictors based on the principles of correlation, multicollinearity, interaction between predictors, transforming predictors into polynomial terms, and regularization techniques using Ridge and Lasso.\nOur final model generates an r-squared value of 71% for training data. R-squared value is defined as the proportion of the variance (difference between actual observed data and modelized output) of life expectancy that can be explained by the model's predictor variables.\nFor test data, we obtained an R-squared value of 66%, which suggests that our model does not fall into overfitting trap. In other words, the model is able adapt to unknown data and generates the same level of accuracy as during the development stage.\nHere are the strongest contributing factors to predict life expectancy value in our model. The figures in parenthesis denote the model's absolute coefficient, which measures the relative weight of each predictor to the model's output.\n\n[0.58] Teen births\n[0.56] Adult smoking\n[0.39] Diabetes prevalence\n[0.32] Food insecurity\n[0.29] Median household income\n[0.26] Mental health providers\n[0.25] Physical inactivity\n[0.25] Mammography screening\n\nConclusions\nFrom the study and models conducted on the impact of various health and lifestyle factors to life expectancy in the US, we came up with following key conclusions:\n\nTeen births, smoking and diabetes prevalence are identified as top contributors to lower life expectancy.\nThere is a trade-off that we needed to take between the model’s accuracy and the ability to predict using unseen data input.\nThe accuracy of our model remains fairly unchanged when applied to new data set. We can conclude that it is a reliable model although more refinement can be done to improve its accuracy further.\n\nRecommendations\nHere are the top 3 action plans that we propose to US federal and state agencies to improve life expectancy and to reduce this inequality between states and counties:\n\nSex education in school: improvement in the quality of sex education curriculum in schools to be prioritized in order to reduce teen pregnancy, which came out as top factor in lowering life expectancy.\nFood stamps programme: food stamps programme to be extended to other vulnerable segments of the population to combat food insecurities via affordable access to food, and to combat diabetes via healthier food options.\nSouthern states are in need the most: states in the south of the US, particularly Mississippi, Alabama and Louisiana are the key areas of focus where life expectancy is the lowest in order to bring the level in par with other parts of the country.\n\n""], 'url_profile': 'https://github.com/algakovic', 'info_list': ['MATLAB', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'MIT license', 'Updated Jan 10, 2020', '1', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Data-Mining-Project-1\nUtilized different regression techniques to observe differences in methods. Includes shrinkage and dimension reduction methods.\nData utilized in the simulation was randomly generated with 70 percent of the rows being used as training data and the rest being test data.\nEach scenario is repeated three times with varying levels of standard deviation for noise generation.\nScenario 1 has the true model having 15 significant variables out of the 15 variables.\nScenario 2 has the true model having 4 significant variables out of the 700 variables.\n'], 'url_profile': 'https://github.com/Zibble55', 'info_list': ['MATLAB', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'MIT license', 'Updated Jan 10, 2020', '1', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': [""This program reads data in a CSV file provided by a race car's datalogger and plots the variables of your choosing\nin a html file that relfects the selected variables. Run the program and you will be prompted to compare X and Y variables of your choosing.\nIn the background this program:\n\ncleans CSV file to identify header\nrenames columns of the CSV file to something that is more recognizeable\nmakes all variables absolute values to accurately represent cause and effect\ncondenses the data to be one dimensional for statistical purposes\nconcatenantes strings with other variables to create labels, legends, and file names\ncalculates all relevant statistical using SciPy, NumPy, and sklearn - prints to terminal\nPlots regression line\n\nYour variable choices are below:\n\nTime: Time throughout the two laps worth of data in 0.05 second increments\nDistance: Distance traveled throughout the lap\nBrake Pressure: Braking force applied by driver\nSteering Angle: Steering angle applied by driver (negative is left, positive is right)\nVertical GForce: The force of the vehicle rising or lowering on the springs\nLateral GForce: The force resulting of braking or accelerating\nLongitudinal GForce: The force resulting of navigating turns\nGear: Gear that the vehicle is in at the given Time\nRPM: Revolutions Per Minute of the engine\nThrottle%: The percent of throttle application by the driver\nMPH: Speed of vehicle in Miles Per Hour\n\nThe program will request X and Y variables, please type the variables you would like to see and the program will plot them\ninto the html file. You will receive an error if the variable that you typed is not in the dataset\nNEW IMPLEMENTATIONS WILL INCLUDE\n\nUse wrapper or a second class for the assign to automatically calculate its executions and therefore execute\nthe Y-Axis language automatically after the X-variable has been assigned\n\n""], 'url_profile': 'https://github.com/crossedapex', 'info_list': ['MATLAB', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'MIT license', 'Updated Jan 10, 2020', '1', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'San Jose, CA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Time-Series-Analysis\nForecasting models built on bike ridership dataset.\nData used uploded inside the data folder.\nFile Guide\nForecasting bike rentals- EDA.ipynb : exploratory data analysis and visualizations, helping better understand the data and modelling decisions.\nForecasting bike ridership.R : .R script to generate a forecast using an ARIMA model of order c(5,1,7) implemented and cross-validated using tsCV().\nResults\n\nARIMA:\nMAPE: 12.07%\nRMSE: 63.02\n\nProphet:\nMAPE: 10.28%\nRMSE: 51.06\n'], 'url_profile': 'https://github.com/raviswanath', 'info_list': ['MATLAB', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'MIT license', 'Updated Jan 10, 2020', '1', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kashwani78nand', 'info_list': ['MATLAB', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'MIT license', 'Updated Jan 10, 2020', '1', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Coursera Regression Models Course Project\nContext\nYou work for Motor Trend,\xa0a magazine about the automobile industry. Looking at a data set of a collection of cars, they are interested in exploring the relationship between a set of variables and miles per gallon (MPG) (outcome). They are particularly interested in the following two questions:\n\nIs an automatic or manual transmission better for MPG\nQuantify the MPG difference between automatic and manual transmissions\n\nQuestion\nTake the mtcars data set and write up an analysis to answer their question using regression models and exploratory data analyses.\nYour report must be:\n\nWritten as a PDF printout of a compiled (using knitr) R markdown document.\nBrief. Roughly the equivalent of 2 pages or less for the main text. Supporting figures in an appendix can be included up to 5 total pages including the 2 for the main report. The appendix can only include figures.\nInclude a first paragraph executive summary.\n\nUpload your PDF by clicking the Upload button below the text box.\nPeer Grading\n\nThe criteria that your classmates will use to evaluate and grade your work are shown below.\xa0\nEach criteria is binary: (1 point = criteria met acceptably; 0 points = criteria not met acceptably)\nYour Course Project score will be the sum of the points and will count as 40% of your final grade in the course.\n\nGrading Questions\n\nDid the student interpret the coefficients correctly?\nDid the student do some exploratory data analyses?\nDid the student fit multiple models and detail their strategy for model selection?\nDid the student answer the questions of interest or detail why the question(s) is (are) not answerable?\nDid the student do a residual plot and some diagnostics?\nDid the student quantify the uncertainty in their conclusions and/or perform an inference correctly?\nWas the report brief (about 2 pages long) for the main body of the report and no longer than 5 with supporting appendix of figures?\nDid the report include an executive summary?\nWas the report done in Rmd (knitr)?\n\n'], 'url_profile': 'https://github.com/Ahmed-Aboshady', 'info_list': ['MATLAB', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'MIT license', 'Updated Jan 10, 2020', '1', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': ['GomokuAI\nAI trained to play Gomoku through linear regression training while utilizing alpha-beta pruning.\nThe provided pickle files are the result of different training sets.\nMain is in PlayGomoku.py\n'], 'url_profile': 'https://github.com/byun-sungwoo', 'info_list': ['MATLAB', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'MIT license', 'Updated Jan 10, 2020', '1', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '121 contributions\n        in the last year', 'description': ['Predicting fuel consumption and CO2 emission of cars using Multiple Linear Regression\nIn this project, I have created a regression model which predicts CO2 emission and fuel consumption depending on\nsome other attributes.\n\nIntroduction to the dataset :\n\nThe dataset FuelConsumptionCo2.csv has these following columns:\n\nMODELYEAR e.g. 2014\nMAKE e.g. Acura\nMODEL e.g. ILX\nVEHICLE CLASS e.g. SUV\nENGINE SIZE e.g. 4.7\nCYLINDERS e.g 6\nTRANSMISSION e.g. A6\nFUELTYPE e.g. z\nFUEL CONSUMPTION in CITY(L/100 km) e.g. 9.9\nFUEL CONSUMPTION in HWY (L/100 km) e.g. 8.9\nFUEL CONSUMPTION COMB (L/100 km) e.g. 9.2\nCO2 EMISSIONS (g/km) e.g. 182   --> low --> 0\n\n\nLoading data :\n\n\'\'\' Importing libraries \'\'\'\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pylab as pl\nimport numpy as np\n%matplotlib inline\n\n# loading the dataset into an pandas.DataFrame object using pandas.read_csv() method\ndf = pd.read_csv(""FuelConsumption.csv"")\ndf.head() # first few rows of the dataset\nHere I have taken only ENGINESIZE, CYLINDERS, FUELCONSUMPTION_CITY, FUELCONSUMPTION_HWY and FUELCONSUMPTION_COMB as independent variables and CO2EMISSIONS is dependent. Goal of this model is to predict CO2EMISSIONS for given values of other attrs.\ncdf = df[[\'ENGINESIZE\',\'CYLINDERS\',\'FUELCONSUMPTION_CITY\',\'FUELCONSUMPTION_HWY\',\'FUELCONSUMPTION_COMB\',\'CO2EMISSIONS\']]\ncdf.head(5)\nNow, lets check dependency of CO2EMISSIONS on ENGINESIZE:\nplt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,  color=\'blue\')\nplt.xlabel(""Engine size"")\nplt.ylabel(""Emission"")\nplt.show()\n\nSimilarly, other variables choosed here will show linear dependency with the CO2EMISSIONS variable.\n\nSplitting training and testing data :\n\nThere need a way to evaluate how good my model is, and for that we need data which was not used in training the model. That\'s why\nsplitting the dataset into two parts: one part will be used for training the model and other part for testing it. For splitting, I have\nused boolean mask indexing on the dataframe to get train. The rest of it will be testing data test.\nmsk = np.random.rand(len(df)) < 0.8\ntrain = cdf[msk]\ntest = cdf[~msk]\nWe need to check if the data is taken randomly and not taking a part of dataset in train data. To check that, plotting CO2EMISSIONS of train data with ENGINESIZE. If the output is not scattered over the dataset, then we need to run again. When I ran it, it produced\nthe output as shown below:\nplt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color=\'blue\')\nplt.xlabel(""Engine size"")\nplt.ylabel(""Emission"")\nplt.show()\n\n\nBuilding the regression model\n\nHere we have multiple independent variables which predicts the CO2EMISSION. We will use linear_model.LinearRegression() from\nsklearn package to built the model.\nfrom sklearn import linear_model\nregr = linear_model.LinearRegression()\nx = np.asanyarray(train[[\'ENGINESIZE\',\'CYLINDERS\',\'FUELCONSUMPTION_COMB\']])\ny = np.asanyarray(train[[\'CO2EMISSIONS\']])\nregr.fit (x, y)\n# The coefficients\nprint (\'Coefficients: \', regr.coef_)\nThe coefficient and intercept are the parameters of the fit hyper-plane.\n\nPrediction :\n\ny_hat= regr.predict(test[[\'ENGINESIZE\',\'CYLINDERS\',\'FUELCONSUMPTION_COMB\']])\nx = np.asanyarray(test[[\'ENGINESIZE\',\'CYLINDERS\',\'FUELCONSUMPTION_COMB\']])\ny = np.asanyarray(test[[\'CO2EMISSIONS\']])\nprint(""Residual sum of squares: %.2f""\n      % np.mean((y_hat - y) ** 2))\n\n# Explained variance score: 1 is perfect prediction\nprint(\'Variance score: %.2f\' % regr.score(x, y))\nIn my model, the Variance score came 0.86. The best possible variance score is 1.0.\n'], 'url_profile': 'https://github.com/t-gos7', 'info_list': ['MATLAB', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'MIT license', 'Updated Jan 10, 2020', '1', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'Bengaluru, India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Advanced-Logistic-regression--Telecom-Churn\nBusiness Problem Overview>>\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\nFor many incumbent operators, retaining high profitable customers is the number one business goal.\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\nIn this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\nUnderstanding and Defining Churn>>\nThere are two main models of payment in the telecom industry - postpaid (customers pay a monthly/annual bill after using the services) and prepaid (customers pay/recharge with a certain amount in advance and then use the services).\nIn the postpaid model, when customers want to switch to another operator, they usually inform the existing operator to terminate the services, and you directly know that this is an instance of churn.\nHowever, in the prepaid model, customers who want to switch to another network can simply stop using the services without any notice, and it is hard to know whether someone has actually churned or is simply not using the services temporarily (e.g. someone may be on a trip abroad for a month or two and then intend to resume using the services again).\nThus, churn prediction is usually more critical (and non-trivial) for prepaid customers, and the term ‘churn’ should be defined carefully.  Also, prepaid is the most common model in India and southeast Asia, while postpaid is more common in Europe in North America.\nThis project is based on the Indian and Southeast Asian market.\nDefinitions of Churn>>\nThere are various ways to define churn, such as:\nRevenue-based churn: Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as ‘customers who have generated less than INR 4 per month in total/average/median revenue’.\nThe main shortcoming of this definition is that there are customers who only receive calls/SMSes from their wage-earning counterparts, i.e. they don’t generate revenue but use the services. For example, many users in rural areas only receive calls from their wage-earning siblings in urban areas.\nUsage-based churn: Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time.\nA potential shortcoming of this definition is that when the customer has stopped using the services for a while, it may be too late to take any corrective actions to retain them. For e.g., if you define churn based on a ‘two-months zero usage’ period, predicting churn could be useless since by that time the customer would have already switched to another operator.\nIn this project, you will use the usage-based definition to define churn.\nHigh-value Churn>>\nIn the Indian and the southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.\nIn this project, you will define high-value customers based on a certain metric (mentioned later below) and predict churn only on high-value customers.\nUnderstanding the Business Objective and the Data\nThe dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively.\nThe business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.\nUnderstanding Customer Behaviour During Churn>>\nCustomers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are three phases of customer lifecycle :\nThe ‘good’ phase: In this phase, the customer is happy with the service and behaves as usual.\nThe ‘action’ phase: The customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‘good’ months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor’s offer/improving the service quality etc.)\nThe ‘churn’ phase: In this phase, the customer is said to have churned. You define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1/0 based on this phase, you discard all data corresponding to this phase.\nIn this case, since you are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month is the ‘churn’ phase.\nThe data dictionary contains meanings of abbreviations. Some frequent ones are loc (local), IC (incoming), OG (outgoing), T2T (telecom operator to telecom operator), T2O (telecom operator to another operator), RECH (recharge) etc.\nThe attributes containing 6, 7, 8, 9 as suffixes imply that those correspond to the months 6, 7, 8, 9 respectively.\nData Preparation>>\nThe following data preparation steps are crucial for this problem:\n\nDerive new features>>\n\nThis is one of the most important parts of data preparation since good features are often the differentiators between good and bad models. Use your business understanding to derive features you think could be important indicators of churn.\n\nFilter high-value customers\n\n\n\n\n\nAs mentioned above, you need to predict churn only for the high-value customers. Define high-value customers as follows: Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).\nAfter filtering the high-value customers, you should get about 29.9k rows.\n\nTag churners and remove attributes of the churn phase>>\n\nNow tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes you need to use to tag churners are:\ntotal_ic_mou_9\ntotal_og_mou_9\nvol_2g_mb_9\nvol_3g_mb_9\nAfter tagging churners, remove all the attributes corresponding to the churn phase (all attributes having ‘ _9’, etc. in their names).\nModelling>>\nBuild models to predict churn. The predictive model that you’re going to build will serve two purposes:\nIt will be used to predict whether a high-value customer will churn or not, in near future (i.e. churn phase). By knowing this, the company can take action steps such as providing special plans, discounts on recharge etc.\nIt will be used to identify important variables that are strong predictors of churn. These variables may also indicate why customers choose to switch to other networks.\nIn some cases, both of the above-stated goals can be achieved by a single machine learning model. But here, you have a large number of attributes, and thus you should try using a dimensionality reduction technique such as PCA and then build a predictive model. After PCA, you can use any classification model.\nAlso, since the rate of churn is typically low (about 5-10%, this is called class-imbalance) - try using techniques to handle class imbalance.\nYou can take the following suggestive steps to build the model:\nPreprocess data (convert columns to appropriate formats, handle missing values, etc.)\nConduct appropriate exploratory analysis to extract useful insights (whether directly useful for business or for eventual modelling/feature engineering).\nDerive new features.\nReduce the number of variables using PCA.\nTrain a variety of models, tune model hyperparameters, etc. (handle class imbalance using appropriate techniques).\nEvaluate the models using appropriate evaluation metrics. Note that is is more important to identify churners than the non-churners accurately - choose an appropriate evaluation metric which reflects this business goal.\nFinally, choose a model based on some evaluation metric.\nThe above model will only be able to achieve one of the two goals - to predict customers who will churn. You can’t use the above model to identify the important features for churn. That’s because PCA usually creates components which are not easy to interpret.\nTherefore, build another model with the main objective of identifying important predictor attributes which help the business understand indicators of churn. A good choice to identify important variables is a logistic regression model or a model from the tree family. In case of logistic regression, make sure to handle multi-collinearity.\nAfter identifying important predictors, display them visually - you can use plots, summary tables etc. - whatever you think best conveys the importance of features.\nFinally, recommend strategies to manage customer churn based on your observations.\nNote: Everything has to be submitted in one Jupyter notebook.\n'], 'url_profile': 'https://github.com/krunalchalakh', 'info_list': ['MATLAB', 'Updated Nov 20, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'R', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 20, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'MIT license', 'Updated Jan 10, 2020', '1', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}"
"{'location': 'United States', 'stats_list': [], 'contributions': '445 contributions\n        in the last year', 'description': ['rotten-tomatoes-prediction\nUsed gradient boosting regression to predict Rotten Tomatoes scores for movies of various genres. Dataset retrieved from Kaggle, and categorical encoding was done on my own. Will update this soon, but some quick facts: mean absolute error for predicting Tomatometer score was about 19. I was able to reduce it further by increasing the number of estimators, but the training time did take much longer. Films rated NR showed the least mean absolute error, about 12.\n\nJump to the program.\nJump to quick discussion.\n\n'], 'url_profile': 'https://github.com/kaisubr', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Updated Jan 10, 2020', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'MATLAB', 'Updated Jun 12, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['An-Analysis-of-Wine-Quality\nProject to find predictors in the quality of wine using linear regression\n'], 'url_profile': 'https://github.com/rtora', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Updated Jan 10, 2020', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'MATLAB', 'Updated Jun 12, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Reg-HousePrices\nKaggle Ames Housing Data Set https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview\n'], 'url_profile': 'https://github.com/rosonaeldred', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Updated Jan 10, 2020', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'MATLAB', 'Updated Jun 12, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/salauayobami', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Updated Jan 10, 2020', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'MATLAB', 'Updated Jun 12, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'Fukuoka, Japan', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['house-price\nThis is a seriously-done project. Spent a lot of time and effort for this. It was a part of the training program I attended when started learning machine learning\n\nHouse Price project from kaggle. https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/WetGlasses', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Updated Jan 10, 2020', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'MATLAB', 'Updated Jun 12, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anna1027', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Updated Jan 10, 2020', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'MATLAB', 'Updated Jun 12, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['wine_quality_machine_learning\nUsing Matlab to compare Logistic Regression and Random Forest when predicting wine quality.\n'], 'url_profile': 'https://github.com/Gordontd16', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Updated Jan 10, 2020', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'MATLAB', 'Updated Jun 12, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'noosphere', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['\n\nInfo:\nBrAInStocker tries to predict (using Linear Regression) the next number within a series of random numbers.\nInstalling:\nThis tool runs on many platforms and it requires Python (3.x.y). To generate graphs, you need to install the following library:\n   python3-matplotlib - Python based plotting system in a style similar to Matlab (Python 3)\n\nOn Debian-based systems (ex: Ubuntu), run:\n   sudo apt-get install python3-matplotlib\n\nOr:\n   pip3 install matplotlib\n\nExecuting:\npython3 brainstocker\n\nSource libs:\n\nPyMatplotlib: https://pypi.python.org/pypi/matplotlib\n\nLicense:\nBrAInStocker is released under the GPLv3.\nContact:\n  - psy (epsylon@riseup.net)\n\nContribute:\nTo make donations use the following hash:\n - Bitcoin: 19aXfJtoYJUoXEZtjNwsah2JKN9CK5Pcjw\n\n\nScreenshots:\n\n'], 'url_profile': 'https://github.com/epsylon', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Updated Jan 10, 2020', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'MATLAB', 'Updated Jun 12, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Non Linear Transformation\n\nImplementation of non linear transformation on non linear data set\nApplying linear regression in multiple variable\nThe equation solve using Gaussian Elimination\n\nExecution\n$ gcc non-linear.c -lm\n$ ./a.out\n'], 'url_profile': 'https://github.com/kindlerprince', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Updated Jan 10, 2020', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'MATLAB', 'Updated Jun 12, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Ham-Spam-Classifier\nA ham-spam classifier implementing logistic regression. Was submitted for a Kaggle competition and ranked in the top 13% with a 95% accuracy.\nImplementation Framework-\n\nExploratory Data Analysis\nFeature Engineering\nClassification using Logistic Regression\n    \nBasic - using top word frequency\nAdvanced - additional feature extraction & transformation - \n      \nSubject length, body length, word counts, punctuation count, capitalized letter count\nMin-Max Scaling\nPower-3 Polynomal features creation\n\n\nModel Evaluation\n\n'], 'url_profile': 'https://github.com/Qutubkhan', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 10, 2020', 'Updated Jan 10, 2020', 'Updated Jan 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'MATLAB', 'Updated Jun 12, 2020', '1', 'Python', 'Updated Jan 12, 2020', 'C', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020']}"
"{'location': 'Vellore', 'stats_list': [], 'contributions': '548 contributions\n        in the last year', 'description': ['A Car Purchase Prediction Model\nA prediction model implemented using artificial neural networks and linear regression.\nAlgorithms Used\nLinear Regression\nA linear approach to modeling linear a relationship between a dependent variable(y) and one or more independent variables(x).\nArtificial Neural Networks\nIt is a multi-layered structure where every node in one layer is connected to every other node in the next layer. It consists of an input layer, multiple hidden layers, and an output layer. This model can be made deeper and more efficient by increasing the number of layers.\nBack Propogation\nBackpropagation is a method used to train ANNs by calculating gradient needed to update network weights. It is commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function.\nImplementation\nDataset Provided\nThe given dataset consists of parameters like :\n\nCustomer Name\nCustomer e-mail\nCountry\nGender\nAge\nAnnual Salary\nCredit Card Debt\nNet Worth\n\nThe model is supposed to provide an estimate amount of money an indivual can spend on buying a car based on these parameters.\n'], 'url_profile': 'https://github.com/VaishnaviNandakumar', 'info_list': ['Jupyter Notebook', 'Updated Oct 4, 2020', 'Python', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020']}","{'location': 'Lahore, Pakistan', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ayeshatahreem', 'info_list': ['Jupyter Notebook', 'Updated Oct 4, 2020', 'Python', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020']}","{'location': 'Lahore, Pakistan', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ayeshatahreem', 'info_list': ['Jupyter Notebook', 'Updated Oct 4, 2020', 'Python', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020']}","{'location': 'Bengaluru, India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Lead-Score-case-study\nProblem Statement>>\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses.\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%.\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as ‘Hot Leads’. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. A typical lead conversion process can be represented using the following funnel:\nLead Conversion Process - Demonstrated as a funnel\nAs you can see, there are a lot of leads generated in the initial stage (top) but only a few of them come out as paying customers from the bottom. In the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.\nX Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.\nData\nYou have been provided with a leads dataset from the past with around 9000 data points. This dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not. The target variable, in this case, is the column ‘Converted’ which tells whether a past lead was converted or not wherein 1 means it was converted and 0 means it wasn’t converted. You can learn more about the dataset from the data dictionary provided in the zip folder at the end of the page. Another thing that you also need to check out for are the levels present in the categorical variables. Many of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value (think why?).\nGoals of the Case Study>>\nThere are quite a few goals for this case study.\nBuild a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.\nThere are some more problems presented by the company which your model should be able to adjust to if the company's requirement changes in the future so you will need to handle these as well. These problems are provided in a separate doc file. Please fill it based on the logistic regression model you got in the first step. Also, make sure you include this in your final PPT where you'll make recommendations.\nResults Expected>>\nA well-commented Jupyter note with at least the logistic regression model, the conversion predictions and evaluation metrics.\nThe word document filled with solutions to all the problems.\nThe overall approach of the analysis in a presentation\nMention the problem statement and the analysis approach briefly\nExplain the results in business terms\nInclude visualisations and summarise the most important results in the presentation\nYou need to submit the following three components:\nPython commented file: Should include detailed comments and should not contain unnecessary pieces of code.\nWord File: Answer all the questions asked by the company in the word document provided.\nPresentation:  Make a presentation to present your analysis to the chief data scientist of your company (and thus you should include both technical and business aspects). The presentation should be concise, clear, and to the point. Submit the presentation after converting it into PDF format.\n""], 'url_profile': 'https://github.com/krunalchalakh', 'info_list': ['Jupyter Notebook', 'Updated Oct 4, 2020', 'Python', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akash97715', 'info_list': ['Jupyter Notebook', 'Updated Oct 4, 2020', 'Python', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Bank-Marketing\nProject for Applied Statistics. Advanced regression techniques to predict term-deposit subscriptions.\n'], 'url_profile': 'https://github.com/cj-nava', 'info_list': ['Jupyter Notebook', 'Updated Oct 4, 2020', 'Python', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/danieldis', 'info_list': ['Jupyter Notebook', 'Updated Oct 4, 2020', 'Python', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['CCF-2019-Sales-Forecast-of-Passenger-Vehicle-Segment-Market\nMethod1: ARIMA\nARIMA.ipynb\ncredit: Yin Tang\nMethod 2: Model Selection and Averaging for Linear Regression.\nModel_selection_averaging.ipynb\ncredit: Xingzhi Sun\n'], 'url_profile': 'https://github.com/xingzhis', 'info_list': ['Jupyter Notebook', 'Updated Oct 4, 2020', 'Python', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tiagofpaa', 'info_list': ['Jupyter Notebook', 'Updated Oct 4, 2020', 'Python', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['R-linear-regression-and-hyp-testing\nR-code for regression and hypothesis testing analyis of GDP vs Currency Exchange rates\n'], 'url_profile': 'https://github.com/Bechkate', 'info_list': ['Jupyter Notebook', 'Updated Oct 4, 2020', 'Python', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Updated Jan 7, 2020']}"
"{'location': 'New York', 'stats_list': [], 'contributions': '137 contributions\n        in the last year', 'description': ['House-Value-Prediction\nThis project aims to build linear regression models to predict median house value(medv) in Boston. The dataset is composed by 506 neighborhoods around Boston, including medv and 13 predictors such as rm (average number of rooms per house), age (average age of houses), and lstat (percent of households with low socioeconomic status).\nThis data frame contains the following columns:\n\n\ncrim: per capita crime rate by town.\n\n\nzn: proportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nindus: proportion of non-retail business acres per town.\n\n\nchas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\n\nnox: nitrogen oxides concentration (parts per 10 million).\n\n\nrm: average number of rooms per dwelling.\n\n\nage: proportion of owner-occupied units built prior to 1940.\n\n\ndis: weighted mean of distances to five Boston employment centres.\n\n\nrad: index of accessibility to radial highways.\n\n\ntax: full-value property-tax rate per $10,000.\n\n\nptratio: pupil-teacher ratio by town.\n\n\nblack: $1000(B_{k}-0.63)^2$ where $B_{k}$ is the proportion of blacks by town.\n\n\nlstat: lower status of the population (percent).\n\n\nmedv: median value of owner-occupied homes in $1000s.\n\n\n'], 'url_profile': 'https://github.com/csuustc', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Go', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'North Dakota', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Predicting Fleet Sales Price\nThe goal of this project is to explore the Seattle fleet sold equipment Disease dataset and use machine learning to predict whether a patient has heart disease.\nData\nThis dataset includes sales data for fleet equipment that was sold in the current and previous three years and includes 11 columns. This dataset does not include sales data for Seattle City Light (SCL) fleet equipment.\nSource: Seattle Sold Fleet Equipment\n'], 'url_profile': 'https://github.com/andrew-block', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Go', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '210 contributions\n        in the last year', 'description': ['Problem Statement\nBusiness Problem Overview\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\nFor many incumbent operators, retaining high profitable customers is the number one business goal.\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\nIn this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\nModel Comparisions\n\n\n\nSr. No.\nModel\nAccuracy\nPrecision\nRecall\nAUC\nF1\n\n\n\n\n0\nSVM (Default)-linear\n0.83\n0.80\n0.30\n0.82\n0.44\n\n\n1\nSVM (Default)-rbf\n0.87\n0.74\n0.37\n0.81\n0.49\n\n\n2\nSVM( rfb ) [Hyper]\n0.92\n0.48\n0.49\n0.72\n0.48\n\n\n3\nRandomForest (Default)\n0.91\n0.49\n0.44\n0.72\n0.46\n\n\n4\nRandomForest (Hyper)\n0.90\n0.67\n0.41\n0.79\n0.51\n\n\n5\nXGBoost (Default)\n0.86\n0.75\n0.33\n0.81\n0.46\n\n\n6\nXGBoost (Hyper Tuned )\n0.85\n0.75\n0.32\n0.81\n0.45\n\n\n\nConculsion\n\nVM with tuned hyperparameters produce best result on this dataset with 0.92 accuracy.\nRandom forest also produce good accuracy with 0.91 (default overfit model) and 0.90 with tuned hyperparameters.\nXGBoost also produce apt accuracy of 0.86 (default overfit model) and 0.85 with tuned hyperparameters.\nAs per our analysis SVM and Random forest produce best accuracy and models can be selected to predict churn data for future dataset or production.\n\n'], 'url_profile': 'https://github.com/bhargrah', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Go', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Regression-using-LASSO-and-Boosting\nMain Tasks:\n\nPre-process data and impute the missing values using ""mean substitution"" method\nPlot a correlation matrix for the features in the dataset\nSelect 11 features with highest Coefficient of Variation and plotting scatter plots and box plots for them\nPerform Linear Regression using least squares, LASSO Regression (with alpha chosen using Cross Validation) and Ridge Regression (with alpha chosen using Cross Validation) and compare the results\nFit a Principal Component Regression model with M (number of principal components) chosen by Cross Validation and comparing the results\nFit L1-penalized gradient boosting tree using XGBoost (with alpha chosen using Cross Validation) and compare the performance\n\n'], 'url_profile': 'https://github.com/ShivaniBiradar', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Go', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'Minsk, Belarus', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['House price prediction with linear regression implemented in\ngolang, csharp, python:\n\n1 investigate the data\n2 prepare the data\n3 train the model\n4 test the model\n5 visualising the model\n\n'], 'url_profile': 'https://github.com/TerminusDeus', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Go', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'Los Angeles, CA', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': [""NBA Draft Regression Analysis.ipynb\nNotebook outlining the methods of analysis, interpretations, and conclusions. The main goal of this analysis is to look at how biometric information\nand player contribution in box-score statistics can be projected onto the draft. The final schema used is as follows:\n\n\n\nVariable\nDescription\n\n\n\n\nplayer_height\nPlayer height (cm)\n\n\nplayer_weight\nPlayer weight (kg)\n\n\ndraft_number\nDraft position (1-82)\n\n\ngp\nGames played per season\n\n\npts\nPoints per game\n\n\nreb\nRebounds per game\n\n\nast\nAssists per game\n\n\nnet_rating\nNet rating - team's point differential per 100 possessions while player is on the floor\n\n\noreb_pct\nOffensive rebound percentage - percentage of available offensive rebounds a player grabbed while on the floor\n\n\ndreb_pct\nDefensive rebound percentage - percentage of available defensive rebounds a player grabbed while on the floor\n\n\nusg_pct\nUsage percentage - percentage of team plays used by a player while on the floor\n\n\nts_pct\nTrue shooting percentage - measure of shooting efficiency that takes into account field goals, 3-point field goals, and free throws\n\n\nast_pct\nAssist percentage - percentage of teammate field goals a player assisted while on the floor\n\n\n\nNote: all variables are career averages\nThe original dataset can be found here.\n""], 'url_profile': 'https://github.com/jeffreylu0', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Go', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['Logistic-Regression-and-Titanic-Prediction\nMSDS Dr. Cao class assignment: using R language and logistic regression to predict survivors\n'], 'url_profile': 'https://github.com/glockkm', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Go', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['DIABETES-PREDICTION-USING-LOGISTIC-REGRESSION\nThe objective of this project is to predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset.         All patients are females and are at least 21 years old of Pima Indian heritage.\n'], 'url_profile': 'https://github.com/kingdukedav', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Go', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'San Francisco ', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/baa256', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Go', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Hard-work-Challenge-Linear-regression\ngiven the past data we have predicted the score that one would get given the amount of time spent by him on coding daily.\n'], 'url_profile': 'https://github.com/raghavangra', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Go', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'R', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['DIABETES-PREDICTION-USING-LOGISTIC-REGRESSION\nThe objective of this project is to predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset.         All patients are females and are at least 21 years old of Pima Indian heritage.\n'], 'url_profile': 'https://github.com/kingdukedav', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Jan 12, 2020', '3', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['This project used Linera-Regression model to predict image of cat or not.\n*sigmoid activation function was used for binary classification.\n'], 'url_profile': 'https://github.com/vikeshkr123', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Jan 12, 2020', '3', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Yakal', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Jan 12, 2020', '3', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Riyadh, Saudi Arabia', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Multivariate-linear-regression-using-MATLAB\nA multivariate linear regression to predict the mean January high temperature of Texas counties given their latitude, longitude, and elevation.\n'], 'url_profile': 'https://github.com/elaugh9', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Jan 12, 2020', '3', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-regression - Air Quality Prediction\nwe have 5 features to be used to predict the air quality index and we have the training data on basis of which we have to predict air quality index using linear regression.\n'], 'url_profile': 'https://github.com/raghavangra', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Jan 12, 2020', '3', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Macaíba/RN', 'stats_list': [], 'contributions': '161 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/WesleyLeocadio', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Jan 12, 2020', '3', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Chennai, India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Unsupervised-Learning---Clustering-cars-based-on-attributes\nAnalyzed cars dataset and performed exploratory data analysis and then categorized them using K means clustering. Used linear regression on the different clusters and estimated coefficients. Skills and Tools: K means clustering, Hierarchical clustering, EDA, Linear Regression\nThe Car dataset was used in the 1983 American Statistical Association Exposition. The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 2 multivalued discrete and 4 continuous variables.\nDataset: cars-dataset.csv\nAttribute Information:\nCar Name – Name of the car Cyl – No of cylinders in the car – Multivalued discrete Disp – Displacement – continuous Hp – Horsepower – continuous Wt – Weight – continuous Acc – Accleration – continuous Yr – Model of the car – Multivalued discrete\nSteps to follow:\nEDA & Pre-processing (Make sure to remove all non-numeric entries from numeric columns)\nUse pair plot or scatter matrix to visualize how the different variables are related (Hint: The amount of Gaussian curves in the plot should give a visual identification of different clusters existing in the dataset)\nUse K Means or Hierarchical clustering to find out the optimal no of clusters in the data. Identify and separate the clusters\nUse linear regression model on different clusters separately and print the coefficients of the models individually\n'], 'url_profile': 'https://github.com/Lalitha-radhakrishnan', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Jan 12, 2020', '3', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/2012vaibhav', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Jan 12, 2020', '3', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['ML-Flow \nMLFlow is the generalized package which is developed using the python and the H2o framework. The purpose of this function is to store the models meta data ie., for both classification and regression model.\n\nPreperation of model meta data:\n\ndata = pandas dataframe or H2o dataframe based on which the below flags are set,\nh2oframe, pdframe\nsplit = If the data is passed then based on this split - it divides the data in train and valid\nIf train & valid is passed direclty then this can be ignored\nmodel_uuid = unique id for user reference (If this is none an Unique ID will be generated automatically)\nmodel_type = classification/ regression\nmodel version = (by default Version1 is passed)\nmodel = actual model data (as of now H2o model)\nfeatures = list of features (X)\ntarget = target variable\nhyperparams_tuning = if the model is tuned based on grid search. Then list of tuned parameters needs to be passed\nmaximising metrics = for train/valid metrics - it generated the metrics details for different sets of threshold, So based on this maximising metrics the threshold and corresponding metrics are stores.\nplots_show = To display all the metrics graphs for the passed model.  For both train and validation data.\n\nThe use case I have solved in this module is to get the model meta data.\n\n\n\nThe modules handled in this pacakges are:\n\nDoes the model training and the predicition, based on the inputs passed. ( The inputs passed can be either a pandas or H2o framework\ncorrespondingly the respective flags should be set.\nIt also handles the hyper parameter tuning to get the best parameters.\nEvaluation fo the model (Please refer to my other repo Model Evaluation\nGetting the best threshold for Classification models by maximising a metrics.\nAnd then creates a overall information about the model - which is the model meta data in the json format\n\n\nSample Execution Output:\ngetting the actual and predicted values...\ngbm prediction progress: |████████████████████████████████████████████████| 100%\ngbm prediction progress: |████████████████████████████████████████████████| 100%\nGetting the metrics details for the train data...\nGetting the metrics details for the validation data...\nModel UUID is not given - random UUID is created\nModel Tag is not given - NULL is passed\nModel version is not given - \'Version-1\' is passed\nModel type is not given - None is passed\nhyperparams_tuning is not given - None is passed\nSeed value is not passed in the Model - seed = 0 is passed as default\nNo cross-validation metrics summary for this model\nCV is not performed in the Model - NULL is passed\n  .........\nSample meta data:\n{`Model_UUID`: `17b68386-d49a-11e9-8bff-3c2c30d1ea45`,\n `Model_Tag`: `None`,\n `Model_Version`: `Version-1`,\n `Model_Type`: `None`,\n `Rundate`: datetime.datetime(2019, 9, 11, 13, 43, 9, 944992),\n `Trainining_Start_Time`: datetime.datetime(2019, 9, 11, 13, 43, 9, 944992),\n `Traning_End_Time`: datetime.datetime(2019, 9, 11, 13, 43, 9, 944992),\n `Total_Training_Time`: None,\n `Trained_By`: `Haribaskar.d`,\n `Input_Features`: ""[`X1`, `X2`, `X3`, `X4`]"",\n `Hyperparams_Tuning`: `None`,\n `Hyperparams`: """",\n `Seed_Value`: 0,\n `Training_Data_Reference`: None,\n `Test_Data_Reference`: None,\n `Model_Path`: None,\n `Model`: `NULL`,\n `Feature_Importance`: ""[(`X1`, 2339.82958984375, 1.0, 0.7399357915895), (`X2`, 449.2460632324219, 0.19199947944175788, 0.14206728680550904), (`X3`, 334.8724670410156, 0.14311831446809675, 0.10589836330690615), (`X4`, 38.25813674926758, 0.016350821835628804, 0.01209855829808488)]"",\n `Feature_Distribution`: ""{`X1`: {`mean`: 0.06662887410836711, `std`: 0.9717070855781386, `missing%`: 0.0}, `X2`: {`mean`: 0.0823587388025514, `std`: 0.9579708217449495, `missing%`: 13.906272740503567}, `X3`: {`mean`: 0.07412937544293083, `std`: 0.9615146723706843, `missing%`: 28.882258768738172}, `X4`: {`mean`: 0.44178431087177994, `std`: 0.7264934356403574, `missing%`: 0.0}}"",\n `Model_Accuracy_Metrics_Cv`: `None`,\n `Model_Accuracy_Metrics_Training`: ""[{`TP`: 7662.0, `FP`: 651.0, `FN`: 1022.0, `TN`: 1659.0, `Accuracy`: 0.8478260869565217, `Precision0`: 0.618798955613577, `Precision1`: 0.9216889209671598, `recall0`: 0.7181818181818181, `recall1`: 0.8823122984799632, `f1`: 0.9015708654468435, `mcc`: 0.5697014920234555, `roc_auc`: 0.8002470583308907}]"",\n `Model_Accuracy_Metrics_Validation`: ""[{`TP`: 1788.0, `FP`: 153.0, `FN`: 383.0, `TN`: 424.0, `Accuracy`: 0.8049490538573508, `Precision0`: 0.5254027261462205, `Precision1`: 0.9211746522411128, `recall0`: 0.7348353552859619, `recall1`: 0.8235836020267158, `f1`: 0.8696498054474707, `mcc`: 0.4993768857270868, `roc_auc`: 0.7792094786563388}]"",\n `Model_Parameters`: None,\n `Model_Summary_Stats`: None,\n `Model_Plots`: None,\n `Training_Count`: 10994,\n `Validation_Count`: 2748,\n `Total_Count`: 13742,\n `Best_Threshold`: 0.67,\n `Best_Metric`: `mcc`,\n `Threshold_Used`: 0.67,\n `Created_Date`: datetime.datetime(2019, 9, 11, 13, 43, 10, 188889),\n `Created_By`: `Haribaskar.d`,\n `Modified_Date`: datetime.datetime(2019, 9, 11, 13, 43, 10, 188889),\n `Modified_By`: None}\n\n\nContributing\nPatches are welcome, preferably as pull requests.\n\n'], 'url_profile': 'https://github.com/hari2594', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Jan 12, 2020', '3', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Madrid', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/charlstown', 'info_list': ['Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 12, 2020', 'MATLAB', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 7, 2020', 'R', 'Updated Jan 12, 2020', '3', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}"
"{'location': 'Chennai, India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Lalitha-radhakrishnan', 'info_list': ['HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tsdmrfth', 'info_list': ['HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Regression-Analysis-of-Systolic-Blood-Pressure\nWe perform data analysis on a dataset containing the blood pressure data of 199 individuals. We begin with some explanatory data analysis, followed by building regression models based on stepwise selection and all subsets selection. The objective is to predict Systolic Blood Pressure from the 8 given explanatory variables.\n'], 'url_profile': 'https://github.com/VidyutRao', 'info_list': ['HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': [""Chances-of-Admit-Predictor-Using-Logistic-Regression\nThis project uses Logistic Regression to estimate the Chances of Admission to a particular class of University for Graduate Studies.\nCitation for the dataset -\nMohan S Acharya, Asfia Armaan, Aneeta S Antony : A Comparison of Regression Models for Prediction of Graduate Admissions, IEEE International Conference on Computational Intelligence in Data Science 2019\nThe dataset used is available here.\nThe dataset consists of the following features -\n\nSerial No. (was not abbreviated in the project)\nGRE Score (abbreviated as GRE in the project)\nTOEFL Score (abbreviated as TOEFL in the project)\nUniversity Rating (abbreviated as UR in the project)\nSOP (was not abbreviated in the project)\nLOR (was not abbreviated in the project)\nCGPA (was not abbreviated in the project)\nResearch (was not abbreviated in the project)\nChance of Admit (abbreviated as COA in the project)\n\nThe task is to predict target feature Chance of Admit using all the input features excluding the Serial No.\nThe dataset shows the target feature in terms of probability which means that Chances of Admit or COA will be a real number between 0 and 1. Thus, the target feature is a continuous variable. However, Logistic Regression is a classification algorithm and needs finite number of target classes for prediction.\nIn order to resolve this, the target feature is split into 3 categories:\n\nClass 0: AMBITIOUS (Chances of Admit <= 0.5 or 50%)\nClass 1: REALISTIC (Chances of Admit between 0.5-0.85 or between 50%-85%)\nClass 2: AMBITIOUS (Chances of Admit > 0.85 or 85%)\n\nAfter training the model and testing it's accuracy, the model is used to predict the Chances of Admit (SAFE, REALISTIC, AMBITIOUS) based on a profile entered by the user.\n""], 'url_profile': 'https://github.com/yashveersinghsohi', 'info_list': ['HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'LIMBE CAMEROON', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Logistic-Regression-on-Epileptic-Seizure-Recognition-Dataset\nEpileptic seizures are caused by a disturbance in the electrical activity of the brain. There are\nmany different types of epileptic seizures. Any of us could potentially have a single epileptic\nseizure at some point in our lives.\nhere we build a model that classifies between those who have epileptic seizures and those who have not.\n'], 'url_profile': 'https://github.com/cyrille-feu', 'info_list': ['HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NIT Surathkal, Mangalore, India', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Classification-And-Regression-Models-K-Nearest-Neighbors\nExploring Machine Learning\nThis was done under the guidance of faculties of Applied ai.\nReference : https://www.appliedaicourse.com/\n'], 'url_profile': 'https://github.com/ranjan103', 'info_list': ['HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'Buffalo', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sonaliye', 'info_list': ['HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'Erode', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['LOGISTIC-REGRESSION-FOR-CUSTOMER-CHURN-PREDICTION\nAIM : TO PREDICT WHICH CUSTOMER HAS CHURN THE SERVICES FROM A COMPANY\nLETS EGI WITH SPARK\n'], 'url_profile': 'https://github.com/dr-rubeshanand', 'info_list': ['HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SangeethaSA', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Predicting-Customer-Revenue-Using-Linear-Regression\nThe model will generalize to future years to predict the next year spending behavior in advance, unless the market or business changed significantly\n'], 'url_profile': 'https://github.com/omolewadavids', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jackdry', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Creating-Model-For-Logistic-Regression-Heart-Disease-\n'], 'url_profile': 'https://github.com/DineshKarnati', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 1, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 1, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 1, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 1, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vyas4853', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 1, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Los Angeles, CA', 'stats_list': [], 'contributions': '301 contributions\n        in the last year', 'description': ['multiple-logistic-regression-to-predict-diabetes\nmy other binary classification project\n'], 'url_profile': 'https://github.com/Rlegaspi562', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 1, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 1, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 1, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'Buffalo, NY', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Breast cancer classification using Logistic Regression\nTools Used : Jupyter Notebook\nProgramming Language: Python\nLibraries used: Scikit-learn, NumPy, Pandas, Logistic Regression library\nWisconsin Diagnostic Breast Cancer dataset was collected from the UCI machine learning repository and a machine learning model was built that can classify the cancer cells to Benign and Malignant. The data was trained using Logistic Regression and the model’s performance was tested on the test data.\n'], 'url_profile': 'https://github.com/aksharas28', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 1, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 1, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 12, 2020', 'R', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Python', 'Updated Jun 20, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Logistic-regreesion\nLogistic Regression is the appropriate regression analysis to solve binary classification problems( problems with two class values yes/no or 0/1). This algorithm analyzes the relationship between a dependent and independent variable and estimates the probability of an event to occur. Like other regression models, it is also a predictive model. But there is a slight difference. While other regression models provide continuous output, Logistic Regression is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick.\n'], 'url_profile': 'https://github.com/Avinash237', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 12, 2020', 'R', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Python', 'Updated Jun 20, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'Nagpur', 'stats_list': [], 'contributions': '157 contributions\n        in the last year', 'description': ['Sinking-of-the-Titanic\nThe objective is to predict if a passenger survived the sinking of the Titanic or not using Logistic Regression Model\nModel Representation:\n\nData Processing:\n\nAlgorithm:\n\nLogistic Regression:\nLogistic regression is a technique borrowed by machine learning from the field of statistics.It is the go-to method for binary classification problems (problems with two class values). SInce the given problem required the output as whether a person travelling on the titanic board survives or not i.e. classification ,we have implemented logistic regression algorithm.\nOur model consists of a single perceptron(or node) in the output layer and consists of no hidden layers.Firstly, we have converted the raw data of both the given datasets into a processable datasets using pandas. The categorical data were transformed into numeric labels and we applied dummy encoding scheme on the numeric labels.The final processed data was then fed into the neural network.\nDetails of how we got the final processed data is given in the flow chart.\nLoss function:\n\nOptimisation Algorithm:\n\n\nHow predict function (in code) works:\nX axis= z   ,Y axis=sigma(z)\n\nThis is graph of sigmoid function. From graph it is seen that sigma(z) varies from 0 to 1.\nIf z -> infinity  , sigma(z)=1 also if z -> -infinity , sigma(z)=0.\nWhat predict function does is:\nif sigma(z)>=0.5 then y=1 and if sigma(z)<0.5 then y=0\nWhere y is the predicted value for a particular z.\nModel\nX : input feature vector in which each column corresponds to a training example’s input features.\nY : Ground truth labels in which each column corresponds to a training example’s ground truth label.\nX=(x(1),x(2)........ , x(m))\nY=(y(1),y(2)....... ,y(m))\n1.Initializing Parameters:\nW was initialized to zero column array having 19 rows(number of input features after dummy encoding).\nB was initialized to scalar zero.\n2.Forward Propagation:\n\n3.Backward Propagation:\n\n4.Update Parameters:\nW := W - alpha x dW\nb := b - alpha x db\nWhere alpha is the learning rate.\n5.Perceptron:\nFor a given number of iterations ,the following steps take place in order:\n1.Forward Propagation\n2.Backward Propagation\n3.Updates of parameters\n4.Calculation of cost and accuracy on every 100th iteration.\nFinally the model function returns lists of cost and accuracy calculated at every iteration. Using them we plotted two graphs :\nCost vs number of iterations (per 100th iteration)\nAccuracy vs number of iterations (per 100th iteration)\nResults:\n\nSince the cost curve is continuously decreasing it is implies that our algorithm is working properly.\n'], 'url_profile': 'https://github.com/nilakshi104', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 12, 2020', 'R', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Python', 'Updated Jun 20, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['ML-Methods-for-ASD-Classification-and-Prediction\nNeural Network, SVM, Logistic Regression, Random Forest, KNN and Decision Tree. Grid for Parameter Selection and Lasso for Variable Selection.\n'], 'url_profile': 'https://github.com/glockkm', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 12, 2020', 'R', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Python', 'Updated Jun 20, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/emerizhang', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 12, 2020', 'R', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Python', 'Updated Jun 20, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['AI\nA repo dedicated to manually all aspects of AI, from basic linear and logistic regression to deep reinforcement learning\n'], 'url_profile': 'https://github.com/IvanRado', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 12, 2020', 'R', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Python', 'Updated Jun 20, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Diabetes\nTried some ML algo like KNN, Logistic Regression and RandomForrest to compare the accuracy for the simple data-set\n'], 'url_profile': 'https://github.com/MRA11145', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 12, 2020', 'R', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Python', 'Updated Jun 20, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'Rockville, MD', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['ufc_fights\nAnalysis of historical UFC fights and development of a logistic regression clasification model to predict UFC fight winners\n'], 'url_profile': 'https://github.com/artlionel', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 12, 2020', 'R', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Python', 'Updated Jun 20, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'Khlong Luang, Pathum Thani, Thailand', 'stats_list': [], 'contributions': '114 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/developeravsk', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 12, 2020', 'R', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Python', 'Updated Jun 20, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NFL-Kick-Predictor\nLogistic Regression Model to predict the proability of making a field goal kick. Trained on NFL kicks from 2009-2018.\nVariables Used\n\nYards\nTemperature\nGrass Type\nStadium Type\nHumidity\nWind\n\nPredicted Proability for notable field goals in NFL History\n\nScott Norwood\'s miss in Superbowl XXV: 0.7272094814510025\nAdam Vinatieri\'s game winning kick in Superbowl XXXVI: 0.7645521102095636\nMatt Bahr\'s 1990 NFC Championship winning kick: 0.823966662091537\nMatt Prater\'s 64 yard kick, the longest kick in NFL history: 0.2557375265517442\nCodey Parkey\'s ""double doink"" vs the Eagles in the 2018 playoffs: 0.8041854375998978\n\n'], 'url_profile': 'https://github.com/taoprajjwal', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Python', 'Updated Jan 12, 2020', 'R', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated May 11, 2020', 'Python', 'Updated Jun 20, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Feb 26, 2020', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 26, 2020']}"
"{'location': 'Brazil', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Laboratory Assistant (v3.0!)\nLightweight symbolic regression algorithms implemented on javascript, to run on the client side.\nThis tool is a proof of concept to show that, by using the Interaction-Transformation representation, it is possible to perform symbolic regression on small datasets.\n'], 'url_profile': 'https://github.com/gAldeia', 'info_list': ['JavaScript', 'GPL-3.0 license', 'Updated Aug 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', '2', 'Python', 'MIT license', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 8, 2020']}","{'location': 'Berlin, Germany.', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dina-deifallah', 'info_list': ['JavaScript', 'GPL-3.0 license', 'Updated Aug 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', '2', 'Python', 'MIT license', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 8, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': [""Big Mart Sales Prediction\nProblem Statement\nThe data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined.\nThe aim is to build a predictive model and find out the sales of each product at a particular store. Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.\nPlease note that the data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly.\nMy Performance - 300 rank out of 36657 participants. (As of 12.07.20)\nThe lowest rmse was 1128.19, my model's rmse was 1148.30.\nhttps://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/#LeaderBoard\nData Description\nData We have train (8523) and test (5681) data set, train data set has both input and output variable(s). You need to predict the sales for test data set.\nVariable : Description\nItem_Identifier : Unique product ID\nItem_Weight : Weight of product\nItem_Fat_Content : Whether the product is low fat or not\nItem_Visibility : The % of total display area of all products in a store allocated to the particular product\nItem_Type : The category to which the product belongs\nItem_MRP : Maximum Retail Price (list price) of the product\nOutlet_Identifier : Unique store ID\nOutlet_Establishment_Year : The year in which store was established\nOutlet_Size : The size of the store in terms of ground area covered\nOutlet_Location_Type : The type of city in which the store is located\nOutlet_Type : Whether the outlet is just a grocery store or some sort of supermarket\nItem_Outlet_Sales : Sales of the product in the particulat store. This is the outcome variable to be predicted.\nAproach to our Problem:\n\nCheck for missing data and the data types - categorical or numerical -  present in the dataset\nCheck the normality of the data. For multiple linear regression, one of the assumptions is that the Actual values should be normal.\nWe analyse the data through boxplots, scatter plots to see the descriptive analysis. These probes could tell us whether some columns/feature are     having an effect on the output variables or not. We can use them feature selection.\nTreating the missing data.\nMultivariate Analysis and then Feature Extraction.\nOLS models have to be built for checking multi-collinearity, heteroskedaticity and other important assumptions for building a succesful multiple linear regression.\nPrediction using various Regression Algorithms. The most important aspects are the prediction score and the rmse error.\n\nObservations:\nAlgorithm,\tRMSE Error\n1)Polynomial Regression -\tVery High Error\n2)Linear Regression -\t1155.952\n3)Random Forest -\t1150\n4)XGBoost\t- 1152.94\n5)Gradient Boosting -\t1155\n6)SVM\t- 1167\n7)Linear SVM -\t1167.27\n8)NuSVM\t- 1164\n9)Ada Boost\t- 1157\n10)KNN\t- 1154\n11)Bagging Regressor -\t1157\n12)Lasso Lars IC\t- 1180\nFinal Obseravtions:\n\n\nThe model was transformed before training and testing. When the input was transformed for non-linearity as well, so that gave us our best result.\n\n\nBoxplots and groupby functions were used to decide on the significant variables for categorical variables\nWhile heatmaps, scatterplots were used for deciding on numerical variables.\n\n\nWhen more features are introduced through feature extraction and feature transformation, accuracy increases but rmse error increases or doesnt get affected.\n\n\nItem_MRP and Item_Outlet_Type are our most significant features.\n\n\nRmse gets affected due to high multicolinearity amonsgt varibles. But if those variables are dropped which show high multicollinearity, the accuracy is compromised.\n\n\nOutlier removal didn't help with the model because they reduce the variance, and then the model underperformed.\n\n\nIt seems that the highest accuarcy possible is 68-69%. Any removal of insignificant variable results the model being underfit.\n\n\nThe model where all the varibles were taken into account, the results were overall better.\n\n\nEnsemble Methods performed the best. Random Forest Regressor gave us the best model - with or without the insignificant features.\n\n\nConclusion:\nRandom Forest performed the best with the lowest RMSE of 1148-1150.\nA random forest is a meta estimator that fits a number of decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. It is a voting type algorithm, that builds multiple trees from the same dataset, and uses voting to decide on the best output on adequate variance and bias from those multiple models. It is a bootstrap model\nInteresting findings:\n\nSupermarket Type 1 had the most sales because it sold the most variety in every Item_Type.\nAn increase in Item MRP also affects the Sales\n\n""], 'url_profile': 'https://github.com/sharmanavika07', 'info_list': ['JavaScript', 'GPL-3.0 license', 'Updated Aug 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', '2', 'Python', 'MIT license', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 8, 2020']}","{'location': 'Italy', 'stats_list': [], 'contributions': '365 contributions\n        in the last year', 'description': ['MNIST_task\nLearn a task like sum, abs_diff, eucl_dist with MNIST pairs.\nThe model is a siamese-like CNN implemented with Tensorflow/Keras.\nTask\nThe model learns to compute the following tasks:\n\nRequirements\n\n\n\nSoftware\nVersion\nRequired\n\n\n\n\nPython\n>= 3.5\nYes\n\n\nNumpy\nTested on v1.17\nYes\n\n\nTensorflow\nTested on v2.0.0\nYes\n\n\nstr2bool\n>= 1.0\nYes\n\n\n\n'], 'url_profile': 'https://github.com/LucaAngioloni', 'info_list': ['JavaScript', 'GPL-3.0 license', 'Updated Aug 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', '2', 'Python', 'MIT license', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['JavaScript', 'GPL-3.0 license', 'Updated Aug 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', '2', 'Python', 'MIT license', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['JavaScript', 'GPL-3.0 license', 'Updated Aug 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', '2', 'Python', 'MIT license', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['JavaScript', 'GPL-3.0 license', 'Updated Aug 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', '2', 'Python', 'MIT license', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['JavaScript', 'GPL-3.0 license', 'Updated Aug 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', '2', 'Python', 'MIT license', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['JavaScript', 'GPL-3.0 license', 'Updated Aug 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', '2', 'Python', 'MIT license', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['JavaScript', 'GPL-3.0 license', 'Updated Aug 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jul 19, 2020', '2', 'Python', 'MIT license', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Python', 'Unlicense license', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Python', 'Unlicense license', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Python', 'Unlicense license', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Predicting-SIC-returns-with-Ordinary-Least-Squares-Regression\n'], 'url_profile': 'https://github.com/adamszequi', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Python', 'Unlicense license', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Python', 'Unlicense license', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Python', 'Unlicense license', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Python', 'Unlicense license', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Python', 'Unlicense license', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Python', 'Unlicense license', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020', 'Python', 'Unlicense license', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'HTML', 'Updated Jan 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'Novi Sad ', 'stats_list': [], 'contributions': '1,463 contributions\n        in the last year', 'description': [' Nacin pokretanja \nRequiremets\n\nKako bi preuzeli igru, mozete jednostavno klonirati repozitorijum na vasem racunaru, ili skinuti zipovan projekat. Kako bi instalirali sve biblioteke i pokrenuli igru pomocu nekog od okruzenja koje podrzava Python, morate uraditi sledece.\n\nPozicionirate se gde se nalazi nasa igra, tacnije folder requirements.txt\nOtvorite tu cmd i ukucate sledece:\n\n$ pip install -r requirements.txt\nPokretanje aplikacije\nPotrebno je da se pozicionirate u okvir direktorijuma vas_path/Connect-Four/ koji ste prethodno skinuli/klonovali. A potom na sledeci nacin mozete pokrenuti aplikaciju:\n$ python igra.py\n\n Nacin igranja \n\nIgranje je vrlo intuitivno, pomeranjem misa levo desno mozete da izaberete kolonu u koju zelite da spustite token(ZETON), klikom na kolonu automatski ce se popuniti prvi slobodan red sa zetonom, zatim je na redu implementirani AI algoritam i tako nazimenicno se smenjujte. Ko ce biti prvi na potezuje AI ili Vi, o tome odlucuje RANDOM generator.\n\n Ukratko o projektu \nIgra sadrži dva moda, pri pokretanju korisniku iskače dialog u kom korisnik bira da li želi da koristi MOD1 ili MOD2\n\n\n\n\n\n\nUpotpunosti je implementiran MIN-MAX algoritam sa alpha-beta odsecanjem stabla, da bi postigli brze pretraživanje stabla.\n\n\n\nAko korisnik izabere MOD1 imaće priliku da iskusi pravu moć MIN-MAX algoritma i da se iskuša protiv skoro nepobedivog algoritma, u tom modu korisnik može da potraži pomoć numerike, tj implementirali smo (linearnu regresiju sa Ridge and Lasso regularizacijom)\nMOD1 nam ujedno služi i za prikupljanje trening seta podataka koji koristimo da bi napravili što bolje predviđanje uz pomoć regresije. Pamtimo odluke MIN-MAX algoritma i te podatke koristimo za regresiju.\nPodaci se čuvaju u .CSV formatu i pri svakom novom pokretanju programa ponovo se učitavaju. Regresiju izvršavamo nad tim podacima.\nIdeja projekta je da se regresija koristi kao mali pomoćnik u borbi protiv AI algoritma, svi su svesni da numerika nije ni približno moćna za predviđanje kao MIN MAX algoritam, ali ako iskoristimo tog malog pomoćnika i našu moć razmišljanja možemo vrlo jako da pariramo AI algoritmu. Klikom na dugme ""POMOC NUMERIKE"" korisnik dobija predlog po predjasnjem iskustvu numerike gde bi bilo najbolje da korisnik odigra sledeci potez.\n\n\n\nTakođe korisnik ako želi može da se oproba u igri protiv numerike, ako pri pokretanju igre izabere MOD2.\nIgra sadrži i edukativni sadržaj, tj želeli smo da korisnicima malo približimo regresiju i regularizaciju, klikom na dugme ""PRINCIPI NUMERIKE"" korisniku isače dialog u kom je u kratkim crtama objašnjena regresija i regularizacija koja šljaka ispod haube.\n\nKorisniku je u svakom trenutku dostupan trening set podataka koji se obnavlja pri zavrsetku svake igre u MOD-u 1. Klikom na dugme ""TRAINING DATA SET"" korisniku dobija prikaz 6 različitih dijagrama, radi demonstracije kako parametar alpha utiče na fitovanje u podatke samim tim i na predviđanje.\n\nIgra sadrži i simulaciju fizike. Simuliramo odskakanje tokena, niti se uspavljuju na odredjeno vreme tako da korisniku nije dozvoljeno da odigra sledeci potez dok token ne zavrsi sa simulacijom fizike.\n\n\n\n'], 'url_profile': 'https://github.com/Pufke', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'Colombo, Sri Lanka', 'stats_list': [], 'contributions': '291 contributions\n        in the last year', 'description': ['Predicting-CO2-emission-using-Linear-regression\nThis is my first Simple Linear model.\nThis is an example that was learnt in the lab session of Coursera online course Machine Learning with Python.\nData set\nFuelConsumption.csv\nPredicting the Co2 emission vs one independent variable\nIn this model the Co2 emission is predicted with respect to Engine Size.\n'], 'url_profile': 'https://github.com/AkshaanB', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'Colombo, Sri Lanka', 'stats_list': [], 'contributions': '291 contributions\n        in the last year', 'description': ['Predicting-CO2-emission-using-Linear-regression\nThis is my first Simple Linear model.\nThis is an example that was learnt in the lab session of Coursera online course Machine Learning with Python.\nData set\nFuelConsumption.csv\nPredicting the Co2 emission vs one independent variable\nIn this model the Co2 emission is predicted with respect to Engine Size.\n'], 'url_profile': 'https://github.com/AkshaanB', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nCovariance and Correlation\nWe start the section by covering covariance and correlation, both of which relate to how likely two variables are to change together. For example, with houses, it wouldn\'t be too surprising if the number of rooms and the price of a house was correlated (in general, more rooms == more expensive).\nStatistical Learning Theory\nWe then explore statistical learning theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': [""Medical-Cost-Personal-Datasets-Insurance-Forecast-by-using-Linear-Regression\nContext Machine Learning with R by Brett Lantz is a book that provides an introduction to machine learning using R. As far as I can tell, Packt Publishing does not make its datasets available online unless you buy the book and create a user account which can be a problem if you are checking the book out from the library or borrowing the book from a friend. All of these datasets are in the public domain but simply needed some cleaning up and recoding to match the format in the book.  Content Columns - age: age of primary beneficiary  sex: insurance contractor gender, female, male  bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9  children: Number of children covered by health insurance / Number of dependents  smoker: Smoking  region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n""], 'url_profile': 'https://github.com/arpitamohanty1991', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rajeshnalamati', 'info_list': ['Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'HTML', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['A Sparse Regression Approach for Evaluating and Predicting NHL Results\nA more rigorous overview of the project can be found in the pdf, but below is a high level (and less technical) description.\nMotivation\nThis repository contains an early draft of a research project I worked on in undergrad at the University of Alabama. The motivation of this project was to:\n\nDevelop models that can accurately predict NHL regular season stands and playoff results.\nUtilize model coefficients to better understand driving factors behind regular season and post-season success.\nAddress the claim ""Defense win championships.""\n\nData\nOriginally data was collected from the population NHL data aggregation site http://war-on-ice.com/. This is the datasource the paper is based off of; however, the site has since closed down with no new owners taking over maintenance of the site. I now have a dataset scraped from https://www.naturalstattrick.com/ that can be used to recreate the analysis.\nThe resulting modelling dataset ends up with over 1,000 features. Given the limited number of observations (teams * # years of data), our feature set is high-dimensional relative to our sample space. We use penalized regression techniques as our means of variable extraction.\nCode\nCode used for data scraping, data cleaning, model building, model validation, and so on will be added to this repo at a later date.\nApproach\nModels were developed to address three different scenarios:\n\nUse regular season data to predict regular season standings\nUse regular season data to predict postseason match winners\nUse postseason data to predict postseason match winners\n\nGLMs were developed for this purpose with the following variations:\n\nDifferent link functions were tested\nDifferent sparse regression techniques (Lasso, Ridge, Elastic Net) were used\nHyperparameters were tuned to prioritize different metrics (sparsity of coefficients vs. minimizing RMSE)\n\nResults\nThe created models are able to predict NHL playoff success at least as well as existing naive approaches. Analysis on the coefficients that remain suggest that in the regular season offense is more predictive of defense of how good a team is. For the playoffs offense still appears to be more important, but by a much smaller margin. One explanation for this is that offense is a larger driver of which teams make the playoffs, so teams that make the playoffs have a larger variation in defensive ability than offensive ability which makes defense appears to be more important in the postseason.\n'], 'url_profile': 'https://github.com/nblaffey', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating regression lines\nLearning Objectives\n\nUnderstand what is meant by the errors of a regression line\nUnderstand how to calculate the error at a given point\nUnderstand how to calculate RSS and why we use it as a metric to evaluate a regression line\nUnderstand the difference between RSS and its variation, the RMSE\n\nIntroduction\nSo far we have seen how lines and formulas can estimate outputs given an input.  We can describe any straight line with two different variables:\n\n$m$ -  the slope of the line, and\n$b$ - the y-intercept\n\nSo far we have been rather fast and loose with choosing a line to estimate our output - we simply drew a line between the first and last points of our data set.  Well today, we go further.  Here, we take our first step toward training our model to match our data.\n\nThe first step in training is to calculate our regression line\'s accuracy --  that is, how well our regression line matches our actual data.  Calculating a regression line\'s accuracy is the topic of this lesson.\n\nIn future lessons, we will improve upon our regression line\'s accuracy, so that it better predicts an output.\nDetermining Quality\nThe first step towards calculating a regression line to predict an output is to calculate how well any regression line matches our data.  We need to calculate how accurate our regression line is.\nLet\'s find out what this means.  Below we have data that represents the budget and revenue of four shows, with x being the budget and y being the revenue.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\n\nRun code above with shift + enter\n\nAn initial regression line\nAs we did in the last lab, let\'s draw a not-so-great regression line simply by drawing a line between our first and last points.  We can use our build_regression_line function to do so.  You can view the code directly here.\n\nEventually, we\'ll improve this regression line.  But first we need to see how good or bad a regression line is.\n\nfrom linear_equations import build_regression_line\nx_values = list(map(lambda show: show[\'x\'],shows))\ny_values = list(map(lambda show: show[\'y\'],shows))\nregression_line = build_regression_line(x_values, y_values)\nregression_line\nWe can plot our regression line as the following using the plotting functions that we wrote previously:\nfrom graph import m_b_trace, plot, trace_values\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\ndata_trace = trace_values(x_values, y_values)\nregression_trace = m_b_trace(regression_line[\'m\'], regression_line[\'b\'], x_values)\nplot([regression_trace, data_trace])\nSo that is what our regression line looks like.  And this the line translated into a function.\ndef sample_regression_formula(x):\n    return 1.5(x) + 100\nAssessing the regression line\nOk, so now that we see what our regression line looks like, let\'s highlight how well our regression line matches our data.\n\n\nLet\'s interpret the chart above.  That first red line shows that our regression formula does not perfectly predict that first show.\n\nOur actual data -- the first blue dot -- shows that when $x = 100$, $y =  150$.\nHowever, our regression line predicts that at $x = 100$, $y = 250$.\n\n\n\nSo our regression line is off by 100, indicated by the length of the red line.\n\nEach point where our regression line\'s estimated differs from the actual data is called an error.  And our red lines display the size of this error.  The length of the red line equals the size of the error.\n\nThe error equals the difference between the actual value and the value expected by our model (that is, our regression line).\nerror = actual - expected\n\nNow let\'s put this formula into practice.  The error is the actual value minus the expected value.  So at point $x = 100$, the actual $y$ is 150.  And at point x = 100, the expected value of $y$ is $250$.  So:\n\nerror = $150 - 250 = -100$.\n\nIf we did not have a graph to display this, we could calculate this error by using our formula for the regression line.\n\nOur regression formula is $y = 1.5x + 100$.\nThen when $x$ equals 100, the formula predicts $y = 1.5 * 100 + 100 = 250$.\nAnd we have the actual data of (100, 150).  So\nactual - expected $ = 150 -250 = -100$.\n\nRefining our Terms\nNow that we have explained how to calculate an error given a regression line and data, let\'s learn some mathematical notation that let\'s us better express these concepts.\n\nWe want to use notation to distinguish between two things: our expected $y$ values and our actual $y$ values.\n\nExpected values\nSo far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the expected value of $y$. So let\'s indicate this, by changing our regression line formula to look like the following:\n\n$\\hat{y} = \\hat{m}x + \\hat{b}$\n\nThose little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat multiplied by $x$ plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  The hats also say that this estimated value of $y$ is based on our estimated values of $m$ and $b$.\n\nNote that $x$ is not a predicted value.  This is because we are providing a value of $x$, not predicting it.  For example, we are providing an show\'s budget as an input, not predicting it.  So we are providing a value of $x$ and asking it to predict a value of $y$.\n\nActual values\nNow remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which look like the following.\nfirst_show = {\'x\': 0, \'y\': 100}\nsecond_show = {\'x\': 100, \'y\': 150}\nthird_show = {\'x\': 200, \'y\': 600}\nfourth_show = {\'x\': 400, \'y\': 700}\n\nshows = [first_show, second_show, third_show, fourth_show]\nshows\nSo how do we represent our actual values of $y$? Here\'s how: $y$.  No extra ink is needed.\nOk, so now we know the following:\n\n$y$: actual y\n$\\hat{y}$: estimated y\n\nFinally, we use the Greek letter $\\varepsilon$, epsilon, to indicate error. So we say that\n\n$\\varepsilon = y - \\hat{y}$.\n\nWe can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\hat{y}$ are at that $x$ value.  This is written as:\n$\\varepsilon {i}$ = $y{i}$ - $\\hat{y}_{i}$\nThose little $i$s represent an index value, as in our first, second or third movie.  Now, applying this to a specific point of say when $ x = 100 $, we can say:\n\n$\\varepsilon {x=100} = y{x=100}$ - $\\hat{y}_{x=100} = 150 - 250 = -100$\n\nCalculating and representing total error\nWe now know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\hat{y_i}$.  Again, this is helpful at describing how well our regression line predicts the value of $y$ at a specific point.\nHowever, we want to see well our regression describes our dataset in general - not just at a single given point.  Let\'s move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.\nAs an initial approach, we simply calculate the total error by summing the errors, $y - \\hat{y}$, for every point in our dataset.\nTotal Error = $\\sum_{i=1}^{n} y_i - \\hat{y_i}$\nThis isn\'t bad, but we\'ll need to modify this approach slightly. To understand why, let\'s take another look at our data.\n\nThe errors at $x = 100$ and $x = 200$ begin to cancel each other out.\n\n$\\varepsilon_{x=100}= 150 - 250 = -100$\n$\\varepsilon_{x=200} = 600 - 400 = 200$\n$\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -100 + 200 = 100 $\n\nWe don\'t want the errors to cancel each other out!  To resolve this issue, we square the errors to ensure that we are always summing positive numbers.\n${\\varepsilon_i^2}$ = $({y_i - \\hat{y_i}})^2$\nSo given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like:\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2 = \\sum_{i = 1}^n \\varepsilon_i^2 $\n\nResidual Sum of Squares is just what it sounds like.  A residual is simply the error -- the difference between the actual data and what our model expects.  We square each residual and add them together to get RSS.\n\nLet\'s calculate the RSS for our regression line and associated data.  In our example, we have actual $x$ and $y$ values at the following points:\n\n$ (0, 100), (100, 150), (200, 600), (400, 700) $.\n\nAnd we can calculate the values of $\\hat{y} $ as $\\hat{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n$RSS = (100 - 100)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$\nwhich reduces to\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\nNow we have one number, the RSS, that represents how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive.\nRoot Mean Squared Error\nRoot Mean Squared Error (RMSE), is just a variation on RSS.  Essentially, it tries to answer the question of what is the ""typical"" error of our model versus each data point.  To do this, it scales down the size of that large RSS number by taking the square root of the RSS divided by the number of data points:\n\n\n$ RSS  = \\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2$\n\n\n$RMSE = \\sqrt{\\frac{RSS}{n}} $\n\n\n\nWhere n equals the number of elements in the data set.\n\nNow let\'s walk through the reasoning for each step.\nTaking the mean\nThe first thing that makes our RSS large is the fact that we square each error.  Remember that we squared each error, because we didn\'t want positive errors and negative errors to cancel out.  Remember, we said that each place where we had a negative error, as in :\n\n$actual - expected = -100$\n\nWe would square the error, such that $(-100)^2 = 10,000$.\nRemember that we square each of our errors and add them together, which led to:\n\n$RSS = 0^2 + (-100)^2 + 200^2 + 0^2 = 50,000$\n\nWe then take the mean to get the average squared error (also called ""mean squared error"" or ""MSE"" for short:\n\n$MSE = \\frac{50,000}{4}=12,500$\n\nWe do this because with each additional data point in our data set, our error will tend to increase. So with increasing dataset size, RSS also increases. To counteract the effect of RSS increasing with the dataset size and not just accuracy, we divide by the size of the dataset.\nTaking the square root\nThe last step in calculating the RMSE, is to take the square root of the MSE:\n$RMSE = \\sqrt{12,500} = 111.8$\nIn general, the RMSE is calculated as:\n$ RMSE  = \\sqrt{\\frac{\\sum_{i = 1}^n ({y_i - \\hat{y_i}})^2}{n}} $\nSo the RMSE gives a typical estimate of how far each measurement is from the expectation.  So this is ""typical error"" as opposed to an overall error.\nSummary\nBefore this lesson, we simply assumed that our regression line made good predictions of $y$ for given values of $x$.  In this lesson, we learned a metric that tells us how well our regression line fits our actual data.  To do this, we started looking at the error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to determine how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these squared errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data.  Lastly, we learned how the RMSE tells us the ""typical error"" by dividing the square root of the RSS by the number of elements in our dataset.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated May 26, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated May 26, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated May 26, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020']}","{'location': 'Bay Area', 'stats_list': [], 'contributions': '492 contributions\n        in the last year', 'description': ['ML_Diabetes_Predictor\nPredicts probability of having diabetes using Logistic Regression Machine Learning Model.\nVisit http://diabetesprob.web.illinois.edu/home.html to see the final project\n'], 'url_profile': 'https://github.com/abehara2', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated May 26, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020']}","{'location': 'Hyderabad | Chennai ', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': [""Comparison-of-ML-models-for-predicting-AQI\nGoal\nIn this project we are comparing various machine learning models to find which model works better for predicting the AQI (Air Quality Index).\nMachine learning models used\nIn this project we are using regression models such as:\n\nMultiple Linear Regression\nPolynomial Regression\nDecision Tree Regression\nRandom Forest Regression\nSupport Vector regression (SVR)\n\nLibraries Used: numpy, pandas, sklearn \nIDE used: spyder (Anaconda 3)\nError Metrics Used\nIn this project we have used the following error metrics to evaluate and compare our models:\n\nCoefficient of determination (R^2)\nRoot Mean Square Error (RMSE)\nMean absolute error (MAE)\nRoot Mean Squared Logarithmic Error (RMSLE)\n\nAQI table\n\nData Source\nThe data set is taken from Open Government Data (OGD) Platform India. The site provides Real time National Air Quality Index values from different monitoring stations across India. The pollutants monitored are Sulphur Dioxide (SO2), Nitrogen Dioxide (NO2), Particulate Matter (PM10 and PM2.5) , Carbon Monoxide (CO), Ozone(O3) etc. The site provides data on hourly basis thus the site's data is refreshed every hour.\nResult\n\nResults on training set:\n\n\n\n\nmodels\nR^2\nRMSE\nMAE\nRMSLE\n\n\n\n\nMLR\n0.9965\n5.9334\n3.2952\n0.0595\n\n\nDecision Tree\n1.0000\n0.0000\n0.0000\n0.0000\n\n\nRandom Forest\n0.9996\n2.0237\n0.7106\n0.0195\n\n\nSVR\n0.9494\n22.628\n16.076\n0.1423\n\n\nPoly R\n1.00\n0.09\n0.018\n0.0012\n\n\n\n\n\nResults on testing set:\n\n\n\n\nModels\nR^2\nRMSE\nMAE\nRMSLE\n\n\n\n\nMLR\n0.9965\n5.4973\n3.4796\n0.0517\n\n\nDecision Tree\n0.9955\n6.2370\n2.354\n0.0563\n\n\nRandom Forest\n0.9982\n3.8577\n1.7016\n0.0422\n\n\nSVR\n0.9164\n27.0025\n19.0722\n0.1686\n\n\nPoly R\n-4.1417\n211.8759\n81.5855\n0.4638\n\n\n\nPrediction results\n \n \n\nConclusion\nFrom the above table it is evident that the Random Forest Regressor performed the best out of all other regression models.\n""], 'url_profile': 'https://github.com/Anindya-Das02', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated May 26, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Zenetor', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated May 26, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020']}","{'location': 'Leiden, Netherlands', 'stats_list': [], 'contributions': '129 contributions\n        in the last year', 'description': [""Supervised Learning to predict individuals' income to identify possible donors for charity\nThis project has been completed as part of the Udacity's Machine Learning Nanodegree requirements.\nThe main file for the project is the: \nfinding_donors.ipynb\nOther jupyter notebook files contain further personal experimentation of tuning various ML algorithms.\nOverview\nIn this project I apply various supervised models to predict if the income of individuals is above or less than $50,000, using 1994 U.S. Census data. The best performing model is then selected for further tuning.\nThe jupyter notebook is structured as follows (more information at the finding_donors.ipynb file):\n\nData Exploration\nData Pre-processing\n\nLogarithmic Transformation of skewed data (to reduce the high range of values caused by the outliers)\nNormalization of numerical features (to ensure each feature is treated equally)\nOne-hot encoding of non-numeric features\nShuffle and split data into train and test sets\n\n\nDevelopment of a Training and Predicting Pipeline for 4 different algorithms\n\nNaive predictor\nLogistic Regression\nSupport Vector Machines (SVMs)\nAdaBoost\n\n\nEvaluation of model performance and selection of best performing model\n\nAccuracy\nF-beta score\n\n\nModel Tuning (of the model parameters to further improve its performance)\n\nGrid Search Cross Validation\nRe-evaluate the model performance\n\n\nFeature Importance (to identify few crucial features)\n\nRe-evaluate the model performance to see how it performs when we only use a subset of all the available features\n\n\n\n""], 'url_profile': 'https://github.com/gepallas', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated May 26, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Caravan-Insurance-Policy-Prediction-Machine Learning\nProblem Statement\nWe want you to predict whether a customer is interested in a caravan insurance policy from other data about the\ncustomer. Information about customers consists of 86 variables and includes product usage data and sociodemographic\ndata derived from zip area codes.\nThe training set contains over 5000 descriptions of customers, including the information of whether or not they\nhave a caravan insurance policy. A test set contains 4000 customers of whom target variable is not shared with\nyou.\nOur Target/Dependent Variable is V86.\n\n\nSolution-\nwe have two data sets-\n1)carvan_train.csv(Training Data)\n2)carvan_test.csv(Testing Data)\nSteps-\n1)Read both train and test data using python pandas library.\n2)Concatenate both train and test data for data preprocessing and data cleaning.\n3)Check for data types and unique values of all the columns.\n4)Columns ""V1"",""V4"",""V5"",""V6"",""V10"",""V42"",""V44"" are categorical columns.\n5)Plot all the categorical columns using catplot of seaborn library.\n6)Create (n-1) dummies of all the categorical columns.\n7)Check Null values and impute Null values by mean in train data except columns ""V86"" and column ""data"".\n8)Data preprocessing is done and now split the data into train and test data.\n9)Again split train data into 80% and 20% using sklearn train test split.\n10)80% data will be used for training and 20% data will be used to check the performance of the model.\n11)Differentiate target variable and predictors in both the data sets.\n12)Build logistics regression model on train data with 10 fold cv using sklearn\'s GridSearchCV.\n13)Find the best estimator of grid search and fit on train data(x_train,y_train).\n14)Do the prediction on 20% test data and find the maximum KS cutoff for hardclasses(0 or 1).\n15)Build confusion matrix,find accuracy and f-beta score.\n16)Then fit on whole train data and do the prediction on whole test data.\n17)To get better performance use other model like Random Forest.\n18)Build random forest model using sklearn\'s RandomizedSearchCV with 10 fold CV.\n19)Find the best estimator of RandomizedSearchCV and fit on 80% train data.\n20)Do the prediction on 20% test data.\n21)Then find KS_cutoff,Accuracy,fbeta score,confusion matrix to check our model performance.\n22)Fit on whole train data using best estimator of RandomizedSearchCV.\n23)Do the prediction on whole test data.\n24)Save prediction(hardclasses) in an csv file.\n25)Try boosting algorithms like GradientBoostingMachines and XGBoost.\n26)Try KNN and Naiye bayes algorithms.\n27)From all the model,XGBoost perform well and gives better results compare to other models.\n\n\nNote-: To understand the data/columns,please refer data dictionary.\n'], 'url_profile': 'https://github.com/hariomjangid687', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated May 26, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '206 contributions\n        in the last year', 'description': ['python_correlation\nA simple tutorial on:\n\nPearson, Spearman, Kendall correlation coefficient calculations\nHow to use SciPy, NumPy, Pandas correlation functions\nHow to visualize data, regression lines, and correlation matrices with Matplotlib\n\n'], 'url_profile': 'https://github.com/rtelles64', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated May 26, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '1,295 contributions\n        in the last year', 'description': ['mess-prediction\nthe project uses machine learning to predict if you should go to the mess or not based on the food reviews and the food wastage.\nto execute each line\nctrl+enter\n'], 'url_profile': 'https://github.com/r-ush', 'info_list': ['Jupyter Notebook', 'Updated Jan 7, 2020', 'HTML', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated May 26, 2020', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'Updated May 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 6, 2020']}"
"{'location': 'Mumbai', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Caravan-Insurance-Policy-Prediction-Machine Learning\nProblem Statement\nWe want you to predict whether a customer is interested in a caravan insurance policy from other data about the\ncustomer. Information about customers consists of 86 variables and includes product usage data and sociodemographic\ndata derived from zip area codes.\nThe training set contains over 5000 descriptions of customers, including the information of whether or not they\nhave a caravan insurance policy. A test set contains 4000 customers of whom target variable is not shared with\nyou.\nOur Target/Dependent Variable is V86.\n\n\nSolution-\nwe have two data sets-\n1)carvan_train.csv(Training Data)\n2)carvan_test.csv(Testing Data)\nSteps-\n1)Read both train and test data using python pandas library.\n2)Concatenate both train and test data for data preprocessing and data cleaning.\n3)Check for data types and unique values of all the columns.\n4)Columns ""V1"",""V4"",""V5"",""V6"",""V10"",""V42"",""V44"" are categorical columns.\n5)Plot all the categorical columns using catplot of seaborn library.\n6)Create (n-1) dummies of all the categorical columns.\n7)Check Null values and impute Null values by mean in train data except columns ""V86"" and column ""data"".\n8)Data preprocessing is done and now split the data into train and test data.\n9)Again split train data into 80% and 20% using sklearn train test split.\n10)80% data will be used for training and 20% data will be used to check the performance of the model.\n11)Differentiate target variable and predictors in both the data sets.\n12)Build logistics regression model on train data with 10 fold cv using sklearn\'s GridSearchCV.\n13)Find the best estimator of grid search and fit on train data(x_train,y_train).\n14)Do the prediction on 20% test data and find the maximum KS cutoff for hardclasses(0 or 1).\n15)Build confusion matrix,find accuracy and f-beta score.\n16)Then fit on whole train data and do the prediction on whole test data.\n17)To get better performance use other model like Random Forest.\n18)Build random forest model using sklearn\'s RandomizedSearchCV with 10 fold CV.\n19)Find the best estimator of RandomizedSearchCV and fit on 80% train data.\n20)Do the prediction on 20% test data.\n21)Then find KS_cutoff,Accuracy,fbeta score,confusion matrix to check our model performance.\n22)Fit on whole train data using best estimator of RandomizedSearchCV.\n23)Do the prediction on whole test data.\n24)Save prediction(hardclasses) in an csv file.\n25)Try boosting algorithms like GradientBoostingMachines and XGBoost.\n26)Try KNN and Naiye bayes algorithms.\n27)From all the model,XGBoost perform well and gives better results compare to other models.\n\n\nNote-: To understand the data/columns,please refer data dictionary.\n'], 'url_profile': 'https://github.com/hariomjangid687', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Jun 21, 2020', 'R', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jul 13, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '206 contributions\n        in the last year', 'description': ['python_correlation\nA simple tutorial on:\n\nPearson, Spearman, Kendall correlation coefficient calculations\nHow to use SciPy, NumPy, Pandas correlation functions\nHow to visualize data, regression lines, and correlation matrices with Matplotlib\n\n'], 'url_profile': 'https://github.com/rtelles64', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Jun 21, 2020', 'R', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jul 13, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Machine_learning_Models_by_Aayan\nVarious Machine Learning Classification and Identification models has been described in various examples using K nearest neighbors, Support Vector Machine, Random Forest, Logistic Regression and many more\n'], 'url_profile': 'https://github.com/1OMKAR7NAM', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Jun 21, 2020', 'R', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jul 13, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sayanchak', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Jun 21, 2020', 'R', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jul 13, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['TAMU-Datathon\nIn Fall 2019, I attended the TAMU Datathon, the first data science hackathon. As part of the Learner Track, I created some Linear and Logistic Regression models for analysis and model fitting.\n'], 'url_profile': 'https://github.com/rdan22', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Jun 21, 2020', 'R', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jul 13, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'Philadelphia', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['Impact Evaluation of Child Aid Programs\nIncreasing research evidence has indicated the meaningful effect nutrition and family income between the ages of 0 to 5 has on child development (Birch & Gussow 1970; Duncan & Brooks-Gunn 1997). This research conducts an evaluation study to examine the effect of the Women, Infant and Children (WIC) Nutrition Program and Aid to Families with Dependent Children (AFDC) program participation during pregnancy on child reading achievement. This is a large evaluation study using a national probability sample, the Child Development Supplement to the Panel Study of Income Dynamics, and thus has meaningful policy implications.\nUsing the Child Development Supplement to the Panel Study of Income Dynamics. This research examines if the effect of WIC program participation during pregnancy is moderated by (1) family income, (2) race, and (3) the current age of the child. Assumptions need to be evaluated and changes were made to the data and model in order to account for assumption violations.\nThe report includes: the descriptive statistics (i.e. N, means/medians/proportions, standard deviations, frequencies, observed ranges, and correlations) of the variables and the multiple regression analyses with interaction effects.\nDefinition of Variables:\n• WICpreg – Women, Infant and Children (WIC) Nutrition Program participant during\npregnancy: 0 = No, 1 = Yes.\n• Race – Centered Binary Coding of Race: -0.5 = Black, 0.5 = White.\n• mathraw97 – Woodcock-Johnson Revised Mathematics Achievement Test Raw Score.\nMinimum = 0, Maximum = 98.\n• age97 – The child’s age in 1997. Minimum = 3, Maximum = 13.\n• faminc97 – Total family income in 1997 (in 2002 constant dollars). Minimum = $-\n72296.26, Maximum = $784610.59.\n• bthwht – Low birth weight status of the child. 0 = non-low birth weight child, 1 = low\nbirth weight child.\n• HOME97 – A composite total score of the emotional and cognitive stimulation at home.\nMinimum = 7, Maximum = 27.\n'], 'url_profile': 'https://github.com/karenyxwang', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Jun 21, 2020', 'R', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jul 13, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'Berlin Kreuzberg', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': [""Applied Data Science @ Columbia University\nFall 2019\nProject 4: Algorithm implementation from scratch and evaluation – Collaborative Filtering\n\nGradient Descent and Alternating Least Squares for movie rating predictions\n\n\nTeam # Sec2 Group8\n\n\nTeam Members:\n\nFateme Sadat Haghpanah\nStefano Longo\nTim Schleicher\nStephanie Wong\nNuanjun Zhao\n\n\n\nProject summary: In this project, we implemented two algorithms for collaborative filtering from scratch: Gradient Descent with Probabilistic Assumtptions (A2 in the assignment) and Alternating Least Squares (A3). Afterwards, we used Kernel Ridge Regression (P3) for post-processing. Our results indicate that KRR is a valuable improvement for the algorithms. Also, we found that the more latent factors we use, the lower the final RMSE. Overall, ALS tends to outperform GD with Probabilistic Assumptions for all levels of latent factors on the the test data.\n\n\nContribution statement:\n\nFateme Sadat Haghpanah: implemented algorithm A2 (Gradient Descent with Probabilistic Assumptions) jointly with Tim. Created the p and q matrix resulted by this algorithm for different factor sizes. Helped on completing the A2 algorithm part of main.Rmd\nStefano Longo: Mainly responsible for understanding the process and theory behind postprocessing. Worked with Tim, Fatima and Stephanie to make sure the algorithm implementation and the post-processing parts were actually speaking to one another. Researched various possible krr functions and chose the one we implemented. Wrote Cross Validation function. Worked with Nuanjun to understand how to best compare the krr prediction with the original ratings. Cleaned repository and supported creation of final report.\nTim Schleicher: implemented algorithm A2 (Gradient Descent with Probabilistic Assumptions) with Fateme. Created the and q matrix resulted by this algorithm for different factor sizes. Pulled together all components of the project to build the final report.\nStephanie Wong: implemented A3 algorithm (Alternating Least Squares)\nNuanjun Zhao: Post-processing and evaluation: mainly contributed to the codes for this part, including all the codes: transformation of algorithms results, building 610 krr models and getting predictions, tuning weight of weighted average of prediction from algorithms and how to compute rmse for test data(evaluation),except tuning lambda; read and understood the second paper and discussed my understanding of the whole post processing and evaluation part with Stefano and make sure that makes sense; came up with the correct idea of how we transform q matrix how to realize krr function; Discussed with group members who work on algorithms to make sure their results seem correct; ran the codes and tuned parameters for this part of A3's results. responsible for the report for this part; explained the details of steps to my partner; helped with presentation; some of github arrangement work\n\n\n\nAll team members approve our work presented in this GitHub repository including this contributions statement.\nFollowing suggestions by RICH FITZJOHN (@richfitz). This folder is organized as follows.\nproj/\n├── lib/\n├── data/\n├── doc/\n├── figs/\n├── output/\n├── Gradient-Descent-w-Prob-Assumptions Algorithm\n└── Alternating-Least-Squares Algorithm\n\nPlease see each subfolder for a README file.\n""], 'url_profile': 'https://github.com/grenzsprung', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Jun 21, 2020', 'R', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jul 13, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Explore-relationship-between-model-complexity-and-Generalization\nTwo-part Coursera assignment on supervised machine learning models that explores the relationship between model complexity and generalization in 1.Regression models 2.Classification models\n'], 'url_profile': 'https://github.com/megs161195', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Jun 21, 2020', 'R', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jul 13, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'Vancouver', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': ['Predicting Customer Churn\nIn this project I try to build a logistic regression based model to predict whether the customer will churn out of telecom service or not.\nCustomer attrition is a big issue in any industry. Not surprisingly, one of the major focus of a data scientist is to reduce customer attrition and increase customer retention. It is relatively easier to predict and detect in the industries where monthly billing service exists Eg: telecom, internet, streaming service etc. From an organizational perspective, it is always cheaper to retain existing customer than to spend money to acquire new customer.\nIn this post, my focus is to try and build a simple model to predict whether a customer will churn or not given a dataset.\nDataset\nThis dataset has total 11 columns including a column called churn which is our dependent variable, and 10 columns which are our predictor variables.\nWe will try and identify the variables which are significant in predicting customer churn and try to build a logistic regression model which will accurately predict the churn.\n\nAll the steps taken are outlined in my blog post here.\nAll the code in the R file, should be self explanatory.\n'], 'url_profile': 'https://github.com/rckclimber', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Jun 21, 2020', 'R', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jul 13, 2020', 'HTML', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cyder11', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'Python', 'Updated Feb 1, 2020', 'Python', 'Updated Jun 21, 2020', 'R', 'Updated Sep 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Jul 13, 2020', 'HTML', 'Updated Jan 7, 2020']}"
"{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TheDeclineOfTheAmericanMiddleClass', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Feb 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Feb 3, 2021', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['User-Ad-Click-Predictive-Model\nUsed Pandas, Numpy, Seaborn, Matplotlib, and Sklearn to analyze an artificial Advertising data and create a Logistic Regression Model to predict whether a user will click on an Ad based off the features or interests of that user.\nThe data and exercise template were provided by an instructor at The Knowledge House (Tech Bootcamp). For easy access, I have added the data (which I do not own) to my Qri data profile: https://qri.cloud/ldiakite/advertising_data\n'], 'url_profile': 'https://github.com/Diakite94', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Feb 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Feb 3, 2021', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/paillacharan', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Feb 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Feb 3, 2021', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Predicing-House-Sale-Prices\nIn this project, I cleaned and transformed features in the dataset, and predicted house prices with linear regression model generated using selected features.\n'], 'url_profile': 'https://github.com/sammyzhy', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Feb 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Feb 3, 2021', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Data_Science_Practical_Approach\nCore Examples for Data Science in Python and R  :Data Preprocessing, Data Cleaning,  Exploratory Data Analysis, Visual Analysis, Regression Analysis, Neural Networks, etc.\n'], 'url_profile': 'https://github.com/xisco891', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Feb 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Feb 3, 2021', 'Updated Jan 8, 2020']}","{'location': 'Here', 'stats_list': [], 'contributions': '249 contributions\n        in the last year', 'description': ['Simple application that just uses some formulas to calculate y = mx + b\nAlso calculates how good of a fit the best fit line is, namely by calculating SS_ret and SS_tot which are used to calculate coefficient of determination (r^2), and correlation coefficient (r)\nScreenshots\n\n\n'], 'url_profile': 'https://github.com/ama6nen', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Feb 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Feb 3, 2021', 'Updated Jan 8, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sheery96', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Feb 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Feb 3, 2021', 'Updated Jan 8, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Black_Friday_Sale_Prediction\nPredicting Prices for the products to be sold on Black Friday in US using Regression Analysis, Feature Engineering, Feature Selection, Feature Extraction and Data analysis - Data Visualizations.\ndataset\nhttps://www.kaggle.com/sdolezel/black-friday\n'], 'url_profile': 'https://github.com/KhizarSultan', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Feb 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Feb 3, 2021', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '331 contributions\n        in the last year', 'description': ['machine-learning-for-business-applications\nAlgorithms for Data Guided Business Intelligence (ADBI)\nProjects (source-code)\n+ Preprocessing \n    - Data-wrangling \n\n+ Apache Spark and Kafka\n    - Graph-embedding-recommender-systems-project\n    - Network-properties-with-apache-spark\n    - Recommendation-systems-using-apache-spark\n\n+ Deep Learning\n    - Dnn-architectures-for-defect-detection-using-keras-neural-networks\n\n+ Predictions\n    - Bitcoin-price-prediction\n    - Stochastic-gradient-descend-for-logistic-regression\n    \n+ Graph Mining\n    - Online-bipartite-graph-matching\n    - Attributed-graph-compunity-detection \n    \n+ Natural Language Processing        \n    - Twitter-sentimental-analysis \n    - Word2vec-and-doc2vec-sentiment-analysis \n\nDatabase Management System (DBMS)\nFinal Project (source-code)\nA publication house database management system using Java and MariaDB\n\nUser Experiences (UX)\n\nsyllabus\nFinal project\n\n'], 'url_profile': 'https://github.com/ssp4all', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Feb 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Feb 3, 2021', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Diabetes-Prediction\nLogistic Regression was uesd to predict whether or not a Patient has Diabetes based on certain diagnostic measures included in the dataset\n'], 'url_profile': 'https://github.com/eksola', 'info_list': ['Python', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 6, 2020', 'R', 'Updated Feb 11, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'HTML', 'Updated Feb 3, 2021', 'Updated Jan 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Nanoparticle-luminescence-\nWe have fitted the time resolved luminescence spectra of Nanoparticles (non linear regression) with the help of tri and 4 exponential function using Python\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(""V2 532NM.CSV"", sep = \',\')\ndf\ndf1 = df.iloc[1033:4997,:]\ndf1\ndf1.reset_index(drop = True, inplace = True)\ndf1\ncols = [\'B\']\ndf1[cols] = df1[df1[cols]>0][cols]\ndf1.dropna()\ndf1\ndf1 = df1[np.isfinite(df1[\'B\'])]\ndf1\nimport matplotlib.pyplot as plt\nplt.plot(df1.A, df1.B,\'o\')\nplt.title(""V2 lifetime"")\nplt.xlim(0,0.005)\nplt.ylim(0,2.0)\nplt.show()\nfrom math import *\n%matplotlib inline\nimport scipy.optimize\nfrom lmfit import Model\nfrom lmfit import minimize, Parameters, Parameter, report_fit\nt = df1[\'A\'].values\nrt = df1[\'B\'].values\nnoisy = rt + 0.001*np.random.normal(size=len(rt))\ndefine objective function: returns the array to be minimized\ndef fcn2min(params, t, noisy):\nc0 = params[\'c0\'].value\nc1 = params[\'c1\'].value\nc2 = params[\'c2\'].value\nc3 = params[\'c3\'].value\nc4 = params[\'c4\'].value\nc5 = params[\'c5\'].value\nc6 = params[\'c6\'].value\nc7 = params[\'c7\'].value\nmodel = c0 + (c1np.exp(-(t-c2)/c3)) + (c4np.exp(-(t-c2)/c5)) + (c6*np.exp(-(t-c2)/c7))\nreturn model - noisy\ncreate a set of Parameters\nparams = Parameters()\nparams.add(\'c0\', value= 0.1, min=0 )\nparams.add(\'c1\', value= 0.04, min=0)\nparams.add(\'c2\', value= 0.00003, min=0)\nparams.add(\'c3\', value= 0.00006, min=0)\nparams.add(\'c4\', value= 0.2, min=0 )\nparams.add(\'c5\', value= 0.0004, min=0)\nparams.add(\'c6\', value= 0.8, min=0)\nparams.add(\'c7\', value= 0.0008, min=0)\nparams[\'c2\'].vary = False\ndo fit, here with leastsq model\nresult = minimize(fcn2min, params, args=(t, noisy))\ncalculate final result\nfinal = noisy + result.residual\nwrite error report\nreport_fit(result.params)\ntry to plot results\ntry:\nimport pylab\npylab.plot(t, noisy, \'ko\')\npylab.plot(t, final, \'r\')\npylab.xlim(0,0.005)\npylab.xlabel(\'Time(ns)\')\npylab.ylabel(\'Fluorescence Intensity\')\npylab.show()\nexcept:\npass\nlmfit_Rsquared = 1 - result.residual.var()/np.var(noisy)\nprint(\'Fit R-squared:\', lmfit_Rsquared, \'\\n\')\nprint(result.params)\nprint(\'Fit X^2: \', result.chisqr)\nprint(\'Fit reduced-X^2:\', result.redchi)\nplt.plot(t,result.residual,\'r\')\n'], 'url_profile': 'https://github.com/Anindachem143', 'info_list': ['1', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 25, 2020', '3', 'Updated Feb 6, 2021', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hmzjanjua', 'info_list': ['1', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 25, 2020', '3', 'Updated Feb 6, 2021', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Credit Risk Analysis of Lending Club Debtors\nAssignment two of the Banking Analytics class taught by Cristian Bravo at Western University. Purpose was to evaluate the risk of debtors of Lending Club by predicting their PD and LGD using logistic regression and decision tree models.\n'], 'url_profile': 'https://github.com/Jay2theWhy', 'info_list': ['1', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 25, 2020', '3', 'Updated Feb 6, 2021', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/okimb', 'info_list': ['1', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 25, 2020', '3', 'Updated Feb 6, 2021', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Baseball-Analytics-and-Modeling\nPreformed ETL using Spark frameworks. Utilized SQL Joins in Spark to combine two data sets. Cleaned various aspects of the data including missing values , and data types. Aggregated, concatenated, and filtered data to prep for EDA and modeling. Used Linear Regression and Random Forests to model . Achieved RMSE of 0.1004 for Regression Model. Random Forest model predicted correct values with 91.75% accuracy.\n'], 'url_profile': 'https://github.com/kody8891', 'info_list': ['1', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 25, 2020', '3', 'Updated Feb 6, 2021', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020']}","{'location': 'Washington', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': [""FIFA-Soccer-Player\nThis is a team repository for analyzing the FIFA soccer players. The data are obtained from Kaggle (https://www.kaggle.com/karangadiya/fifa19). We built linear regression, logistic regression, KNN models to predict the soccer players' wage, international reputation, favoratie foot. The mixed work can be found in FIFA.py.\n""], 'url_profile': 'https://github.com/cying4', 'info_list': ['1', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 25, 2020', '3', 'Updated Feb 6, 2021', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sharma-harish', 'info_list': ['1', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 25, 2020', '3', 'Updated Feb 6, 2021', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['##K Nearest Neighbors with Python|ML##\nHow It Works ?\nK-Nearest Neighbors is one of the most basic yet essential classification algorithms in Machine Learning. It belongs to the supervised learning domain and finds intense application in pattern recognition, data mining and intrusion detection. ->The K-Nearest Neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems.\n->The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other. KNN captures the idea of similarity (sometimes called distance, proximity, or closeness) with some mathematics we might have learned in our childhood— calculating the distance between points on a graph. There are other ways of calculating distance, and one way might be preferable depending on the problem we are solving. However, the straight-line distance (also called the Euclidean distance) is a popular and familiar choice. ->It is widely disposable in real-life scenarios since it is non-parametric, meaning, it does not make any underlying assumptions about the distribution of data (as opposed to other algorithms such as GMM, which assume a Gaussian distribution of the given data).\nPre-requisites: Numpy, Pandas, matplotlib, sklearn\n->We’ve been given a random data set with one feature as the target classes. We’ll try to use KNN to create a model that directly predicts a class for a new data point based off of the features.\n'], 'url_profile': 'https://github.com/Sairaj9920', 'info_list': ['1', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 25, 2020', '3', 'Updated Feb 6, 2021', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020']}","{'location': 'Toulouse', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['income-prediction-94-95\n'], 'url_profile': 'https://github.com/BrunoPlzk', 'info_list': ['1', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 25, 2020', '3', 'Updated Feb 6, 2021', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harishkumar92', 'info_list': ['1', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 25, 2020', '3', 'Updated Feb 6, 2021', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020']}"
"{'location': 'London, UK', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Ames Iowa Housing Price Prediction\nPredicting housing prices using the Ames dataset using feature engineering, LASSO, and tree modelling.\nDetailed blog post here: https://nycdatascience.com/blog/student-works/property-analysis-i-house-prices-in-iowa/\nThe associated Kaggle competition that this code was designed for can be found at this link.\n'], 'url_profile': 'https://github.com/mfgriffin467', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2021', '1', 'HTML', 'Updated Oct 9, 2020', 'HTML', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Jan 10, 2020']}","{'location': 'Munich', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Data Modeling GUI\nApplication for small data modeling tasks with GUI\n\nFeatures\nWith this application the user is able to load data, preprocess and explore it (to some degree), and make predictions for regression or classification tasks.\n\nSelect Feature types (inputs/outputs, numeric/categorical)\nautomatic handeling of missing data\nautomatic data summary and visualization\nclassification and regression tasks (with models below)\ncustom model parameters\ndata and model export (.csv, .pkl)\n\nModels\nClassifiers\n\nDecision Tree Classifier\nRandom Forest Classifier\nGradient Boosting Classifier\n\nRegressors\n\nLinear Model\n$k$-Nearest Neighbors Regression\nDecision Tree Regression\nGaussian Process Regression\nNeural Network\nSupport Vector Regression\nXGB Regressor\n\nPossible Future Features\nBetter support for categorical data: use scikit ColumnTransfromer! Include more models, add a real progressbars, and add cross validation.\nDependencies\nThe following packes are required\n\ntkinter\nsklearn\nxgboost\nnumpy\npandas\npickle\nctypes\nmatplotlib\nseaborn\npandastable\ninspect\nwebbrowser\n\nFeedback is appreciated!\n'], 'url_profile': 'https://github.com/kohring', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2021', '1', 'HTML', 'Updated Oct 9, 2020', 'HTML', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Gaze Estimation\nGaze Estimation project for master thesis at Wroclaw University of Science and Technology.\nAbstract\nThe work aimed to propose a method of controlling the mouse cursor using eye movement based on the image from a video camera. The solution to the task required the development of a multi-stage input image processing pipeline, the main element of which was to predict the viewpoint using an artificial neural network model. The effectiveness of the method was evaluated experimentally. In addition to the benchmark set, the study used a self-collected data set. Analysis of the test results showed that the proposed method allows us to move the mouse cursor to the desired place, but it was not possible to move it exactly to the place where the user was looking at.\nTechnical details\nProject implemented in Python 3. It includes implementation of convolutional neural network in Tensorflow 2.0 and some classical methods of image processing.\n'], 'url_profile': 'https://github.com/lukaszsus', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2021', '1', 'HTML', 'Updated Oct 9, 2020', 'HTML', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Jan 10, 2020']}","{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '356 contributions\n        in the last year', 'description': ['Wine Quality\nPredict the quality of wine (red wine) based on different attributes.\nGoal\n\nUse Random Forest algorithm to create machine learning models\nEvaluate the model using cross-validation\nSelect the best model using grid-search\nPredict using the best model\n\nBackground\nA typical machine learning process involves training different models on the dataset, evaluating the performance of algorithm and selecting the one with best performance.\nThere are several factors that determine which algorithm performs the best:\n\nPerformance of the algorithm on cross validation set\nChoice of hyperparameters for the algorithm\n\nCross Validation for model accuracy\nThe training set is used to train the model and the test set is used to evaluate the performance of the model. However, this may lead to variance problem where the accuracy obtained on one test set is very different to accuracy obtained on another test set using the same algorithm.\nThe solution to this is the process of K-Fold Cross-Validation:\n\nDivide the data into K folds.\nOut of the K folds, K-1 sets are used for training while the remaining set is used for testing.\nThe algorithm is trained and tested K times, each time a new set is used as testing set while remaining sets are used for training.\nFinally, the result of the K-Fold Cross-Validation is the average of the results obtained on each set.\n\nGrid Search for Hyperparameter selection\nRandomly selecting the hyperparameters for the algorithm can be exhaustive. It is also not easy to compare performance of different algorithms by randomly setting the hyper parameters because one algorithm may perform better than the other with different set of parameters. And if the parameters are changed, the algorithm may perform worse than the other algorithms.\nGrid Search is an algorithm which automatically finds the best parameters for a particular model.\n\nCreate a dictionary of all the hyperparameters and their corresponding set of values that are set to test for best performance.\n\nThe name of the dictionary items corresponds to the parameter name and the value corresponds to the list of values for the parameter.\n\n\nCreate an instance of the algorithm class.\nPass the values for the hyperparameter from the dictionary.\nCheck the hyperparameters that return the highest accuracy.\nFind the accuracy obtained using the best parameters.\n\n\nDependencies\n\nPandas\nScikit-learn\n\npip install -r requirements.txt\nDataset\nUCI archive ML data: https://archive.ics.uci.edu/ml/datasets/wine+quality\nSaved in: data/winequality-red.csv\nData Preprocessing\n\nSeparate the features and labels\nPrepare data for cross-validation\n\nAll the data is kept in the training set\n\n\nScale the training data\n\nImplementing the algorithm\n\nRandom Forest Classifier\nEstimators = 300\n\nImplementing Cross Validation\n\nCross Validation Accuracy\nNumber of Folds = 5\n\nParameter selection for best model\n\nGrid Search\nEstimators: 100, 300, 500, 700, 1000\nCriteria: gini, entropy\nWith and without bootstrap\n\nConclusion\n\nK-Fold Cross-Validation is used to evaluate performance of a model by handling the variance problem of the result set.\nTo identify the best algorithm and best parameters, the Grid Search algorithm is used.\n\n'], 'url_profile': 'https://github.com/likarajo', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2021', '1', 'HTML', 'Updated Oct 9, 2020', 'HTML', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Description\nSmartBoard is a smart business dashboard that allows users to apply one or more Data Science algorithms to a dataset (e.g. Regression, Clustering, Classification, Association) to solve a business data problem (such as fraud detection, sales forecasting, customer segmentation ... etc).\nObjectif\n•\tProvide an easy to use yet powerful solution for Data Analytics using a predefined algorithms and ready to use data model  for most common data problem.\nHow it works?\n•\tCompanies can upload their data to the dashboard (via http or ftp) in different formats (csv, xlsx)\n•\tThey can then select an appropriate algorithm (Regression, Classification, Clustering, Association) to solve a specific business data problem (things like: Fraud detection, Customer Segmentation, Sales forecast)\n•\tGenerate a Machine Learning model for different purposes (Prediction, Classification…etc.)\n•\tSave the generated data model for further analysis (each user/company has his own space to store and restore data model)\n•\tUse the predefined model on a new uploaded dataset for analysis\nShow clustering algorithm\n• This screenshoot shows the clustering algorithm\n\nInstall the requirements inside the app folder\n• $ pip install -r requirements.txt\nSmartboard_test\nTest repo for the Smartboard project\nSheet Js file\nwith Some functions  generate different views of the sheets:\nsheet_to_csv generates CSV\nsheet_to_txt generates UTF16 Formatted Text\nsheet_to_html generates HTML\nXsheet_to_json generates an array of object.\nsheet_to_formulae generates a list of formulae\n'], 'url_profile': 'https://github.com/CorpoSense', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2021', '1', 'HTML', 'Updated Oct 9, 2020', 'HTML', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['GraduateAdmissions\nGraduate Admissions is all about predicting whether the student will get admission into the school or not .The analysis has been done by exploring the overall structure of the dataset and summary statistics of each variable. Also, analyzed the number of observations, their data type. As the target value is continuous so, I have used Regularized linear Regression( Lasso, Ridge, Enet), Neural Network, Random Forest, Gradient Boosted Trees and Linear Regression to find the results.\nThe objective is to help students to predict graduate admissions from an Indian Perspective. The target value shows chance of admit in a university for a student. Target variable is “Chance of Admit “.It ranges from 0 to 1.The dataset is inspired by the UCLA Graduate Dataset. The dataset is ""Admission_Predict"" which is available on Kaggle\'s website . There are 400 observations of 9 variables. The attributes are  Serial No. , GRE Score, TOEFL Score, University Rating, SOP, LOR, CGPA, Research, Chance of Admit. All the given attributes are numerical. I have dropped the “Serial No.” variable as it is not a useful predictor. Created the factor of “Research” variable as it had binary values.\n'], 'url_profile': 'https://github.com/DograSurbhi', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2021', '1', 'HTML', 'Updated Oct 9, 2020', 'HTML', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['PredictingDefault\nThis repository contains Python code that implements logistic regression, decision tree, random forests, and naive bayes classifiers to predict the probability that a firm will go bankrupt within the next one-year and compare the performance of these models.\n'], 'url_profile': 'https://github.com/priyakal', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2021', '1', 'HTML', 'Updated Oct 9, 2020', 'HTML', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Jan 10, 2020']}","{'location': 'France', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['Classifying-Songs-Genres-From-Audio-Data\nExploring a music dataset by examining correlations between numerical variables,  running a principal component analysis for dimensionality reduction and finally\nfitting both Decision Tree Classification and Logistic Regression models to compare their performance.\n'], 'url_profile': 'https://github.com/femtonelson', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2021', '1', 'HTML', 'Updated Oct 9, 2020', 'HTML', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Jan 10, 2020']}","{'location': 'Halifax', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aakashpatel379', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2021', '1', 'HTML', 'Updated Oct 9, 2020', 'HTML', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vyas4853', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Mar 28, 2020', '1', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2021', '1', 'HTML', 'Updated Oct 9, 2020', 'HTML', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'Updated Jan 10, 2020']}"
"{'location': 'Shanghai, China ', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['项目介绍\n\n这个项目旨在通过运用统计课程中所有的内容，来解决某网站的运营问题。\n\n\n具体包括了概率计算、A/B测试（假设检验）、回归模型（逻辑回归）等统计学上的知识以及数据清洗的方法。\n\n\n针对该门课程中的具体内容，可以通过打开 analyze-ab-test-results.ipynb 文件获得内容。\n\n若加载该文件失败，可以访问这个网站：https://nbviewer.jupyter.org/ ，把github上对应的文件的url输进去，就可以在线渲染jupyter文件了。\n'], 'url_profile': 'https://github.com/Jakejck', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 9, 2020', '1', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['Classification of QSAR Biodegradation data set\nIntroduction\nQuantitative Structure Activity Relationship (QSAR) biodegradation data set.\nThe data set source is UCI machine learning repository.\nThe data set has 1,055 instances (molecules) and their 41 features (chemical and physical properties).\nEach molecule is either readily biodegradable (RB) or not readily biodegradable (NRB).\nLogistic regression, logistic with lasso, logistic with ridge, radial SVM,\nand random forests are used here to classify each molecule as RB or NRB based on\nits 41 features.\nTrain and Test Error Rates\nThe following was repeated 100 times to capture the spread of train error, test error,\nand minimum CV error rates for all 5 classification methods.\nThrough random sampling, the data set was separated into 90% train set and the\nrest was used as test set data. The data was weighted through resampling since there\nwas some imbalance of 66% NRB and 34% RB.\nUsing the train set, the hyper parameters of logistic lasso, logistic ridge, and\nradial svm were tuned with 10-fold CV. The minimum CV error rates were extracted.\nThe fitted models were used to capture train error, test error, false positive (fp)\ntrain error, false negative (fn) train error, fp test error, and fn test error rates.\nAs shown in the figure below, of the 5 classification methods rf and svm models have the\nlowest train error rates with the train set data. However, these two models perform much\nworse with test set data, while the rest of the models, although have more spread\nout error rates, on average have error rate similar to those with their train set data.\nThe box plot of test fn errors stands out, here, svm and rf have the worst performance\nand should not be used for identification of RB molecules, instead logistic ridge\nappears to have the best overall performance for purpose of identifying RB molecules.\n\nMinimum Cross-Validation Error Rates\nThe following figure shows the spread of minimum CV error rates for logistic lasso,\nlogistic ridge, and svm.\n\nCross-Validation Error vs L2-Norm of Beta Coefficients for Logistic Lasso and Ridge\nFrom the figure below, the smallest cross-validation errors for logistic lasso and\nlogistic ridge are similar to the cv error of unregularized logistic regression.\n\nHeatmap of radial SVM CV Error Rates\nThe heatmap is used to determine the SVM parameters (gamma and cost) that give the\nsmallest cross-validation error rate. In the case of QSAR biodegradation data set,\nthe best cost parameter appears to be equal to 100 and the best gamma parameter is 0.1.\n\nVariable Importance of Logistic Lasso, Logistic Ridge, and SVM Methods\nV19 was too sparse and was not used with any of the classification methods.\nFeature definitions can be found here.\nThe logistic lasso and logistic ridge have similar patterns for coefficient importance.\nHowever, logistic lasso tends to emphasize some\ncoefficients and reduce others to zero, while logistic ridge tends to reduce all\ncoefficients in proportion to their importance. Hence, lasso coefficients are either\nlarge or small, while those of ridge are somewhere inbetween. The variable importance\npattern of random forests method is completely different from the patterns of logistic\nlasso and logistic ridge, because random forests method is non-linear method and logistic\nregression is a linear method, two different methods produce two different patterns for\nvariable importance.\n \n\n'], 'url_profile': 'https://github.com/asyakhl', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 9, 2020', '1', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Bloomington, IN', 'stats_list': [], 'contributions': '228 contributions\n        in the last year', 'description': ['Overwork, Specialization, and Wealth\n\n\n\nAbout\nThis repository was written in 2017-2018 and was used to publish this paper. With data from the Survey of Consumer Finances (SCF), this repository estimates unconditional quantile regression models to investigate how overwork and household specialization are associated with household wealth across socioeconomic strata and over time. Results indicate that overwork has the greatest absolute benefits at the top of the wealth distribution but the greatest relative benefits in lower portions of the wealth distribution. Specialization yields distinct advantages for high-wealth households that have grown over time, whereas specialization comes with tradeoffs for low-wealth households that outweigh its benefits. The scripts were initially written with SAS, but R is used in later steps for estimating unconditional quantile regression models and visualizing the results.\n'], 'url_profile': 'https://github.com/BrianAronson', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 9, 2020', '1', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vyas4853', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 9, 2020', '1', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Waterloo', 'stats_list': [], 'contributions': '175 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lingwei-gu', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 9, 2020', '1', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Sunnyvale, California', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bhaskarnn9', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 9, 2020', '1', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'London, United Kingdom', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Dataset\n\nThis dataset was provided by Udacity as part of their Data Analyst nanodegree program. The data consisted of results from a fictional e-commerce website, testing whether the 'conversion' rate of their customers improved with the aid of a new website.\n\n\nThe following libraries were used to complete this project in Jupyter Notebook:\n\n\npandas\nnumpy\nmatplotlib\nrandom\n\nSummary of Findings\n\nStatistical analysis was used to determine whether the results from the A/B test followed that the new website outperformed the old website.\nBootstrapped samples of data were used to analyse the results, failing to reject the null hypothesis that the old website performed at least as well as the new website, if not better.\nLogisstic regression was then used to confirm the findings.\n\n""], 'url_profile': 'https://github.com/yasirgani', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 9, 2020', '1', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '246 contributions\n        in the last year', 'description': ['Breast-Cancer-Detection\nThis project deals with binary classification by employing a supervised Learning technique called Logistic regression to predict whether a person has a breast cancer or not based on the fine needle aspirate (FNA) of a breast mass.\nABSTRACT\nMachine learning, since its inception has been trying to solve several real-world problems that hinder the progress in society. It has succeeded in many domains, medical field being one of the most successful domains where the use of machine learning has proven to be very effective and has become very vital.  This project is related to medical field and deals with binary classification by employing a supervised Learning technique called Logistic regression to predict whether a person has a breast cancer or not based on the fine needle aspirate (FNA) of a breast mass. This report records the training, validation and testing accuracy along with precision and recall for each. |\nIntroduction\nLogistic regression falls under the category of supervised learning that operates on discrete data. The Wisconsin cancer data set given for this project contains a set of 30 features along with a separate column that classifies if the patient has a cancer or not. The dataset is divided into 3 parts for training, validation and testing. The model is trained using the training dataset, validated immediately with the newly updated learnable parameters (weight and bias). With the finally calculated gradients, the accuracy of a trained model can be tested on a totally new subset of the data. The model, based on the patient\'s feature set would classify which class the patient belongs to; class 0 is benign and class 1 is malignant.\nDataset\nThe Wisconsin dataset consist of 30 real valued input features which were computed from a digitized image of a Fine Needle Aspirate (FNA) of a breast mass.  The computed feature describes the following characteristics of the cell nuclei present in the image.\n\n\n\nS.NO\nCharacteristics of a cell Nuclei\n\n\n\n\n1\nRadius (mean of distances from center to points on the perimeter)\n\n\n2\nTexture (standard deviation of gray-scale values)\n\n\n3\nPerimeter\n\n\n4\nArea\n\n\n5\nSmoothness (local variation in radius lengths)\n\n\n6\nCompactness (perimeter2/area − 1.0)\n\n\n7\nConcavity (severity of concave portions of the contour)\n\n\n8\nConcave points (number of concave portions of the contour)\n\n\n9\nSymmetry\n\n\n10\nFractal dimension (""coastline approximation"" - 1)\n\n\n\nTable 2.1 Tabulation of the Wisconsin dataset displaying the input features\nThe mean, standard error, and ""worst"" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features.\nThe dataset consists of 569 patient information. The dataset is divided into a set of three for training, validation and testing int the ratio of 8:1:1 respectively.\nData preprocessing\nThe dataset provided to us consist of the ID, Diagnosis information along with 30 features of the patient.\n\nThe id of the patient is dropped because of its irrelevancy for training our model.\nThe diagnosis information of the patient provided as Malignant and Benign has been replaced with 1 and 0 respectively. This step is essential as this information in form of a matrix would be given as an input to the cost function and confusion matrix.\nEach feature is normalized with min-max normalization which will convert each feature in the range of 0 to 1. Normalization is very useful in the cases where the feature is of different range. Having a different range might end up delaying the process of gradient descent as the weight and bias might take a lot of time to find a global minimum. Normalization also makes sure that if there are few data in the feature set that deviates to a greater than the normal, it doesn\'t vary the calculation of mean or standard deviation to a greater extent.\nThe normalized feature set and the diagnostic information is split into a set of 3, for training, validation and testing in the ratio of 8:1:1 respectively.\nThe transpose of each of the features along with the diagnosis information is calculated, this is useful while performing dot products with randomized weights.\nThe bias, weight, learning rate and epochs are initialized to a random value, the gradients are updated for each epoch while the hyper parameter is given by the user.\n\nArchitecture\nTraining the logistic regression model involves finding out the global minimum of gradients (weight and bias) and improving the accuracy of the model by varying the hyper parameters (Epoch and Learning rate). The first step to calculate the logistic regression involves calculating the hypothesis function which is described in the image given below.\n\nFigure 5.1 Hypothesis function that limits the output of loss equation between 0 and 1\nThe first step involves multiplying randomized weight matrix with the features and adding it to the bias. The resulting matrix is then passed to the Sigmoid function. Sigmoid function is given by the equation.\n\nFigure 5.2 Formula of the Sigmoid function\nThe sigmoid function converts the real valued data Z in the range of 0 to 1. In the case of cancer classification, the output value of the sigmoid function would be helpful to determine whether a cancer is malignant or benign based on decision boundary.\n\nFigure 5.3 Output of the sigmoid function with respect to the decision boundary\nThe performance of the classification model is determined using Cross-Entropy or a Cost function. This function works with values between 0 and 1. The output of the cost function is a single value. The Cost equation J(θ) is given below equation.\n\nFigure 5.4 Cost equation for convergence of gradients\nThe forward propagation ends with the calculation of cross entropy. The gradients (weights and bias) have to be updated for the next epoch and the equation to update the weights and bias is given below,\n\nFigure 5.5 Equation to update the weights and biases.\nThe dw and db in the equation stands for derivative weight and derivative bias.\n\nFigure 5.6 Equation to calculate the derivative bases and derivative weights\nThe derivative weight (dw) is calculated by taking the dot product of input feature matrix and the difference of hypothesis function and output matrix and finding their mean.\nThe derivative bias (db) is calculated by taking the mean of difference between the hypothesis function and the output matrix. The equations to calculate the dw and db are given below.\nThe gradients are updated and the forward propagation takes place for the validation dataset. The updated values of the gradients are then passed to the forward propagation of the training set again for epoch number of times.\nFinally, the precision, recall and f-measure are calculated using the confusion matrix which takes the actual test outputs and predicted outputs as its parameters and provides performance measure in terms of accuracy, recall and precision.\nThe formulas to calculate Accuracy, Recall and f-measure is given below.\ntext: \nFigure 5.7 Formula to calculate Accuracy, Recall and Precision from a confusion matrix\nWhen the final gradients are obtained they are then tested on an unseen testing dataset and the accuracy is calculated. The hyper parameters are tuned until higher accuracy is obtained for the model.\nResult\nThe hyperparameters (learning rate and epochs) were tuned to find the best model. The graphs below show training accuracy, training cost, validation accuracy and validation cost along with their Accuracy precision, recall and f-measure.\n\n\n\nFor an epoch of 4100 and a learning rate of 0.78\n\n\n\n\nFigure 6.1 (A) and (B) Figures representing Accuracy vs Epoch and Cost vs Epoch for epoch of 4100 and learning rate of 0.78\n\n\n\nTest Accuracy\nPrecision\nRecall\nf-Measure\n\n\n\n\n96.49%\n93%\n100%\n96%\n\n\n\nTable 6.2 Representing the test accuracy, precision, recall and f-measure calculated from the predicted values for the epoch of 4100 and learning rate of 0.78\n\n\n\nEpochs\nConvergence of cost\n\n\n\n\nCost at 0 epochs\n0.692156\n\n\nCost at 1200 epochs\n0.103530\n\n\nCost at 2400 epochs\n0.084981\n\n\nCost at 3600 epochs\n0.076571\n\n\n\nTable 6.3 Representing the convergence of cost for the epoch of 4100 and learning rate of 0.78     |   |   |   |   |    |   |\n\n\n\nFor an epoch of 7500 and a learning rate of 0.054\n\n\n\n\nFigure 6.4 (A) and (B) Figures representing Accuracy vs Epoch and Cost vs Epoch for epoch of 7500 and learning rate of 0.054\n\n\n\nTest Accuracy\nPrecision\nRecall\nf-Measure\n\n\n\n\n92.98%\n90%\n96%\n93%\n\n\n\nTable 6.5 Representing the test accuracy, precision, recall and f-measure calculated from the predicted values for the epoch of 7500 and learning rate of 0.054\n\n\n\nEpochs\nConvergence of cost\n\n\n\n\nCost at 0 epochs\n0.692601\n\n\nCost at 1200 epochs\n0.263673\n\n\nCost at 2400 epochs\n0.203133\n\n\nCost at 3600 epochs\n0.175513\n\n\nCost at 4800 epochs\n0.158583\n\n\nCost at 3600 epochs\n0.146742\n\n\nCost at 3600 epochs\n0.137830\n\n\n\nTable 6.6 Representing the convergence of cost for the epoch of 7500 and learning rate of 0.054\n\n\n\nFor an epoch of 5000 and a learning rate of 0.1\n\n\n\n\nFigure 6.7 (A) and (B) Figures representing Accuracy vs Epoch and Cost vs Epoch for epoch of 5000 and learning rate of 0.1\n\n\n\nTest Accuracy\nPrecision\nRecall\nf-Measure\n\n\n\n\n94.73%\n93%\n96%\n95%\n\n\n\nTable 6.8 Representing the test accuracy, precision, recall and f-measure calculated from the predicted values for the epoch of 5000 and learning rate of 0.1\n\n\n\nEpochs\nConvergence of cost\n\n\n\n\nCost at 0 epochs\n0.693530\n\n\nCost at 1200 epochs\n0.208896\n\n\nCost at 2400 epochs\n0.162891\n\n\nCost at 3600 epochs\n0.141495\n\n\nCost at 4800 epochs\n0.128327\n\n\n\nTable 6.9 Representing the convergence of cost for the epoch of 5000 and learning rate of 0.1\n\n\n\nLearning rate\nEpochs\nTraining accuracy** In % **\n** Validation accuracy **In %\nTest accuracy In %\n\n\n\n\n0.01\n4000\n92.74725\n96.49123\n92.98246\n\n\n0.01\n5000\n93.40659\n96.49123\n92.98246\n\n\n0.01\n5700\n93.40659\n96.49123\n92.98246\n\n\n0.01\n6000\n93.18681\n96.49123\n92.98246\n\n\n0.2\n5000\n97.8022\n98.24561\n96.49123\n\n\n0.2\n6000\n97.8022\n98.24561\n96.49123\n\n\n0.24\n7500\n98.24176\n98.24561\n96.49123\n\n\n0.27\n7500\n98.24176\n98.24561\n96.49123\n\n\n0.3\n7500\n98.46154\n98.24561\n96.49123\n\n\n0.34\n5500\n98.24176\n98.24561\n96.49123\n\n\n0.6\n5500\n98.46154\n98.24561\n96.49123\n\n\n0.6\n6000\n98.46154\n98.24561\n96.49123\n\n\n0.55\n6000\n98.46154\n98.24561\n96.49123\n\n\n\nTable 6.10 Representing the test training accuracy, validation accuracy and test accuracy obtained by tuning the hyper parameters.\nConclusion\nA highest accuracy of 96.5 percentage was achieved by this logistic regression model by tuning the hyperparameters and setting the value of learning rate to 0.2 and epoch to 5000. Thus, logistic regression is one of the effective supervised machine learning technique for two class classifier problem involving discrete data.\nReferences\n[1] – Cost function in logistic regression - https://www.internalpointers.com/post/cost-function-logistic-regression\n[2] – Introduction to logistic regression - https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148\n[3] – Beyond Accuracy: Precision and Recall - https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c\n'], 'url_profile': 'https://github.com/adhithyakrishna', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 9, 2020', '1', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Magdeburg, Germany', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Chinmaya-Hegde', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 9, 2020', '1', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Baltimore', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rohitkumar1438', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'R', 'Updated Jan 9, 2020', '1', 'R', 'Updated Jan 6, 2020', 'R', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}"
"{'location': 'Washington', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['Seattle-Housing-price\nThis is a team repository for a housing market prediction model for the Seattle area based on the kc_house_data.csv dataset, retrieved on kaggle.com. Our initial EDA and visualization section can be found in the Project 1 folder, while use of regression models can be found in the Project 2 folder.\n'], 'url_profile': 'https://github.com/cying4', 'info_list': ['HTML', 'Updated Jan 8, 2020', 'R', 'Updated Jan 10, 2020', '1', 'R', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 8, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'MIT license', 'Updated Jun 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vyas4853', 'info_list': ['HTML', 'Updated Jan 8, 2020', 'R', 'Updated Jan 10, 2020', '1', 'R', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 8, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'MIT license', 'Updated Jun 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vivianluckyzhou', 'info_list': ['HTML', 'Updated Jan 8, 2020', 'R', 'Updated Jan 10, 2020', '1', 'R', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 8, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'MIT license', 'Updated Jun 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Google_Play_Store_Analysis_Using_R\nHere i have used Google Play store dataset from Kaggle.com.In this Whole Project My main hypothesis was to predict Google App Rating based upon Google App Reviews and Google App Installs using simple linear regression and in order to do this i have gone through several steps. First, i have tried to visualize whole dataset to get insight than i have done Exploratory Data Analysis of all three variables on which i  applied bivariate and univariate analysis.At last i have tried to apply linear regression model to find relationship or to predict Google App Rating based upon Google App Reviews and Google App Installs.\n'], 'url_profile': 'https://github.com/viraj1131', 'info_list': ['HTML', 'Updated Jan 8, 2020', 'R', 'Updated Jan 10, 2020', '1', 'R', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 8, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'MIT license', 'Updated Jun 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NLP-with-Python\nYorumları analiz edebilmek için orjinal kelime, N-gram, kelime vektörleri yöntemleri kullanılarak her bir yöntem ile tf-idf şeklinde veri setleri oluşturuldu. Analiz için Random Forest, Naive Bayes, LinearSVC, SGD ve Logistic Regression sınıflandırıcıları kullanılmış ve transfer yöntemleri denenmiştir. Transfer öğrenme konusunda en başarılı metin sınıflandırıcının n-gram, en başarılı sınıflandırıcının ise LinearSVC olduğu görülmüştür.\n'], 'url_profile': 'https://github.com/bkaracas', 'info_list': ['HTML', 'Updated Jan 8, 2020', 'R', 'Updated Jan 10, 2020', '1', 'R', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 8, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'MIT license', 'Updated Jun 13, 2020']}","{'location': 'Chicago, Illinois', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Homezilla-Attracting-home-buyers-through-better-photos\nNote: Data cannot be disclosed due to Confidentiality and Non-Disclosure Agreement.\nProblem Statement: To analyze the user browsing data in an attempt to learn which photos were considered to be the best and worst photos and how to keep users’ attention on the photos as long as possible. Also, to determine if the sequence and type of the photos that attracted the customers the most.\nMethodology: Analyzed 62 Properties’ 29,492 records of browsing data for Homezilla. Performed data transformation, data cleaning and univariate analysis as a part of initial understanding of the data. Applied logistic regression model and determined the importance of the sequence of photos and type of photos on customer attraction.\n'], 'url_profile': 'https://github.com/sheethalsridhar', 'info_list': ['HTML', 'Updated Jan 8, 2020', 'R', 'Updated Jan 10, 2020', '1', 'R', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 8, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'MIT license', 'Updated Jun 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/victoriakalin', 'info_list': ['HTML', 'Updated Jan 8, 2020', 'R', 'Updated Jan 10, 2020', '1', 'R', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 8, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'MIT license', 'Updated Jun 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Google Analytics Project\nIntroduction:\nCompanies big or small should always leverage the data they have about the customers to improve their experience, target new customers etc.\nThe goal of this project was to help management of a solar energy based start-up to understand factors influencing sign-ups.\nData source :\nThere are two different datasets.\n\n4 months of data scrapped from google analytics - summary level data\nIndividual level data from a third-party.\n\nTechnologies:\nPython 3.7.3, Big Query\nAlgorithms :\n\nLogistic Regression\nRandom Forest\nXGboost\n\nApproach :\n\nSummary level data – This data has a behavioral features like bounce rate, exit rate, referres etc\n..*\tUsed tableau to understand trends in the data and derive insights\n..*\tUsing logistic regression, random forest and XGboost to understand important features\nIndividual level data – This is the data from the third party. The data has features like –\n..*\tUsed logistic regression and random forest to understand feature importances\n\nReccomendations :\nThe company should go ahead and use a third party vendor to get the data as it is economical. Recommended marketing channels which will boost number of sign-ups.\n'], 'url_profile': 'https://github.com/Asayesha', 'info_list': ['HTML', 'Updated Jan 8, 2020', 'R', 'Updated Jan 10, 2020', '1', 'R', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 8, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'MIT license', 'Updated Jun 13, 2020']}","{'location': 'Riyadh, Saudi Arabia', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Heart-disease-detection-using-Machine-Learning\nMany predictive models are being used in different medical domains. With the variety of options, the most used models in diagnostic and prognostic tasks are logistic regression (LR) and artificial neural networks (ANN). We will use UCL Cleveland Heart Disease dataset that consists of 303 examples and 14 features. The dataset is split into a training set, validation set, and testing set. The project workflow consists of two steps, training the models, and testing them. In this report, we show the results of training artificial neural networks, and logistic regression with and without normalization. Then we proceeded by comparing the results and explaining why training artificial neural networks outperform logistic regression\nIntroduction\nMany predictive models are being used in different medical domains. With the variety of options, the most used models in diagnostic and prognostic tasks are logistic regression (LR) and artificial neural networks (ANN) [1]. We will use UCL Cleveland Heart Disease dataset that consists of 303 examples and 14 features. The dataset is split into a training set, validation set, and testing set. The project workflow consists of two steps, training the models, and testing them. In this report, we show the results of training artificial neural networks, and logistic regression with and without normalization. Then we proceeded by comparing the results and explaining why training artificial neural networks outperform logistic regression.\nFunctions\nIn order to accomplish the task, we implemented 7 functions:\n\nnormalizeFeatures(x) : x\nsigmoid(z): g\ncomputeLRCost(x, y, theta, lambda): [cost, grad]\nlearnLRTheta(x, y, lambda): theta\ntrainLRModel(x, y, lambda_values): [theta, lambda]\npredictClass(x, theta, threshold): y\ntestPerformance(y, y_predicted): [acc, recall, precision, fScore]\n\nExperiment\nAfter completing our implementation, we applied the following experiments and reported their results below:\nExperiment 1: logistic regression without feature normalization\nDue to some mathematical complications, we had to use a very small learning rate (0.0003), otherwise, the log() = infinity. Training (blue) and validation (yellow) errors Vs. lambda values curve is shown in the following figure:\n'], 'url_profile': 'https://github.com/elaugh9', 'info_list': ['HTML', 'Updated Jan 8, 2020', 'R', 'Updated Jan 10, 2020', '1', 'R', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 8, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'MIT license', 'Updated Jun 13, 2020']}","{'location': 'Washington D.C.', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': [""Reducing Staff Attrition\nAn R project that analyzes a company's employee data through a logistic regression and decision tree machine learning model in order to investigate the primary factors, along with their patterns, that impact and lead to high staff attrition rates and help develop actionable insights and comprehensive strategies to significantly reduce attrition.\nResults\n\n\n""], 'url_profile': 'https://github.com/pkhiyara', 'info_list': ['HTML', 'Updated Jan 8, 2020', 'R', 'Updated Jan 10, 2020', '1', 'R', 'Updated Jan 9, 2020', 'HTML', 'Updated Jan 12, 2020', 'Python', 'Updated Jan 8, 2020', 'Updated Jan 11, 2020', 'HTML', 'Updated Jan 6, 2020', '1', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'R', 'MIT license', 'Updated Jun 13, 2020']}"
"{'location': 'Windsor, Canada', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sami14996', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,075 contributions\n        in the last year', 'description': ['Google Play Store Apps\n*Note:\nDue to a GitHub bug (issue #3035 & #3555), sometimes the notebook files (files ending in "".ipynb"") may not render. Please reload the page until the content can be displayed, or click here to view the shared Google Colab notebook file.\nSource\n""Google Play Store Apps - App data of 1.1 Million applications"" dataset collected from this kaggle repository and created by G. Prakash.\nDescription & Objectives\nPerformed exploratory data analysis with descriptive statistics and data visualization, and applied various machine learning techniques. The analysis undertaken was in the context of the CS982 ""Big Data"" module of Strathclyde University (Glasgow, UK) in the academic year 2019-20.\nPurpose\nThe results and the diagnostic analysis of the project, using both supervised (classification) and unsupervised methods (clustering), led to insights that can be useful for identifying which attributes are linked with the most popular apps in the Google Play Store.\n'], 'url_profile': 'https://github.com/dimi-fn', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Tunisia', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': [""ASHRAE-Great-Energy-Predictor-III\nWe are using a dataset related to ASHRAE – Great Energy Predictor III (How much energy will a building consume?). The goal is to develop models from ASHRAE’s 2016 data in order to better understand metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a one-year timeframe. The method chosen to solve the problem is Linear Regression.\nObjective\nThe objective of this notebook is to provide a prdictive models using LSTM to predict\nHow much energy will a building consume?\nThe train dataset has our target variable called “meter reading” with datatype float, hence the task could be solved by RNN. The following methodology is used:\nOutline\n1.Data Understanding\n2.Data Preparation\n2.1 Merge tables\n2.2 Droping columns and filling null value for column: 'air_temperature', 'wind_speed', 'precip_depth_1_hr', 'cloud_coverage'\n2.3 Prepare train & test data for LSTM\n3.Modeling\n""], 'url_profile': 'https://github.com/abirkorched', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'new york city', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['airbnbprice\n• Used machine learning algorithms such as linear regression, logistics regression, random forest and XGBoost to predict price per night of Airbnb based on a large dataset with R.\n• Implemented data cleaning and data transformation, such as dealing with NAs in some levels and conducting logarithmic transformation of price to improve the result.\n• Utilized data visualization to explore the relationship between price and other variables with ggplot2, and selected variables that have strong relationships with price to advance the model performance.\nLink:https://www.kaggle.com/c/airbnblala1\n'], 'url_profile': 'https://github.com/Superxujwbruce', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Austin, Texas', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Employee-Attrition\nProblem Statement\nA large company named XYZ, employs, at any given point of time, around 4000 employees. However, every year, around 15% of its employees leave the company and need to be replaced with the talent pool available in the job market. The management believes that this level of attrition (employees leaving, either on their own or because they got fired) is bad for the company, because of the following reasons -\n\nThe former employees’ projects get delayed, which makes it difficult to meet timelines, resulting in a reputation loss among consumers and partners\nA sizeable department has to be maintained, for the purposes of recruiting new talent\nMore often than not, the new employees have to be trained for the job and/or given time to acclimatise themselves to the company\n\nAim\nIn order to curb attrition\n\nProbability of attrition using Logistic Regression\nThey want to know which of these variables is most important and needs to be addressed right away.\n\n'], 'url_profile': 'https://github.com/Aniketkumar18', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ujjwal12398', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['K-Nearest-Neighbor\nK-nearest neighbor is a non-parametric lazy learning algorithm, used for both classification and regression. KNN stores all available cases and classifies new cases based on a similarity measure. The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other. When a new situation occurs, it scans through all past experiences and looks up the k closest experiences. Those experiences (or: data points) are what we call the k nearest neighbors.\n'], 'url_profile': 'https://github.com/Avinash237', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Melbourne, Australia', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': [""UBER-Ride-Data-Wrangling-Analysis\nFolder Structure:\n 1.) 'data' folder:  Which has 5 different data files with their specific usecases defined in the notebook. \n 2.) 'dirty_data_solutions.csv' file: This is a solution file for dirty data (which has semantic or syntactic errors) which has got the data after correction. \n 3.) 'outliers_solution.csv' file: This is a solution file for outlier data which has got the modified data after handling the outliers.\n 4.) 'missing_value_solution' file: This is a solution file with imputed/ predicted values for missing data.\n 5.) 'UBER_Data_Wrangling.ipynb' file: This is python notebook which can be opened in  Jupyter Notebook or any other platform which supports python notebook.\n 6.) 'UBER_Data_Wrangling.pdf' file:  This is a pdf version for the python notebook which includes console output and codebase. This can be useful if someone does not want to run the code again and just want to understand the whole procedure.\nTo Run:\nYou need Jupyter Notebook or any other IDE to work on the codebase.\nDescription:\nHere we are working with the UBER ride sharing dataset of Victoria, Australia. \nWe will load the given datasets:  \n1.) dirty_data.csv, \n2.) outliers.csv, \n3.) missing_values.csv, \n4.) edges.csv, \n5.) nodes.csv \nWe will try to do some data wrangling & EDA to find different types of errors (Syntactic/Semantic) and understand the data, also we will try to analyse and find outliers(if any) in all the dimensions and try some linear regression models to predict the missing values.\n""], 'url_profile': 'https://github.com/himanshu2393', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'Italy', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/marcogulli01', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['My first repo on GitHub, a portfolio of some data science projects I undertook\n'], 'url_profile': 'https://github.com/dsn00b', 'info_list': ['2', 'Jupyter Notebook', 'Updated Jan 11, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 15, 2021', 'Jupyter Notebook', 'Updated Jan 8, 2020', 'R', 'Updated Jan 11, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', '1', 'Jupyter Notebook', 'Updated Apr 12, 2020', '1', 'Jupyter Notebook', 'Updated Feb 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Data-ANZ-Virtual-Experience\nThis was a virtual experience offered by ANZ through InsideSherpa as part of their Data@ANZ program, which provides some insights into the sort of work the data science team at ANZ do. I completed both modules:\n\n\nExploratory Data Analysis: I provided some preliminary descriptive statistics and segmented the data set by date and account, visualising transaction volume and spending and assessing the effect of outliers.\n\n\nPredictive Analytics: I estimated yearly salary of each account holder and explored correlations between salary and various customer attributes. Then I fit two predictive models - a multiple regression model and a decision-tree model and compared their in and out of sample predictive accuracy.\n\n\nI coded both these modules in R and produced a small set of slides for the first module, explaining the findings of the preliminary exploration as well as a report for the whole project with code, discussion and visualisations integrated.\n'], 'url_profile': 'https://github.com/BenCtree', 'info_list': ['Updated Jul 26, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 2, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Estimating-the-3D-pose-of-a-person-given-their-2D-pose\nIn this project I worked on estimating the 3D pose of a person given their 2D pose. Turns out, just regressing the 3D pose coordinates using the 2D pose works pretty well.In Part One, we are going to work on reproducing the results of the paper, in Part Two, we are going to try to find a way to handle noisy measurement.\n'], 'url_profile': 'https://github.com/shreyapriya700', 'info_list': ['Updated Jul 26, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 2, 2021']}","{'location': 'India', 'stats_list': [], 'contributions': '169 contributions\n        in the last year', 'description': ['bike-sharing-predictor\nBike sharing systems have been gaining prominence all over the world with more than 500 successful systems being deployed in major cities like New York, Washington, London. With an increasing awareness of the harms of fossil based mean of transportation, problems of traffic congestion in cities and increasing health consciousness in urban areas, citizens are adopting bike sharing systems with zest. Even developing countries like India are adopting the trend with a bike sharing system in the pipeline for Karnataka. This paper tackles the problem of predicting the number of bikes which will be rented at any given hour in a given city, henceforth referred to as the problem of ‘Bike Sharing Demand’. In this vein, this paper investigates the efficacy of standard machine learning techniques namely SVM, Regression, Random Forests, Boosting by implementing and analyzing their performance with respect to each other.This paper also presents two novel methods, Linear Combination and Discriminating Linear Combination, for the ‘Bike Sharing Demand’ problem which supersede the aforementioned techniques as good estimates in the real world.\n'], 'url_profile': 'https://github.com/VANRao-Stack', 'info_list': ['Updated Jul 26, 2020', 'Jupyter Notebook', 'Updated Jan 8, 2020', '1', 'Jupyter Notebook', 'Updated Feb 2, 2021']}",,,,,,,
