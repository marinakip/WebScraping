"{'location': 'NONE', 'stats_list': [], 'contributions': '136 contributions\n        in the last year', 'description': [""SiamCAR\n1. Environment setup\nThis code has been tested on Ubuntu 16.04, Python 3.6, Pytorch 0.4.1/1.2.0, CUDA 9.0.\nPlease install related libraries before running this code:\npip install -r requirements.txt\n2. Test\nDownload the pretrained model:\ngeneral_model code: lw7w\ngot10k_model code: p4zx\nLaSOT_model code: 6wer\n(The model in google Driver)\nand put them into tools/snapshot directory.\nDownload testing datasets and put them into test_dataset directory. Jsons of commonly used datasets can be downloaded from BaiduYun or Google driver. If you want to test the tracker on a new dataset, please refer to pysot-toolkit to set test_dataset.\npython test.py                                \\\n\t--dataset UAV123                      \\ # dataset_name\n\t--snapshot snapshot/general_model.pth  # tracker_name\nThe testing result will be saved in the results/dataset_name/tracker_name directory.\n3. Train\nPrepare training datasets\nDownload the datasets：\n\nVID\nYOUTUBEBB (code: v7s6)\nDET\nCOCO\nGOT-10K\nLaSOT\n\nNote: train_dataset/dataset_name/readme.md has listed detailed operations about how to generate training datasets.\nDownload pretrained backbones\nDownload pretrained backbones from google driver or BaiduYun (code: 7n7d) and put them into pretrained_models directory.\nTrain a model\nTo train the SiamCAR model, run train.py with the desired configs:\ncd tools\npython train.py\n4. Evaluation\nWe provide the tracking results (code: 8c7b) (results in google driver )of GOT10K, LaSOT, OTB and UAV. If you want to evaluate the tracker, please put those results into  results directory.\npython eval.py \t                          \\\n\t--tracker_path ./results          \\ # result path\n\t--dataset UAV123                  \\ # dataset_name\n\t--tracker_prefix 'general_model'   # tracker_name\n\n5. Acknowledgement\nThe code is implemented based on pysot. We would like to express our sincere thanks to the contributors.\n6. Cite\nIf you use SiamCAR in your work please cite our paper:\n\n@InProceedings{Guo_2020_CVPR,\nauthor = {Guo, Dongyan and Wang, Jun and Cui, Ying and Wang, Zhenhua and Chen, Shengyong},\ntitle = {SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking},\nbooktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2020}\n}\n\n""], 'url_profile': 'https://github.com/ohhhyeahhh', 'info_list': ['159', 'Python', 'Updated Oct 17, 2020', '9', 'Python', 'Updated Jan 14, 2020', '3', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 9, 2020', '4', 'Lua', 'GPL-3.0 license', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', '4', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', '2', 'R', 'Updated Dec 26, 2020']}","{'location': 'Lyon', 'stats_list': [], 'contributions': '407 contributions\n        in the last year', 'description': [""Streamlit app on regression\nStreamlit app for building a demo for regression to a somewhat arbitrary function. It's like regression-ipywidgets-viz but with Streamlit ;)\n\nInstall\nconda create -n regression-streamlit-viz python=3.7\nconda activate regression-streamlit-viz\npip install -r requirements.txt\n\nTo regenerate requirements.txt, run pip-compile requirements.in\nRun\nstreamlit run app.py\n\n""], 'url_profile': 'https://github.com/andfanilo', 'info_list': ['159', 'Python', 'Updated Oct 17, 2020', '9', 'Python', 'Updated Jan 14, 2020', '3', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 9, 2020', '4', 'Lua', 'GPL-3.0 license', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', '4', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', '2', 'R', 'Updated Dec 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['SeRGS\nSequential linear Regression Slopes\nA remote sensing based method developed to analyse changes in functioning of dryland vegetation.\nThe method aims at analysing functioning rather than pure productivity indices (e.g. NDVI).\nSince vegetation productivity in drylands in mainly determined by precipitation, this relationship may be described through a linear regression. The slope of a linear regression is known to reflect the unit change in the dependent variable (vegetation productivity) per unit change in the independent variable (precipitation) and can therefore be used as an indicator for vegetation functioning in drylands.\nAs the slope is representing vegetation functioning - changes in the slope over time may represent changes in vegetation biophysical processes, thus indicate changes in vegetation functioning.\nThe output of the SeRGS code will be a time series of slopes that then can be subject to either linear or segmented trend analysis or a driver (see below).\nFor details see:\nhttps://www.sciencedirect.com/science/article/pii/S0034425719300653\nDriver analysis - Principal Component Analysis & Regression\nTo estimate the relative importance of various variables potentially driving annual variations and long-term changes in vegetation functioning (SeRGS) we introduces a systematic data-driven, multiple regression approach (based on Seddon et al. (2016)).\nOn a per-pixel level, a Principal Component Analysis (PCA) is performed on the time series data of all potential driving variables chosen in the analysis (e.g. Temperature, soil moisture, population density, ...), to remove any impact from co-linearity between them. The PCA produces a set of new variables, the Principal Components (PCs), and With these PCs as independent variables and SeRGS as the dependent variable, a multiple linear, or Principal Component Regression (PCR) is calculated. By multiplying the slopes from the PCR with the loadings inherent to each PC (relating to the share of input variables they include), and summing the absolute values of these scores, a per-pixel estimate of the relative importance of each potentially driving variable on changes in vegetation functioning (SeRGS) can be obtained.\nFor details see:\n*** Manuscript currently under revision.\n'], 'url_profile': 'https://github.com/Frangi10ne', 'info_list': ['159', 'Python', 'Updated Oct 17, 2020', '9', 'Python', 'Updated Jan 14, 2020', '3', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 9, 2020', '4', 'Lua', 'GPL-3.0 license', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', '4', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', '2', 'R', 'Updated Dec 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['159', 'Python', 'Updated Oct 17, 2020', '9', 'Python', 'Updated Jan 14, 2020', '3', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 9, 2020', '4', 'Lua', 'GPL-3.0 license', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', '4', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', '2', 'R', 'Updated Dec 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['This repository contains code for simulation experiments and walkthroughs on using the estimator in Alidaee, Auerbach, and Leung (2020), ""Recovering Network Structure from Aggregated Relational Data Using Penalized Regression."" We provide implementations in Python and R, found in their respective folders. Walkthroughs are given in the notebook files walkthrough.ipynb. The file in the Python folder can be viewed in two ways.\n\n\nIt can be opened in your browser using this binder link:  Note that this might take a little bit of time to start up.\n\n\nYou can open the file on your desktop using the notebook viewer nteract. This method requires an installation of Python, the Python kernel, and the required modules (see instructions below), but it also allows you to use your own datasets instead of those provided in our example. To do so, save walkthrough.ipynb, nuclear_norm_module.py, and your datasets in CSV format to the same working directory, open the walkthrough file in nteract, and change the name of the CSVs in the walkthrough within the nteract user interface.\n\n\nThe file in the R folder can also be viewed in nteract after installation of R, the R kernel, and our R package. Instructions are given below.\nContents of Python folder\n\nwalkthrough.ipynb: walkthrough of Python implementation.\nARD_data.csv, type_data.csv: artificial data for the walkthrough.\nnuclear_norm_module.py: implementation of accelerated gradient descent method. The code builds on this repository.\nmonte_carlo.py: code for simulation experiments summarized in Table 2 of the paper.\neffective_rank.py: produces Table 1 of the paper, which simulates the effective ranks of M^* under three network formation models.\n\nTo run the .py files, we require Python 3 and installation of numpy, pandas, and scipy. See this walkthrough for installing Python via the Anaconda distribution, which also includes the required packages. If Python 3 is already installed on your machine, then you only need to install the packages, which can be done by entering into command line\npython3 -m pip install --user numpy scipy pandas\n\nIf you are using nteract to view the walkthrough file, you also have to install an ipython kernel. See here for instructions.\nTo execute monte_carlo.py, download our Python folder and its contents, change your current working directory to the Python folder, and enter into command line\npython3 monte_carlo.py\n\nContents of R folder\n\nwalkthrough.ipynb: walkthrough of R implementation.\nARD_data.csv, type_data.csv: artificial data for the walkthrough.\nnuclearARD_0.1.tar.gz: R package for our estimator.\nnuclear_ard: contents of R package.\n\nYou can install R here. To install our R package, download our R folder, open the R console, change the working directory to the location of our R folder, and input the following command:\ninstall.packages(\'nuclearARD_0.1.tar.gz\', repos=NULL, type=\'source\')\n\nTo view the walkthrough file in nteract, you also need to install the IRkernel. Follow the installation instructions here.\n'], 'url_profile': 'https://github.com/mpleung', 'info_list': ['159', 'Python', 'Updated Oct 17, 2020', '9', 'Python', 'Updated Jan 14, 2020', '3', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 9, 2020', '4', 'Lua', 'GPL-3.0 license', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', '4', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', '2', 'R', 'Updated Dec 26, 2020']}","{'location': 'Columbus, Ohio', 'stats_list': [], 'contributions': '614 contributions\n        in the last year', 'description': ['RbxPolyReg\nPolynomial regression solver.\nFull credit to the original author, Paul Lutus, and his PolySolve site. This is simply a port of his JavaScript code into Lua, which is covered under the GNU General Public License.\nThe Roblox ModuleScript is available here.\n\nUsage\nlocal RbxPolyReg = require(rbxPolyRegModuleScript)\n\n-- Get matrix functions:\nlocal matFuncs = RbxPolyReg.MatFunctions.new()\n\n-- Each data entry can be 1 of 3 different types:\n\t-- 1) {Number, Number}\n\t-- 2) {X = Number; Y = Number} OR {x = Number; Y = Number}\n\t-- 3) Vector2\nlocal data = {\n\t{-1, -1},\n\t{0, 3},\n\t{1, 2.5},\n\t{2, 5},\n\t{3, 4},\n\t{5, 2},\n\t{7, 5},\n\t{9, 4}\n}\n\n-- Degrees can be between 0 and 18:\nlocal degrees = 2\n\n-- Solve polynomial regression:\nlocal result = matFuncs:ProcessData(data, degrees)\n-- result[1]: Table of terms\n-- result[2]: Correlation coefficient\n-- result[3]: Standard error\n\n-- List terms:\nlocal terms = result[1]\nfor i,v in ipairs(terms) do\n\tprint((""%.16e * x^%i""):format(v, i - 1))\nend\n\n-- Create source code for the polynomial regression:\nlocal clampMin, clampMax = 0, 1000\nlocal funcSrcCode = RbxPolyReg.ToFunctionSourceCode(terms, clampMin, clampMax)\nprint(funcSrcCode)\n\n'], 'url_profile': 'https://github.com/Sleitnick', 'info_list': ['159', 'Python', 'Updated Oct 17, 2020', '9', 'Python', 'Updated Jan 14, 2020', '3', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 9, 2020', '4', 'Lua', 'GPL-3.0 license', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', '4', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', '2', 'R', 'Updated Dec 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Used Cars Price Prediction\nDSCI-522 - Group 308 Authors: Andrés Pitta, Braden Tam, Serhiy\nPokrovskyy\nAbout\nIn this project we attempt to build a regression model which can predict\nthe price of used cars based on numerous features of the car. We tested the following models: support vector machine regression, stochastic gradient descent regression, linear regression, K-nearest neighbour regression as well as random forest regression and gradient boosted trees.  We found that support vector machine regression shown the best results, producing an\n score of 0.877\non the training set,  score of 0.832 on the validation set and\n score of 0.830\non the test set. The training and validation scores are computed from a\nvery small subset of the data while the test score used a much larger\nsubset. Given that the dataset was imbalanced by manufacturers, this led to a bit worse prediction of the classes that were quite sparse because the model was not able to learn enough about those classes in order to give good predictions on unseen data.\nThe data set used in this project is ""Used Cars Dataset"" created by Austin\nReese. It was collected from Kaggle.com (Reese 2020) and can be found\nhere.\nThis data consists of used car listings in the US scraped from\nCraigslist that contains information such as listed price, manufacturer,\nmodel, listed condition, fuel type, odometer, type of car, and which\nstate it’s being sold in.\nReport\nThe final report can be found\nhere.\nUsage\nTo replicate the analysis, clone this GitHub repository and follow these instructions:\n\nReset the repository and clean any prebuilt results:\n\nmake clean\n\n\nRun the complete pipeline:\n\nmake all\n\nQuick Analysis\nThe original dataset’s size is 1.35GB. Fitting the model may take many hours (seriously!) Thus, we provided a convenient method to replicate a quick version of the model with just a portion of the data:\n\nReset the repository (without removing the data file data/vehicles.csv):\n\nmake partial_clean\n\n\nRun the quick version of the pipeline:\n\nmake quick TRAIN_SIZE=0.01\n\nYou may choose other percentage value (0-to-1) For 1% (TRAIN_SIZE=0.01) expected runtime is 5 minutes. Keep in mind that lower dataset size decreases accuracy. Also, train / test metrics are automatically embedded in generated reports, thus the reports will reflect those metrics and not the best ones we had prebuilt by default.\nUsing Docker\nIf you do not want to install all the software dependencies, you may want to use Docker. Running the Docker pipelines will download a prebuilt Docker container image for our project. Be advised, that the size of container is approximately 1.86 GB, plus the original data file of 1.35 GB will also be downloaded afterwards.\nTo replicate the quick version of this analysis using Docker run the following commands (on Windows use git bash):\n\nReset the repository (without removing the data file data/vehicles.csv):\n\nmake partial_clean\n\n\nRun the quick version of the pipeline using Docker:\n\nmake quick_docker\n\nSimilarly, you may use make all_docker to build full model with Docker (same advisory on time / resources applies)\nMakefile dependency maps\nPlease consider the following dependency maps for the make processes:\n\nThe dependencies for the complete version of this pipeline are shown in the following diagram:\n\n\n\nThe dependencies for the quick version of this pipeline are shown in the following diagram:\n\n\nDependencies\nNOTE When installing the following packages, conda install is preferred:\n\nPython 3.7.3 and Python packages:\n\naltair==3.2.0\nselenium==3.141.0\ndocopt==0.6.2\npandas==0.24.2\nnumpy==1.16.4\nstatsmodel==0.10.0\nplotly==4.3.0\nplotly-orca==1.2.1\nscikit-learn==0.20.4\n\n\nR version 3.6.1 and R packages:\n\nknitr==1.24\ndocopt==0.6.1\ntidyverse==1.3.0\nreadr==1.3.1\n\n\n\nNotes\nPlease be advised that in order to reproduce the analysis, the\nscripts/download.py script will download approximately 1.35GB original\ndata file. Also, training the full model with scripts/train_model.py may\ntake several hours. You may consider using command line arguments to\ntrain on a configurable subset of training data (which may affect the\ntrained model) Please consult usage data for each script for further\ndetails and command line arguments.\nLicense\nThe Used Cars Prediction materials here are licensed under the Creative\nCommons Attribution 2.5 Canada License (CC BY 2.5 CA). If\nre-using/re-mixing please provide attribution and link to this webpage.\nReferences\n\n\nReese, Austin. 2020. “Used Cars Dataset.” Kaggle.\nhttps://www.kaggle.com/austinreese/craigslist-carstrucks-data.\n\n\n'], 'url_profile': 'https://github.com/UBC-MDS', 'info_list': ['159', 'Python', 'Updated Oct 17, 2020', '9', 'Python', 'Updated Jan 14, 2020', '3', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 9, 2020', '4', 'Lua', 'GPL-3.0 license', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', '4', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', '2', 'R', 'Updated Dec 26, 2020']}","{'location': 'Tokyo', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['IDAO 2020 online roud\nTeam: QuMantumPhysicists\nDirectory\n.\n├── input/\n  ├── train.csv\n  └── Track 1\n    └── test.csv\n├── requirements.txt\n├── Makefile\n├── README.md\n├── model_description.PNG\n├── fit_param_dictAA.dump\n├── fit_param_dictB2.dump \n├── lr_hyperparam.csv\n├── main.sh\n├── trackA_predict.py\n├── trackB_predict.py\n└── train.py\n\n\n\ncommon\n\ninput/\n\ninput data folder from https://disk.yandex.ru/d/0zYx00gSraxZ3w\n\n\nrequirements.txt\n\ndevelopment environment\n\n\ntrain.py\n\nfit by linear regression and save results\ngenerate fit_param_dictAA.dump and fit_param_dictB2.dump\n\n\nlr_hyperparam.csv\n\nlist of hyper parameters\n\n\n\n\nfor track A\n\nfit_param_dictAA.dump\ntrackA_predict.py\n\nmake prediction\n\n\n\n\nfor track B\n\nfit_param_dictB2.dump\ntrackB_predict.py\n\nmake prediction\n\n\nMakefile\nmain.sh\n\n\n\n\nInstructions\n\nPlace input data in input/ (shown in Directory section).\nIf necessary, install packages in requirements.txt.\nTrain models for both tracks:  python train.py.\n\nPretrained models(fit_param_dictAA.dump and fit_param_dictB2.dump) will be generated.\nIt takes around 1 hour with 4 cores.\n\n\n\nfor track A:\n\nMake submission: python trackA_predict.py.\n\nsubmission_trackA.csv will be generated.\n\n\n\nfor track B:\n\nZip current working directory.\n\n\nModel description\nAll you need is linear regression\n\n'], 'url_profile': 'https://github.com/analokmaus', 'info_list': ['159', 'Python', 'Updated Oct 17, 2020', '9', 'Python', 'Updated Jan 14, 2020', '3', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 9, 2020', '4', 'Lua', 'GPL-3.0 license', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', '4', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', '2', 'R', 'Updated Dec 26, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '1,702 contributions\n        in the last year', 'description': ['BengaluruHousePriceRegression\n\nData Analysis With Jupyter Notebook In Python 3.6+\n\n\nFeatures\n\nArea_type – describes the area\nAvailability – when it can be possessed or when it is ready(categorical and time-series)\nLocation – where it is located in Bengaluru\nPrice – Value of the property in lakhs(INR)\nSize – in BHK or Bedroom (1-10 or more)\nSociety – to which society it belongs\nTotal_sqft – size of the property in sq.ft\nBath – No. of bathrooms\nBalcony – No. of the balcony\n\nHow To Run\n\nInstall python 3.6+\nPip insall\n\nnumpy\npandas\nmatplotlib\nseaborn\nsklearn\n\n\n\nInstallation\nFork Project and contribute to improving Analysis\nCheck here for setup your notebook first time.\nStart With Jupyter NoteBook\nUsage\nA good start with data-analysis for beginner\nSupport\ncontributors\nContributing\nBefore submitting a bug, please do the following:\nPerform basic troubleshooting steps:\n\nMake sure you are on the latest version. If you are not on the most recent version, your problem may have been solved already! Upgrading is always the best first step.\nTry older versions. If you are already on the latest release, try rolling back a few minor versions (e.g. if on 1.7, try 1.5 or 1.6) and see if the problem goes away. This will help the devs narrow down when the problem first arose in the commit log.\nTry switching up dependency versions. If the software in question has dependencies (other libraries, etc) try upgrading/downgrading those as well.\n\nAuthors and acknowledgment\nShow your appreciation to those who have contributed to the project.\nLicense\nFor open-source projects, Under MIT License.\nProject status\nAuthor\n\nProject: BengaluruHousePriceRegression\nAuthor: CodePerfectPlus\nLanguage: Python\nGithub: https://github.com/codePerfectPlus\nWebsite: http://codeperfectplus.github.io/\n\n'], 'url_profile': 'https://github.com/codePerfectPlus', 'info_list': ['159', 'Python', 'Updated Oct 17, 2020', '9', 'Python', 'Updated Jan 14, 2020', '3', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 9, 2020', '4', 'Lua', 'GPL-3.0 license', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', '4', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', '2', 'R', 'Updated Dec 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '170 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/duckmayr', 'info_list': ['159', 'Python', 'Updated Oct 17, 2020', '9', 'Python', 'Updated Jan 14, 2020', '3', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '4', 'Jupyter Notebook', 'Updated Mar 9, 2020', '4', 'Lua', 'GPL-3.0 license', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Feb 9, 2020', '4', 'Python', 'Updated Feb 15, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 30, 2020', '2', 'R', 'Updated Dec 26, 2020']}"
"{'location': 'Austin, Texas', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Marketing-Mix-Modeling-Automobile-Industry\nOptimized marketing mix- effective promotion channels and price for different product types, for car industry using regression model\nProject Title:\nMarketing Mix Modeling - Regression\nMotivation:\n""Half the money I spend on advertising is wasted; the trouble is, I don\'t know which half."" - John Wanamaker\nE. Jerome McCarthy proposed the marketing mix with 4 Ps - Product, Price, Promotion and Place. The optimal mix of these 4 Ps defines a successful marketing effort.\nIn this project, I am analyzing these 4 Ps to see how automobile industry is performing as per their marketing efforts.\nData Source:\nMy Professor at University of Texas, Austin helped with us with the confidential data for the industry. The data had information about the models, their years sales and prices. In the advertisement data, there was retail and corporate level advertising efforts at month level.\n** Note : The data is from year 1995 to 2005. Hence the insights are not applicable for the current car industry\nAnalysis Steps:\n\nData wrangling was the biggest challenge in this project as the data was very cluttered and in different files.\n\nFirst we extracted the brand and model information from the advertising channel information\nCleaned the channel descriptions to extract maximum information about channel and retail markeitng effort\nFiltered unlikely combination of brand and model-line\nAggregated the channel spending at model ID level as the sales data was yearly and advertising data was monthly.\nMerged the files to obtain sales and ad spend data in the same table\n\n\nCreated flags for luxury and non-luxury models; and domestic and international brands manually\nExploratory Data Analysis\nManually create flags for luxury vs non-luxury brands; and domestic vs international brands\nOverall regression model to understand the price elasticity. Since the price elasticity in the initial iterations was positive, we decided to capture the granularity of the data using year flags and brand flags. When the results were intuitive, we proceeded with product split regression analysis\nBuilt regression model (OLS) and log-log model at following levels  - overall, luxury brands, non-luxury brands, domestic brands, and international brands. Dependent variable : sales\nLog-log model coefficients represent the elasticity of the variable with the sales. Hence, we checked the correlation of the variables using OLS and elasticity using log-log model\n\nFindings:\n\n\nSignificant Variables with their elasticities:\n\nOverall:\n\nTV (0.21%); Price (-0.54%)\n\n\nLuxury :\n\nTV (0.24%); Price (-0.34%)\n\n\nNon-luxury models:\n\nTV (0.17%); Price (-0.54%)\n\n\nInternational Brands:\n\nTV (0.13%); Radio(0.07%) ; Print(0.08%); Price (-2.32%)\n\n\nDomestic Brands:\n\nTV (0.21%);  Online (0.02%) ; Price (-0.54%)\n\n\n\n\n\nFor Luxury models and international brands, the current marketing mix is not optimal and hence there is a potential to optimize it by changing price and channel ad spend mix\n\n\nRecommendations:\n\nFor Non-Luxury models, the market is more sensitive towards prices as compared to Luxury models\n\nPrice centric advertisement campaigns are more likely to be effective for non-luxury segment\n\n\nAmong the channels, television has the most impact on driving sales across  segments\nFor International brands, print media and radio , along with TV are observed to have potential of driving sales\nBy smaller change in price, International brands would be able to capture more sales compared to domestic models; as they are more price sensitive\n\nFuture Scope:\n\nIncorporation of monthly trends to calculate inter-advertising spends precisely\n\nSeasonal  effect on channel mix\nLag effect on channel mix\n\n\nWith online channel information, sales attribution for the channels would be possible. This attribution is not possible with current level of information for offline ad spend\nUsing marketing mix model for future vehicle launches to optimize marketing strategy for pricing and ad spend\n\n'], 'url_profile': 'https://github.com/aishwarya-pawar', 'info_list': ['13', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'HTML', 'MIT license', 'Updated Feb 8, 2020', '2', 'R', 'Updated Jul 9, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'R', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Avocado Price Predictors\n\nCreators: Katie Birchard, Ryan Homer, Andrea Lee\nDataset\nWe will be analyzing the avocado prices dataset retrieved from Kaggle and compiled by the Hass Avocado Board using retail scan data from the United States [1]. The dataset consists of approximately 18,000 records over 4 years (2015 - 2018). The dataset contains information about avocado prices, Price Look-Up (PLU) codes, types (organic or conventional), region purchased in the United States, volume sold, bags sold, and date sold.\nResearch Question\nWe will be answering the research question: What is the strongest predictor of avocado prices in the United States?\nOur goal is to find the feature that most strongly predicts the average price of avocados in the United States. A natural inferential sub-question would be to first determine if any of the features correlate with avocado prices and if there is any multicollinearity among the features. From our results, we can also compute a rank of features by importance.\nAnalysis\nTo answer our research question, we will first need to determine our target and features. Our target will be average price and our features will be type, region, date, total volume, PLU_code, no_variety_sold, Total Bags, bag_size, bags_sold, month, and year_month. Some of these features do not match the column names of the original dataset, such as month or PLU_code, as they were created during data wrangling step of the exploratory data analysis.\nNext, we will need to determine if the features are correlated with the target. We will do this by fitting an additive linear model. We will then conduct a hypothesis test and interpret the p-values and confidence intervals to determine which features are significant. This hypothesis test will also serve as a validation for the feature importances computed in the last step.\nTo get a better understanding of our features, we will also test for multicollinearity by computing their variance inflation factors. We will then check for and remove any redundancies between features. This will allow us to build a more accurate model in the next step.\nOnce we have confirmed which features are correlated with the target and are non-redundant, we will fit a Random Forest Regressor model using these features. We will then compute the feature importances using the feature_importances_ attribute. This attribute will return an importance value for each feature that indicates how important that feature is at explaining the target (the higher the value, the more important the feature is). The importance value is based on the decrease in impurity measure. The decrease in impurity is calculated by the model by computing how much each feature contributes to decreasing the weighted impurity. The model then averages each feature\'s impurity decrease over the trees.\nLastly, we will plot our results in order to find out which feature is the strongest predictor of avocado prices.\nExploratory Data Analysis\nBefore we can perform our hypotheses tests and create our random forest regression model, we will partition the complete dataset into an 80% training set and a 20% test set. We will then perform exploratory data analysis on the training set to assess the validity of our dataset, as well as assess possible correlations between the features and average avocado prices. To ensure the validity of our dataset, we will include a table of summary statistics and compare these values to a previous study looking at average avocado prices over a similar time period [2]. To get an idea of which features may be of importance, we will include bar plots depicting the relationship between month and price, type and price, and region and price. We will also display a plot depicting how avocado prices have varied by week between 2015 and 2018.\nResults\nTo communicate our results, we will create a bar chart ranking the features by importance, from most to least important. The plot will have the features on the x-axis and the importance values on the y-axis.\nUsage\nHere are two suggested ways to run this analysis.\nRun with Docker (recommended)\n\nInstall Docker.\nDownload or clone this repository.\nOpen a terminal session and navigate to the root of the project directory.\nRun the analysis with the following command:\n\ndocker run --rm -v /$(pwd):/avocado murage/avocado_predictors:v1.0 make -C /avocado all\n\nWindows Users: You may need to replace /$(pwd) above with the absolute path to the root of your project directory.\nWhen the process has completed, you can find the analysis report at doc/avocado_predictors_report.html or doc/avocado_predictors_report.md.\nTo clean out all temporary files:\ndocker run --rm -v /$(pwd):/avocado murage/avocado_predictors:v1.0 make -C /avocado clean\n\nRun without Docker\n\nMake sure you\'ve installed all of the dependencies listed in the Dependencies section below.\nDownload or clone this repository.\nOpen a terminal session and navigate to the root of the project directory.\nRun the analysis with the following command:\n\nmake all\n\nTo clean out all temporary files:\nmake clean\n\nRun Individual Pieces\nTo retrieve and prepare the data:\nRscript src/get_data.R --url=https://raw.githubusercontent.com/ryanhomer/dsci522-group411-data/master/avocado.csv --destfile=data/avocado.csv\nRscript src/prepare_data.R --datafile=data/avocado.csv --out=data\n\nTo generate markdown versions of the notebooks we used during EDA:\nRscript -e ""rmarkdown::render(\'src/DSCI_522_EDA.Rmd\')""\nRscript -e ""rmarkdown::render(\'src/hypothesis_test.Rmd\')""\nRscript -e ""rmarkdown::render(\'src/multicoll/multicoll.Rmd\')""\n\nTo generate the final report:\nRscript src/conduct_hypothesis_test.R --datafile=data/train.feather --out=doc/img\nRscript src/multicoll/mc_create_assets.R --datafile=data/train.feather --out=doc/img\nRscript src/render_EDA.R --datafile=data/train.feather --out=doc/img\npython src/regression.py data/train.feather results/\nRscript -e ""rmarkdown::render(\'doc/avocado_predictors_report.Rmd\', output_format = \'github_document\')""\n\nDependencies\nOS-level Dependencies\n\n\n\nPackage Name\nVersion\n\n\n\n\nchromedriver\n79.0.3945.36\n\n\nPython\n3.7\n\n\nR\n3.6.2\n\n\n\nR Package Dependencies\n\n\n\nPackage Name\nVersion\n\n\n\n\nbroom\n0.5.3\n\n\ncaret\n6.0-85\n\n\ncar\n3.0-6\n\n\ndocopt\n0.6.1\n\n\nfeather\n0.3.5\n\n\nggpubr\n0.2.4\n\n\nhere\n0.1\n\n\nkableExtra\n1.1.0\n\n\nknitr\n1.25\n\n\nlubridate\n1.7.4\n\n\nmagick\n2.3\n\n\nRCurl\n1.98-1.1\n\n\nreshape2\n1.4.3\n\n\ntidyverse\n1.2.1\n\n\n\nPython Package Dependencies\n\n\n\nPackage Name\nVersion\n\n\n\n\naltair\n4.0.0\n\n\nnumpy\n1.17\n\n\npandas\n0.25.3\n\n\npyarrow\n0.15.1\n\n\nscikit-learn\n0.22.1\n\n\nselenium\n3.141.0\n\n\n\nReports\n\nExploratory Data Analysis\nHypothesis Test (Intermediate Analysis)\nMulticollinearity Test (Intermediate Analysis)\nRandom Forest Feature Importances (Final Analysis)\nFinal Report\n\nScripts\n\nDownload Data\nPrepare Data\nRender EDA\nConduct Hypothesis Test\nConduct Multicollinearity Test\nConduct Feature Importances Analysis\nFinal Report\n\nFile Dependency Graph\n\nReferences\n[1] Kiggins, J. ""Avocado Prices: Historical data on avocado prices and sales volume in multiple US markets."" May 2018. Web Link.\n[2] Shahbandeh, M. ""Average sales price of avocados in the U.S. 2012-2018."" February 2019. Web Link.\n'], 'url_profile': 'https://github.com/UBC-MDS', 'info_list': ['13', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'HTML', 'MIT license', 'Updated Feb 8, 2020', '2', 'R', 'Updated Jul 9, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'R', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['vglmer: Variational Generalized Linear Mixed Effects Regression   \nA package to estimate non-linear hierarchical models using the variational algorithms described in Goplerud (2020). It also provides the option to improve an initial approximation using marginally augmented variational Bayes (MAVB) also described in the same paper. It can be installed using devtools\nlibrary(devtools); devtools::install_github(""mgoplerud/vglmer"", dependencies = TRUE)\n\nAt present, it can fit logistic and negative binomial outcomes with an arbitrary number of random effects. Details on negative binomial inference can be found here and are more experimential at the moment.\nThis model accepts ""standard"" glmer syntax of the form:\nvglmer(formula = y ~ x + (x | g), data = data, family = \'binomial\')\n\nvglmer(formula = y ~ x + (x | g), data = data, family = \'negbin\')\n\nMany standard methods from lme4 work, e.g. fixef, coef, vcov, ranef, predict. Use format_vglmer to parse all parameters into a single data.frame. Estimation can be controlled via the numerous arguments to control using vglmer_control. At the moment, Schemes I, II, and III in Goplerud (2020) correspond to strong, partial, and weak. Unless the model is extremely large, the default (weak) should be used.\nThe package is still ""experimental"" so some of the inputs and outputs may change! Please make an issue with any concerns you have.\nOne known issue is that it is not possible to include a random effect without a main effect (e.g. y ~ x + (b | g) will fail). This will be fixed shortly.\n'], 'url_profile': 'https://github.com/mgoplerud', 'info_list': ['13', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'HTML', 'MIT license', 'Updated Feb 8, 2020', '2', 'R', 'Updated Jul 9, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'R', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['تابع LSR_Iterative\nاین تابع به دو صورت متفاوت نوشته شده است. در صورت اول این تابع با دریافت تعداد\nEpoch پس از تکرار آموزش داده متوقف می شود که در زیر آمده است.این کد در فایل\n2_2.py می باشد\ndef LSR_Iterative(landa,xtrain, ytrain, epoch):\nERMS = []\ncost = costfunc(xtrain, ytrain, weight)\nERMS.append(cost)\nfor i in range(epoch):\nfor j in range(len(xtrain)):\ny = weight[1]*xtrain[j]+weight[0]\nweight[0] = weight[0] + landa * (ytrain[j]-y)\nweight[1] = weight[1] + landa * (ytrain[j]-y) *xtrain[j]\ncost = costfunc(xtrain, ytrain, weight)\nprint(cost)\nERMS.append(cost)\nplt.clf()\nplt.plot(trainx, trainy, \'o\', mfc=\'r\', mec=\'r\' )\naxes = plt.gca()\nyline=[]\nfor k in axes.get_xlim():\nyline.append(weight[0]+weight[1]*k)\nplt.plot(axes.get_xlim(), yline)\nplt.xlabel(\'train X\')\nplt.ylabel(\'train Y\')\nplt.show()\nplt.title(\'EMRS vs Iteration\')\nplt.xlabel(""Iteration"")\nplt.ylabel(\'ERMS\')\nplt.plot(ERMS)\nplt.show()\nدر این تابع با قرار دادن مقدار 20 برای تعداد Epoch ده بار الگوریتم کل داده ها را\nمی بیند و سعی در بهبود خود می کند. که در تصاویر زیر نمونه خط کشیده شده در مرحله\nاول و مرحله آخر نشان داده شده است.\n\nشکل 2 مرحله اول آموزش برای دادگان اول\n\nشکل 3 مرحله بیستم آموزش برای دادگان اول\n\nشکل 4 مقدار EMRS در بیست دوره مختلف آموزش با نرخ یادگیری 0005/0 برای دادگان\nاول\n\nشکل 5 مرحله اول آموزش برای دادگان دوم\n\nشکل 6 مرحله بیستم آموزش برای دادگان دوم\n\nشکل 7 مقدار EMRS در بیست دوره مختلف آموزش با نرخ یادگیری 0005/0 برای دادگان\nدوم\nدر حالات بالا نرخ یادگیری برابر 0005/0 می باشد که با بزرگتر کردن آن به مقدار\n01/0 نمودار خطای آن پس از چند مرحله صعودی می شود. که بهترین میزان خطا 67/2% است\nکه در مرحله اول بدست آمده است.\n\nشکل 8 میزان خطا در حالتی که نرخ یادگیری برابر 01/0 می باشد.\nحالت دیگر این تابع با تعیین شرط میزان خطای تخمین پایان می یابد. که در زیر آمده\nاست.\ndef LSR_Iterativeemrs(landa,xtrain, ytrain, emrs):\nERMS = []\nCost = 100\nit =0\nwhile (it < 100) & (Cost > emrs):\nit += 1\nfor j in range(len(xtrain)):\ny = weight[1]*xtrain[j]+weight[0]\nweight[0] = weight[0] + landa * (ytrain[j]-y)\nweight[1] = weight[1] + landa * (ytrain[j]-y) *xtrain[j]\nCost = costfunc(xtrain, ytrain, weight)\nprint(Cost)\nERMS.append(Cost)\nplt.clf()\nplt.plot(trainx, trainy, \'o\', mfc=\'r\', mec=\'r\' )\naxes = plt.gca()\nyline=[]\nfor k in axes.get_xlim():\nyline.append(weight[0]+weight[1]*k)\nplt.plot(axes.get_xlim(), yline)\nplt.xlabel(\'train X\')\nplt.ylabel(\'train Y\')\nplt.show()\nplt.title(\'ERMS vs Iteration\')\nplt.xlabel(""Iteration"")\nplt.ylabel(\'ERMS\')\nplt.plot(ERMS)\nplt.show()\nاین تابع با مقدار emrs = 0 هیچوقت به پایان نمی رسد. و تابع تا اتمام حافظه رم به\nرفع خطا ادامه خواهد داد. اما با قرار دادن emrs = 2.3 این برنامه پس از 20 مرحله\nآموختن کل داده ها متوقف می شود\n\nشکل 9 مقادیر EMRS با شرط پایان کمتر از 2.3\nتابع LSR_NonIterative که با دریافت داده های ورودی رگراسیون خطی غیرتکراری را اجرا کرده و خطی را برای داده ها تقریب می زند.\nتابع نوشته شده در فایل 2_3.py قرار دارد.\ndef LSR_NonIterative(xtrain, ytrain):\nweight = np.array([0,0],np.float64)\nweight[1] = np.sum( (ytrain - np.mean(ytrain)) * (xtrain -\n(np.mean(xtrain))))/np.sum(np.power( xtrain - np.full(xtrain.shape,\nnp.mean(xtrain)),2))\nweight[0] = np.mean(ytrain - weight[1]*xtrain)\nprint(weight)\nplt.clf()\nplt.plot(trainx, trainy, \'o\', mfc=\'r\', mec=\'r\')\naxes = plt.gca()\nyline = []\nfor k in axes.get_xlim():\nyline.append(weight[0] + weight[1] * k)\nplt.plot(axes.get_xlim(), yline)\nplt.xlabel(\'train X\')\nplt.ylabel(\'train Y\')\nplt.show()\nکه خروجی آن برای دو دادگان به صورت زیر است.\n\nشکل 10 خروجی تابع LSR_NonIterative برای دادگان اول\n\nشکل 11 خروجی تابع LSR_NonIterative برای دادگان دوم\nنتایج حاصل از دو تابع LSR_Iterative و LSR_NonIterative را مقایسه کنید؟ و در یک نمودار برای داده ها رسم کنید. (برای هر دو دادگان انجام شود)\nکد های این مقایسه در فایل 2_4.py می باشد که خروجی آن به صورت زیر است.\n\nشکل 12 نتیجه مقایسه دو تابع LSR_Iterative و LSR_NonIterative برای دادگان اول\n\nشکل 13 نتیجه مقایسه دو تابع LSR_Iterative و LSR_NonIterative برای دادگان دوم\nنحوه فراخوانی و استفاده از تابع polyfit در کتابخانه \xa0scipyیا numpy را مشخص کرده و سپس با استفاده از آن یک خط و یک منحنی با درجه 7 برای دادهها تقریب بزنید و رسم نمایید. (مشابه شکل زیر فقط برای دادگان (getdata(برای هر دو دادگان انجام شود)\nnumpy.polyfit(x, y, deg, rcond=None, full=False, w=None,\ncov=False)\ndef mmtpolyfit(X,Y,degree):\npoly = np.polyfit(X,Y,degree)\nPolyid = np.poly1d(poly)\nplt.plot(X, Y, \'o\', mfc=\'r\', mec=\'r\')\nplt.plot(X, Polyid(X))\nplt.legend([""data""])\nplt.show()\nreturn Polyid\nاین قسمت در فایل 2_5.py نوشته شده است که خروجی آن به شرح زیر است.\n\nشکل 14 استفاده از تابع پلی فیت با درجه 1 برای دادگان اول\n\nشکل 15 استفاده از تابع پلی فیت با درجه 1 برای دادگان دوم\n\nشکل 16 استفاده از تابع پلی فیت با درجه7 برای دادگان اول\n\nشکل 17 استفاده از تابع پلی فیت با درجه 7 برای دادگان دوم\nتابع polyfit\nاین قسمت در فایل 2_6.py قرار دارد.\ndef costfunc(x, y, poly):\nemrs = []\nfor i in range(len(poly)):\nPolyid = np.poly1d(poly[i])\ninner = np.power(Polyid(x) - y, 2)\nsmm = np.sum(inner)\nemrs.append(np.sqrt(np.sum(smm) / 2*(len(y))))\nplt.plot(range(len(poly)), emrs)\nplt.legend([""EMRS""])\nplt.show()\ntrainx, validationx, testx, trainy, validationy, testy = getdata1()\ndef mmtpolyfit(X, Y, degree):\npoly = np.polyfit(X, Y, degree)\nPolyid = np.poly1d(poly)\nreturn poly\nply = []\nfor i in range(20):\nply.append(mmtpolyfit(trainx, trainy, i))\ncostfunc(validationx, validationy, ply)\n\nشکل 18 میزان EMRS برای درجه های پلی فیت 1 تا 20 برای دادگان اول\n\nشکل 19 شکل 20 میزان EMRS برای درجه های پلی فیت 1 تا 20 برای دادگان اول\n\nدرج کمترین خطا\n\nاین قسمت در فایل 2_7.py نوشته شده است که خروجی آن به شرح زیر است.\n\nشکل 21 رسم بهترین تقریب برای دادگان اول\n\nشکل 22 محاسبه خطای تقریب برای داده test\n\nشکل 23 رسم بهترین تقریب برای دادگان دوم\n\nشکل 24 محاسبه خطای تقریب برای داده test\n'], 'url_profile': 'https://github.com/mmtalaie', 'info_list': ['13', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'HTML', 'MIT license', 'Updated Feb 8, 2020', '2', 'R', 'Updated Jul 9, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'R', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Bangalore ', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anandkodakkoly', 'info_list': ['13', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'HTML', 'MIT license', 'Updated Feb 8, 2020', '2', 'R', 'Updated Jul 9, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'R', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': [""A prediction result of a game between team 1 and team 2, based on who's home and who's away, and on whether or not the game is friendly (include rank in your training).\n\nDefining the question\na) Objectives\nMain Objective\n\nPredicting the result of a game between team 1 and team 2, based on who's home and who's away, and on whether or not the game is friendly (include rank in your training).\nSpeecific Objectives\nCreate two models where:\nModel 1: Predict how many goals the home team scores.\nModel 2: Predict how many goals the away team scores.\nb) Metrics For Success\n1)Exhaustively performed polynomial regression done by performing feature engineering where necessary.\n2)Exhaustively perform Logistic regression by performing feature engineering where necessary\nc) Context\nThe prediction sites and comapnies have become relatively on - demand sites .Everyone is beginning to recognise the opportunity of making money through sports betting. Unlike other forms of gambling, football prediction has a wider range of selection of possibilities to bet from, which makes it a lot easier for punters to place bets based on their knowledge of teams and leagues. Hence the need to create a model for a company that helps them know the odds\nd) Experimental Designs\ni) Data Exploration Importing Libraries Loading Dataset Cleaning Data Determine Relationship between variables(Univariate, Multivariate)\nii) Feature engineering\niii) Statistical Inferences\niv) Logistic regression\nv) Polynomial regression\n""], 'url_profile': 'https://github.com/Jaslinegati', 'info_list': ['13', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'HTML', 'MIT license', 'Updated Feb 8, 2020', '2', 'R', 'Updated Jul 9, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'R', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ravindrasap', 'info_list': ['13', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'HTML', 'MIT license', 'Updated Feb 8, 2020', '2', 'R', 'Updated Jul 9, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'R', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/smitj92', 'info_list': ['13', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'HTML', 'MIT license', 'Updated Feb 8, 2020', '2', 'R', 'Updated Jul 9, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'R', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Moscow', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': [""Predicting data using one-factor regression analysis\nThe data frame http://d396qusza40orc.cloudfront.net/statistics/lec_resources/states.csv\nHere you can see a piece of data:\n\nLet's build a scatter plot for our data. Note that the relationship is linear.  We use regression analysis to determine the significance of the relationship between the dependent variable - the level of salary and the predictor - the level of education. Our goal is to predict the future indicators.\n\nUsing regression analysis in R, we obtained the following values:\nCoefficients:\n        Estimate Std. Error t value Pr(>|t|)    \n\n(Intercept) 64.78097    6.80260   9.523 9.94e-13 ***\nhs_grad     -0.62122    0.07902  -7.862 3.11e-10 ***\nResidual standard error: 2.082 on 49 degrees of freedom\nMultiple R-squared:  0.5578,\tAdjusted R-squared:  0.5488\nF-statistic: 61.81 on 1 and 49 DF,  p-value: 3.109e-10\nNext, using the regression coefficients (b0=64.7810, b1=-0.6212), we construct the regression line equation.\ny = 64,67-0,62*x (where x = education level)\nAnd if we now go to find out what percentage of poverty will be if the predictor is 98%, we get the value 3,91\nThe obtained value of the determination coefficient, equal to 56%, indicates that 56% of the variability of the dependent variable is explained by our model.\n""], 'url_profile': 'https://github.com/VickyEllison', 'info_list': ['13', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'HTML', 'MIT license', 'Updated Feb 8, 2020', '2', 'R', 'Updated Jul 9, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'R', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['regression\n\n'], 'url_profile': 'https://github.com/KirillMaslov9', 'info_list': ['13', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'HTML', 'MIT license', 'Updated Feb 8, 2020', '2', 'R', 'Updated Jul 9, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'R', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020']}"
"{'location': 'Denver, CO', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['regression\nIn this project I investigated Denver’s current traffic problem by trying to identify any relationships between Denver neighborhoods\nwith longer commute times and Denver neighborhood characteristics, such as percentage of education levels,\nrent prices, home value, poverty, race, age, income, and sex. The paper attached explains my regression analysis and provides relevant\nexplanations for decisions made through out the process.\n'], 'url_profile': 'https://github.com/duffme', 'info_list': ['R', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '3', 'Python', 'MIT license', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '3', 'JavaScript', 'Updated Jan 19, 2020']}","{'location': 'Columbus, OH', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/YuvrajSinghAutomotive', 'info_list': ['R', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '3', 'Python', 'MIT license', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '3', 'JavaScript', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/karineiji', 'info_list': ['R', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '3', 'Python', 'MIT license', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '3', 'JavaScript', 'Updated Jan 19, 2020']}","{'location': 'Tehran-Iran', 'stats_list': [], 'contributions': '823 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KiLJ4EdeN', 'info_list': ['R', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '3', 'Python', 'MIT license', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '3', 'JavaScript', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wardaharshad', 'info_list': ['R', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '3', 'Python', 'MIT license', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '3', 'JavaScript', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wardaharshad', 'info_list': ['R', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '3', 'Python', 'MIT license', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '3', 'JavaScript', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wardaharshad', 'info_list': ['R', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '3', 'Python', 'MIT license', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '3', 'JavaScript', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wardaharshad', 'info_list': ['R', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '3', 'Python', 'MIT license', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '3', 'JavaScript', 'Updated Jan 19, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abubakaar', 'info_list': ['R', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '3', 'Python', 'MIT license', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '3', 'JavaScript', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Debug-ml5-pixel-regression\nI'm stuck!\n""], 'url_profile': 'https://github.com/CodingTrain', 'info_list': ['R', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '3', 'Python', 'MIT license', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '3', 'JavaScript', 'Updated Jan 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ankitsood007', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'JavaScript', 'Updated Jan 14, 2020']}","{'location': 'Sweden', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/salamamohammedawad1986', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'JavaScript', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/peacetoyo', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'JavaScript', 'Updated Jan 14, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['polynomial-regression\nA regression model to predict the house rental price based on certain features like size, floor, number of bed rooms etc.\nIt generates polynimial regression models upto the degree specified.\nUses sklearn - PolynomialFeatures for generating polynomial features.\nOutputs mean_absolute_error for each degree between 1 to the degree specified.\nDecide the best polynomial degree based on the minimum value of mean_absolute_error for testing data.\n'], 'url_profile': 'https://github.com/parthmehta', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'JavaScript', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ramuvadlamudi', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'JavaScript', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['IndependentStudy\nLogistic regression\n'], 'url_profile': 'https://github.com/nikki10nms', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'JavaScript', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sadeg', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'JavaScript', 'Updated Jan 14, 2020']}","{'location': 'Seoul, Republic of Korea', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': [""sensitivity_analysis\nsensitivity analysis on linear regression and logistic regression\n< simple procedure >\n\n(1) after fitting linear regression or logistic regression model\n(2) select all X variables from an observation (1 row)\n(3) then, predict y value by using only 1 variable's existing value and setting '0' for other variables\n: ie. for loop iteration of all variables one by one\n(4) compare predicted y value by variables\n\n\nused abalone.txt open dataset as an example\n\n""], 'url_profile': 'https://github.com/hdlee4u', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'JavaScript', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vlventure', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'JavaScript', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rsicher1', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Updated Jan 18, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '2', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'JavaScript', 'Updated Jan 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression\nProjects on Linear Regression\n'], 'url_profile': 'https://github.com/NGBGR', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', '1', 'HTML', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['SparClur.jl\n\n'], 'url_profile': 'https://github.com/SparClur', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', '1', 'HTML', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Data-Science-Regression-examples\nSimple and Multiple Linear Regression operations and working written in python 3\n'], 'url_profile': 'https://github.com/aryamonani', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', '1', 'HTML', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020']}","{'location': 'Ankara', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/murathanakdemir', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', '1', 'HTML', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['AutomobilePricePrediction\nLinear Regression implementation Project\n'], 'url_profile': 'https://github.com/sushaantbansal', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', '1', 'HTML', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Airline-Price-prediction-_-GradientBoostingRegressor\nRegression model using GradientBoostingRegressor\nusing this project we can estimate the price of an aircraft ticket from different locations with different operators.\n'], 'url_profile': 'https://github.com/dilipvaleti', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', '1', 'HTML', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020']}","{'location': 'MA', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Veera-123', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', '1', 'HTML', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/edusysvijay', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', '1', 'HTML', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020']}","{'location': 'Ankara', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/murathanakdemir', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', '1', 'HTML', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Linear-Regression-master\nLinear-Regression-master\n'], 'url_profile': 'https://github.com/Chethan-Dasaiah', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', '1', 'HTML', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020']}"
"{'location': 'Houston', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['data-mining-linear-regression\nLinear regression using python\n'], 'url_profile': 'https://github.com/Hou-dev', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 16, 2020', '1', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""mlr\nmultiple linear regression model illustrating gradient descent, SGD, mini batch GD and sklearn's lr\n""], 'url_profile': 'https://github.com/shasa271189', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 16, 2020', '1', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Polynomial-Linear-Regression-master\nPolynomial-Linear-Regression-master\n'], 'url_profile': 'https://github.com/Chethan-Dasaiah', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 16, 2020', '1', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Random-Forest-Regression-master\nRandom-Forest-Regression-master\n'], 'url_profile': 'https://github.com/Chethan-Dasaiah', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 16, 2020', '1', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic-Regression\nProjects on Logistic Regression\n'], 'url_profile': 'https://github.com/NGBGR', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 16, 2020', '1', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/VladKrupin', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 16, 2020', '1', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Rehovot. Israel', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Implemantaion of:\n\nLog linear classifier + train\ngradiants check for sanity checks\n3.MLP with one layer- implement garadiants\n4.MLP with n layers- implement gradiants\n\n'], 'url_profile': 'https://github.com/dmoshayof', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 16, 2020', '1', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['CO2-emission-prediction\nHere I practice building regression models (multi linear regression, polynomial regression, SVR, decision tree, and random forest. To replicate results use the data file emmision_clean.csv. The data was cleaned using R.\n'], 'url_profile': 'https://github.com/Tisi-theeconomist', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 16, 2020', '1', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Machine-Learning-Projects\nRegression , Classification and Segmentation projects\n'], 'url_profile': 'https://github.com/KumaraGuruM', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 16, 2020', '1', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Ankara', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/murathanakdemir', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Python', 'Updated Jan 16, 2020', '1', 'HTML', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 14, 2020']}"
"{'location': 'Ankara', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/murathanakdemir', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'MATLAB', 'MIT license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kavyashreekp12', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'MATLAB', 'MIT license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020']}","{'location': 'gandhinagar', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ravikiradoo', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'MATLAB', 'MIT license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Janghu1407', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'MATLAB', 'MIT license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['House-Prices\nHouse Prices: Advanced Regression Techniques\nKaggle Dataset:\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/ihpolash', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'MATLAB', 'MIT license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020']}","{'location': 'Mysore, India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Abalone Project\nThis is a Linear Regression model on Abalone dataset taken from UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Abalone)\nAbalone is a shellfish and it's age is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope. This model tries to predict the age of the Abalone by it's external features like Length, Weight etc.\nThis model is coded in MATLAB\n""], 'url_profile': 'https://github.com/Nandy-006', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'MATLAB', 'MIT license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': [""Classification and Regression tree\n\nLinks\nClassification and Regression tree: having data frame as dataset\nClassification and Regression tree: having sql database as dataset\nClassification and Regression tree test: data frame\nClassification and Regression tree test: sql database\nClassification tree benchmark code\nClassification tree profile code\nClassification tree benchmark and profile results\n\n\nFor classification_tree.py, using data frame to build and prune classification tree. Then, making random forest based on the tree that we built before.\n\n\nFor sql_tree.py, it uses exactly the same algorithms as the classification_tree.py except some of the codes can only be applied to the SQL database rather than the data frame. One thing that needs to notice is that sql_tree doesn’t have random forest nor the calculation of the minimum error alpha, which would be used in the prune function. Therefore, users need to identify their own minimum error alpha in order to prune the tree.\n\n\nFor both test_classification_tree.py and SQLtest.py, they are unit tests for the classification_tree.py and sql_tree.py.\n\n\nFor benchmark.py, Profile_code.py, and benchmark.txt, they are measuring how fast the classification and regression tree code execute. They aim to find where the bottlenecks are and how to optimize them.\n\n\nWe are going to introduce the use of functions in the following and all of the following functions are designed in the classification_tree.py\n\n\nThe following functions are in the Class Node\n\ndef init(self.dataset. Impurity_function, past_split):\nThe init function needs users to input their desired dataset, which is a matrix, their desired impurity function, which can be Gini index, Bayes error, cross-entropy or any users defined function, and the past_split, which is an empty list [ ].\nAfter having these inputs, the init function is going to help users to build a balanced tree with nodes and leaves. Each node contains the split point, split variable, its connection to the right child and left child, the number of zero and one of this node, and the probability of zero and one of this node.\ndef impurity(self, p_value, impurity_function):\nThe impurity function takes both p_value and impurity function in order to return the impurity center value, which is the parent node, for future calculations in impurity reduction function.\ndef impurity_reduction(self, p_value, left_dataset, right_dataset, impurity_function):\nThe impurity reduction function takes in p-value, left dataset, right dataset, and impurity function. In addition, the entered p-value should be between 0 and 1, otherwise, the function would throw an error.\nThe maximum impurity value is calculated by using the impurity center value, which gets from the impurity function minus the probability of 1 in left dataset times the impurity value of the left dataset, which also gets from impurity function minus the probability of 1 in right dataset times the impurity value of the left dataset, which gets from impurity function.\nmax_impurity_reduction = impurity_center - prob_left * impurity_left - prob_right * impurity_right\ndef best_split(self, dataset, impurity_function):\nThe best split function takes dataset and impurity function as inputs and would return the best split variable and split point for each node.\nIn order to find the best split variable and best split point for each node, this function would use the values returned by the impurity_reduction and the datasets returned by the get_data function. Each testing split variable and each split point would be corresponding with an impurity reduction value and for which split variable and split point who has the maximum impurity reduction would be the best split point and best split variable for that node.\ndef get_data(self, split_variable, split_point, dataset):\nThis get_data function inputs split variable, split point, and dataset and would return the sub dataset for left and right side nodes, which are also children nodes.\nThe split variable and split point that input in this function is usually not the best split variable and split point but rather the testing split variable and split point that we examined in the best_split function. The way that we get the data is that we deduced the lines from the whole dataset based on the split point and split variable.\ndef prune(self, node, min_error_alpha, whole_dataset):\nFor prune function, it takes a node, the min_error alpha, and the whole dataset and it is going to return the modified tree after we prune.\nThe min_error_alpha gets from the cross_validation function in the tree class which we will explain later.\nThis function would also employ the G_T function to get the alpha star. If the alpha star is smaller than the min_error alpha, we would prune the tree.\ndef G_T(self, whole_dataset):\nThis G_T function takes the whole dataset and it would calculate the alpha start which would be used in the prune function that we mentioned above.\ndef query_node(self, row):\nThis query_node function takes every row of the data frame and traverse through the tree that we built and pruned before in order to get the prediction.\n\nThe following functions are in the Class Tree.\n\ninit(self, dataset, impurity_function):\nThis function takes the dataset and impurity function, which aims to build a tree and then call the prune function in this class.\nprune(self, dataset, impurity_function):\nThe prune function takes in the dataset and impurity function. This function would call the corss_validation function to get the minimum error alpha and then call the prune function in the Node class.\ndef cross_validation(self, whole_dataset, impurity_function):\nThe cross_Validation takes in the whole dataset and impurity function. This function would return the minimum error alpha which would be used in the prune function to compare with the alpha star to decide whether we need to prune a tree or not.\ndef query_node(self, dataset)\nThis query_node function takes the dataset and it aims for the benchmark. Traverse through the tree and get the prediction. Each row of test_dataset will be compared with the split_variable's split point in order to decide which path, left or right, should go in order to get the predicted outcomes\n\nThe following functions are in the Class tree_to_forest and it is a super function of Node Class.\n\ninit(self, whole_dataset, k, impurity_function):\nThis init function takes in the whole dataset, k, which is the number of variables that the users wish to select from all the covariates, and impurity function. This function aims to set the k as an attribute and inherit the functions in the Node class.\ndef best_split(self, dataset, impurity_function):\nThis best split takes in dataset and impurity function and it would return the best split point and best split variable for each node. This function is similar to the best_split function in the Node class, however, it doesn’t use the whole dataset nor all the variables to choose the best split point and best split variable but rather use the k number oof variables that users defined and then a random sample dataset.\n\nThe following functions are included in the Class Forest\n\ninit(self, whole_dataset, k, n_tree, impurity_function):\nThis init function takes in the whole dataset, k, which is the number of variables that users pick, n_tree, which is the number of trees that the users wish to have in the forest and the impurity function. This function would build a forest.\nprediction(self, user_dataset):\nThis prediction function takes in the user’s dataset and it would return a list that has all the predictions that we calculated through our tree.\n""], 'url_profile': 'https://github.com/yuehanx05', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'MATLAB', 'MIT license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020']}","{'location': 'Palakkad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nImplementation Of Simple Linear Regression\n'], 'url_profile': 'https://github.com/AneeshAIDev', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'MATLAB', 'MIT license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020']}","{'location': 'kasargod', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anagha99', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'MATLAB', 'MIT license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['LRinR\nSimple Linear Regression with R\n'], 'url_profile': 'https://github.com/bayusantiko', 'info_list': ['Python', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'MATLAB', 'MIT license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020']}"
"{'location': 'La Jolla, CA', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['CSE253_PA1\nLogisitic and Softmax Regression Implementation for CSE253\n'], 'url_profile': 'https://github.com/adamklie', 'info_list': ['Jupyter Notebook', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', '2', 'Python', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'Ankara', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/murathanakdemir', 'info_list': ['Jupyter Notebook', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', '2', 'Python', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'Nigeria ', 'stats_list': [], 'contributions': '269 contributions\n        in the last year', 'description': ['Use of a simple Linear Regression Technique.\n\n\nProject Description\nProject Aim\nTools\n\nProject Description\nThis project was carried out to learn how a linear regression model can be used to predict continuous values.\nProject Aim\nwe are to explore a given dataset for an E-commerce company trying to figure out ways to improve sales.\nTools\n\nI worked with the jupyter notebook under the anaconda package using python 3.6.\n\n'], 'url_profile': 'https://github.com/Ik-Emmanuel', 'info_list': ['Jupyter Notebook', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', '2', 'Python', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'Ankara', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/murathanakdemir', 'info_list': ['Jupyter Notebook', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', '2', 'Python', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'New Jersey Newark', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Linear-Regression-Diagnostics\nLinear Regression Diagnostics using R\n'], 'url_profile': 'https://github.com/ashique93', 'info_list': ['Jupyter Notebook', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', '2', 'Python', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'Palakkad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\nImplementation of Multiple Linear Regression\nOnce you run the code in Python, you’ll observe three parts:\n(1) The first part shows the output generated by sklearn:\nLinear Regression in Python\nThis output includes the intercept and coefficients. You can use this information to build the multiple linear regression equation as follows:\nStock_Index_Price = (Intercept) + (Interest_Rate coef)*X1 + (Unemployment_Rate coef)*X2\nAnd once you plug the numbers:\nStock_Index_Price = (1798.4040) + (345.5401)*X1 + (-250.1466)*X2\n(2) The second part displays the predicted output using sklearn:\nLinear Regression\nImagine that you want to predict the stock index price after you collected the following data:\nInterest Rate = 2.75 (i.e., X1= 2.75)\nUnemployment Rate = 5.3 (i.e., X2= 5.3)\nIf you plug that data into the regression equation, you’ll get the same predicted result as displayed in the second part:\nStock_Index_Price = (1798.4040) + (345.5401)(2.75) + (-250.1466)(5.3) = 1422.86\n'], 'url_profile': 'https://github.com/AneeshAIDev', 'info_list': ['Jupyter Notebook', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', '2', 'Python', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'Berlin', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ozlemozkan', 'info_list': ['Jupyter Notebook', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', '2', 'Python', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Multiple Linear Regression Predicting Cholesterol\nFor the full presentation please read: https://github.com/benlaird/health-project/blob/master/Multiple%20Linear%20Regression%20predicting%20Cholesterol.pptx\nChris Park and Ben Johnson-Laird\nNational Health and Nutrition Examination Survey (NHANES) (2013-2016)\n16,881 survey respondents\nVariables of Interest\nDependent Variable\nCholesterol - Chol\nIndependent Variables\nTriglycerides - Trig\nBody Mass Index - BMI\nBlood Pressure - BP\nBlood Urea Nitrogen - BUN\nAge - Age\nGender\nRace (7 Dummies)\nAssumptions\nDistribution of Residuals\nQQ Plot - Normality Testing\nPredicting Cholesterol with no interaction & no interpolation for missing values\nEnhancements to the initial model\nLog transformation of all continuous variables\nMin-Max Scaling\nKNN Imputation of Missing Values\nInteraction effect between age and BUN in predicting total cholesterol\nPredicting Cholesterol with K nearest neighbor interpolation for missing values (k = 15)\nConclusion\nOverall Adjusted-R2 Score increases after log transformation and normalization\nBoth models were found to be statistically significant (p < 0.001)\nChange in cholesterol levels were found to be different in older population\n'], 'url_profile': 'https://github.com/benlaird', 'info_list': ['Jupyter Notebook', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', '2', 'Python', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['-ML-Regression-Model-\nRegression Techinque with data provided\nYou no need to go anywhere for the data , it is provided in the repository itself.\nJust Clone the repository and use it.\n'], 'url_profile': 'https://github.com/Prajwal270', 'info_list': ['Jupyter Notebook', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', '2', 'Python', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Code repository for ""Two-stage Variational Mode Decomposition and Support Vector Regression""\nThis study heavily relies on the open-source software. Pandas (McKinney, 2010) and numpy (Stéfan et al., 2011) were used to manage and process streamflow data. Matlab was used to perform variational mode decomposition (VMD), ensemble empirical mode decomposition (EEMD), and discrete wavelet transform (DWT) of streamflow and compute the partial autocorrelation coefficient (PACF) of subsignals. The matlab implementations of VMD and EEMD come from Dragomiretskiy and Zosso (2014) and Wu and Huang (2009), respectively. The DWT was performed based on matlab build-in toolbox (“Wavelet 1-D” in “Wavelet Analyzer”). The SSA was performed based on a python program developed by  Jordan D\'Arcy. The SVR model in scikit-learn (Pedregosa et al., 2011) was used to train SVR models. scikit-optimize (Tim et al., 2018) was used to tune the SVR models. Matplotlib (Hunter, 2007) was used to draw figures.\nHow to validate the research results\n\n\nClone this repository from Github. Run the following code in CMD or git-bash.\ngit clone https://github.com/zjy8006/MonthlyRunoffForecastByAutoReg\n\n\n\nOpen MATLAB for decomposing monthly runoff using EEMD, VMD, DWT and MODWT, and building ARIMA model. Go to the root directory.\n% Run the following code in command window.\n>> cd where_you_save:/MonthlyRunoffForecastByAutoReg\n\n\n\nOpen this repository with vscode for other tasks. You can run code with code runner extension.\n\n\nShift-invariant test and  boundary effect analysis of VMD\n\nRun ""tools/vmd_shift_invariant_test.m""\n\nMonthly runoff decomposition\n\nRun ""/tools/RUN_EEMD.mlx"" for EEMD of monthly runoff.\nRun ""/tools/RUN_VMD.mlx"" for VMD of monthly runoff.\nRun ""/tools/RUN_DWT.mlx"" for DWT of monthly runoff.\nRun ""/tools/RUN_MODWT.mlx"" for MODWT of monthly runoff.\nRun ""/tools/ssa_decompose.py"" for SSA of monthly runoff.\n\nDetermine the input predictors\n\nRun ""/tools/compute_pacf.m"" for computing PACF.\nRun ""/**/projects/variables.py"" for selecting input predictors.\npredictors.\n\nGenerate training, development and testing samples\n\nRun ""/**/projects/generate_samples.py""\n\nSupport Vector Regression\nThe \'SVR\' in scikit-learn was used to build support vector regression (SVR) models. The \'gp_minimize\' (Bayesian optimization based on Gaussian process) in scikit-optimize was used to optimize SVR models. The SVR model optimized by Bayesian optimization are organized in \'./tools/models.py\'.\nReference\n\nMcKinney, W., 2010. Data Structures for Statistical Computing in Python, pp. 51–56.\nStéfan, v.d.W., Colbert, S.C., Varoquaux, G., 2011. The NumPy Array: A Structure for Efficient Numerical Computation. A Structure for Efficient Numerical Computation. Comput. Sci. Eng. 13 (2), 22–30.\nDragomiretskiy, K., Zosso, D., 2014. Variational Mode Decomposition. IEEE Trans. Signal Process. 62 (3), 531–544.\nWu, Z., Huang, N.E., 2009. Ensemble Empirical Mode Decomposition: a Noise-Assisted Data Analysis Method. Adv. Adapt. Data Anal. 01 (01), 1–41.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, É., 2011. Scikit-learn. Machine Learning in Python. Journal of Machine Learning Research 12, 2825–2830.\nTim, H., MechCoder, Gilles, L., Iaroslav, S., fcharras, Zé Vinícius, cmmalone, Christopher, S., nel215, Nuno, C., Todd, Y., Stefano, C., Thomas, F., rene-rex, Kejia, (K.) S., Justus, S., carlosdanielcsantos, Hvass-Labs, Mikhail, P., SoManyUsernamesTaken, Fred, C., Loïc, E., Lilian, B., Mehdi, C., Karlson, P., Fabian, L., Christophe, C., Anna, G., Andreas, M., and Alexander, F.: Scikit-Optimize/Scikit-Optimize: V0.5.2, Zenodo, 2018.\nHunter, J.D., 2007. Matplotlib. A 2D Graphics Environment. Computing in Science & Engineering 9, 90–95.\n\n'], 'url_profile': 'https://github.com/UnIcOrn7618', 'info_list': ['Jupyter Notebook', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', '2', 'Python', 'MIT license', 'Updated Jan 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TheDataMentor', 'info_list': ['Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 14, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['RegressionForests\nRegression Forests from scratch using only numpy and no sklearn.\nUsing all the 21 columns from the sarcos.inv.csv file to predict the last column.\nThe target column is the the torque of a motor of a robotic arm.\nThe hyperparameters are not tuned.\nFeel free to make any changes and improvements.\n'], 'url_profile': 'https://github.com/charalambos02', 'info_list': ['Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 14, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Chengdu, China', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/xuechao-chen', 'info_list': ['Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 14, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Simple-linear-regression-using-R\nPredicting the Salary according to the experience of a candidate\nThe CSV file must be included in the download folder or else, you can can give the URL of the data set.\nThe file is python notebook file, the language is R, so you will be required to setup an R environment before using it.\nHappy MAchine Learning!!\n'], 'url_profile': 'https://github.com/Premm98', 'info_list': ['Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 14, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Multiple-linear-Regression-using-R\nPredicting probably approximate value for profit of a startup company.\nThe CSV file must be included in the download folder or else, you can can give the URL of the data set.\nThe file is python notebook file, the language is R, so you will be required to setup an R environment before using it.\nHappy MAchine Learning!!\n'], 'url_profile': 'https://github.com/Premm98', 'info_list': ['Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 14, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/thkim1011', 'info_list': ['Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 14, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Indonesia', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic-harunalazies\n'], 'url_profile': 'https://github.com/harunalazies', 'info_list': ['Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 14, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '264 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abhishekaashu', 'info_list': ['Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 14, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': [""Supervised Machine Learning Project\nHi! The reason I do this porject is to test my skills and also try to use all the supervised machine learning technique I've learned so far. The project contain 2 parts. The detail is down below.\nDataset\nI got the dataset from Kaggle (an online community of data scientists and machine learning practitioners).\nOne is about used cars price, the other is about income classification.\nThe link is down below:\nRegression: autos\nClassification: income\n\nPart 1\nThe Part-1.ipynb file contain 2 sections, the first section is classification part and the second section is regression part.\nSection1\nIt contains EDA part, KNN classification, Logistic Regression, Linear Support Vector Machine, Kerenilzed Support Vector Machine, Decision Tree. I implemented grid search with cross validation to find the best parameter for the models and wrote down my conclusion and my discover along the project.\nSection2\nIt  contains EDA part, KNN regressor, linear regression, Ridge, Lasso, polynomial regression, SVM both simple and with kernels. I used cross-validation to find average training and testing score. Finally, I found the best regressor for this dataset and train my models on the entire dataset using the best parameters and predict the target values for the test_set.\n\nPart 2\nI seperated part 2 into two files: Part-2_classification.ipynb & Part-2_regression.ipynb . I thought that would be more clear if I seperated my analysis into two files.\nClassification\nI kept the EDA part and my analysis contains voting classifiers(both hard and soft), bagging & pasting, adaboost, gradient boost, PCA (for dimension reduction) and deep learning model.\nRegression\nLike the classification part, I kept my EDA from part1 and my analysis includes bagging & pasting, adaboost, gradient boost, PCA and deep learning model.\n\nEnd\nThis wraped up my practice, hope it hlped you guys, thank you very much.\n""], 'url_profile': 'https://github.com/harry-hwang', 'info_list': ['Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 14, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Ankara', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/murathanakdemir', 'info_list': ['Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 14, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', '2', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Python', 'Updated Jan 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '99 contributions\n        in the last year', 'description': ['LogisticRegression\nClassification Problem Solving using Logistic Regression Algorithm\n'], 'url_profile': 'https://github.com/hafizmrf3', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,134 contributions\n        in the last year', 'description': ['binder-git-test20\nhttps://mybinder.org/v2/gh/op07n/binder-git-test20.git/master?urlpath=proxy/8501/\nhttp://binder.pangeo.io/v2/gh/op07n/binder-git-test20.git/master?urlpath=proxy/8501/\nhttps://notebooks.gesis.org/binder/v2/gh/op07n/binder-git-test20.git/master?urlpath=proxy/8501/\n'], 'url_profile': 'https://github.com/op07n', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '3,973 contributions\n        in the last year', 'description': ['Diabetes Prediction\nIn this mini-project, I trained a logistic regression model to correctly predict a diabetes diagnosis in patients.\nThe file prediction.py is the main file where the model was trained, and then saved. The file check.py is the file where I use some validation data and a sample to check for the probability of the prediction and check if the model iis working correctly.\n'], 'url_profile': 'https://github.com/yashg160', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'mysore', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': [""Linear Regression\nObjective: The primary objective is to implement the Linear Regression Algo on the Boston House Prediction Dataset and find the price of houses.\nDataset\nThe Dataset has to be imported from sklearn.datasets.\nData Set Characteristics:\n:Number of Instances: 506 \n\n:Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n\n:Attribute Information (in order):\n    - CRIM     per capita crime rate by town\n    - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n    - INDUS    proportion of non-retail business acres per town\n    - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n    - NOX      nitric oxides concentration (parts per 10 million)\n    - RM       average number of rooms per dwelling\n    - AGE      proportion of owner-occupied units built prior to 1940\n    - DIS      weighted distances to five Boston employment centres\n    - RAD      index of accessibility to radial highways\n    - TAX      full-value property-tax rate per $10,000\n    - PTRATIO  pupil-teacher ratio by town\n    - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n    - LSTAT    % lower status of the population\n    - MEDV     Median value of owner-occupied homes in $1000's\n\nEDA\n:BY doing Basic EDA,i came to know that two features are highly correlated. so, we have to remove either of the features.\nTechniques Used\n\nStochastic Gradient Descent\nSGDRegressor from sklearn.linear_model\nLinearRegression from sklearn.linear_model\n\nModel Performance Metrics\n\nmean_squared_error\nr2 score\n\nPython Library (required)\n\ntqdm (pip install tqdm)\n\nadd progress bars to Python code.\n\n\npandas_profiling(pip install pandas_profiling)\n\nused for EDA\n\n\n\n""], 'url_profile': 'https://github.com/vishalkumar199', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': [""Kaggle-titanic\nDependancies:\nSee environment.yml for the complete dependency list.\nEnvironment:\nYou can create a conda environment with all required dependencies by running conda env create in the root of the repository. Install the environment with:\nconda env create -f environment.yml\nKaggle Competition | Titanic Machine Learning from Disaster\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\nFrom the competition (https://www.kaggle.com/c/house-prices-advanced-regression-techniques).\nGoal for this Notebook:\n\nregression techniques\nfeature creation\nmanual outlier removal\nsklearn pipelines for transformation of data and model training\ncategory encoders for encoding methods\nimbalanced learn for outlier detection and removal (not yet implemented)\nscikit optimization for parameter tuning\nmlxtend for model stacking\ncross-validation\n\n""], 'url_profile': 'https://github.com/imaccormick275', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'Cyprus, Nicosia', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Problem Definations\nChanges in exchange rates affect both domestic and international investment and consumption decisions and affect the economy as a whole through the real and financial sectors. Considering the weight of this activity on the economy, the volatilities in exchange rates have the capacity to affect the economy as a whole. Therefore, analyzing the volatility of the exchange rate is extremely important in terms of foreseeing the risks that may arise and taking precautions.\nWith artificial intelligence, organizations can benefit from growing data pools, better meet formal regulations, increase profits, improve customer experience and more, moreover, diversify retail financial services and make the right budget. In cases where the data is not linear, Polynomial Regression is used. The aim of linear regression is to find the line that passes on the two-dimensional line closest to all points.\nIn addition, the use of regressions which are the estimation method is discussed.\nIn this regard, linear and polynomial regression estimation is examined. These two regression models are used for different situations.\nLiterature Review\nIn order to follow the correct way in the evaluation of the project, the last projects, techniques, and technologies were examined. In our literature review, research articles between 2000-2018 the estimation was taken into consideration for accuracy. The more valued and cited papers are only considered for our literature review to get a strong foundation for implementing the project.\nArticles related to regression models, their use and applications are taken into consideration.\nMETHODOLOGY\nIn this section, regression model and estimation are used. In line with the researches, information is given about the project. We used 2 models: polynomial regression and linear regression. We experienced in our project which of these two models is important for us through trial and error. In short, Linear Regression is an analysis method that enables us to examine the statistical relationship between two scalar variables. In predictive (statistical) relationships, looking at some of the data we have, a hypothetical relationship is drawn between them.In order to make a prediction in linear regression, we need to create a model.\nThis model is as follows;\ny=b0+b1.x1\n\nb0 is  constant value.\nb1 represents the slope of  model.\nThe value x1 represents the point to be estimated.\nThe process we will do in Python is about finding b0 and b1.\nThe aim of linear regression is to obtain the function of the relationship between parameters. The simplest machine learning model. The main objective is to determine whether there is a linear\n\nAdvantages of Linear Regression\n\n-In the linear model, a linear class boundary is created in the classification problem.\n-The theory is very well developed, its properties and behavior are well known.\n-Parameters are easy to estimate and develop according to problems.\n-Very wide and varied relationships can be expressed.\n\nData may sometimes be non-linear. In these cases, Polynomial Regression is also used.\nLinear Regression\ny=α+βx,\nyi=α+βxi+£i.\nMultiple Linear Regression\ny=β0+β1x1+β2x2+β3x3+£.\nPolynomial Regression\ny=β0+β1x+β2x²+...+βhXh+£.\nIn the above equation h is the polynomial degree.\nAccording to the polynomial and linear regression, we can now go through the sample project step by step together. I will try my best to be descriptive.\nAdding The Necessary Libraries To The Project.\nWe will use the numpy library as we will do all the operations on the matrix.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.preprocessing import PolynomialFeatures\nSince the incoming data will probably come as a DataFrame, which is the data retention object of the pandas library, we have imported the pandas to make the necessary coding for that case.\nx = data[""Day""]\nWe introduce the Pyplot library for the visualization process we will use at the end.\ny = data[""Price""]\nWe give the values in the .csv file from the user into the variable named data.\nx=x.ravel().reshape(len(data),1)\ny=y.ravel().reshape(len(data),1)\nIn the above function we took the extension of the variable with len (). Because we will apply the number of data reads from csv. In the meantime, the process here is to create a new order to create data matrix.\nplt.title(""Linear and Polynomial Regression"")\nplt.xlabel(""Day"")\nplt.ylabel(""Price"")\nplt.grid()\nplt.scatter(x,y)\nHere we charted the raw data of our data and identified the names of the labels.\nThe reason for graphing our data was that we can easily see how much error rate regressions are.\nIn this way we get a graph. Then we go to the linear regression event.\n#Linear Regression\nestimate_linear=LinearRegression()\nestimate_linear.fit(x,y) #For fitting on x and y axis.\nestimate_linear.predict(x) #We are looking for prices by days.Predict for forecasting.\nplt.plot(x,estimate_linear.predict(x),c=""red"") #For plotting\n#print(estimate_linear.predict(x)) #to print forecasts ...\n#plt.show() #to show linear ...\nIn this section, we first created a variable named estimatelineer. We equalized our constant method called LinearRegression () to this variable. Then, we fit our x, y values to their axes with the fit (x, y) function. We are looking for prices according to our function in the form of predict (x).\n#Linear Regression\n#To find out which degree is better.\na=0\nhatakaresipolinom=0\nfor a in range(50):\n    tahminpolinom = PolynomialFeatures(degree=a+1)\n    Xyeni = tahminpolinom.fit_transform(x)\n    polinommodel = LinearRegression()\n    polinommodel.fit(Xyeni,y)\n    polinommodel.predict(Xyeni)\n    for i in range(len(Xyeni)):\n        hatakaresipolinom = hatakaresipolinom + (float(y[i])-float(polinommodel.predict(Xyeni)[i]))**2\n    print(""----------"")\n    print(a+1,"":"", hatakaresipolinom)\n    print(""----------"")\n    hatakaresipolinom = 0\nHere, we have determined the error rate of the degree of the polynomial regression according to which we have seen which would be good.\nhatakaresilineer=0\nhatakaresipolinom=0\n#Polinom regresyonun hatasını görmek için....\nfor i in range(len(Xyeni)):              #(Gerçek değerim - tahmini değerim)**2\n    hatakaresipolinom=hatakaresipolinom+(float(y[i]-float(polinommodel.predict(Xyeni)[i])))**2  \nfor i in range(len(y)):\n    hatakaresilineer = hatakaresilineer + (float(y[i])-float(tahminlineer.predict(x)[i]))**2\nAnd next;\nWe first defined an array. To understand how it looks like we\'re wondering. Then we said that if this graphic is the same color, we don\'t understand anything.\nAnd where ”c” is written, we\'d better draw the colors from the array again.\n•\t2nd degree black ones,\n•\t3rd degree green,\n•\t9th degree gray\nConclusion\nThis project includes a study on dollar estimation with artificial intelligence.\nIn our project, linear and polymer regression analyzes were used.We chose the Python programming language because of its library width and compatibility.\nAs we think that artificial intelligence will provide serious conveniences and benefits in a serious sector such as dollar rate estimation as in all fields, we have implemented such a project.\nIn conclusion, polynomial regression is more appropriate and 9th degree polynomial is more appropriate in this study.\n'], 'url_profile': 'https://github.com/RamazanBakir', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'Poland', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['pszt-project-2\nXGBoost and logistic regression algorithm comparison\nDataset\nDownload the bank marketing dataset and place all files (not folders) into data folder.\n'], 'url_profile': 'https://github.com/brodzik', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': [""House Prices: Advanced Regression Techniques\nProblem-link: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\nDescription:\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nFile descriptions :\ntrain.csv - the training set\ntest.csv - the test set\ndata_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\nsample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms.\nData fields:\nHere's a brief version of what you'll find in the data description file.\nSalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\nMSSubClass: The building class\nMSZoning: The general zoning classification\nLotFrontage: Linear feet of street connected to property\nLotArea: Lot size in square feet\nStreet: Type of road access\nAlley: Type of alley access\nLotShape: General shape of property\nLandContour: Flatness of the property\nUtilities: Type of utilities available\nLotConfig: Lot configuration\nLandSlope: Slope of property\nNeighborhood: Physical locations within Ames city limits\nCondition1: Proximity to main road or railroad\nCondition2: Proximity to main road or railroad (if a second is present)\nBldgType: Type of dwelling\nHouseStyle: Style of dwelling\nOverallQual: Overall material and finish quality\nOverallCond: Overall condition rating\nYearBuilt: Original construction date\nYearRemodAdd: Remodel date\nRoofStyle: Type of roof\nRoofMatl: Roof material\nExterior1st: Exterior covering on house\nExterior2nd: Exterior covering on house (if more than one material)\nMasVnrType: Masonry veneer type\nMasVnrArea: Masonry veneer area in square feet\nExterQual: Exterior material quality\nExterCond: Present condition of the material on the exterior\nFoundation: Type of foundation\nBsmtQual: Height of the basement\nBsmtCond: General condition of the basement\nBsmtExposure: Walkout or garden level basement walls\nBsmtFinType1: Quality of basement finished area\nBsmtFinSF1: Type 1 finished square feet\nBsmtFinType2: Quality of second finished area (if present)\nBsmtFinSF2: Type 2 finished square feet\nBsmtUnfSF: Unfinished square feet of basement area\nTotalBsmtSF: Total square feet of basement area\nHeating: Type of heating\nHeatingQC: Heating quality and condition\nCentralAir: Central air conditioning\nElectrical: Electrical system\n1stFlrSF: First Floor square feet\n2ndFlrSF: Second floor square feet\nLowQualFinSF: Low quality finished square feet (all floors)\nGrLivArea: Above grade (ground) living area square feet\nBsmtFullBath: Basement full bathrooms\nBsmtHalfBath: Basement half bathrooms\nFullBath: Full bathrooms above grade\nHalfBath: Half baths above grade\nBedroom: Number of bedrooms above basement level\nKitchen: Number of kitchens\nKitchenQual: Kitchen quality\nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\nFunctional: Home functionality rating\nFireplaces: Number of fireplaces\nFireplaceQu: Fireplace quality\nGarageType: Garage location\nGarageYrBlt: Year garage was built\nGarageFinish: Interior finish of the garage\nGarageCars: Size of garage in car capacity\nGarageArea: Size of garage in square feet\nGarageQual: Garage quality\nGarageCond: Garage condition\nPavedDrive: Paved driveway\nWoodDeckSF: Wood deck area in square feet\nOpenPorchSF: Open porch area in square feet\nEnclosedPorch: Enclosed porch area in square feet\n3SsnPorch: Three season porch area in square feet\nScreenPorch: Screen porch area in square feet\nPoolArea: Pool area in square feet\nPoolQC: Pool quality\nFence: Fence quality\nMiscFeature: Miscellaneous feature not covered in other categories\nMiscVal: $Value of miscellaneous feature\nMoSold: Month Sold\nYrSold: Year Sold\nSaleType: Type of sale\nSaleCondition: Condition of sale\n""], 'url_profile': 'https://github.com/ShubhankSinghal', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sadanandm2020', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'Durham, North Carolina', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Iris-data\nLinear regression using iris dataset in python\n'], 'url_profile': 'https://github.com/souminator', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}"
"{'location': 'Singapore', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Singapore\'s ""Kiasu"" parents? Understand HDB resale prices\nThis project aims to understand the HDB resale pricing trend and predict new entries. We are interested particularly in whether primary school presence and location has effect on the HDB resale prices.\nData source from data.gov.sg, OneMap and HDB.\nThe map below shows our model\'s prediction result v.s. actual resale price.\n\nHere\'s the link to my Medium blog.\n'], 'url_profile': 'https://github.com/ZeeTsing', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Julia', 'Updated Jan 17, 2020', '1', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['ORSVR\nan online robust support vector regression\n'], 'url_profile': 'https://github.com/Fisher1991', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Julia', 'Updated Jan 17, 2020', '1', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wardaharshad', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Julia', 'Updated Jan 17, 2020', '1', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2021']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['ML-Regression-Models\nThis repository contains some ml regression examples\nrelated to polynomial regression and comparision with leniar regression model and its drawbacks.\n'], 'url_profile': 'https://github.com/Sundaram81200', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Julia', 'Updated Jan 17, 2020', '1', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2021']}","{'location': 'Isfahan,Iran', 'stats_list': [], 'contributions': '197 contributions\n        in the last year', 'description': ['engineering-statistics-probability-project\nA simple linear regression project via R\n'], 'url_profile': 'https://github.com/AMK9978', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Julia', 'Updated Jan 17, 2020', '1', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2021']}","{'location': 'California', 'stats_list': [], 'contributions': '701 contributions\n        in the last year', 'description': ['Seattle-House-Sales-Prices\nMultilinear Regression for Seattle House Sales Prices.\nData Source\nSeattle House Sales Prices\nThis data was obtained from kaggle datasets\nThanks\n'], 'url_profile': 'https://github.com/pau-lo', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Julia', 'Updated Jan 17, 2020', '1', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2021']}","{'location': 'Pune', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Medical-Cost-Prediction-Heroku-Deployment\nThis Project has been sucessfully deployed in Heroku Cloud Platform:\nCheck here: https://medical-cost-predictor.herokuapp.com\nProblem Statement\nWe need to predict the medical cost with the input data like age, bmi, number of children, smoker-non smoker, region\nPlease refer this link for information related to the dataset: https://www.kaggle.com/mirichoi0218/insurance\nModel\nLinear Regression model is used here\nContent of this Repository\n\ntemplates >> Basic HTML pages (landing page, results page)\ninsurance.csv >> Dataset\nLinear_Regression.ipynb >> Exploratory Data Analysis and Model creation\nProcfile >> Needed for Cloud deployment\napp.py >> Actual Flask app\nmodel.pickle >> File where the model is stored\nrequirements.txt >> contains all the necessary libraries (Needed for cloud deployment)\n\nDeployment Procedure (on Heroku Cloud Platform)\n\nDownload all the files from this repository and save it in a folder\nGo to https://www.heroku.com and Sign up/Login\nOn the Heroku homepage, click on New button >> create app\nOpen Git Bash\ncd ""/Path of the project folder where all the files are stored locally""\ngit init\ngit add .\ngit commit -m ""your comments""\ngit remote add origin https://git.heroku.com/demo.git (note: this url is available under settings tab of the heroku app)\ngit push origin master\n\n------------------------------------------------------THANK YOU---------------------------------------------------------\n'], 'url_profile': 'https://github.com/saurabhkolawale', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Julia', 'Updated Jan 17, 2020', '1', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2021']}","{'location': 'Norway', 'stats_list': [], 'contributions': '287 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/roelstappers', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Julia', 'Updated Jan 17, 2020', '1', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': [""Kaggle Competition: 'House Prices: Advanced Regression Techniques' - Top 20%\nLink to competition: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\nAchieved score: 0.12085 (root mean squared logarithmic error) - top 20% at time of submission\n\nAs one of my first coding projects, I decided to compete in a popular Kaggle competition to predict house prices.\n""], 'url_profile': 'https://github.com/christoph1stehling1', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Julia', 'Updated Jan 17, 2020', '1', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2021']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': [""symbolic_experiments\n\nRepository for symbolic regression/classification experiments. The experiment files are sc.py and sr.py, run on a SLURM cluster using sbatch sr.sh. The number of trials are specified using a job array index in the shell scripts, and SLURM will schedule these sequentially. Other scripts within the experiment directories are provided to collect and analyze model results.\nRunning locally -\nDownload data set from here: \nCode will look for this file ('sym_data_4.csv') alongside repository (in same directory that symbolic_experiments is in).\nRun the following line from the same directory as sc.py or sr.py, and scoop will automatically collect available processors to distribute model evaluations over. You can watch this happen in your system's task manager.\npython -m scoop sc.py 1\nThe integer provided as an argument at the end (the job index in the cluster version) will be used to initialize random processes, do training-test splits, and label the trial folder and results files.\nDependencies and versions:\nFor experiments:\nRequired:\npython                    3.7.1\nnumpy                     1.16.1\npandas                    0.25.3\nscipy                     1.2.0\ndeap                     1.2.2\nscoop                     0.7.1.1(as written, can be modified to run single processor)\nOptional*(for immediate plotting of results):\nmatplotlib                3.1.0\npygraphviz                1.3\nscikit-learn              0.20.2\n*Remote installations of python may not have plotting, so can turn these off in sr.py and sc.py. All results are stored in a pickle within a directory labeled with the trial number.\nFor sensitivity testing, clustering, and most figures:\nsalib                     1.3.8\nscikit-learn              0.20.2\nmatplotlib                3.1.0\nseaborn                   0.9.0\nFor networks and maps:\npygraphviz                1.3\ncartopy                   0.17.0\nOther data:\nThe code that extracts the IWFM water pumping and delivery estimates from C2VSim can be found here.\nThe code that extracts the land use data from the California Pesticide Use Reports can be found here.\nThe California County Ag Commissioner data (economic) can be found here.\n""], 'url_profile': 'https://github.com/ekblad', 'info_list': ['Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Julia', 'Updated Jan 17, 2020', '1', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2021']}"
"{'location': 'San Francisco Bay Area', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SophiePlt', 'info_list': ['R', 'Updated Jan 13, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', '2', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'MATLAB', 'Updated Jan 14, 2020', '2', 'Python', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zhou671', 'info_list': ['R', 'Updated Jan 13, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', '2', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'MATLAB', 'Updated Jan 14, 2020', '2', 'Python', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'Houston', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['data-mining-linear-regressionv2\nProject done using liner regression to analyze a set of data.\n'], 'url_profile': 'https://github.com/Hou-dev', 'info_list': ['R', 'Updated Jan 13, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', '2', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'MATLAB', 'Updated Jan 14, 2020', '2', 'Python', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Predicting-revenue-using-simple-linear-regression\nIn this project, We have created a model which will predict the revenue in dollars.\n'], 'url_profile': 'https://github.com/jigzz18', 'info_list': ['R', 'Updated Jan 13, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', '2', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'MATLAB', 'Updated Jan 14, 2020', '2', 'Python', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'Pune, Maharashtra, India', 'stats_list': [], 'contributions': '501 contributions\n        in the last year', 'description': ['automobile-price-prediction\nPredicting the price of a car from its specifications using Linear Regression, Random Forest Regression and XGBoost.\nThe dataset can be found here\n'], 'url_profile': 'https://github.com/parth-paradkar', 'info_list': ['R', 'Updated Jan 13, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', '2', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'MATLAB', 'Updated Jan 14, 2020', '2', 'Python', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/olumide2021', 'info_list': ['R', 'Updated Jan 13, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', '2', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'MATLAB', 'Updated Jan 14, 2020', '2', 'Python', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'Guangzhou,China', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['ADGSGP\nThis project is implemented based on DEAP framework.\nBefore running the project, you should install DEAP, and convert the Python 2.x features in the original DEAP to Python 3.x features.\nThe list of newly changed components in DEAP:\ndeap/algorithms.py;\ndeap/semantic.py;\ndeap/StatisticFile.py;\nSBP/GP_SBP.py;\n'], 'url_profile': 'https://github.com/Zhixing1020', 'info_list': ['R', 'Updated Jan 13, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', '2', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'MATLAB', 'Updated Jan 14, 2020', '2', 'Python', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rk2100994', 'info_list': ['R', 'Updated Jan 13, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', '2', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'MATLAB', 'Updated Jan 14, 2020', '2', 'Python', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '264 contributions\n        in the last year', 'description': ['Linear-Regression-on-Donors_choose\nImplementation of Linear Regression Algorithm on Donors_Choose_Dataset\n'], 'url_profile': 'https://github.com/abhishekaashu', 'info_list': ['R', 'Updated Jan 13, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', '2', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'MATLAB', 'Updated Jan 14, 2020', '2', 'Python', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Vimalgshettigar', 'info_list': ['R', 'Updated Jan 13, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', '2', 'Jupyter Notebook', 'Updated Jan 15, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'MATLAB', 'Updated Jan 14, 2020', '2', 'Python', 'Updated Sep 3, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['ML Linear Regression\nML with Linear Regression\n'], 'url_profile': 'https://github.com/bnaoe', 'info_list': ['Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Nairobi, Kenya', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Mchezopesa Football Predictions\nBy Sharon Maswai\nDescription\nThis is a project on the predictions of wins, losses, draws and winning teams based on the fifa-ranking and results datasets.\nRequiraements\n*Google colab notebook\nSetup instruction\n*Save a copy of the notebook in your drive and open it to access.\nTechnologies used\n\nPython 3.6\nPandas Python Library\nNumpy Python Library\nSeaborn library\nMatplotlib library\nsklearn library\n\nBugs\nAt the time of completion there were no known bugs.\nSupport\nIn case of any clarifications or suggestions with regards to this project email me at chepsharonmaswai@gmail.com\nLicense\nCopyright (c) 2019 Sharon Maswai\n'], 'url_profile': 'https://github.com/sharonmaswai', 'info_list': ['Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Italy', 'stats_list': [], 'contributions': '127 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ferromauro', 'info_list': ['Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Houston', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['data-mining-dtree-regress\nProject involving making decision trees using the gini coefficient and entropy.\n'], 'url_profile': 'https://github.com/Hou-dev', 'info_list': ['Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Princeton, NJ', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['Logistic regression\nJulia code for fitting various logistic regression models to the rat behavioral\ndata from the Poisson clicks task. Allows us to assess L/R bias, as well as\ntemporal discounting. Uses the GLM.jl package.\nIn short, fits the following four models:\n1D click difference\n\n2D, total number of right and left clicks\n\nClick difference across time\n\nNumber of right and left clicks across time\n\nProducing, for each rat, a figure that looks like the following:\n\nTo do\n\n Major refactor to make the code easier to use\n Use stratified (across gammas) kfold cv for the estimation of params and their error bars\n\n Make sure that the X matrices contain gammas\n\n\n Check that other generalization metrics also show same trends as AUC(ROC)\n Visualize how separable the w * X + beta distributions are prior to feeding into the logit model\n Metarat analysis: could fit to one giant collated matrix or average logit weights\n Explore different bin sizes - are 20Hz rats fit worse bc of low click count per bin?\n\n'], 'url_profile': 'https://github.com/jyanar', 'info_list': ['Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression-car-Price-Prediction\nLinear Regression Goal is to fit a best line. The best fit line is the one for which total prediction error (all data points) are as small as possible. Error is the distance between the point to the regression line. Linear Regression is a statistical technique which is used to find the linear relationship between dependent and one or more independent variables. This technique is applicable for Supervised Learning Regression problems where we try to predict a continuous variable.\nProblem\nWe have cars dataset which includes all the type of cars with their features. A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nGoal\nGoal is to build a model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels.\nRFE The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\nIt uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\nVIF The Variance Inflation Factor (VIF) is a measure of colinearity among predictor variables within a multiple regression.\n'], 'url_profile': 'https://github.com/PriyaKashyap', 'info_list': ['Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sundaram81200', 'info_list': ['Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'West Bengal, India', 'stats_list': [], 'contributions': '99 contributions\n        in the last year', 'description': ['predict_restaurant_revenue\nPredict revenue of a restaurant using simple regression techinques\n'], 'url_profile': 'https://github.com/samiranberahaldia', 'info_list': ['Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '219 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jjaysuriya5', 'info_list': ['Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['logistic_regression\npython实现logistic regression 逻辑回归\n学习自：https://www.jianshu.com/p/eb5f63eaae2b\n'], 'url_profile': 'https://github.com/muyun001', 'info_list': ['Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}"
"{'location': 'kasargod', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anagha99', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Java', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic Regression with example : Diabetes Detection\nLogistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability.\n\nAs shown in the picture if you are trying to figure out the test will pass or fail; that is known as Logistic regression. Lets see some real world example\n\nTo predict whether an email is spam (1) or (0)\nWhether the tumour is malignant (1) or not (0)\nIf you are diabetic  (1) or not (0)\n\nConsider a scenario where we need to classify whether an email is spam or not. If we use linear regression for this problem, there is a need for setting up a threshold based on which classification can be done. Say if the actual class is malignant, predicted continuous value 0.4 and the threshold value is 0.5, the data point will be classified as not malignant which can lead to serious consequence in real time.\nFrom this example, it can be inferred that linear regression is not suitable for classification problem. Linear regression is unbounded, and this brings logistic regression into picture. Their value strictly ranges from 0 to 1.\nProject:\nTo make the learning project easier and productive, we are going to learn Logistic Regression by an example of creating a logistic regression model to predict a user is Diabetic or not.\nFirst we are going to collect the dataset and clean it. Then we will create a Logistic regression model with fitting the dataset.\nFileName: diabetic_analysis_logistic_regression.ipynb\nFinally after a successive training it’s our main goal to integrate our machine learning model with the GUIs. You can use tkinter for making a desktop application like we did in Linear Regression example, but here we are going to integrate it with a web framework i.e Flask, to create a website that will take the input parameters from the user and predict if they are Diabetic or not.\nCommand to run the script: python flask_integration\\app.py\n\nHope you have enjoyed learning this, if so share this with others and for more such contents you can connect with me on\nYouTube: https://www.youtube.com/channel/UCmF8qppe02J1ot4Jfwl_lFg\nLinkedIn: https://www.linkedin.com/in/jagwithyou/\nMedium: https://medium.com/@jagwithyou\n'], 'url_profile': 'https://github.com/explorewithjag', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Java', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/waliyani09', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Java', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kanduri2000', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Java', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['Linear functional regression with truncated signatures (preprint)\nWe propose a novel methodology for regressing a real output on vector-valued functional covariates. This methodology is based on the notion of signature, which is a representation of a function as an infinite series of its iterated integrals. The signature depends crucially on a truncation parameter for which an estimator is provided, together with theoretical guarantees. We provide here the code to compute an estimator of the truncation parameter, which is then used to implement a linear regression with signature features. The procedure is summarised below:\n\n\n\nThe code\nAll the code to fit a linear regression on signature features is in main.py. Given data matrices Y of size n and X of size n x p x d, the following lines of code will give you the estimator of the truncation order and a vector of regression coefficients:\nfrom main import orderEstimator\n\nest=orderEstimator(d)\nhatm=est.get_hatm(Y,X,M,Kpen=Kpen,alpha=alpha)[0]\nreg,Ypred=est.fit_ridge(Y,X,hatm,alpha=alpha,norm_path=False)\nwhere alpha is the regularization parameter, Kpen is the constant in the penalization of the truncation order estimator and M is the range of truncation orders considered. If you wish to calibrate Kpen with the slope heuristics method, the command\nK_values=np.linspace(10**(-7),10**1,num=200)\nest.slope_heuristic(K_values,X,Y,max_k,alpha)\nwill output a plot of the estimator as a function of the constant. Then, you can choose the best constant by looking at the biggest jump and picking twice the x-value corresponding to this jump.\nReproducing the experiments\nWe give below the steps to reproduce the results of the paper.\nEnvironment\nAll the necessary packages may be set up by running\npip install -r requirements.txt\nData\nFirst create a directory in the root directory data/. Then download the Canadian Weather and UCR & UEA datasets from the following sources:\n\nThe Canadian Weather dataset comes from the fda R package and can be downloaded by running in R the scrip get_data/get_canadian_weather.R\nThe UCR & UEA datasets may be downloaded at http://www.timeseriesclassification.com and should all be stored in data/ucr/ as .arff files.\n\nRunning the scripts\nThere are 3 scripts that reproduce the various experiments of the paper.\n\nRun python script_cvg_hatm_Y_sig.py to get the results on simulated datasets. Beware that this script may take a while to run. Its results are stored in results/, together with the corresponding plots.\nRun python script_canadian_weather.py to get the results on the Canadian Weather dataset. You will be asked to enter the constant of the penalization chosen by the slope heuristics method.\nRun python script_ucr.py <name> where <name> should be the name of one of the UCR & UEA datasets to reproduce the results on these datasets.\n\nCitation\n@article{fermanian2020linear,\n  title={Linear functional regression with truncated signatures},\n  author={Fermanian, Adeline},\n  journal={arXiv:2006.08442},\n  year={2020}\n}\n'], 'url_profile': 'https://github.com/afermanian', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Java', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sinchana26', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Java', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'Phoenix, AZ, USA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression-Models\nPortfolio of a few regression projects carried out by me\n'], 'url_profile': 'https://github.com/tahaS23', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Java', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '158 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/burrussmp', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Java', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/himeag', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Java', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jasmithaps123', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Java', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 12, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': [""multiple-reg-analysis\nMultiple Regression Analysis on Factors Contributing to College Affordability\nThis is an extension of my Applied Regression Models Final Project using R. I also replicated the R code in Python because my college doesn't offer any Python courses in my department, and it would be a good way to learn regression techniques. I may try to replicate this code in SAS if time permits. I used R version 3.6.2 and Python version 3.6.5. Package versions from both languages can be made available upon request.  I've attached the original dataset and a data dictionary along with my R and Python files.\nBackground\nData was collected on multiple attributes by College Scorecard (2017-2018) for colleges across the U.S. and neighboring territories (𝑁=7112, 𝑛=790 for cleaned dataset).  The data was retrieved from data.gov, and there is a new College Scorecard 2018-2019 dataset available there now.\nMy research question of interest: What are the major contributing factors (i.e. best predictors) to college affordability?\nMethods/Results\nSee attached code files.\nConclusions\n-The reduced model performed well for NDSU with the coefficient matrix holding PCTPELL constant \n-ACTCMMID seemed to be the best predictor of NDSU’s total cost holding all else constant \n-ADM_RATE_ALL and NPT4_PRIV (set coefficient to 0 for public schools) seemed to be the worst individual predictors of NDSU’s cost \n-The reduced model didn’t quite work using all our coefficients \nReferences\nIntroduction to Regression Modeling by Bovas Abraham and Johannes Ledolter \nStatistical Analysis with R For Dummies by Joseph Schmuller \nR for Data Science: Import, Tidy, Transform, Visualize,\tand Model Data by Hadley Wickham and Garrett Grolemund \nhttps://datascience-enthusiast.com/R/ML_python_R_part1.html \nhttps://catalog.data.gov/dataset/college-scorecard \nhttps://www.rdocumentation.org/packages/base/versions/3.6.1/topics/any \nhttps://www.rdocumentation.org/packages/dplyr/versions/0.5.0/topics/select_if \nhttps://towardsdatascience.com/a-beginners-guide-to-linear-regression-in-python-with-scikit-learn-83a8f7ae2b4f \nhttps://stackoverflow.com/questions/32827269/adding-columns-to-matrix-in-python \nhttps://www.statsmodels.org/dev/examples/notebooks/generated/ols.html \n""], 'url_profile': 'https://github.com/Daniel-Palmer-56', 'info_list': ['Updated Jan 18, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Julia', 'MIT license', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Mar 10, 2020', 'Python', 'Updated May 14, 2020', 'R', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tobimichigan', 'info_list': ['Updated Jan 18, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Julia', 'MIT license', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Mar 10, 2020', 'Python', 'Updated May 14, 2020', 'R', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chernyavskiy99', 'info_list': ['Updated Jan 18, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Julia', 'MIT license', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Mar 10, 2020', 'Python', 'Updated May 14, 2020', 'R', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020']}","{'location': 'San Francisco Bay Area', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tobanw', 'info_list': ['Updated Jan 18, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Julia', 'MIT license', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Mar 10, 2020', 'Python', 'Updated May 14, 2020', 'R', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020']}","{'location': 'Bloomington', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['California-House-Pricing-Prediction\nToy project to learn ML taking a regression problem\n'], 'url_profile': 'https://github.com/singhvis29', 'info_list': ['Updated Jan 18, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Julia', 'MIT license', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Mar 10, 2020', 'Python', 'Updated May 14, 2020', 'R', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['spotify-analysis\nMusic contains so much information. We can not only compare all the hit songs to conclude the popular music trend, but also analyse the song behaviour of a particular person and get to know more about him/her through just a small music application.\nSpotify is one of the top music streaming service around . With 35+ million songs and 170+ million monthly active users, musicians can spread their music and reach the audience easily. Also, it is an ideal platform for users to find their favorite music quickly and conveniently due to its large catalog, collaborative playlists, podcasts, and other attractive features. On the app, music can be browsed or searched via various parameters — such as artists, album, genre, playlist, or record label. Users can create, edit, and share playlists, share tracks on social media, and make playlists with other users.\nRecently, I discovered that the Spotify API provides audio features for each song in its database, with which we are able to quantify a song. Therefore, I would like to conduct some music analysis based on it.\nThis project is divided into two parts. The goal in Part I is to analyze what song characteristics would affect its popularity. To achieve this, I built up a regression model and set the popularity as the dependent variable. The goal in Part II is to create a like song prediction system for a specific spotify user. A Shiny App is deployed to show my analysis result.\nFor the study, I will access the Spotify Web API, which provides data from the Spotify music catalog. This can be accessed via standard HTTPS requests to an API endpoint.\nYou can access to the full analysis report here: https://yyyoment.github.io/yan-yun/work/spotify.html\n'], 'url_profile': 'https://github.com/yyyoment', 'info_list': ['Updated Jan 18, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Julia', 'MIT license', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Mar 10, 2020', 'Python', 'Updated May 14, 2020', 'R', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020']}","{'location': 'Brooklyn, NY', 'stats_list': [], 'contributions': '254 contributions\n        in the last year', 'description': [""NBA Win Percentage Predictor\nMultiple regression model predicting season win percentage of an NBA team based on its statistics\nProject Partner: Robert Alterman\nGoal\nCreate a model to predict final season win percentage for an NBA team based on its current per-game statistics.\nData Cleaning/Preprocessing\nBasic NBA statistics for the years 1996-2019\nVariables: WIN%, PTS, FGM, FGA, FG%, 3PM, 3PA, 3P%, FTM, FTA, FT%, OREB, DREB, REB, AST, TOV, STL, BLK, BLKA, PF\n\nUsed Selenium to scrape stats.nba.com\n\nRemoved endline character and team rank from each row (using a regular expression); kept only columns of interest\nCreated dataframe from list of lists, where the first sub-list was the column names and the rest of the sub-lists were one team's          stats from that season\nInserted 'Season' column for each row to keep track of NBA season\nConcatenated each season's stats to the previous season\nRenamed columns that contained non-alphabetic characters\n\n\nLog Transformations and Normalization\n\nRan log transformations using NumPy on variables deemed necessary due to the distribution shape of the variable and/or OLS p-value,        see below (can possibly be explained by more recent trends in basketball, i.e. dramatic increase in three-point shots since 2014)\nNormalized all of the variables (including those that were log transformed) so that they would all be on the same scale given how          they differ\n\n\n\nExploratory Data Analysis\n\nCorrelation heatmap of basketball statistics\n\n\n\nScatter matrix and histograms of the distribution shapes of all of the variables — normal vs. skewed (small snippet of diagram)\n\n\nOLS Tables\n\nBegan by creating Ordinary Least Squares (OLS) regression table on all variables prior to any transformations\nFollowed that up by creating an OLS table with every variable logarithmically transformed\nFinally, made an OLS table only logarithmically transforming the variables that had a lower p-value when logarithmically transformed\n\n\n\n\nInitial Refinement\n\nRemoved variables with high p-values\nR2 = 0.844\nChecked for multicolinearity between variables (where variance inflation factor is greater than 5)\n\n\n\n\nSecond Refinement\n\nRemoved one variable from each of the multicolinear pairs and ran OLS again\nR2 = 0.558\nP-values are all well below α = 0.05\nAppears to no longer be multicolinearity between variables\nP(F-statistic) = 6.88e-107, can conclude that the model proves a better fit than the intercept-only model\n\n\n\n\nAssessing Refined Model\n\nCheck for normal distribution with Q-Q plot\n\n\n\nCheck for homoscedasticity with scatter plot of the error terms (should be random)\n\n\nTesting the Model\n\nPerfomed Scikit-Learn train/test split\nNo need for Ridge/Lasso regularization since unpenalized linear regression was good — very low difference between error of trained        model and error of test model\n\nFinal Model\n\nR2 = 0.558\nMSE = 0.0108\n\n\n\n\nCheck out the blog on Medium about this project!\n""], 'url_profile': 'https://github.com/Vajrasamaya', 'info_list': ['Updated Jan 18, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Julia', 'MIT license', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Mar 10, 2020', 'Python', 'Updated May 14, 2020', 'R', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['nlfitr\nR functions for nonlinear regression of biological data. These include simulation and fitting functions.\nVery much a beta version. This is a work in progress but the functions in the package so far work fine.\nTo install the package, in the R console run: install_github(""TJMurphy/nlfitr"")\n'], 'url_profile': 'https://github.com/TJMurphy', 'info_list': ['Updated Jan 18, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Julia', 'MIT license', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Mar 10, 2020', 'Python', 'Updated May 14, 2020', 'R', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kavyashreekp12', 'info_list': ['Updated Jan 18, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Julia', 'MIT license', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Mar 10, 2020', 'Python', 'Updated May 14, 2020', 'R', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/artirocks', 'info_list': ['Updated Jan 18, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Julia', 'MIT license', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Mar 10, 2020', 'Python', 'Updated May 14, 2020', 'R', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['\nGaussian Process Models for Scalar and Functional Inputs\nDescription: funGp is a regression package based on Gaussian process models. It allows inputs to be either scalar, functional (represented as vectors), or a combination of both. A dimension reduction functionality is implemented in order aid keeping the model light while keeping enough information about the inputs for the model to predict well. Moreover, funGp offers a model selection feature which allows to tune different characteristics of the model such as the active scalar and functional inputs, the type of kernel function and the family of basis function used for the projection of the inputs. This is an extension of the work presented in Betancourt et al. (2020).\nMain functionalities \n🔹 Creation of regression models \n🔹 Output estimation at unobserved input points \n🔹 Random sampling from a Gaussian process model \n🔹 Heuristic optimization of model structure \nNote: funGp was first developed in the frame of the RISCOPE research project, funded by the French Agence Nationale de la Recherche (ANR) for the period 2017-2021 (ANR project No. 16CE04-0011, RISCOPE.fr), and certified by SAFE Cluster.\nThis project is licensed under the GPL-3 License. \nInstallation\n# Install release version from CRAN\ninstall.packages(""funGp"")\n\n# Install release version from GitHub\n# way 1\nlibrary(devtools)\ninstall_github(""djbetancourt-gh/funGp"", dependencies = TRUE)\n\n# way 2\nlibrary(githubinstall)\ngithubinstall(""funGp"", dependencies = TRUE)\n\n\n# Install development version from GitHub\n# way 1\nlibrary(devtools)\ninstall_github(""djbetancourt-gh/funGp@develop"", dependencies = TRUE)\n\n# way 2\nlibrary(githubinstall)\ngithubinstall(""funGp@develop"", dependencies = TRUE)\n\n\nManual 📖  Gaussian Process Regression for Scalar and Functional Inputs with funGp - The in-depth tour \nAuthors: José Betancourt 🔧 (IMT, ENAC), François Bachoc (IMT) and Thierry Klein (IMT, ENAC).\nContributors: Déborah Idier (BRGM) and Jérémy Rohmer (BRGM).\n🔧 maintainer - djbetancourt@uninorte.edu.co \nAcknowledgments: we are grateful to Yves Deville from Alpestat for his advice on the documentation of R packages and to Juliette Garcia from ENAC for her assistance on the stabilization of the Ant Colony algorithm for structural parameter optimization. \nReferences \nBetancourt, J., Bachoc, F., Klein, T., Idier, D., Pedreros, R., and Rohmer, J. (2020), ""Gaussian process metamodeling of functional-input code for coastal flood hazard assessment"". Reliability Engineering & System Safety, 198, 106870. [RESS] - [HAL]\nBetancourt, J., Bachoc, F., Klein, T., and Gamboa, F. (2020), Technical Report: ""Ant Colony Based Model Selection for Functional-Input Gaussian Process Regression. Ref. D3.b (WP3.2)"". RISCOPE project. [HAL]\nBetancourt, J., Bachoc, F., and Klein, T. (2020), R Package Manual: ""Gaussian Process Regression for Scalar and Functional Inputs with funGp - The in-depth tour"". RISCOPE project. [HAL]\n'], 'url_profile': 'https://github.com/djbetancourt-gh', 'info_list': ['R', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'Durham, North Carolina', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Boston-house-prices-dataset\nLinear regression on boston house prices dataset using python\n'], 'url_profile': 'https://github.com/souminator', 'info_list': ['R', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear Regression With Sales Prediction Project\nVideo Link Of Complete Project https://youtu.be/ZPPiZvvpU6s\nLinear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting.\nIt performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output).\nThe simplest form of a simple linear regression equation with one dependent and one independent variable is represented by:\n\nSimilarly for multiple variable\n\n\nProject\n\nLet’s consider a problem statement, a company wants to increase its sells to a certain amount. Now the challenge is to find the amount of investment on advertisement that will result the gain in sells.\nTo solve this problem we have to get the history of the investment and the sales value and prepair our dataset.\nNext we have to create a Logistic Regression model and fit the dataset with the model. After testing if we get that model is working fine then we will save the model for future use.\nFinally we have to integrate the model with a GUI, in this project we will create a tkinter GUI.\nCode Link:\nLinear_regression: simple_and_multiple_linear_regression.ipynb\nGUI Integration : integrating_model_with_gui.ipynb\nHope you have enjoyed learning this, if so share this with others and for more such contents you can connect with me on\nYouTube: https://www.youtube.com/channel/UCmF8qppe02J1ot4Jfwl_lFg\nLinkedIn: https://www.linkedin.com/in/jagwithyou/\nMedium: https://medium.com/@jagwithyou\n'], 'url_profile': 'https://github.com/explorewithjag', 'info_list': ['R', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Prediction-for-Housing-Prices\nI performed a brief analysis on the ""Boston House Prices"" dataset from Kaggle and fit/trained a linear regression model to make predictions on housing prices.\n'], 'url_profile': 'https://github.com/KevinQMBui', 'info_list': ['R', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '158 contributions\n        in the last year', 'description': ['Predicting Hospital Length Of Stay\nRegression analysis to predict hospital length of stays\nObjective:\nOptimize BIDMC’s resourcing strategy by understand how intake factors can predict hospital Length of Stay (LOS)\nData Sources:\n\nDatabase: https://physionet.org/content/mimiciii/1.4/\nData Info: https://mimic.physionet.org/mimictables/callout/\nWiki for ICD9 Codes: https://en.wikipedia.org/wiki/List_of_ICD-9_codes_001%E2%80%93139:_infectious_and_parasitic_diseases\n\nFeatured Techniques\n\nEDA\nFeature Engineering\nLinear Regression\n\nDefinitions:\nLength of Stay (LOS): Amount of time spent in the hospital from admission time to discharge time\nICD9_codes: 5 digit code that identifies each diagnoses\nTime Shifting as Defined by MIMIC-III:\n""All dates in the database have been shifted to protect patient confidentiality. Dates will be internally consistent for the same patient, but randomly distributed in the future. This means that if measurement A is made at 2150-01-01 14:00:00, and measurement B is made at 2150-01-01 15:00:00, then measurement B was made 1 hour after measurement A.\nThe date shifting preserved the following:\n\n\nTime of day - a measurement made at 15:00:00 was actually made at 15:00:00 local standard time. Day of the week - a measurement made on a Sunday will appear on a Sunday in the future. Seasonality - a measurement made during the winter months will appear during a winter month. The date shifting removed the following:\n\n\nYear - The year is randomly distributed between 2100 - 2200. Day of the month - The absolute day of the month is not preserved. Inter-patient information - Two patients in the ICU on 2150-01-01 were not in the ICU at the same time. Dates of birth""\n\n\n'], 'url_profile': 'https://github.com/mollyliebeskind', 'info_list': ['R', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '210 contributions\n        in the last year', 'description': [""Problem Statement\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market.\nThe company wants to know:\n\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\n\nBusiness Goal\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\nData Preparation\nThere is a variable named CarName which is comprised of two parts - the first word is the name of 'car company' and the second is the 'car model'. For example, chevrolet impala has 'chevrolet' as the car company name and 'impala' as the car model name. You need to consider only company name as the independent variable for model building.\nModel Evaluation:\nWhen we're done with model building and residual analysis, and have made predictions on the test set, just make sure you use the following two lines of code to calculate the R-squared score on the test set.\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_pred)\n""], 'url_profile': 'https://github.com/bhargrah', 'info_list': ['R', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['bert-regressor\nbert regressor scripts\nbash run.sh\n'], 'url_profile': 'https://github.com/ahclab', 'info_list': ['R', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['moneyball_ols_regression\nmoneyball_ols_regression\n'], 'url_profile': 'https://github.com/michaelpallante', 'info_list': ['R', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'Brooklyn, NY | New York City, NY', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['market-regression\n'], 'url_profile': 'https://github.com/RobbieZifchak', 'info_list': ['R', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Linear-Regression\nDescrição do curso: Regressão Linear\nAtualmente uma grande quantidade de dados é gerada na grande rede, a cada minuto1, por exemplo, o Google realiza 3.877.140 buscas, no Instagram, usuários postam 49.380 fotos e no twitter, 473.400 tweets são enviados.\nAlém disso, o número de pessoas conectadas à Internet gerando dados vem crescendo significativa- mente, saltando de 2.5 bilhões de usuários em 2012 para 7.5 bilhões em 2019.\nO volume de dados disponíveis aumenta e consequentemente existe a necessidade de analise dessa grande quantidade de informação.\nA utilização de inteligência artificial, através de modelos que usam maquinas de aprendizagem re- duzem o custo de análise e tempo de processamento de informações, apresentando como um dos outputs desse processo de modelagem de dados o melhor entendimento do comportamento de con- sumidores e a aplicação de estratégias para atender as necessidades de empresas publico/privadas através da aplicação da técnica adequada para cada situação.\nEste curso apresenta um dos modelos de maquinas de aprendizagem mais utilizados na indústria e na academia. Através da exploração dessa técnica, conceitos e exemplos serão apresentados.\nConteúdo do curso: Regressão Linear\n\nRegressão Linear Unidimensional\nRegressão Linear Multidimensional\nHiperparâmetros\nAvaliação: Bias e Variância, Underfitting e Overfitting\n\nSoftwares/Compilers necessários\nPython 3+ e Scikitlearn serão utilizados durante o curso. Recomenda-se instalar o Anaconda/Jupyter/Python (gratuitamente) no web site: https://jupyter.org/install\nReferência Bibliográfica\n\nHands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems\nCS229 Lecture Notes by Andrew Ng\n\nPalestrantes: Marcos Machado e Ivaldo Tributino\nMarcos Machado\nÉ atualmente doutorando pela Universidade de Ontário Instituto de Tecnologia. Sua pesquisa atual envolve a exploração de modelos de máquinas de aprendizagem para estudar problemas em Business. Marcos trabalha atualmente sobre supervisão da professora doutora Salma Karray. Marcos tem título de mestre em Engenharia de Produção, obtido pela Universidade de São Paulo (USP), Pós-graduação em administração e em estatística, e mais de 6 anos de experiência na indústria bancária. O primeiro título de Marcos foi obtido no Instituto Federal de Educação, Ciências e Tecnologia (IFCE).\nIvaldo Tributino\nÉ atualmente professor no LaSalle College em Vancouver, Canadá. As pesquisas atuais do Dr. Ivaldo são relacionadas com máquinas de aprendizagem em diferentes cenários. Ivaldo obteve seu doutorado na Universidade Federal do Ceará (UFC) em parceria com a Universidade de Seville (Espanha), mestrado na Universidade Federal da Paraíba (UFPB) e graduação pelo Instituto Federal de Educação, Ciências e Tecnologia (IFCE). Todos esses títulos foram em Ciências – Matemática.\n'], 'url_profile': 'https://github.com/Tributino', 'info_list': ['R', 'Updated Nov 27, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Feb 1, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 16, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistique-Regression\n'], 'url_profile': 'https://github.com/SatyaKaruturi', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 17, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'OCaml', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'GPL-3.0 license', 'Updated Nov 9, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/scoding123', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 17, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'OCaml', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'GPL-3.0 license', 'Updated Nov 9, 2020']}","{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/CliveWatson13', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 17, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'OCaml', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'GPL-3.0 license', 'Updated Nov 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression\nA machine learning model that Predicts the price of houses.\n'], 'url_profile': 'https://github.com/Meetkumar1', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 17, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'OCaml', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'GPL-3.0 license', 'Updated Nov 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Linear-Regression\nProject containing Linear Regression exercises with Scikit-Learn and Statsmodels\n'], 'url_profile': 'https://github.com/wadler91', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 17, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'OCaml', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'GPL-3.0 license', 'Updated Nov 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yatin9988', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 17, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'OCaml', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'GPL-3.0 license', 'Updated Nov 9, 2020']}","{'location': 'Jakarta', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Linear Regression\n\nSimple Linear Regression\nMultiple Linear Regression\nDocumentation : https://medium.com/@arifromadhan19/linear-regression-35f8a65d83b9\n\n'], 'url_profile': 'https://github.com/arifromadhan19', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 17, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'OCaml', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'GPL-3.0 license', 'Updated Nov 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['linear_regression\nLinear regression using gradient method in Ocaml\n'], 'url_profile': 'https://github.com/glegendr', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 17, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'OCaml', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'GPL-3.0 license', 'Updated Nov 9, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/msjahid', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 17, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'OCaml', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'GPL-3.0 license', 'Updated Nov 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['pyRegression\nLinear and nonlinear fit functions that can be used e.g. for curve fitting.\nIs not meant to duplicate methods already implemented e.g. in NumPy or SciPy,\nbut to provide additional, specialized regression methods or higher computation\nspeed.\nLinear regression (in linear_regression.py)\n\ndataset_regression: Does a classical linear least squares regression. Treats\nthe input data as a linear combination of the different components from\nreference data. Can be used for example to fit spectra of mixtures with spectra\nof pure components. Produces the same result like, but much faster than using\nsklearn.linear_model.LinearRegression().fit(...).\nlin_reg_all_sections: Does linear regressions on a dataset starting with the\nfirst two datapoints and expands the segment by one for each iteration. The\nregression metrics are useful to determine if a dataset behaves linearly at its\nbeginning or not, and when a transition to nonlinear behavior occurs.\n\nPolynomial regression (in polynomial_regression.py)\nGeneral nonlinear regression (in nonlinear_regression.py)\nPrincipal component regression and partial least squares regression (in multivariate_regression.py)\n'], 'url_profile': 'https://github.com/AlexanderSouthan', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 17, 2020', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Apr 28, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'OCaml', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'GPL-3.0 license', 'Updated Nov 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SiphokuhleZwane', 'info_list': ['Python', 'Updated Jan 19, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 24, 2020']}","{'location': 'Wisconsin, United States', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mbeaver0803', 'info_list': ['Python', 'Updated Jan 19, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ktlp', 'info_list': ['Python', 'Updated Jan 19, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Cosin-regression\nGiven (x,y) pair regress y on x via cosin function model of the form f(x) = acos(bx+c) + d.\nThe function is first transformed into a simpler form with only 1 non-linear parameter and then we use linear regression on linear parameters. Non-linear paramater is then chosen using grid search.\n'], 'url_profile': 'https://github.com/IsoArm', 'info_list': ['Python', 'Updated Jan 19, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Louis192', 'info_list': ['Python', 'Updated Jan 19, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 24, 2020']}","{'location': 'Phoenix, AZ', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Linear Regression\n'], 'url_profile': 'https://github.com/bigislandtak', 'info_list': ['Python', 'Updated Jan 19, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 24, 2020']}","{'location': 'South Jordan, Utah', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jeremyrodgers123', 'info_list': ['Python', 'Updated Jan 19, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yogitagoswami08', 'info_list': ['Python', 'Updated Jan 19, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lakshmidk', 'info_list': ['Python', 'Updated Jan 19, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sarahjamaal', 'info_list': ['Python', 'Updated Jan 19, 2020', 'R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'R', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Implementation of Polynomial Regression with python\n'], 'url_profile': 'https://github.com/abutair', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 12, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Multiple-Regression-\ni used boston house price data to do further analysis\n'], 'url_profile': 'https://github.com/ashishstat08', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Daniel-Santos9', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 12, 2020']}","{'location': 'West Lafayette, IN, USA', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anaspatankar', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Bayesian-Regression\nUnivariate Baesian Regression 1.ipynb is the file containing the code.\n\nBostonHousingData.csv.txt is the csv file of the data.\n\n'], 'url_profile': 'https://github.com/EvAp0rAt0', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression\nApplying Linear Regression, SGD on different kinds of scenarios, different dataset types.\n'], 'url_profile': 'https://github.com/aswiniNK', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yatin9988', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 12, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/iaw2110', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 12, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': ['Multi-Regression\nUsing hand-writted LR code to process data\n'], 'url_profile': 'https://github.com/Azure-Whale', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Linear-Regression\nDescription\nIn statistics, linear regression is a linear approach to modeling the relationship between a scalar response and one or more explanatory variables.\nInstallation\nUse the package manager pip to install requirements.\npip3 install -r requirements.txt\nExample\npython linear_regression.py data.csv\n\n'], 'url_profile': 'https://github.com/bgeorges35', 'info_list': ['Python', 'Updated Feb 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 12, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear Regression\nA basic walk-through of a single and multiple linear regression model in Python using Statsmodel and SKLearn. The model predicts car prices based on a single variable and multiple variables.\n'], 'url_profile': 'https://github.com/VarinSingh', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated Jan 19, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '540 contributions\n        in the last year', 'description': ['Linear_Regression\n'], 'url_profile': 'https://github.com/kritikaparmar-programmer', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Atlanta', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Regression project focused on predicting the number of uber pickups in a NYC borough for a given day with weather data and day of the week data\n'], 'url_profile': 'https://github.com/grantcloud', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Linear_regression\nSpringboard 10.1.5\n'], 'url_profile': 'https://github.com/jlzhang93', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '951 contributions\n        in the last year', 'description': [""Generalised Linear Models for Dependent Binary Outcomes with Applications to Household Stratified Pandemic Influenza Data\nContains the code for the paper Generalised linear models for dependent binary outcomes with applications to Househols stratified pandemic influenza data - https://arxiv.org/abs/1911.12115\nAbstract\nMuch traditional statistical modelling assumes that the outcome variables of interest are independent of each other when conditioned on the explanatory variables. This assumption is strongly violated in the case of infectious diseases, particularly in close-contact settings such as households, where each individual's probability of infection is strongly influenced by whether other household members experience infection. On the other hand, general multi-type transmission models of household epidemics quickly become unidentifiable from data as the number of types increases. This has led to a situation where it is has not been possible to draw consistent conclusions from household studies of infectious diseases, for example in the event of an influenza pandemic. Here, we present a generalised linear modelling framework for binary outcomes in sub-units that can (i) capture the effects of non-independence arising from a transmission process and (ii) adjust estimates of disease risk and severity for differences in study population characteristics. This model allows for computationally fast estimation, uncertainty quantification, covariate choice and model selection. In application to real pandemic influenza household data, we show that it is formally favoured over existing modelling approaches.\n""], 'url_profile': 'https://github.com/timothykinyanjui', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/scoding123', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression\nNumerical Computing based project using python.\n'], 'url_profile': 'https://github.com/wasiabbas', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,202 contributions\n        in the last year', 'description': ['polynomial_regression\n'], 'url_profile': 'https://github.com/msteknoadam', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Logistic-Regression\nExercise using Logistic Regression for classification\n'], 'url_profile': 'https://github.com/wadler91', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '137 contributions\n        in the last year', 'description': ['linear_regression\nLinear regression is useful for finding relationship between two continuous variables. One is predictor or independent variable and the\nother is response or dependent variable. Here we have considered the length of the automobile as independent variable and the width as the\nindependent variable.At the end we predict the value of the width of the automobile based on the length of the automobile.\nWe caculate the values such as sum,mean,variance,covariance in order to calculate the values for the parameters of the regression line.\ny=b0+b1*x.\nOnce we get values of the b0 and b1 which are the intercept and the slope we can calculate the value of y when the value of x is given.\n'], 'url_profile': 'https://github.com/deviprajwala', 'info_list': ['Jupyter Notebook', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'MIT license', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2021', 'Jupyter Notebook', 'Updated Apr 28, 2020', '1', 'Python', 'Updated Jan 19, 2020']}"
"{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['#Regression-Regularization\nLinear and Logistic Regression in R and Python Codes and Datasets\nThis repository includes programs in both R and Python on Linear and Logistic Regression and Regularization techniques,\nincluding open source datasets such as Ozone data, Red wine, Zip Number Recognition and the Titanic.\n'], 'url_profile': 'https://github.com/mmp909', 'info_list': ['R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'C#', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/patel577', 'info_list': ['R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'C#', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saamm', 'info_list': ['R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'C#', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['LogisticRegression\nUsing a logistic regression model to find the factors that were most important in predicting survival on the Titanic.\n'], 'url_profile': 'https://github.com/PhumlaniKubeka', 'info_list': ['R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'C#', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yogitagoswami08', 'info_list': ['R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'C#', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '148 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gandhi-21', 'info_list': ['R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'C#', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abstract195', 'info_list': ['R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'C#', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['linear-regression\nthis is linear regression part of the machine learning internship\n'], 'url_profile': 'https://github.com/aneez007', 'info_list': ['R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'C#', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'ESAN-UFMS,Campo Grande-MS, Brazil', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amrofi', 'info_list': ['R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'C#', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression_Analysis\nPlease view PDF to see my write up on regression analysis of pokemon images.\n'], 'url_profile': 'https://github.com/Kaipie5', 'info_list': ['R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'C#', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'HTML', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}"
"{'location': 'San Diego, CA', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['Machine Learning Regression Models\nExamples of Machine Learning Regression Models Built in Python and R\nThese models are academic projects in Python and R to explore machine learning and its applications.\nMeet the Models\n\nSimple Linear  : Statistical model\nMultiple Linear  : Multiple linear regression (MLR) or simply multiple regression: statistical technique that uses several explanatory variables to predict the outcome.\nPolynomial  : Relationship between the independent variable x and the dependent variable y, modelled as an nth degree of x.\nSupport Vector  (SVM): More often developed into a machine (SVM), maintains all the main features that characterize the algorithm (maximal margin).\nDecision Tree : Data Mining : Builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller subsets while at the same time an associated decision tree is incrementally developed.\nRandom Forest : Supervised Learning algorithm which uses ensemble learning method for classification and regression\n\nEach folder contains a sample data file, mostly a single comma-separated value spreadsheet, and its Python and R machine learning versions.\n\n\nThis is an academic course project with additional changes to use my customized Anaconda environment, as part of SuperDataScience instruction found on Udemy 2019\n\n'], 'url_profile': 'https://github.com/jeremywood-ai', 'info_list': ['Python', 'MIT license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['LinearRegression\n'], 'url_profile': 'https://github.com/SatyaKaruturi', 'info_list': ['Python', 'MIT license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/UditArora2000', 'info_list': ['Python', 'MIT license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/UditArora2000', 'info_list': ['Python', 'MIT license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/lakshmidk', 'info_list': ['Python', 'MIT license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Logistic_regression\nSpringboard 10.1.6\n'], 'url_profile': 'https://github.com/jlzhang93', 'info_list': ['Python', 'MIT license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Reression\nThe data set consists of customers data of an insurance company.\nThe aim is to predict insurance charges payable by a customer, given the attributes like Age, Gender, whether s/he is a smoker or not, region and BMI.\nI have used Linear Regression technique to predict insurance charges, which is a continuous variable.\n'], 'url_profile': 'https://github.com/AjayprathapCh', 'info_list': ['Python', 'MIT license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic-Regression\nA machine learning model that will predict whether or not user will click on an advertisement.\n'], 'url_profile': 'https://github.com/Meetkumar1', 'info_list': ['Python', 'MIT license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NewYork, United States', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['ML_RidgeRegression\n'], 'url_profile': 'https://github.com/vichu259', 'info_list': ['Python', 'MIT license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Logistic-Regression-Mini-Project\nSpringboard\n'], 'url_profile': 'https://github.com/Mangalalaxmy', 'info_list': ['Python', 'MIT license', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/from-earth-import-dev', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': [""linear_regression_python\nLinear Regression Programming Exercise - from Andrew Ng's 'Machine Learning' Coursera course - written in Python\nIncluded:\n\nLinear Regression with one variable:\n\ncost function computation\ngradient descent update\nsurface plot for visualizing cost vs theta0, theta1\n\nLinear Regression with Two variables:\n\ncost function computation\ngradient descent update\n""], 'url_profile': 'https://github.com/mhiyer', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 13, 2020']}","{'location': 'Austria-Vienna', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['linear regression\nsetup\n\nnpm i\nnode index.js\n\n'], 'url_profile': 'https://github.com/chargome', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 13, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KarthikSwasaka', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 13, 2020']}","{'location': 'Brasil', 'stats_list': [], 'contributions': '734 contributions\n        in the last year', 'description': [""Bone Age Regression with Deep Learning\nThis is my code for the I2A2 Bone Age Regression competition. I learned a lot building this pipeline from scratch and was able to experiment with different model architectures and optimizers. This was my first end-to-end image regression model and it was very nice seeing my theoretic knowledge work in practice.\nThis competition was inspired by RSNA's Bone Age challenge, in which given hand X-ray images, the model should predict the patient's bone age.\n  \n\nX-ray images provided in the competition's dataset.\n\nMy final solution used a ResNet50 architecture, a Rectified Adam optimizer and geometric data augmentations. This model achieved a Mean Average Error of 13.2 after 20 epochs of training, which I believe could be improved given more training time and a better preprocessing pipeline (using object detection to segment the hands and normalizing hand rotation, for example). I didn't saved all the hyperparameters I experimented with (neither their results) but you'll find in the code the ones I used for my last submission.\nI used tensorboard to log the training curves and tqdm to track progress. I also used FCMNotifier, a tool I made to send logs as notifications to my phone.\nRequirements\nSee requirements.txt.\nUsage\n\nDownload the requirements with pip install -r requirements.txt\nDownload the dataset and sample submission with sh download_data.sh. You may need to log in with your Kaggle account in order to do it.\nTrain the ResNet50 model with python boneage.py\nTry different models and hyperparameters editing the train script or use the boneage.ipynb notebook to do it interactively.\n\nCredits\nI used the vision models already implemented in torchvision with small changes. You can actually try other torchvision models by only adding in_channels parameter to generalize the number of input channels, since torchvision models work with RGB images.\n""], 'url_profile': 'https://github.com/bryanoliveira', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 13, 2020']}","{'location': 'Minneaoplis, Minnesota', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/leeschmalz', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 13, 2020']}","{'location': 'Dublin', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alexmillea', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nTheoretical Illustration  :\nSimple linear regression is a statistical method which is used for finding relationship between two continuous variables.\nIt assumes that there is approximately a linear relationship between ""x"" and ""y"".\nThere are two variables, one is the dependent variable(""y"") and the other is the independent variable(""x""). \n\nDependent variable - The variable whose values depend on any other variable/variables is known as dependent variable.\nIndependent variable - The variable whose values does not depend on any other variable is known as independent variable.\nSimple linear regression is used to find a relationship between the dependent and the independent variable. It is important to know that simple linear regression method is not used to find the deterministic relationship. If one variable(independent variable) can accurately determine the other variable(dependent variable), then the relationship between the two variables is said to be deterministic. Simple linear regression is rather used to find the best relationship between the dependent and independent variable for predicting the future values of the dependent variable more accurately.\nMathematical Illustration  :\nSimple linear regression is used to predict a quantitative outcome ""y"" on the basis of one single predictor variable ""x"".\nThe main aim is to build a mathematical model that defines ""y"" as a function of the ""x"".\nThe model thus created can be used to predict the future values of ""y"" on the basis of new values of ""x"".\n\nMathematically, linear regression can be written as  :  y = b0 + b1*x + e, where:\nb0 -> Intercept of the regression line i.e., value of ""y"" when ""x"" = 0\nb1 -> Slope of the regression line i.e., rate of change in ""y"" as ""x"" changes.\ne -> Residual error i.e., diference between original value of ""y"" and predicted value of ""y""\n\nb0 and b1 are known as coefficients or parameters that tells us how important is the term ""x"" for predicting ""y"".\nImportant property of b0  :\nIf there is no ‘b0’ term, then regression line will pass over the origin.\n\nImportant properties of b1  :\nIf b1 > 0, then ""x"" and ""y"" have a positive relationship i.e., as ""x"" increases, ""y"" also increases.\nIf b1 < 0, then ""x"" and ""y"" have a negative relationship i.e., as ""x"" increases, ""y"" decreases.\n\nResidual Sum of Squares(RSS) - The sum of the squares of the residual errors are known as the residual sum of squares or RSS.\nMathematically, RSS can be written as  :  RSS = e1^2 + e2^2 + e3^2 + ..... + en^2\nStandard error(SE) - Standard error is defined as average distance that the observed values(""y"") fall from the regression line.\nMathematically, SE can be written  as  :  SE = (σ^2)/n\nσ -> Standard deviation\nn -> Total number of observations\n\nFor those who don\'t know what is standard deviation.\nStandard Deviation(σ) - Standard deviation is a measure of how spread apart the data is from the mean. A low standard deviation indicates that the values tend to be close to the mean, while a high standard deviation indicates that the values are spread out over a wider range.\nMathematically, Standard deviation can be written  as  :  σ = √(Σ(x-x̄)^2/(n-1)) or (Σ(x-x̄)^2/(n-1))^(1/2)\nx -> value of variable ""x""\nx̄ -> Mean value of variable ""x""\nn -> Total number of observations\n\nResidual standard error(RSE) - Residual Standard Error is defined as the average variation of points around the fitted regression line. It is used to evaluate the overall quality of the regression model. The lower the RSE, the better it is.\nMathematically, Residual standard error can be written  as  :  RSE = √(RSS/(n-1)) or (RSS/(n-1))^(1/2)\nRSS -> Residual sum of squares\nn -> Total number of observations\n\nVisualization of predicted data via simple linear regression model  :\nVisualization of data means expressing the data in the form of graphs, charts, etc.\nI have performed data visualization using ""ggplot2"" package, and the function used is ""ggplot()"".\n\nThe concept behind ggplot2 divides plot into three different fundamental parts  :  Plot = Data + Aesthetics + Geometry.\nThe principal components of every plot can be defined as follow:\nData is a data frame\nAesthetics is used to indicate x and y variables. It can also be used to control the color, the size or the shape of points,etc.\nGeometry defines the type of graph(histogram, box plot, line plot, density plot, dot plot, etc.)\n\nsyntax  :\nggplot()+\ngeom_point(data = ""data frame"", aes(x = ""variable-x"", y = ""variable-y""), colour = ""..."")+\nggtitle(""Main title"", ""Sub-title"")+\nxlab(""x-label"")+\nylab(""y-label"")+\nxlim(c(""initial limit"", ""final limit""))+\nylim(c(""initial limit"", ""final limit""))\n\nNOTE  :\nAll the steps of the code have been explained in detail.\nExplanation is given in comments along with the code.\nAll the pre-processing methods and the reasons why to choose them are also explained in the comments.\nRead each and every line of the code along with the comments.\nIt will help you understand the concept of simple linear regression very easily.\nAfter going through the code, you will be able to perform simple linear regression very conveniently.\n\nFiles attached  :\ncode -> Simple linear regression code along with comments\ngraph -> Simple linear regression graph\nSalary_Data -> Original dataset \ntraining_set -> Trained dataset\ntest_set -> Test dataset\nn_test_set -> Dataset consisting of actual and predicted data values\n\n'], 'url_profile': 'https://github.com/kapsxx', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 13, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '312 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/radheysm', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 13, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['BlogFeedback-Regression-Problem\nInstances in this dataset contain features extracted from blog posts. The task associated with the data is to predict how many comments the post will receive. \nData \nData Set Information:\n\nThis data originates from blog posts. The raw HTML-documents\nof the blog posts were crawled and processed.\nThe prediction task associated with the data is the prediction\nof the number of comments in the upcoming 24 hours. In order\nto simulate this situation, we choose a basetime (in the past)\nand select the blog posts that were published at most\n72 hours before the selected base date/time. Then, we calculate\nall the features of the selected blog posts from the information\nthat was available at the basetime, therefore each instance\ncorresponds to a blog post. The target is the number of\ncomments that the blog post received in the next 24 hours\nrelative to the basetime.\nIn the train data, the basetimes were in the years\n2010 and 2011. In the test data the basetimes were\nin February and March 2012. This simulates the real-world\nsitutation in which training data from the past is available\nto predict events in the future.\nThe train data was generated from different basetimes that may\ntemporally overlap. Therefore, if you simply split the train\ninto disjoint partitions, the underlying time intervals may\noverlap. Therefore, the you should use the provided, temporally\ndisjoint train and test splits in order to ensure that the\nevaluation is fair.\nAttribute Information:\n\n1...50:\nAverage, standard deviation, min, max and median of the\nAttributes 51...60 for the source of the current blog post\nWith source we mean the blog on which the post appeared.\nFor example, myblog.blog.org would be the source of\nthe post myblog.blog.org/post_2010_09_10 \n51: Total number of comments before basetime \n52: Number of comments in the last 24 hours before the\nbasetime \n53: Let T1 denote the datetime 48 hours before basetime,\nLet T2 denote the datetime 24 hours before basetime.\nThis attribute is the number of comments in the time period\nbetween T1 and T2 \n54: Number of comments in the first 24 hours after the\npublication of the blog post, but before basetime \n55: The difference of Attribute 52 and Attribute 53 \n56...60:\nThe same features as the attributes 51...55, but\nfeatures 56...60 refer to the number of links (trackbacks),\nwhile features 51...55 refer to the number of comments. \n61: The length of time between the publication of the blog post\nand basetime \n62: The length of the blog post \n63...262:\nThe 200 bag of words features for 200 frequent words of the\ntext of the blog post \n263...269: binary indicator features (0 or 1) for the weekday\n(Monday...Sunday) of the basetime \n270...276: binary indicator features (0 or 1) for the weekday\n(Monday...Sunday) of the date of publication of the blog\npost \n277: Number of parent pages: we consider a blog post P as a\nparent of blog post B, if B is a reply (trackback) to\nblog post P. \n278...280:\nMinimum, maximum, average number of comments that the\nparents received \n281: The target: the number of comments in the next 24 hours\n(relative to basetime) \nResults\nMultilinear Least Square Regression:\n\nRidge Results:\n\nLASSO Results:\n\nLASSO Feature Importance:\n\n'], 'url_profile': 'https://github.com/adesh-gadge', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Python', 'Updated Feb 18, 2021', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Jan 18, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 13, 2020']}"
"{'location': 'Munich, Germany', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': [""Multi-level regressions with survey data\nMulti-level regressions with survey data, a topic that is a far from trivial. Survey data can be tricky to analyse because it has many discontinuous variables, and missing data, which introduces quite a bit of uncertainty in the results as well us increasing heterogeneity. In general, social country-scale surveys hide so much noise that make everything significant while prodicing barely measurable marginal effects, which are not convinsing, as a result. To demonstrate how to report marginal effects from large scale survey data, I post an R code, where I use World Values Survey data from wave 6 for 50 countries to explore relationship between respondents' self-reported protest participation and online media consumption. I run multi-level logistic regression and simulations to estimate marginal effects.\n""], 'url_profile': 'https://github.com/norakirkizh', 'info_list': ['1', 'R', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Unlicense license', 'Updated Jan 17, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '312 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/radheysm', 'info_list': ['1', 'R', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Unlicense license', 'Updated Jan 17, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ohijea', 'info_list': ['1', 'R', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Unlicense license', 'Updated Jan 17, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Automobile-MPG-Regression-Model\nIn this analysis report we are going to be analyzing different car attributes in order to determine which attributes have the largest impact on a car’s miles per gallon (MPG). In this report we are specifically going to be looking at factors such as: the number of cylinders that a car has, engine displacement, horsepower, the weight of the car, the acceleration of the car, and the year that the car was made. One of the main results that we are trying to figure out is which of these factors have an impact on a car’s miles per gallon. From there we can ask the question: if this factor does impact a car’s miles per gallon, does it increase the miles per gallon or does it decrease the miles per gallon. This dataset was found through the UC Irvine data repository.\n'], 'url_profile': 'https://github.com/amadorosebery', 'info_list': ['1', 'R', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Unlicense license', 'Updated Jan 17, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amykelly36', 'info_list': ['1', 'R', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Unlicense license', 'Updated Jan 17, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'Kiev', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fox998', 'info_list': ['1', 'R', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Unlicense license', 'Updated Jan 17, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Selective-regression-model\nThe codes of paper ""Risk-Controlled Selective Prediction for Regression Deep Neural Network Models"".\n(We are still updating this code repository.)\nTropic Cyclone (TC) Intensity Estimation\nDataset of Tropical Cyclone for Image-to-intensity Regression (TCIR) [^TCIR] was put forward by Boyo Chen, BuoFu Chen and Hsuan-Tien Lin. Please browse web page TCIR for detail.\nSingle image in the TCIR dataset has the size of  201 (height) * 201 (width) * 4 (channels). Four channels are Infrared, Water vapor, Visible and Passive microwave, respectively. We just use Infrared and Passive microwave channels.\nFile TCIntensityEstimation has all the file about tropic cyclone intensity estimation problem, including source code, ""how to get data"" and a trianed model weights file.\n\ndownload source dataset (~13GB) and unzip\n\nwget https://learner.csie.ntu.edu.tw/~boyochen/TCIR/TCIR-ALL_2017.h5.tar.gz\nwget https://learner.csie.ntu.edu.tw/~boyochen/TCIR/TCIR-ATLN_EPAC_WPAC.h5.tar.gz\nwget https://learner.csie.ntu.edu.tw/~boyochen/TCIR/TCIR-CPAC_IO_SH.h5.tar.gz\n\nUnzip the compressed files and save the data into the dir Selective-regression-model/TCIntensityEstimation/data.\n\npreprocessing\n\ncd Selective-regression-model/TCIntensityEstimation/Src\npython preprocess.py \n\nThis step need more than 32GB computer memory.\n\nrunning\n\npython clean_rotated_Sel_PostNet.py\n\nTrained model will be saved in dir Selective-regression-model/TCIntensityEstimation/result_model/. In result_model/, weightsV2-improvement-450.hdf5 is a trained model weight file.\nApparent Age Estimation\nFile ApparentAgeEstimation has all the file about apparent age estimation problem, including  model .prototxt, mean file, ""how to get data"" and a trianed model weights file, validation dataset, test dataset and ""how to download a trained model"".\nApparent age estimation, which tries to estimate the age as perceived by other humans from a facial image, is different from the biological (real) age prediction. You could get more detail about dataset and model from ApparentAgeEstimation [^AGE]. The code of Apparent Age Estimation Model has a lot hard codes, which need to change depended on own enviroment.\n\ndownload validation and test dataset (face only)\n\nwget https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/wiki_crop.tar\n\n\ndownload trained caffe model\n\nwget https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/dex_chalearn_iccv2015.caffemodel\n\n\nrunning (need Caffe installed)\n\ncd Selective-regression-model/ApparentAgeEstimation/\npython LAPClassification.py\n\n###Draw Figures in the Paper\nThe steps to rebuild the models and results in the paper is easy but fussy. The source dataset and neural network models need be downloaded, and the softwares TensorFlow and Caffe should be installed. So, we open the results (.npy files and .csv files in the dir ./DrawFigures/), including the following information.\n\n\n\nBlend-Var or MC-dropout\ndy (y-f(x))\nMSE or MAE\nthe gap t\n(MSE or MAE) +t\n\n\n\nIf you want to draw the figures in the paper:\ncd ./DrawFigures/AGE/\npython draw_all_figures.py\n\nThe figures will be save into dir ./DrawFigures/AGE/Figure/.\nIf you still have some questions, welcome to contact me.\nMy email address : jwm17@mails.tsinghua.edu.cn.\n[^TCIR]: Boyo Chen, Buo-Fu Chen, and Hsuan-Tien Lin. Rotation-blended CNNs on a new open dataset for tropical cyclone image-to-intensity regression. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), August 2018.\n[^AGE]: Rothe R, Timofte R, Van Gool L. Dex: Deep expectation of apparent age from a single image[C]//Proceedings of the IEEE international conference on computer vision workshops. 2015: 10-15.\n'], 'url_profile': 'https://github.com/Wenming-Jiang', 'info_list': ['1', 'R', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Unlicense license', 'Updated Jan 17, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shamika92', 'info_list': ['1', 'R', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Unlicense license', 'Updated Jan 17, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/terao0724', 'info_list': ['1', 'R', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Unlicense license', 'Updated Jan 17, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Non-Parametric-Regression\nUGP\n'], 'url_profile': 'https://github.com/iamsarthakk', 'info_list': ['1', 'R', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020', 'Updated Jan 17, 2020', 'R', 'Updated Apr 17, 2020', 'Python', 'Updated Feb 16, 2020', 'Python', 'Unlicense license', 'Updated Jan 17, 2021', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020']}"
"{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shamika92', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Aug 17, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/terao0724', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Aug 17, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Non-Parametric-Regression\nUGP\n'], 'url_profile': 'https://github.com/iamsarthakk', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Aug 17, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['This project is designed which predicts the cost of a house using certain features. I used Linear Regression methododology from scratch to predict the prices.\n'], 'url_profile': 'https://github.com/amajeti', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Aug 17, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['simple-Linear-regression\n'], 'url_profile': 'https://github.com/Lakshya-Rastogi', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Aug 17, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020']}","{'location': 'NYC', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['nfl_draft_regression\n'], 'url_profile': 'https://github.com/j-gilkey', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Aug 17, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShwethaGowriNagaraj', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Aug 17, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020']}","{'location': 'Shenzhen, China', 'stats_list': [], 'contributions': '401 contributions\n        in the last year', 'description': ['Linear Regression Implementation\nThree machine learning practices to implement linear regression, namely\nnormal equation, skelearn package, and gradient descent.\n\n\n\nSource Code Preview:\n.py version (naiveLinearRegression.py)\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n\n# Manipulate linear regression using normal regression\n# θ = (XTX)-1XTY\ndef normalEquation(x,y):\n\t#ax1 = fig.add_subplot(221)\n\t#ax4 = fig.add_subplot(224)\n\tconstant = np.ones_like(x)\n\tnew_x = np.vstack((x, constant))\n\tnew_x = new_x.transpose()\n\tnew_y = y.reshape((-1, 1))\n\ttheta = (np.linalg.inv(new_x.transpose().dot(new_x))).dot(new_x.transpose()).dot(new_y)\n\ttheta = theta.reshape((1, -1))\n\tnor_x = np.linspace(0, 10, N)\n\tnor_y = np.array(theta[0, 0] * nor_x + theta[0, 1])\n\t# plot the stimulated line generated by normal equation\n\tax1.plot(nor_x, nor_y, c=\'c\', marker=\'o\', alpha=0.2, label=\'Normal Line\')\n\tax4.plot(nor_x, nor_y, c=\'c\', marker=\'o\', alpha=0.2, label=\'Normal Line\')\n\n\n# Manipulate linear regression using sklearn library\ndef sklearn(x, y):\n\t# ax2 = fig.add_subplot(222)\n\t# ax4 = fig.add_subplot(224)\n\tmodel = LinearRegression(fit_intercept=True)\n\tmodel.fit(x[:, np.newaxis], y)\n\txfit = np.linspace(0, 10, N)\n\tyfit = model.predict(xfit[:, np.newaxis])\n\tax2.plot(xfit, yfit, c=\'y\', marker=\'o\', alpha=0.2, label=\'Sklearn Line\')\n\tax4.plot(xfit, yfit, c=\'y\', marker=\'o\', alpha=0.2, label=\'Sklearn Line\')\n\n\n# Manipulate linear regression using gradient descent algorithm\n# y = mx + b\n\n# Method 1: for loop\n# def gradientDescent(x, y):\n# \t# ax3 = fig.add_subplot(223)\n# \t# ax4 = fig.add_subplot(224)\n# \tpoints = np.vstack((x, y))\n# \tlearning_rate = 0.01\n# \tinitial_m = 0\n# \tinitial_b = 0\n# \tnum_iterations = 10000\n# \tm_current = initial_m\n# \tb_current = initial_b\n#\n# \tfor i in range(num_iterations):\n# \t\tb_gradient = 0\n# \t\tm_graident = 0\n# \t\tfor j in range(N):\n# \t\t\ttemp_x = points[0, j]\n# \t\t\ttemp_y = points[1, j]\n# \t\t\tb_gradient += (1 / N) * ((m_current * temp_x + b_current) - temp_y)\n# \t\t\tm_graident += (1 / N) * ((m_current * temp_x + b_current) - temp_y) * temp_x\n# \t\tnew_m = m_current - learning_rate * m_graident\n# \t\tnew_b = b_current - learning_rate * b_gradient\n# \t\tm_current = new_m\n# \t\tb_current = new_b\n#\n# \tm = m_current\n# \tb = b_current\n# \tgra_x = np.linspace(0, 10, N)\n# \tgra_y = np.array(m * gra_x + b)\n# \tax3.plot(gra_x, gra_y, c=\'g\', marker=\'o\', alpha=0.2, label=\'Gradient Descent Line\')\n# \tax4.plot(gra_x, gra_y, c=\'g\', marker=\'o\', alpha=0.2, label=\'Gradient Descent Line\')\n\n# Method 2: matrix multiplication\ndef gradientDescent(x, y):\n\tx0 = np.ones_like(x)\n\tnew_x = np.vstack((x0, x))\n\tx_parameters = new_x.transpose()\n\ty = np.vstack(y)\n\tlearning_rate = 0.01\n\tinitial_m = 0.\n\tinitial_b = 0.\n\tnum_iterations = 10000\n\tm_current = initial_m\n\tb_current = initial_b\n\tb_gradient = 0\n\tm_gradient = 0\n\ttheta = np.array([[b_current], [m_current]], dtype=\'float64\')\n\n\tfor i in range(num_iterations):\n\t\tb_gradient = sum((1/N) * ((x_parameters.dot(theta)) - y))\n\t\tm_gradient = sum((1/N) * ((x_parameters.dot(theta)) - y).flatten() * (x_parameters[:, 1]).flatten())\n\t\tnew_b = theta[0, 0] - learning_rate * b_gradient\n\t\tnew_m = theta[1, 0] - learning_rate * m_gradient\n\t\tb_current = new_b\n\t\tm_current = new_m\n\t\ttheta[0, 0] = b_current\n\t\ttheta[1, 0] = m_current\n\n\tm = m_current\n\tb = b_current\n\tgra_x = np.linspace(0, 10, N)\n\tgra_y = np.array(m * gra_x + b)\n\tax3.plot(gra_x, gra_y, c=\'g\', marker=\'o\', alpha=0.2, label=\'Gradient Descent Line\')\n\tax4.plot(gra_x, gra_y, c=\'g\', marker=\'o\', alpha=0.2, label=\'Gradient Descent Line\')\n\n# Define plot instance\nfig = plt.figure(figsize=(18, 8))\n# plot for original line and normal line\nax1 = fig.add_subplot(221)\n# plot for original line and sklearn line\nax2 = fig.add_subplot(222)\n# plot for original line and gradient descent line\nax3 = fig.add_subplot(223)\n# plot for original line and all the others\nax4 = fig.add_subplot(224)\n\n# Scatter plot and positive correlation\nN = 200\n# x = np.array(np.random.rand(1, N) * 10)\nx = np.linspace(0, 10, N)\noy = np.array(0.5 * x + 4)\ny = np.array(0.5 * x + 4 + np.random.randn(N))\n\n\n# Plot original line and scatter diagram\nax1.plot(x, oy, c=\'m\', marker=\'o\', alpha=0.2, label=\'Original Line\')\nax2.plot(x, oy, c=\'m\', marker=\'o\', alpha=0.2, label=\'Original Line\')\nax3.plot(x, oy, c=\'m\', marker=\'o\', alpha=0.2, label=\'Original Line\')\nax4.plot(x, oy, c=\'m\', marker=\'o\', alpha=0.2, label=\'Original Line\')\nax1.scatter(x, y, s=30, c=\'r\', marker=\'*\', alpha=1)\nax2.scatter(x, y, s=30, c=\'r\', marker=\'*\', alpha=1)\nax3.scatter(x, y, s=30, c=\'r\', marker=\'*\', alpha=1)\nax4.scatter(x, y, s=30, c=\'r\', marker=\'*\', alpha=1)\nnormalEquation(x,y)\nsklearn(x, y)\ngradientDescent(x, y)\n# add legend\nax1.legend(loc=\'upper left\')\nax2.legend(loc=\'upper left\')\nax3.legend(loc=\'upper left\')\nax4.legend(loc=\'upper left\')\nfig.suptitle(\'Three Machine Learning Algorithm to Manipulate Linear Regression\', va=""baseline"")\nplt.show()\n# plt.savefig(\'linearRegression.png\', format=\'png\')\n\n.ipynb version (naiveLinearRegression.ipynb)\n\n#%%\n\n""""""\n@title: Three Machine Learning Algorithm to Manipulate Linear Regression\n@author: LeslieWong & HuangJiaXun\n@group: Five\n""""""\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n#%%\n\n# Manipulate linear regression using normal regression\n# θ = (XTX)-1XTY\ndef normalEquation(x,y):\n    #ax1 = fig.add_subplot(221)\n    #ax4 = fig.add_subplot(224)\n    constant = np.ones_like(x)\n    new_x = np.vstack((x, constant))\n    new_x = new_x.transpose()\n    new_y = y.reshape((-1, 1))\n    theta = (np.linalg.inv(new_x.transpose().dot(new_x))).dot(new_x.transpose()).dot(new_y)\n    theta = theta.reshape((1, -1))\n    nor_x = np.linspace(0, 10, N)\n    nor_y = np.array(theta[0, 0] * nor_x + theta[0, 1])\n    # plot the stimulated line generated by normal equation\n    ax1.plot(nor_x, nor_y, c=\'c\', marker=\'o\', alpha=0.2, label=\'Normal Line\')\n    ax4.plot(nor_x, nor_y, c=\'c\', marker=\'o\', alpha=0.2, label=\'Normal Line\')\n\n\n#%%\n\n# Manipulate linear regression using sklearn library\ndef sklearn(x, y):\n    # ax2 = fig.add_subplot(222)\n    # ax4 = fig.add_subplot(224)\n    model = LinearRegression(fit_intercept=True)\n    model.fit(x[:, np.newaxis], y)\n    xfit = np.linspace(0, 10, N)\n    yfit = model.predict(xfit[:, np.newaxis])\n    ax2.plot(xfit, yfit, c=\'y\', marker=\'o\', alpha=0.2, label=\'Sklearn Line\')\n    ax4.plot(xfit, yfit, c=\'y\', marker=\'o\', alpha=0.2, label=\'Sklearn Line\')\n\n#%%\n\n\n# Manipulate linear regression using gradient descent algorithm\n# y = mx + b\ndef gradientDescent(x, y):\n    # ax3 = fig.add_subplot(223)\n    # ax4 = fig.add_subplot(224)\n    points = np.vstack((x, y))\n    learning_rate = 0.01\n    initial_m = 0\n    initial_b = 0\n    num_iterations = 10000\n    m_current = initial_m\n    b_current = initial_b\n\n    for i in range(num_iterations):\n        b_gradient = 0\n        m_graident = 0\n        for j in range(N):\n            temp_x = points[0, j]\n            temp_y = points[1, j]\n            b_gradient += (1 / N) * ((m_current * temp_x + b_current) - temp_y)\n            m_graident += (1 / N) * ((m_current * temp_x + b_current) - temp_y) * temp_x\n        new_m = m_current - learning_rate * m_graident\n        new_b = b_current - learning_rate * b_gradient\n        m_current = new_m\n        b_current = new_b\n    \n    m = m_current\n    b = b_current\n    gra_x = np.linspace(0, 10, N)\n    gra_y = np.array(m * gra_x + b)\n    ax3.plot(gra_x, gra_y, c=\'g\', marker=\'o\', alpha=0.2, label=\'Gradient Descent Line\')\n    ax4.plot(gra_x, gra_y, c=\'g\', marker=\'o\', alpha=0.2, label=\'Gradient Descent Line\')\n\n\n\n\n#%%\n\n#Define plot instance \nfig = plt.figure(figsize=(18, 8))\n# plot for original line and normal line\nax1 = fig.add_subplot(221)\n# plot for original line and sklearn line\nax2 = fig.add_subplot(222)\n# plot for original line and gradient descent line\nax3 = fig.add_subplot(223)\n# plot for original line and all the others\nax4 = fig.add_subplot(224)\n\n# Scatter plot and positive correlation\nN = 200\n# x = np.array(np.random.rand(1, N) * 10)\nx = np.linspace(0, 10, N)\noy = np.array(0.5 * x + 4)\ny = np.array(0.5 * x + 4 + np.random.randn(N))\n\n\n# Plot original line and scatter diagram\nax1.plot(x, oy, c=\'m\', marker=\'o\', alpha=0.2, label=\'Original Line\')\nax2.plot(x, oy, c=\'m\', marker=\'o\', alpha=0.2, label=\'Original Line\')\nax3.plot(x, oy, c=\'m\', marker=\'o\', alpha=0.2, label=\'Original Line\')\nax4.plot(x, oy, c=\'m\', marker=\'o\', alpha=0.2, label=\'Original Line\')\nax1.scatter(x, y, s=30, c=\'r\', marker=\'*\', alpha=1)\nax2.scatter(x, y, s=30, c=\'r\', marker=\'*\', alpha=1)\nax3.scatter(x, y, s=30, c=\'r\', marker=\'*\', alpha=1)\nax4.scatter(x, y, s=30, c=\'r\', marker=\'*\', alpha=1)\nnormalEquation(x,y)\nsklearn(x, y)\ngradientDescent(x, y)\n# add legend\nax1.legend(loc=\'upper left\')\nax2.legend(loc=\'upper left\')\nax3.legend(loc=\'upper left\')\nax4.legend(loc=\'upper left\')\nfig.suptitle(\'Three Machine Learning Algorithm to Manipulate Linear Regression\', va=""baseline"")\nplt.show()\n# plt.savefig(\'linearRegression.png\', format=\'png\')\n\n'], 'url_profile': 'https://github.com/Leslie-Wong-H', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Aug 17, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020']}","{'location': 'Bangladesh', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Akramwired', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Aug 17, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020']}","{'location': 'Peshawar, Pakistan', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/waqarakbar', 'info_list': ['Jupyter Notebook', 'Updated Feb 7, 2020', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Aug 17, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['DeepNeuralNet_Regression\n'], 'url_profile': 'https://github.com/Kaipie5', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'SAS', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['ML---Regularized-logistic-regression\nWe will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly. Suppose we are the product manager of the factory and we have the test results for some microchips on two different tests. From these two tests, we would like to determine whether the microchips should be accepted or rejected. To help you make the decision, we have a dataset of test results on past microchips, from which we can build a logistic regression model.\n\nWe are using complex calculations to optimize the cost function in order to find the boundary. Here is the result:\n\n'], 'url_profile': 'https://github.com/Manteliz', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'SAS', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pspabhandari', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'SAS', 'Updated Mar 7, 2020']}","{'location': 'Indore ,India', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['simple_linear_Regression_model\nthis model is related to the salary vs year of experience of the employee\n'], 'url_profile': 'https://github.com/shubham22121998', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'SAS', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/floblanc', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'SAS', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': [""Multivariate_Linear_Regression\nA simple implementation of Multivariate Linear Regression using python's sklearn module.\nThe .ipynb file in this repository contains the solution for the exercise described on time 11:30 of this video\n""], 'url_profile': 'https://github.com/yashveersinghsohi', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'SAS', 'Updated Mar 7, 2020']}","{'location': 'Ghana', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Logistic-Regression-with-Pytorch\nWe are Using Logistic regression for classication with Pytorch. The dataset we have used is MNIST dataset which contains images with\ndigits from 0 to 9\nand we have described all the steps throughout the code.\n'], 'url_profile': 'https://github.com/dassebedjibril', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'SAS', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Slava-Nor', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'SAS', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Angel-GM', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'SAS', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '425 contributions\n        in the last year', 'description': ['SAS Statistical Business Analysis Using SAS 9 : Regression and Modeling\n\n1. 시험소개\n\n60-65문항의 객관식, 단답형 주관식 문제\n합격 기준 : 정답률 68% 이상\n시험 시간 : 2시간\n시험 ID : A00-240\n\n2. 시험내용\n\nANOVA의 가정 확인 - 10%\n\nANOVA의 가정 확인\nGLM 및 TTEST 프로시저를 사용해 얻은 모집단 평균 간의 차이 분석\n치료효과 평가를 위한 ANOVA post hoc test 실행\n요인 간 교호작용 탐지 및 분석\n\n\n선형 회귀분석 - 20%\n\nREG 및 GLM 프로시저를 이용한 다중선형 회귀모형 적합\n다중선형회귀 모형에 대한 REG, PLM 및 GLM 프로시저의 결과 분석\nREG 또는 GLMSELECT 프로시저를 이용한 모형 선택\n진단 및 잔차 분석을 통해 주어진 회귀모형의 유효성 평가\n\n\n로지스틱 회귀분석 - 25%\n\nLOGISTIC 프로시저를 통해 로지스틱 회귀분석 수행\n변수 선택을 통한 모형 성능 최적화\nLOGISTIC 프로시저의 결과 해석\nLOGISTIC 및 PLM 프로시저를 이용한 새로운 데이터 셋에 대한 Scoring\n\n\n예측모형 성능을 위한 input준비 - 20%\n\n모형에 대한 input 데이터 준비 시 발생 가능한 문제 식별\nDATA 스텝에서 루프, 배열, 조건문 및 함수로 데이터 처리\n범주형 데이터에 대한 예측력 향상\nCORR 프로시저를 이용한 비선형적 관계 또는 상관성 없는 변수에 대한 식별\nEmpirical logit plot을 이용한 변수 식별\n\n\n모형 성능 평가 - 25%\n\n모형 성과 측정에 정직한 평가원칙 적용\n오차행렬(confusion matrix)을 이용한 분류 성능 평가\nTraining 데이터와 validation 데이터를 이용한 모형 선택 및 검증\n모형비교, 선택을 위한 그래프(ROC, Lift, Gains Charts) 작성 및 해석\nScoring을 위한 효과적인 컷오프 값 설정\n\n\n\n3. e-Learning\n\nhttps://support.sas.com/edu/elearning.html?ctry=kr&productType=library\n\nSAS 기초통계 분석\nLogistic Regression을 이용한 예측 모델링\n\n\n\n4. SAS 기초통계 분석\n\nStatistics 1 : Introduction to ANOVA Regression, and Logistic Regression\n\nChapter 1. Introduction to Statistics\nChapter 2. Analysis of Variance (ANOVA)\nChapter 3. Regression\nChapter 4. Regression Diagnostics\nChapter 5. Categorical Data Analysis\n\n\n\n5. Logistic Regression을 이용한 예측 모델링\n\nPredictive Modeling Using Logistic Regression\n\nChapter 1. Predictive Modeling\nChapter 2. Fitting the Model\nChapter 3. Preparing the Input Variables\nChapter 4. Measuring Classifier Performance\n\n\n\n'], 'url_profile': 'https://github.com/jmpark0118', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'SAS', 'Updated Mar 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '425 contributions\n        in the last year', 'description': ['SAS Statistical Business Analysis Using SAS 9 : Regression and Modeling\n\n1. 시험소개\n\n60-65문항의 객관식, 단답형 주관식 문제\n합격 기준 : 정답률 68% 이상\n시험 시간 : 2시간\n시험 ID : A00-240\n\n2. 시험내용\n\nANOVA의 가정 확인 - 10%\n\nANOVA의 가정 확인\nGLM 및 TTEST 프로시저를 사용해 얻은 모집단 평균 간의 차이 분석\n치료효과 평가를 위한 ANOVA post hoc test 실행\n요인 간 교호작용 탐지 및 분석\n\n\n선형 회귀분석 - 20%\n\nREG 및 GLM 프로시저를 이용한 다중선형 회귀모형 적합\n다중선형회귀 모형에 대한 REG, PLM 및 GLM 프로시저의 결과 분석\nREG 또는 GLMSELECT 프로시저를 이용한 모형 선택\n진단 및 잔차 분석을 통해 주어진 회귀모형의 유효성 평가\n\n\n로지스틱 회귀분석 - 25%\n\nLOGISTIC 프로시저를 통해 로지스틱 회귀분석 수행\n변수 선택을 통한 모형 성능 최적화\nLOGISTIC 프로시저의 결과 해석\nLOGISTIC 및 PLM 프로시저를 이용한 새로운 데이터 셋에 대한 Scoring\n\n\n예측모형 성능을 위한 input준비 - 20%\n\n모형에 대한 input 데이터 준비 시 발생 가능한 문제 식별\nDATA 스텝에서 루프, 배열, 조건문 및 함수로 데이터 처리\n범주형 데이터에 대한 예측력 향상\nCORR 프로시저를 이용한 비선형적 관계 또는 상관성 없는 변수에 대한 식별\nEmpirical logit plot을 이용한 변수 식별\n\n\n모형 성능 평가 - 25%\n\n모형 성과 측정에 정직한 평가원칙 적용\n오차행렬(confusion matrix)을 이용한 분류 성능 평가\nTraining 데이터와 validation 데이터를 이용한 모형 선택 및 검증\n모형비교, 선택을 위한 그래프(ROC, Lift, Gains Charts) 작성 및 해석\nScoring을 위한 효과적인 컷오프 값 설정\n\n\n\n3. e-Learning\n\nhttps://support.sas.com/edu/elearning.html?ctry=kr&productType=library\n\nSAS 기초통계 분석\nLogistic Regression을 이용한 예측 모델링\n\n\n\n4. SAS 기초통계 분석\n\nStatistics 1 : Introduction to ANOVA Regression, and Logistic Regression\n\nChapter 1. Introduction to Statistics\nChapter 2. Analysis of Variance (ANOVA)\nChapter 3. Regression\nChapter 4. Regression Diagnostics\nChapter 5. Categorical Data Analysis\n\n\n\n5. Logistic Regression을 이용한 예측 모델링\n\nPredictive Modeling Using Logistic Regression\n\nChapter 1. Predictive Modeling\nChapter 2. Fitting the Model\nChapter 3. Preparing the Input Variables\nChapter 4. Measuring Classifier Performance\n\n\n\n'], 'url_profile': 'https://github.com/jmpark0118', 'info_list': ['SAS', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['TradingBinary Polynomial Regression\n\nAlgorithm to predict the best parameters in a polynomial regression and verify if it is a good method to trade binary options\nAssumption:\nThe assumption is that the market has trendings, and since a polynomial regression has a good fit in the ends of the interval, the next predicted point is supposed to follow the trend.\nChallenge:\nTo predict if the next closing price will be higher or lower, based on a polynomial regression model.\nMethod\nUsing the polynomial regression model, assume that the next closing price will be the value predicted by the model.\nIf the predicted price is lower, the guess is that the price will go down.\nIf the predicted price is higher, the guess is that the price will go up.\n_The following parameters will be under control:\n\nThe size of the window in wich the regression line will be calculated.\nThe degree of the polynomial regression\n\nResults\nRunning the algorithm with a window size varying from 5 to 50 and degree varying from 2 to 7, the best result was:\nWin ratio: 52.09%\nWindow size: 14\nDegree: 4\n\nPrediction Plot\nHere we see all the closing prices as the small blue dots. The red ones are the predictions without profit, and the green stars as the profit ones.\n\nConclusion\nDespite the idea that a polynomial regression would predict the price based on the trend, it was possible to realize that, even with lots entry points, and lots of profits, the overnight variation does not necessarily follow the general trend, wich is observed over a longer period. So for this same dataset a simple linear regression proved more efficient, but with much more less entry points although.\n'], 'url_profile': 'https://github.com/stdevelopr', 'info_list': ['SAS', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abubakaar', 'info_list': ['SAS', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'Linkoping, Sweden', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Titanic Problem using Logistic Regression\nIn this model, the surviours are predicted in the titanic shipwreck based on type of passengers with multiple variables as predictors\nThe dats is initially cleaned as there are multiple missing values. We are missing 20% of age fields. We can sometimes take the average value if there is any missing data. In this example we are imputing age based on class. We build a function where we allote some random age as if lower class it is 29, middle class 33 and so on.\nThe test error of the model is determined as 0.2014925\nAccuracy of the model is determined 79.85075%\n'], 'url_profile': 'https://github.com/abhi-vellala', 'info_list': ['SAS', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '218 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JP-Vela', 'info_list': ['SAS', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'Santiago, Chile', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juanmadepalacios', 'info_list': ['SAS', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DLArchbold', 'info_list': ['SAS', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ryan-Earl', 'info_list': ['SAS', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aysua', 'info_list': ['SAS', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'Linkoping, Sweden', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Kaggle Project\nLinear Regression Model\nBike Sharing Demand\nURL: https://www.kaggle.com/c/bike-sharing-demand/overview\nYou are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\n'], 'url_profile': 'https://github.com/abhi-vellala', 'info_list': ['SAS', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 27, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'R', 'Updated Jan 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kartik2207', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'C++', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Separating-Chemicals---Logistic-Regression\nTwo chemicals are to be identified on the basis of features given for each chemical.\n'], 'url_profile': 'https://github.com/raghavangra', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'C++', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'Dublin', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alexmillea', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'C++', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'Boston, MA, USA', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Molecular-Property-Prediction\nProject on prediction of molecular properties i.e. scalar coupling constant based on \n\nnearby molecular positions given by one dimensional two dimensional coordinates.\nImplemented using linear regression and tuned model using xgboost\n\nData obtained from active competition under kaggle \nPlease refer the report pdf for complete guide of the project \njust showcasing my RMD skills\n'], 'url_profile': 'https://github.com/anishnitin', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'C++', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'Buffalo, New York', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Logistic-Regression-WDBC-Dataset\nOne of the most common classification algorithm is regression. Regression can be linear or multivariate depending on the number of features available. Regression maps the input vector to an output using a polynomial or basis function. Logistic regression is useful in determining the class to which the input vector belongs. It is called binary logistic regression when the number of classes is dichotomous(binary). In this project binary logistic regression algorithm is employed to determine the malignancy status of the patients in the Wisconsin Diagnostic Breast Cancer (WDBC) dataset.\n'], 'url_profile': 'https://github.com/soumita0210', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'C++', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'Hyperbolic Time Chamber ', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Bryson-P', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'C++', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/arash26m', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'C++', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['AQI-Deployment\n'], 'url_profile': 'https://github.com/lakkanalalitha', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'C++', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Linear-Regression-Mini-Project\nSpringboard\n'], 'url_profile': 'https://github.com/Mangalalaxmy', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'C++', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/makarandmandolkar', 'info_list': ['Python', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 18, 2020', 'R', 'Updated Dec 13, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'C++', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/davidmilligan', 'info_list': ['C#', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 22, 2020', 'Python', 'Updated Feb 29, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 15, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,202 contributions\n        in the last year', 'description': ['Polynomial Regression In Python Using TensorFlow 2.0\nThis is my project to learn about polynomial regression, how optimizers/loss functions work etc. You can get more information about this project (such as the math behind MSE and SGD which are used in the code) at my blog post which you can go with this link.\n'], 'url_profile': 'https://github.com/msteknoadam', 'info_list': ['C#', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 22, 2020', 'Python', 'Updated Feb 29, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 15, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'South Jordan, Utah', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jeremyrodgers123', 'info_list': ['C#', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 22, 2020', 'Python', 'Updated Feb 29, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 15, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/FlyingSpark-Infotech', 'info_list': ['C#', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 22, 2020', 'Python', 'Updated Feb 29, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 15, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'Santiago, Chile', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juanmadepalacios', 'info_list': ['C#', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 22, 2020', 'Python', 'Updated Feb 29, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 15, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['\nFiles:\n\nsport_articles.csv: dataset to build classification tree\nsmall_train.csv: small training dataset to implement unit tests\nsmall_test.csv: small test dataset to implement unit tests\nNode.py: a class to store all elements of a node\nTree.py: a class to do operations about a tree, like building, pruning\ntest_CART.py: units test to test tree model\nsplit.py: functions of splitting region\ncredentials.py: store username and password\nsql.py: store dataset into database\ntest_sql_and_data_frame.py: implement unit tests to compare the results of classifier on SQL version and the data frame version\nsk_learn.py: build decision tree classifier using sk-learn\nbenchmark.txt: general summary comments\ncomplexity.txt: theoretical complexity of the main parts of my implementation\nerror.png: plot of error rate\nrunning_time.png: plot of running time\nperformance.png: plot of performance\n\nDatasets:\n\nThe dataset is loaded from UCI Machine Learning Repository(https://archive.ics.uci.edu/ml/datasets/Sports+articles+for+objectivity+analysis). It contains 1000 samples with 53 attributes.1000 sports articles were labeled using Amazon Mechanical Turk as objective or subjective. However, this dataset is too large to build SQL Tree. Therefore, after consulting professor, I decide to build the classification tree using a smaller subset of this dataset with 700 samples and 31 attributes.\n\nCommand Line Arguments:\n\npython test_CART.py [args....]\nWhere above [args...] is a placeholder for five command-line arguments: <file_name><number_alpha><low_alpha><high_alpha><max_depth>. These arguments are described in detail below:\n1.<file_name>: the file name of dataset\n\n2.<number_alpha>: the number of alpha which will be generated. \n\n3.<low_alpha>: the lowest alpha. \n\n4.<high_alpha>: the highest alpha. \n\n5.<kFold>: the number of fold used for cross-validation. \n\n6.<criterion>: different methods to calculate impurity, where 1 is bayes error, 2 is cross-entropy, 3 is gini index\n\n7.<max_depth>: the maximum depth of tree\n\nAn example of arguments: sports_articles.csv 3 0 1 5 3 4\n'], 'url_profile': 'https://github.com/1717KK', 'info_list': ['C#', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 22, 2020', 'Python', 'Updated Feb 29, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 15, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['ft_linear_regression\nIn this project, you will implement your first machine learning algorithm.\n'], 'url_profile': 'https://github.com/hmney', 'info_list': ['C#', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 22, 2020', 'Python', 'Updated Feb 29, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 15, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mborn91', 'info_list': ['C#', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 22, 2020', 'Python', 'Updated Feb 29, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 15, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'Sunnyvale, California', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bhaskarnn9', 'info_list': ['C#', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 22, 2020', 'Python', 'Updated Feb 29, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 15, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Boustong-House-Pricing-Regression\n'], 'url_profile': 'https://github.com/rbdbhardwaj123', 'info_list': ['C#', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 22, 2020', 'Python', 'Updated Feb 29, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 15, 2020', 'HTML', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020']}"
"{'location': 'Portland, Oregon', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Multi-Level_Modeling_Water_Customers\nNon Linear Mixed Effects Logistic Regression to model water customer support for watershed protection.\n'], 'url_profile': 'https://github.com/dc-larson', 'info_list': ['R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 21, 2020']}","{'location': 'Scotland', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': [""2D-Simple-Linear-Regression-Algorithm\nSimple linear regression algorithm that works in 2D for simple Machine Learning\nThe project simply reads in data from a CSV file and then processes it before calculatig a line of best fit and displaying it in a 2D scatter plot. There are also functions for calculating where a point will lie on the line.\nGoing into the project I wanted to use as few modules as possible for the logic side of things, and in the end I only used random for randomly splitting the data set into training and testing data, and numpy for preping the data for graphical representation which I made using matplotlib.pyplot.\nI also tried to comment for the first time and went a bit overboard but hey, it is what is is and the file is like 3 times as long as a result.\nTo run use the run function in the code. I have provided a CSV file with the code as the project was created as a Physics project so feel free to test that or use your own. It might be easier to re-run each individual function to fit your needs rather than the run function, so if you fo just run the functions in the order you see them in the run fuction, just change the parameters to suit your needs.\nIn the future i'm going to try and do a multiple dimentional linear regression algorithm but we'll see how that goes...\n""], 'url_profile': 'https://github.com/angushenderson', 'info_list': ['R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 21, 2020']}","{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['adhere_logreg\nLogistics Regression Analysis in R to determine effects of Literacy Levels on Diabetic 2 Medication Adherence.\n'], 'url_profile': 'https://github.com/alsabay', 'info_list': ['R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DeltaOptimist', 'info_list': ['R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 21, 2020']}","{'location': 'Kharagpur', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Monsoon-Rainfall-Prediction\n'], 'url_profile': 'https://github.com/ArcNem', 'info_list': ['R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 21, 2020']}","{'location': 'New Brunswick', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Credit-Card-Fraud-Detection\nUsing Logistic Regression, LDA, Xgboost, Random forests to find credit card fraud\n'], 'url_profile': 'https://github.com/kranjitha', 'info_list': ['R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Introduction\nImplementation of common algorithms in Machine Learning using the Julia langugage as part of the Optimization for Learning course at Factulty of Engineering, Lund University.\nDescription\nThe folders contain implementaion of:\n\n\nLasso / tikohonov regularized regression\n\n\nSupport vector machines using Fenchel-Young duality instead of Lagrangian\n\n\nDensely connected neural network i.e. foward-/backpropagation\n\n\nMulti-class SVM using average confidence loss and Fenchel-Young duality\n\n\nRunning the code\nClone the repository and install Julia and Juno using the instructions on this website http://docs.junolab.org/latest/man/installation/.\nRun desired file using for instance the Atom text editor along with Juno.\n'], 'url_profile': 'https://github.com/hampusrosvall', 'info_list': ['R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 21, 2020']}","{'location': 'Herndon, Virginia', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NGS_Pipeline_Utilities\n\ntesting_data_generator\nfile_comparator\njson_parser\ndeployment_inspector\n\n'], 'url_profile': 'https://github.com/wyp1125', 'info_list': ['R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 21, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['Happiness Prediction\nMain Jupyter notebook is World_Happiness.ipynb.\n-- Project Status: Completed\nProject Intro/Objective\nThe purpose of this project is to analyze socio-economic and political data to predict how happy a country’s citizens are using regression models\nMethods Used\n\nMachine Learning\nData Visualization\nLinear Regression Models\n\nTechnologies\n\nPython\nPandas\nJupyter\n\nProject Description\nThe data is from Kaggle.\n\nExploratory Data Analysis\n\nCleaned data and performed Exploratory data analysis using pandas, numpy and visualizations\nWhat are the top 10 countries with highest happiness scores\n\n\n\nRan Linear Regression models iteratively to refine and refit the model for better results using python packages\nPowerpoint Presentation\nPointpoint slide\nContact\nFeel free to contact me with any questions or if you are interested in contributing!\n'], 'url_profile': 'https://github.com/anitaguo49', 'info_list': ['R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 21, 2020']}","{'location': 'Seattle', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': [""Predicting time evolution of a pigment in water\nTechnologies: Python, Keras, NumPy, Scikit-learn, Pandas, Plotly, Altair. \nTopics: deep learning, principal component analysis (PCA), time-series, model discovery. \nDescription\n\n\n\nThe goal of this project is to explore techniques for predicting the behavior of food coloring in water. I started by recording several videos of the diffusion of pigment of colored candy immersed in water, to be used as data source.\nI used PCA to reduce the dimensionality of the data, and then trained a feed-forward neural network based on several videos from the dataset. Once the network was trained, I used it to predict the behavior in an entire video starting from just the first frame. Given the first frame, the neural network predicts the second frame, which I feed back into the network to predict the third frame, and so on.\nNext, I used a model discovery technique to find a partial differential equation (PDE) that describes the evolution of this physical phenomenon. The left-hand side of the PDE  is ut, and for the right-hand side, I considered a library of possible terms, such as ux, uyy, and x ux. I then used Lasso linear regression to find the appropriate coefficients for the terms, and obtained an equation that models the spreading of the food coloring. The main challenge of this task was the calculation of the derivative terms. I tried several techniques based on finite differences, but those led to noisy data and poor results. Therefore, I  decided to find derivatives by fitting a polynomial to the data within a small neighborhood around each pixel, and calculate the derivatives of the polynomial at the pixel of interest. Although this approach is more costly to compute, it produces much smoother results and a more accurate prediction.\nIf you'd like more details, you can read the report detailing my findings.\nThis was my final project for the Inferring Structure of Complex Systems class\n(AMATH 563) at the University of Washington, which I completed as part\nof my masters in Applied Mathematics.\nRunning\nTo run this project on macOS:\nconda env create -f environment_mac.yml\nconda activate pigment-in-water\npython main.py\nTo run this project on Windows:\nconda env create -f environment_win.yml\nconda activate pigment-in-water\npython main.py\n""], 'url_profile': 'https://github.com/bstollnitz', 'info_list': ['R', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'R', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Julia', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Python', 'Updated Jun 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['r-test\nThis repository serves as a container for regression test data for system level\nand module level testing of OpenFAST. The repository contains:\n\nInput files for test case execution\nBaseline solutions for various machine and compiler combinations\nTurbine models used in the regression test cases\n\nThe baseline solutions serve as ""gold standards"" for the regression test suite\nand are updated periodically as OpenFAST and its modules are improved.\n\nmodules/\nThis directory contains module level tests for the modules found in the source\ncode at openfast/modules.\n\nbeamdyn/\nThese BeamDyn specific cases are configured to run with the BeamDyn driver\nprogram rather than with a glue code.\n\nglue-codes/\nThis directory contains system level tests for the various ""glue codes"" or\nhigh-level drivers found in the source at openfast/glue-codes.\n\nopenfast/\nThese are the system level test cases for OpenFAST. This collection of test\ncases was adapted from the FAST V8 CertTests\nand new ones have been added. Each test case directory contains the OpenFAST\ninput file, .fst, and all other case-specific inputs. All turbine-specific\ninputs are linked by relative paths to their corresponding turbine data\ndirectory. See the individual test case README\'s for more information regarding\nthe particular turbine model and portion of the OpenFAST system that is being\ntested.\nThe included turbine directories are:\n\n5MW_Baseline - NREL offshore 5-MW baseline wind turbine\nAOC - Atlantic Orient Company 15/50 wind turbine\nAWT27 - Advanced Wind Turbine program blade 27\nSWRT - Small Wind Research Turbine\nUAE_VI - Unsteady Aerodynamics Experiment research wind turbine\nWP_Baseline - WindPACT 1.5-MW baseline wind turbine\n\nThe CertTest cases have renamed from the old style of TestNN to more\ndescriptive names. In general, the turbine abbreviation begins the case name\nfollowed by a concise description of the physics involved. A mapping of the\nlegacy to current names is given below.\n\n\nLegacy\nCurrent\n\n\n\nTest01\nAWT_YFix_WSt\n\nTest02\nAWT_WSt_StartUp_HighSpShutDown\n\nTest03\nAWT_YFree_WSt\n\nTest04\nAWT_YFree_WTurb\n\nTest05\nAWT_WSt_StartUpShutDown\n\nTest06\nAOC_WSt\n\nTest07\nAOC_YFree_WTurb\n\nTest08\nAOC_YFix_WSt\n\nTest09\nUAE_Dnwind_YRamp_WSt\n\nTest10\nUAE_Upwind_Rigid_WRamp_PwrCurve\n\nTest11\nWP_VSP_WTurb_PitchFail\n\nTest12\nWP_VSP_ECD\n\nTest13\nWP_VSP_WTurb\n\nTest14\nWP_Stationary_Linear\n\nTest15\nSWRT_YFree_VS_EDG01\n\nTest16\nSWRT_YFree_VS_EDC01\n\nTest17\nSWRT_YFree_VS_WTurb\n\nTest18\n5MW_Land_DLL_WTurb\n\nTest19\n5MW_OC3Mnpl_DLL_WTurb_WavesIrr\n\nTest20\n5MW_OC3Trpd_DLL_WSt_WavesReg\n\nTest21\n5MW_OC4Jckt_DLL_WTurb_WavesIrr_MGrowth\n\nTest22\n5MW_ITIBarge_DLL_WTurb_WavesIrr\n\nTest23\n5MW_TLP_DLL_WTurb_WavesIrr_WavesMulti\n\nTest24\n5MW_OC3Spar_DLL_WTurb_WavesIrr\n\nTest25\n5MW_OC4Semi_WSt_WavesWN\n\nTest26\n5MW_Land_BD_DLL_WTurb\n\n\n\n\nBaselines\nThe regression test compares locally generated solutions to the baseline\nsolutions generated on a series of machine and compiler combinations.\nCurrently, the supported machine/compiler combinations for successful\nregression testing are:\n\nlinux-intel\nlinux-gnu\nmacos-gnu\nwindows-intel\n\nThe regression test only supports double precision solutions, so all\nbaseline solutions are generated with a double precision build.\n\nlinux-intel\nThese results were generated on NREL\'s Eagle HPC cluster\nrunning on CentOS 7 with Intel Skylake processors. The OpenFAST binary was\ncompiled with the Intel Fortran compiler at version 18.0.3 and MKL 2018.3.222.\n\nlinux-gnu\nThese results were generated on NREL\'s Eagle HPC cluster\nrunning on CentOS 7 with Intel Skylake processors. The OpenFAST binary was\ncompiled with the GNU Fortran compiler at version 7.3.0. The math libraries in\nthis build are openblas at version 0.3.6.\n\nmacos-gnu\nThese results were generated on a MacBook Pro running on macOS Sierra 10.12.6.\nThe OpenFAST binary was compiled with gfortran installed through Homebrew\'s gcc\npackage at gcc version 7.2.0 (Homebrew GCC 7.2.0).\nThe math libraries in this build are found in the\nAccelerate Framework\ninstalled with Xcode command line tools at version 2347.\n\nwindows-intel\nThese results were generated on a Dell Precision 3530 laptop running\nWindows 10. The OpenFAST binary was compiled with Intel\'s Fortran compiler\nand the Visual Fortran toolset with MKL 2017.\n\nUpdating the baselines\nThe baseline directories can be updated with the included\nupdateBaselineSolutions.py. This script copies locally generated OpenFAST\nsolutions into the appropriate machine - compiler baseline solution directory.\nUsage:\npython updateBaselineSolutions.py source_directory target_directory system_name compiler_id\nExample:\npython updateBaselineSolutions.py local/solution/TestName target/solution/TestName [Darwin,Linux,Windows] [Intel,GNU]\n\nNOTE: ServoDyn external controllers for 5MW_Baseline cases\nThe cases using the 5MW turbine require an external controller for ServoDyn.\nThe source code for three external controllers are provided, but they must be\ncompiled and installed.\nOn Linux and Mac, cmake projects exist to compile the controllers with\nmake. For Windows systems, cmake can generate a Visual Studio project\nto compile and install the controllers.\nFor all system types, create build directories at\n\nr-test/glue-codes/openfast/5MW_Baseline/ServoData/DISCON/build\nr-test/glue-codes/openfast/5MW_Baseline/ServoData/DISCON_ITI/build\nr-test/glue-codes/openfast/5MW_Baseline/ServoData/DISCON_OC3/build\n\nand run cmake .. in each one. For Windows, add your Visual Studio version and\narchitecture in the following command:\ncmake -G ""Visual Studio 14 2015 Win64"" ..\nUltimately, three .dll libraries should be compiled and placed in the\n5MW_Baseline parallel to the test cases that will be executed. For example,\nif the regression test is executed automatically with ctest or\nmanualRegressionTest.py, all case files will be copied to\nopenfast/build. In this case, these three controller libraries must exist:\n\nopenfast/build/reg_tests/glue-codes/openfast/5MW_Baseline/ServoData/DISCON.dll\nopenfast/build/reg_tests/glue-codes/openfast/5MW_Baseline/ServoData/DISCON_ITIBarge.dll\nopenfast/build/reg_tests/glue-codes/openfast/5MW_Baseline/ServoData/DISCON_OC3Hywind.dll\n\n'], 'url_profile': 'https://github.com/D-ICE', 'info_list': ['Roff', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Python', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020']}","{'location': 'Hong Kong', 'stats_list': [], 'contributions': '153 contributions\n        in the last year', 'description': ['Predict-Customer-clicks-on-Facebook-ads-by-Logistic-Regression-Model\nPROBLEM STATEMENT\nYou have been hired as a consultant to a start-up that is running a targetted marketing ads on facebook. The company wants to anaylze customer behaviour by predicting which customer clicks on the advertisement. Customer data is as follows:\nInputs:\n\nName\ne-mail\nCountry\nTime on Facebook\nEstimated Salary (derived from other parameters)\n\nOutputs:\n\nClick (1: customer clicked on Ad, 0: Customer did not click on the Ad)\n\n'], 'url_profile': 'https://github.com/tattwadarshi', 'info_list': ['Roff', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Python', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020']}","{'location': 'Montreal', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['Popularity-Prediction\nIn this project the popularity prediction of comments was investigated based on linear regression technique. The chosen website for obtaining dataset and verification of developed model was Reddit. To achieve this goal, some useful features were extracted from the dataset. Rather than comment\'s content which usually determines the comment popularity, there are some other useful factors including number of replies on the comment (named children), whether the comment is in the main root or not (named is root) and how controversial the comment is (named controversiality), most frequent word counting, etc. Furthermore, it was seen that considering the square of children and the product of controversiality and children as features can enhance the performance of estimation. Based on the idea that punctuation and stop words cannot usually determine the popularity of a comment, the inverse number of these words was considered as another feature for the model. By adding the last four mentioned features, the mean squared error (MSE) decreased by 0.025. The model parameters were obtained based on both closed-form and gradient descent approaches. Higher stability and less runtime were seen in the closed-form approach compared to the gradient descent approach (valid when the features are independent). Finally, the developed model was examined on the test data and achieved MSE was 1.2443.\n\nThe data used in this project can be found in ""proj1_data.json"".\n'], 'url_profile': 'https://github.com/MiladGhanbari', 'info_list': ['Roff', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Python', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['zakaria\n1 Problem setting\n1.1 Africa Economic, Banking and Systemic Crisis :\nContext : This dataset is a derivative of Reinhart et. al’s Global Financial Stability\ndataset which can be found online at: https://www.hbs.edu/behavioral-ﬁnance-and-ﬁnancial-\nstability/data/Pages/global.aspx The dataset will be valuable to those who seek to understand\nthe dynamics of ﬁnancial stability within the African context.\n1.2 Content :\nThe dataset speciﬁcally focuses on the Banking, Debt, Financial, Inﬂation and Systemic Crises that\noccurred, from 1860 to 2014, in 13 African countries, including: Algeria, Angola, Central African\nRepublic, Ivory Coast, Egypt, Kenya, Mauritius, Morocco, Nigeria, South Africa, Tunisia, Zam-\nbia and Zimbabwe. Acknowledgements Reinhart, C., Rogoff, K., Trebesch, C. and Reinhart, V.\n(2019) Global Crises Data by Country. [online] https://www.hbs.edu/behavioral-ﬁnance-and-\nﬁnancial-stability/data. Available at: https://www.hbs.edu/behavioral-ﬁnance-and-ﬁnancial-\nstability/data/Pages/global.aspx [Accessed: 17 July 2019].\n1.3 Inspiration :\nWhich factors are most associated with Systemic Crises in Africa ?\n'], 'url_profile': 'https://github.com/zikosdata', 'info_list': ['Roff', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Python', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020']}","{'location': 'Bengaluru, Karnataka', 'stats_list': [], 'contributions': '174 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Reactor11', 'info_list': ['Roff', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Python', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Washington DC Housing Market Analysis\nContributors:\nNate Lu\nProject Goal:\nThe goal for this analysis is to estimate the median house price for Condo in DC area. We perform Time-Series Linear Regression framework in our analysis. This analysis is statistical-based using two feature seletion methodolgies: stepwise regression and K-Best. In our next project, we will improve this model by applying more machine learning techniques.\nSummary of Data:\nThis analysis uses DC housing market data from Refin and macroeconomic data from Fred with the time periods between February 2012 and October 2019. Housing data includes data for prices (median sale price, percentage of homes sold above list price, percentage of homes that had price drop, etc.), inventory (number of homes on market, new listings, months of supply, etc.), and sales (number of homes sold, median days on market, etc.). For economic data, refer to readme\nSummary of Files:\nData Source:\n\nRedfin\n\nFRED(Federal Reserve Economic Data)\nPython Notebooks:\n\nExploratory Data Analysis\n\nTechnical Notebook\nSlide Deck:\n\nPresentation Slides\nData Folder (all downloaded data and modified datasets used in our final python notebooks):\n\nData\nData Visualizations Folder (any visualizations used in our final python notebooks or presentation):\n\nData Visualizations\nPython Folder (any python files used in our final python notebooks, and other python notebooks used for testing):\n\nPython Files and Test Notebooks\n'], 'url_profile': 'https://github.com/iuniorhsiung', 'info_list': ['Roff', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Python', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '153 contributions\n        in the last year', 'description': ['NYC Taxi Fare Prediction using Simple Linear Regression with BigQuery and PySpark\nDescription\nRunning a Simple Linear Regression on a regression problem might not be the state-of-the-art way to learn parameters. However, when the volumn of data increases drastically, state-of-the-art algorithms will not be able to learn parameters quick enough even with the best computing resources.\nAs such, in this repo, I attempt the use of a simple linear regression to predict NYC taxi fares which has over 100 million records per year in a single SQL table.\nThe data is stored in BigQuery and PySpark would be the tool to go for such big data tasks.\nHow to use\nFor Local Machine\nrun main.py with python and give a mode parameter with tells the code to run on a limited sized dataset or the full SQL table \n> python main.py [mode] \n[mode]:\n--small runs a dataset of size 100,000 nyc taxi records\nLocal Setup Requirements\n\n\nGet a Google Cloud Platform account\n\n\nCreate a GCP project or use an existing GCP project of yours\nYou will need a GCP project in order to run BigQuery api calls. Create a GCP project through your GCP console from https://console.cloud.google.com/project\n\n\nEnable GCP BigQuery API on your aforementioned project You will need to run BigQuery api calls through javascript using BigQuery\'s javascript package. Make sure you have enabled it to the project at https://console.cloud.google.com/flows/enableapi?apiid=bigquery.googleapis.com\n\n\nSetup Authentication and a private key GCP\'s Authentication allows your local machine to make authenticated api calls to GCP\'s services. You will need to create a private Service account key and set up your local machine to use the specified private key by adding it to your PATH by running the following command in your terminal export GOOGLE_APPLICATION_CREDENTIALS=""[PATH to your .json private key]"" or add to your ./bash_profile.\n\n\nFor Google Cloud Platform\n[mode]: --large runs the full SQL table bigquery-public-data.new_york.tlc_yellow_trips_2016 of 131,165,043 nyc taxi records\nGCP Setup Requirements\nSetup a GCP Dataproc Cluster to run on the full dataset.\nhttps://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml#spark-ml-tutorial_subset-python\n\nEnable Dataproc API\nCreate a cluster - choose a datacenter in your region\nUpload the main.py file to the project storage bucket\nSubmit a dataproc job - fill up the job parameters as per the link above also add in the --large optional parameter\nHit the Submit button and wait for the job to complete!\n\n'], 'url_profile': 'https://github.com/leebond', 'info_list': ['Roff', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Python', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': [""CSH Introductory Evaluator\nWhat is CSH?\nCSH, or Computer Science House, is a fantastic\ncommunity at RIT that I am fortunate enough to be a member of. We all have a\npassion for learning and teaching in regards to anything we're passionate about.\nWe try to help each other develop as professionals and individuals with whatever we do.\nWhat is our intro process?\nIn order to join the organization, a prospective member goes through our intro\nprocess and attempts to meet the requirements we set out for them. They are as\nfollows:\n\nObtain a certain percentage of signatures from our current upperclassmen,\nwhich implies that they have spoken to you and think you would be a good fit\nfor the organization.\nAttend a certain number of our House Meetings or general meetings, where we\ngo over important updates and discussions for the organization\nAttend a certain number of our Directorship Meetings, where we discuss\nupdates for each of our executive board positions\nAttend a certain number of our Technical Seminars, where we provide\ninformation on some topic, ranging from programming tips to AI math\nAttend a certain number of our social events, where we just get together\nand do something social\n\nOnce the Introductory Evaluations meeting occurs, each voting member uses these\nmetrics to inform their decision on whether or not to welcome the applicant into\nthe organization. I thought it would be a cool idea to create something that\ncould predict how the organization would end up voting, so I did. The name that\nwe came up with for it is, The Evaluator.\nWhat is The Evaluator, how does it work?\nWhat it is, generally\nIt is simply a tertiary classification model which takes in an applicant's intro\ndata, which are the numerical representations of the requirements described\nabove, and says whether or not CSH is more likely to pass, fail, or conditional\nthat applicant.\nHow it was made\n\nA copy of the CSH databases was acquired in the form of a raw NoSQL dump. A\nlocal SQL database was created from that dump. Various SQL queries were made to\nget the appropriate features and results.\nVarious design considerations were taken into account when trying to figure out\nwhat model / approach to use, but in the end I settled with a basic logistic\nregression model using the One Vs. All approach. (Of course, I'm still toying\naround with other possibilities)\nTesting was done to find the optimal hypothesis function, and eventually\nI settled on one that has decent prediction accuracy of around 83%.\nThe data available for this project was about 200 entries, so a traditional\nsplit of 60/30/10 (or whatever standard you like) was not very feasible. Thus,\nattempts are being made to find what others have done in similar situations and\nhow they solved this issue.\n\nUsage\nWant to see whether or not you'd make it into CSH? Follow these steps\n(If you have any questions about this, message me on Slack or email me!)\n\nGo ahead and download ai_vote.py.\nMake sure you have numpy and scipy installed in your working directory.\nGet your conditional data from conditional.csh.rit.edu, or ask an RTP to grab your info from the server.\nNow run the program! You can do this in one of two ways :\n\nFile mode : Simply run the file with '-f' and then the filename afterwards\n\nEx : python ai_vote.py -f example_file.csv\nFor convenience sake, make your file a .csv with the columns in the following format :\nname,signatures_missed,house_meetings_missed,directorship_meetings_attended,technical_seminars_attended,socials\nsocials is just a 1 or 0 for if you did or did not do any social events.\nIf you're confused about the formatting, just check my example file, 'example_file.csv'\n\n\nCommand line mode : Supply all of the conditional data through command line argument in the below format\n\npython ai_vote.py s_m h_m d_a t_a s\n\ns_m : number of signatures missed\nh_m : number of house meetings missed\nd_a : number of directorship meetings attended\nt_a : number of technical seminars attended\ns : whether or not you attended social events. Put 1 if you attended more 0 events, put 0 otherwise.\n\n\nEx : python ai_vote.py 0 0 60 20 1\n\n\n\n\n\nSome SQL Queries I ran to get various data\n\n\nTo retrieve all House Meetings missed for the 2019 class\nselect uid, count(attendance_status) from member_hm_attendance\nwhere attendance_status = 'Absent'\nand meeting_id in\n(select id from house_meetings\nwhere date <= '10-27-19' and date >= '08-25-19')\nand uid in\n(select uid from freshman_eval_data\nwhere eval_date >= '2019-11-01 00:00:00')\ngroup by uid\norder by uid\n\n\nTo get evaluation results\nselect uid, freshman_eval_result\ncase\nwhen freshman_eval_result = 'Pending' then 2\nwhen freshman_eval_result = 'Passed' then 1\nelse 0\nend\n\n\n""], 'url_profile': 'https://github.com/DylanPJackson', 'info_list': ['Roff', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Python', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['ml-tools\nSome ML utilities functions for multiple tasks such as classification, regression, NLP, visualization\n'], 'url_profile': 'https://github.com/riadghorra', 'info_list': ['Roff', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Python', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020']}","{'location': 'Toronto, ON', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Legendary-Pokemon\nAnalyzing Pokémon statistics, and trying to predict legendary status using logistic regression.\n'], 'url_profile': 'https://github.com/bushrahaque', 'info_list': ['Roff', 'Apache-2.0 license', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Feb 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Sep 30, 2020', 'Python', 'Updated Jan 15, 2020', 'Updated Jan 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['stats2-project1\nMultiple regression using Airbnb data set to predict review scores in Seattle, WA.\n'], 'url_profile': 'https://github.com/bstephan-smu', 'info_list': ['R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Medical-Care-Costs\nAnalysis and Linear Regression using the Medical Care Costs simulated dataset\n'], 'url_profile': 'https://github.com/acarolyn86', 'info_list': ['R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akhil-jha', 'info_list': ['R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'Charlotte, US', 'stats_list': [], 'contributions': '135 contributions\n        in the last year', 'description': ['House-Price-Prediction-Linear-Regression\nThis project is one of the first project, I started working with. In this project, I am using Linear Regression model to predict house prices for king county, this dataset is taken from Kaggle platform.\n'], 'url_profile': 'https://github.com/Minsifye', 'info_list': ['R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Introduction\nAccess to health care services is defined as ""the timely use of personal health services to achieve the best health outcomes."" (IOM, 1993). Having or not having access to health care services can have a great impact on a person\'s physical and mental health, and consequently affect a person\'s quality of life. More broadly it has an effect on the public health of a given society.\nAccess to health care services requires the ability to find health care services within a specific geographic area, having insurance coverage, finding a good health care provider and other factors.\nThis project will focus on studying select disparate geographic regions in the United States to discover similarities and differences in to access health care services. The project will focus on answering the following questions:\n\nAre there certain regions that have better access to health care services than others?\nWhat are the shared characteristics of these regions that affect access to health care services?\n\nAnecdotal evidence appears to indicate low income neighborhoods often have fewer health care providers, hospitals and other health care resources than areas with higher income. This project will use available data to attempt to answer these questions more definitively.\nThe answer to these questions could provide valuable insights to public health officials and guide policy making needed to expand health care services to areas that really need it.\nData\nThis project will use the following data to attempt to answer the questions defined in the introductory section:\n\n\nFoursquare API used to obtain health care service provider information for a given geographic location. Specifically, the API will be queried for locations that match specific categories including: Hospitals, Doctor\'s Offices, Pharmacies, Dentist offices, Urgent care clinics and other medical facilities.\n\n\nUS Census Bureau population demographics and economic data for a given location. This data will be used to provide additional information on the geographic regions being studied. Data to be used includes household income, population, race, ethnicity, age and other population demographics.\n\n\n'], 'url_profile': 'https://github.com/jmm312', 'info_list': ['R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Predict Gentrification with Yelp Data\nQuestions:\nWhat are the top predictors of home prices?\n\n\n\nnumber of business\nyelp reviews per business\nprice point of business\ntype of business (eg. coffee shops, grocery stores, bars, ect.\nfancy coffee shops specifically - how do I define a “fancy” coffee shop?\n\n\n\nData\nResults\n'], 'url_profile': 'https://github.com/sauer3', 'info_list': ['R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '453 contributions\n        in the last year', 'description': [""song-popularity-predictor\npredict a song's popularity score using Linear Regression and One Hot Encoding\n""], 'url_profile': 'https://github.com/spe301', 'info_list': ['R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'Salt Lake City', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['House-Prices-Prediction\nPrediction of house prices in Ames, Iowa using elastic net regression\n'], 'url_profile': 'https://github.com/RamyaDeepthiAvula', 'info_list': ['R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'Chicago, IL, USA', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['R-project\nSanFrancisco Employee data\nDataset obtained from Kaggle (https://www.kaggle.com/san-francisco/sf-employee-compensation)\nOur application is based on San Francisco Employee compensation data which describes the various features related to employee department, organization, job profile, salary and benefits.\nIn a corporate structure, employees are the integral part of the organization. No matter your company size, your people are your most important asset. They are the backbone of your business. So, one of the most important aspects of running your business is keeping your employees happy by offering them high-quality employee benefits and compensation.\nEmployee benefits refer to all non-wage compensation or bonus provided to employees in addition to their salaries. The type of benefits your company decides to offer will vary based on the organization and job profile.\nBut, sometimes many companies don’t realize how much time and money ineffective HR processes are costing them. Providing benefits to those type of job profile which have a very low productivity has often come into wrong consideration. So, there must be some solution in which company can know in advance about the compensation structure based on job profile and organization. This provided us the opportunity to develop model which can predict compensation and benefits based on different factors. Employers can use this model to imbibe some knowledge regarding the compensation factors and employees can use it to decide which job profiles are receiving maximum benefits\nWe will take following steps in evaluating our model:\n\nSince our database contains 0.8 million records, we will use Hold-out evaluation for our model. We will split the data into 80-20 ratio i.e training set (80%) and test set (20%) and will start building the model using training data.\nWe will build different models and evaluate them using coefficient of determination on training dataset to check the accuracy of model. We will then apply the prediction methods on test dataset and evaluate the results by comparing all the models and choose the best out of them.\nMethods used in predicting variables:\nWe have used Multiple linear regression to predict the compensation and benefits given to the employee based on salary, organizationand job profile using R Statistical tool.\nWe have used Multiple linear regression to predict the total salary given to the employee based on organization and job profile and other factors using R Statistical tool.\nWe have used ANOVA to compare average salaries of different employees based on job profiles and organization\n\n'], 'url_profile': 'https://github.com/rainapra', 'info_list': ['R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '483 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/karanaryan07', 'info_list': ['R', 'Updated Feb 17, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'HTML', 'Updated Jan 14, 2020', 'R', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020']}"
"{'location': 'Chennai', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Machine-Learning-Templates\nThis repository contains basic templates of Data pre-processing,Regression,Classification.Clustering\nWhat is Data Preprocessing ?\nData preprocessing is a data mining technique that involves transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent, and/or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues.\nWhy we use Data Preprocessing ?\nIn Real world data are generally incomplete: lacking attribute values, lacking certain attributes of interest, or containing only aggregate data. Noisy: containing errors or outliers. Inconsistent: containing discrepancies in codes or names.\nSteps in Data Preprocessing\nStep 1 : Import the libraries\nStep 2 : Import the data-set\nStep 3 : Check out the missing values\nStep 4 : See the Categorical Values\nStep 5 : Splitting the data-set into Training and Test Set\nStep 6 : Feature Scaling\nRegression\nRegression models are used to predict a continuous value. Predicting prices of a house given the features of house like size, price etc is one of the common examples of Regression. It is a supervised technique. A detailed explanation on types of Machine Learning and some important concepts is given in my previous article.\nTypes of Regression\nSimple Linear Regression\nPolynomial Regression\nSupport Vector Regression\nDecision Tree Regression\nRandom Forest Regression\nSimple Linear Regression\nThis is one of the most common and interesting type of Regression technique. Here we predict a target variable Y based on the input variable X. A linear relationship should exist between target variable and predictor and so comes the name Linear Regression.\nConsider predicting the salary of an employee based on his/her age. We can easily identify that there seems to be a correlation between employee’s age and salary (more the age more is the salary). The hypothesis of linear regression is\nY represents salary, X is employee’s age and a and b are the coefficients of equation. So in order to predict Y (salary) given X (age), we need to know the values of a and b (the model’s coefficients).\nPolynomial Regression\nIn polynomial regression, we transform the original features into polynomial features of a given degree and then apply Linear Regression on it. Consider the above linear model Y = a+bX is transformed to something like\nSupport Vector Regression\nIn SVR, we identify a hyperplane with maximum margin such that maximum number of data points are within that margin. SVRs are almost similar to SVM classification algorithm. We will discuss SVM algorithm in detail in my next article.\nInstead of minimizing the error rate as in simple linear regression, we try to fit the error within a certain threshold. Our objective in SVR is to basically consider the points that are within the margin. Our best fit line is the hyperplane that has maximum number of points.\nDecision Tree Regression\nDecision trees can be used for classification as well as regression. In decision trees, at each level we need to identify the splitting attribute. In case of regression, the ID3 algorithm can be used to identify the splitting node by reducing standard deviation (in classification information gain is used).\nA decision tree is built by partitioning the data into subsets containing instances with similar values (homogenous). Standard deviation is used to calculate the homogeneity of a numerical sample. If the numerical sample is completely homogeneous, its standard deviation is zero.\nRandom Forest Regression\nRandom forest is an ensemble approach where we take into account the predictions of several decision regression trees.\nSelect K random points\nIdentify n where n is the number of decision tree regressors to be created. Repeat step 1 and 2 to create several regression trees.\nThe average of each branch is assigned to leaf node in each decision tree.\nTo predict output for a variable, the average of all the predictions of all decision trees are taken into consideration.\nRandom Forest prevents overfitting (which is common in decision trees) by creating random subsets of the features and building smaller trees using these subsets.\nClassification\nClassification is the process of predicting the class of given data points. Classes are sometimes called as targets/ labels or categories. Classification predictive modeling is the task of approximating a mapping function (f) from input variables (X) to discrete output variables (y).\nFor example, spam detection in email service providers can be identified as a classification problem. This is s binary classification since there are only 2 classes as spam and not spam. A classifier utilizes some training data to understand how given input variables relate to the class. In this case, known spam and non-spam emails have to be used as the training data. When the classifier is trained accurately, it can be used to detect an unknown email.\nClassification belongs to the category of supervised learning where the targets also provided with the input data. There are many applications in classification in many domains such as in credit approval, medical diagnosis, target marketing etc.\nClustering\nClustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups.\nTwo types-1.K-Means 2.Heirarchical\n'], 'url_profile': 'https://github.com/workwithanvay', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Jan 15, 2020', 'R', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2021', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020']}","{'location': 'Jakarta', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gstdl', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Jan 15, 2020', 'R', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2021', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vyshalic', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Jan 15, 2020', 'R', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2021', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020']}","{'location': 'Pune, India', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Sentiment-analysis\nThe goal of this notebook is to predict wheter a review is positive or negative using Logistic Regression.\nData set can be found here - https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\nGetting Started\nPromptCloud extracted 400 thousand reviews of unlocked mobile phones sold on Amazon.com to find out insights with respect to reviews, ratings, price and their relationships.\nThis notebook explains how to clean text data. Trend visualization using reviews.\nI trained logistic regression model to predict whether review is positive or not. I also used bigrams to get more accurate result based on some tricky review.\nThis notebook contains visualization for below topics:\n1.  Top 10 most reviewed products\n2.  Top 10 best brand\n3.  Top 10 worst products\n4.  Best budget product\n5.  Best high end product\n6.  Price vs Rating distribution\n\nLive app\nSentiment Prediction\nInstalling\nUse below command to install dependency to build and deploy model\npip install -r requirements.txt\nAdditionally, to run notebook install below packages\n    1. pip install matplotlib\n    2. pip install seaborn\n    3. pip install pandas\nLicense\nThis project is licensed under the MIT License - see the LICENSE.md file for details.\n'], 'url_profile': 'https://github.com/ashishchoure23', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Jan 15, 2020', 'R', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2021', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020']}","{'location': 'Vancouver, BC', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['The Swan\n\nData-driven method to infer stellar surface gravities from Kepler power spectra using local linear regression. The code also references, and is based on the infrastructure, of The Cannon (Ness et al. 2015; The Cannon).\nAuthors\n\nMaryum Sayeed (UBC)\nDaniel Huber (IfA)\nAdam Wheeler (Columbia)\nMelissa Ness (Columbia, Flatiron)\n\nLicense\nCopyright 2020 the authors. The Swan is free software made available under the MIT License. For details see the file LICENSE.md.\nAt a Glance:\n\nRun quicklook.py on light curves to generate power spectra.\nRun psmaketrainfile_one.py to convert stellar power spectra to log(power spectra).\nRun LLR_logg.py to run local linear regression on both samples.\n\nFor any questions, please get in touch: maryum.sayeed@alumni.ubc.ca.\n'], 'url_profile': 'https://github.com/MaryumSayeed', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Jan 15, 2020', 'R', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2021', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Naresh392', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Jan 15, 2020', 'R', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2021', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020']}","{'location': 'Hamilton', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Machine-learning-Project\nRegression analysis on Black Friday dataset (Dataset from Kaggle competition) (Using Python)\nStarted off from the scratch by cleaning the data, analyzing and visualization, feature selection, Built four different regression models – Ridge regression, Polynomial regression, Decision tree regressor and Random forest regressor. Learnt the difference in their approach towards the type of data. Fine-tuned the parameters for the same estimators using Grid search CV and Random search CV and rebuilt the models and notice the difference in their R2 score.\nThorough understanding of the above regression methods and their usage.\n'], 'url_profile': 'https://github.com/Harshida93', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Jan 15, 2020', 'R', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2021', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020']}","{'location': 'Boston, MA, USA', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Heart-Disease-Prediction\nPrediction of heart disease using classification techniques such as KNN and Logistic Regression.\n'], 'url_profile': 'https://github.com/anishnitin', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Jan 15, 2020', 'R', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2021', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020']}","{'location': 'San Diego, California', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yashj21', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Jan 15, 2020', 'R', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2021', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['customer_churn\ntelecom database when processed with Logistic Regression would predict the loyalty of the customer.\nThe customer churn dataset is private hence cannot be released, and it contained much fewer training samples than I expected, hence the best model to implement the churn prediction was a logistic regression model.\nThe regularized logistic regression using the ‘liblinear’ library, ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers have predicted with similar accuracy. Hence either of them could be used for the training.\n'], 'url_profile': 'https://github.com/adityagm', 'info_list': ['Python', 'Updated Feb 25, 2020', 'Updated Jan 15, 2020', 'R', 'Updated Jan 16, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'MIT license', 'Updated Jan 7, 2021', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'R', 'Updated Jan 13, 2020', 'MATLAB', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Nov 6, 2020']}"
"{'location': 'Chennai', 'stats_list': [], 'contributions': '373 contributions\n        in the last year', 'description': ['Weight_Prediction\n\nIt is an End to End Data Science project using Linear Regression Machine Learning model API which will predict the weight in kilograms for height input given in the web interface.\nThe web interface was done using Flask framework.\nHere, I had used a Simple Linear Regression model to predict the weight in kgs for the given height input in centimeters.\n\n'], 'url_profile': 'https://github.com/ASHOKKUMAR-K', 'info_list': ['HTML', 'Updated Jun 5, 2020', '12', 'Updated Feb 5, 2021', 'R', 'Updated Jan 13, 2020', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020']}","{'location': 'bangalore', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['##K Nearest Neighbors with Python|ML##\nHow It Works ?\nK-Nearest Neighbors is one of the most basic yet essential classification algorithms in Machine Learning.\nIt belongs to the supervised learning domain and finds intense application in pattern recognition, data mining and intrusion detection.\n->The K-Nearest Neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can\nbe used to solve both classification and regression problems.\n->The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\nKNN captures the idea of similarity (sometimes called distance, proximity, or closeness) with some mathematics we might have\nlearned in our childhood— calculating the distance between points on a graph. There are other ways of calculating distance,\nand one way might be preferable depending on the problem we are solving. However, the straight-line distance (also called the Euclidean distance)\nis a popular and familiar choice.\n->It is widely disposable in real-life scenarios since it is non-parametric, meaning, it does not make any underlying assumptions\nabout the distribution of data (as opposed to other algorithms such as GMM, which assume a Gaussian distribution of the given data).\nPre-requisites: Numpy, Pandas, matplotlib, sklearn\n->We’ve been given a random data set with one feature as the target classes.\nWe’ll try to use KNN to create a model that directly predicts a class for a new data point based off of the features.\n'], 'url_profile': 'https://github.com/mallikarjunyadav27', 'info_list': ['HTML', 'Updated Jun 5, 2020', '12', 'Updated Feb 5, 2021', 'R', 'Updated Jan 13, 2020', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020']}","{'location': 'Linkoping, Sweden', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Logistic Regression Project.\nIn this project we will be working with the UCI adult dataset. We will be attempting to predict if people in the data set belong in a certain class by salary, either making <=50k or >50k per year.\nInitially data is cleaned and made some changes for the convenience of model.\nThe data is then plotted for Exploratory Data Analysis.\nThen the data is divided as Train/Test in a ration of 70/30.\nLogistic regression is performed with ""Income"" as target variable and all othe variables as predictors.\nThe model is then predicted with the test data. Class matrix is taken with the priciple class p(x>0.5) = >50K, else <=50K\nFinally test error and accuracy of the model is determined.\nThe test error of the model is 0.148329\nThe accuracy of the model is 85.1671 %\n'], 'url_profile': 'https://github.com/abhi-vellala', 'info_list': ['HTML', 'Updated Jun 5, 2020', '12', 'Updated Feb 5, 2021', 'R', 'Updated Jan 13, 2020', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '128 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rajath1995', 'info_list': ['HTML', 'Updated Jun 5, 2020', '12', 'Updated Feb 5, 2021', 'R', 'Updated Jan 13, 2020', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020']}","{'location': 'Bangladesh', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Akramwired', 'info_list': ['HTML', 'Updated Jun 5, 2020', '12', 'Updated Feb 5, 2021', 'R', 'Updated Jan 13, 2020', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020']}","{'location': 'Bergen', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/EndreTB', 'info_list': ['HTML', 'Updated Jun 5, 2020', '12', 'Updated Feb 5, 2021', 'R', 'Updated Jan 13, 2020', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nbafna04', 'info_list': ['HTML', 'Updated Jun 5, 2020', '12', 'Updated Feb 5, 2021', 'R', 'Updated Jan 13, 2020', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Customer-subscription-using-Logistic-Regression\nInvestigated bank dataset of 45,211 client information which had 17 features for each client in it. The 17th feature, y which is dependent, represented whether a client is going to subscribe a term deposit in the bank or not.\nDataset\nTotal number of samples in this dataset is 45211\nNumber of ""yes"" target = 5289 out of 45211 (that is ~12%)\nNumber of ""no"" target = 39922 out of 45211 (that is ~88%)\n'], 'url_profile': 'https://github.com/saprs', 'info_list': ['HTML', 'Updated Jun 5, 2020', '12', 'Updated Feb 5, 2021', 'R', 'Updated Jan 13, 2020', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020']}","{'location': 'New Jersey', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['LinearRegressionDiagnostics-on-Investments\nLinear regression is a statistical technique to represent relationships between two or more variables using a linear equation.\nThe project explains the linear regression diagnostics on multiple companies to predict the best company for investment in the market.\nThe project shows future scope which could be done by redefining data and using suitable regularization method if any collinearity exists or model doesn’t prove to be a good one.\n'], 'url_profile': 'https://github.com/AlekhyaVV-gif', 'info_list': ['HTML', 'Updated Jun 5, 2020', '12', 'Updated Feb 5, 2021', 'R', 'Updated Jan 13, 2020', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020']}","{'location': 'Nagpur, Maharashtra, India', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/risx3', 'info_list': ['HTML', 'Updated Jun 5, 2020', '12', 'Updated Feb 5, 2021', 'R', 'Updated Jan 13, 2020', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jasmithaps123', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zm17943', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['predicting a startup success rate using multiple linear regression\n'], 'url_profile': 'https://github.com/Yerra-Ajay', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Manglore', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Supreethnayak', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Linear-Regression-on-Boston-Housing\n'], 'url_profile': 'https://github.com/aswiniNK', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '196 contributions\n        in the last year', 'description': [""House-Prices-Advanced-Regression-Techniques\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nPractice Skills\nCreative feature engineering\nAdvanced regression techniques like random forest and gradient boosting\nAcknowledgments\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.\n""], 'url_profile': 'https://github.com/Satnam00', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Regression-Models-for-Protein-Prediction\n'], 'url_profile': 'https://github.com/zm17943', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '312 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/radheysm', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['News-Group-Classification-Using-Logistic-Regression\nThis repository has code for predicting class of the news using logistic regression and compare the performance model run-time with parallel logistic regression using Spark. Both models are made from scratch.\nData\nhttp://qwone.com/~jason/20Newsgroups/\nApproach\nGoal of the project is to classify the news group into positive and negative class by building logistic regression algorithm from scratch.\nI used argparse library to tune the multiple parameters of model through command line. This is complete machine learning pipeline where you\ngive input .txt file and pipeline gives you results. I used OOP to make class as Sparsevector({}) to handle the data more efficiently.\nSpark\nI built parallel logistic regression from scratch to compare the required time to train model and save the output. I used pypark to parallelize the data accross the desired node(changeable through args) and leverage parallelism in model training.\n'], 'url_profile': 'https://github.com/patel577', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Frisco, TX', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['The-scikit-learn-library-\nK-nearest neighbors, Decision trees, Random forests, Logistic regression, Support vector machine implemented using the scikit-learn library.\nDatasets: MNIST, Gamma ray, Solar x-rays and particles(except for Logistic regression)\nWritten by Maliheh Zargaran\n'], 'url_profile': 'https://github.com/malihehz', 'info_list': ['Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020']}"
"{'location': 'Frisco, TX', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Ensembles-from-scratch\nEnsemble of decision and regression trees using Randomization, Bagging and Boosting\nDatasets: Gamma ray detection experiment for decision tree and Solar particle dataset for regression tree\nWritten by Maliheh Zargaran\n'], 'url_profile': 'https://github.com/malihehz', 'info_list': ['Python', 'Updated Jan 19, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'Austin, Texas', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': [""How to Develop a Popular App?\nGoogle Play Store Analysis and Popularity Prediction\n\nIntroduction\nIn this project, we investigate the different variables of Apps on Google Play Store that affect the application and the top 100 most relevant user reviews. We attempt to use our analysis to answer the following questions:\n\nWhat makes an app popular?\nWhat are some interesting trends that we can observe about user behaviour & sentiment on app usage?\nCan we predict app popularity if given a set of features about the app?\n\nMobile app market has grown ~20% in the last 4 years. Android apps market comprises 90% of its market share. Our analysis has the potential to be scaled and applied to identify and solve the following problems for the much wider mobile app industry:\n\nPrediction of app popularity to gauge revenue generated & optimize investment strategy for app development\nIdentifying untapped segments in the app market\nIdentifying fake/junk apps that spam the play store\n\nData Cleaning\nWe started the analysis by creating a data dictionary to understand the structure of the dataset and what each feature represents. Post that, we handled the missing values in some of the columns by either dropping the rows or imputing them with the mode values, depending on the percentage of nulls in each feature. We transformed and some of the columns like Installs, Size and Price to numeric type for ease of analysis.\nExploratory Data Analysis\nAfter establishing a good sense of each feature, we proceeded with plotting a pairwise plot between all the quantitative variables to look for any evident patterns or relationships between the features.\nAndroid Market Breakdown\nWe broke down the apps by category and found that the Family and Game categories have the highest market prevalence. The Business, Tools and\nMedical apps are also catching up. We also checked how each category performed in terms of number of reviews, size and installation count. We found that the gaming category has the maximum number of reviews and install count but the family apps consume more space in the play store.\nSizing Strategy\nWe analysed the sizing distribution of the top-rated apps (rating greater than 4.5) and observed that most top-rated apps are optimally sized between ~2MB to ~40MB i.e., neither too light nor too bulky. We found that the Game and Family categories have the highest number of bulky apps. We also observed that despite this, these bulky apps are fairly highly rated indicating that they are bulky for a purpose. Majority of the paid apps that are highly rated have small sizes which implies that most paid apps are designed and developed to cater to specific functionalities and hence are not bulky. Users prefer to pay for apps that are light-weighted. A paid app that is bulky may not perform well in the market but it also depends on the category of the app.\nPricing Strategy\nThe initial intuition for pricing strategy for apps to be popular would be to make them free. That is confirmed by the data as free apps dominate the store at 92.6%, making it over 12.5 times their paid counterparts. It follows that cheaper apps are also more popular which is supported by the fact that 90% of paid apps are less than $10 and 80% of apps are less than $5. Categorically, the most expensive apps are Family and Medical apps which would suggest that people are willing to pay more for medical apps, possibly due to a belief in a reliability or highly specialized applications.\nWhen cleaning the data, one of the major outliers we found were useless apps priced over $250 which we considered ‘junk’ apps. The names of these apps were typically something to the effect of “I am rich” which would do nothing other than prove the user paid an exorbitant amount it. An interesting insight for app developers for games is that all of the games are within $20.\nInstall Count and Rating\nTo answer the question of whether highly installed apps are rated better, we plotted the install count and rating. We saw that there was no pattern in apps that had installs less than 500 Million. However, for apps with installs greater than this threshold, the minimum rating rises to 3.7 out of 5. To confirm this, we bucketed the apps into bins based on number of installs and plotted the minimum rating for each bucket. We observed that the lowest rating for an app increases with the number of installs. We can say that popular apps are less likely to be rated badly.\nRating Distribution\nThe rating distribution revealed that most apps perform reasonably well with an average rating of 4.17. We broke down the average rating by category to check if any category performs exceedingly good or bad. We conducted a One-way Anova Test and confirmed that the average ratings across categories is statistically different. The Health and Fitness and Books and Reference produce the best apps with 50% apps having a rating greater than 4.5. Interestingly, half of the Dating apps have a rating lower than the average.\nBasic Sentiment Analysis – User Reviews\nWe plotted the fraction of positive, negative and neutral reviews for each category and observed that the Health and Fitness apps perform the best with more than 85% positive reviews. On the other hand, Game and Social apps have a higher fraction of negative reviews. We compared the reviews between free and paid apps and found that people are harsher towards free apps whereas users are more tolerant when they are paying for it.\nFrequency of Words in Reviews\nWe created a word cloud of commonly occurring words in positive and negative reviews and found that the words – “love”, “great” and “good” were the most commonly occurring words in the positive reviews. On the other hand, the negative words that were prevalent were “bad”, “hate” and “ads”.\nOur aim was to analyse the reviews and get a better idea of the common issues that people face with apps or the attributes that make an app popular. We extracted phrases from the reviews and observed that positive reviews had phrases like “user friendly”, “free version”, “works great” and “highly recommend”. The negative reviews contained phrases like “waste time”, “many ads”, “spend money” and “takes forever”. We can see that loading time and ads were one of the main concerns amongst users. On the other hand, usability is one of the reasons that users give positive reviews.\nPredictive Modelling\nWe proceeded to predict the popularity of app based on its features and chose the install count as the measure of popularity. We split the number of installs into four buckets – Not so popular, Intermediate Popular, Popular and Extremely Popular, so that we have a classification problem at hand.\nFeature Selection\nWe plotted a Pearson Correlation Matrix which showed a moderate positive correlation exists between the number of reviews and the number of installs. Our initial analysis also revealed that the size of the app affects the rating. Most of the top-rated apps were sized between 2 Mb and 40 Mb. Intuitively, the number of reviews and size of the app would explain the\nmajority of the popularity of the app. The rating distribution showed that categories had statistically different mean ratings which in turn contributes to the popularity. We decided to use the features - ‘Installs’, 'Reviews', 'Category', 'Content Rating', 'Type', 'Genres' and 'Size'. We dropped the features – Android Ver, Current Ver and Last Updated since these did not have any significant relationship with the install count.\nImplementation\nWe implemented three classification models to predict the popularity of the app, amongst which Random Forest with tuning gave us the best accuracy with 81.08 % accuracy.\nLogistic Regression – 58.67% accuracy\nDecision Tree Classifier – 76.10% accuracy\nRandom Forest Classifier – 78.16% accuracy\nRandom Forest with Hyperparameter tuning - 81.08% accuracy\nWe used Randomized Search CV to loop over a set of random values for the parameters of the model. We ran the grid search and chose the optimal number of tress, number of features at each split, maximum number of levels and the method of selecting samples. Using these parameters, we predicted the popularity of the app on our test set and obtained an accuracy of 88.05%.\nFindings and Conclusion\n\nDevelopers should aim to keep the apps optimally sized (between 2 MB to 40 MB) to increase the likelihood of it being popular.\nPaid apps designed to fulfil specific functions tend to be small sized and are more likely to meet user expectations.\nMajority of the apps should be free. For apps with no ads, apps can be priced under $10.\nFree apps tend to outperform paid apps both in volume, install counts and ratings.\nThere is a positive correlation between installs and ratings\nThe category (Game) is a potential unsaturated space for app developers. Apps in this category tend to have high installs.\nHighly installed apps such as Communication, Productivity & Photography are not as highly rated indicating dissatisfaction among customers and potential untapped market segments.\nFor paid apps, users review harshly.\nBasic sentiment analysis revealed some common issues like loading time and inconvenience with apps may contribute to a negative rating. Usability was one of the main reasons apps were rating positive.\n\n""], 'url_profile': 'https://github.com/sahanasub', 'info_list': ['Python', 'Updated Jan 19, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ani5ha', 'info_list': ['Python', 'Updated Jan 19, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'Frisco, TX', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['k-nearest-neighbors-algorithm-from-scratch\nImage processing and regression task using four versions of k-nearest-neighbors-algorithm are implemented from scratch which   are: Unweighted and Weighted Classification and Unweighted and Weighted Regression.\nTwo datasets are used:MNIST Handwritten Digit Images for classification and solar particle flux for regression task\nAlgorithm performance improved using Attribute Selection, Principal Component Analysis and k-d tree.\nWritten by Maliheh Zargaran\n'], 'url_profile': 'https://github.com/malihehz', 'info_list': ['Python', 'Updated Jan 19, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'Nairobi, Kenya', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': [""Problem statement(still a work in progress)\nCreate a model that predicts results of a game between team 1 and team 2, based on who's home and who's away, and on whether or not the game is friendly (include rank in your training).\npossible approaches\nInput: Home team, Away team, Tournament type (World cup, Friendly, Other)\nApproach 1: Polynomial approach\nWhat to train given:\nRank of home team\nRank of away team\nTournament type\nModel 1: Predict how many goals the home team scores.\nModel 2: Predict how many goals the away team scores.\nApproach 2: Logistic approach\nFeature Engineering:\nFigure out from the home team’s perspective if the game is a Win, Lose or Draw (W, L, D)\n""], 'url_profile': 'https://github.com/gideonkipkorir', 'info_list': ['Python', 'Updated Jan 19, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '125 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abinabrahamjohn', 'info_list': ['Python', 'Updated Jan 19, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/devil33-rgb', 'info_list': ['Python', 'Updated Jan 19, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'Trichy, India', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jaiganesh-MK', 'info_list': ['Python', 'Updated Jan 19, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'Dhaka', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fkjubo', 'info_list': ['Python', 'Updated Jan 19, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Jan 19, 2020', '3', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Apr 9, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020']}"
"{'location': 'Denver, CO', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/egeneroli', 'info_list': ['R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'Turkey', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/emirhanylmzz', 'info_list': ['R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'Karachi,Pakistan', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['Bouston-Housing-Price-ML-Project-Regression\n'], 'url_profile': 'https://github.com/Daniyal56', 'info_list': ['R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akshatAgrawal14', 'info_list': ['R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'Atlanta, GA', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['House Price Prediction using Lasso Regression - obtained a MAPE of 8%\n'], 'url_profile': 'https://github.com/saisatyashreev1996', 'info_list': ['R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/khpraful', 'info_list': ['R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gauri05', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'Frisco, TX', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Decision-and-regression-trees-from-scratch\nDatasets: Gamma ray detection experiment for decision tree and Solar particle dataset for regression tree\nWritten by Maliheh Zargaran\n'], 'url_profile': 'https://github.com/malihehz', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'Greece', 'stats_list': [], 'contributions': '137 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/IoannisTsirovasilis', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'Kyiv. Ukraine', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nonamephysics', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Loan-prediction\nBanks today are facing a lot of problems and giving out free money to wrong people and people who genuinely require money are not given any help because of Improper Faud detection. Machine Learning can help solve that problem for us.\nA classification Program solved with the help of Logistic Regression Algorithm and used KNN for some Increased efficiency\nIn this Data set, details of 535 people were available to us and we had to predict whether the loan would get approved or not using the given data.\nUsed the average values to fill up the missing data in the dataset.\nUsed the dummy variables function of Pandas for some attributes given in the form of String to convert them to integers and get their corresponding values.\n'], 'url_profile': 'https://github.com/Foxtrot2000', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['Machine Learning Classfication\nFollowing techniques of machine learning classification are discussed.\n\nLogistic Regression\nDecision Trees\nRandom Forest\nNaïve Bayes\nSupport Vector Machines (SVM)\n\n'], 'url_profile': 'https://github.com/sureshmanem', 'info_list': ['Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated May 14, 2020']}"
"{'location': 'Bermuda', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['Kaggle Competitions\nHouse Prices\nThe Challenge: Predict the price of homes in Ames, Iowa using 79 explanatory variables.\n\nProject Code\nView on Kaggle\n\nDisaster Tweets\nThe Challenge: Predict whether there is a real disaster using the content of social media posts.\n\nProject Code\nView on Kaggle\n\nTitanic\nThe Challenge: Predict passenger survival using machine learning basics.\n\nProject Code\nView on Kaggle\n\nLearn Together\nThe Challenge: Classify forest types based on geographic features (team-based).\n\nProject Code\nView on Kaggle\n\n'], 'url_profile': 'https://github.com/amypeniston', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Jan 13, 2020', 'R', 'MIT license', 'Updated Jan 30, 2020', '1', 'R', 'Updated May 14, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 31, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/William-Wang-github', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Jan 13, 2020', 'R', 'MIT license', 'Updated Jan 30, 2020', '1', 'R', 'Updated May 14, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 31, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Norman, OK', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Kaggle House Price Prediction Competition\nThis is my attempt to play around with a beginners Kaggle Data Science competition.\nThe competition can be found here.\nAnd my Kaggle profile can be found here!\n'], 'url_profile': 'https://github.com/gradylynn', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Jan 13, 2020', 'R', 'MIT license', 'Updated Jan 30, 2020', '1', 'R', 'Updated May 14, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 31, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Hamburg, Germany', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': [""Double-Machine-Learning-and-IV-Estimation\nThis dissertation project combines methods from machine learning and econometrics to conduct causal inference.\nSpecifically, this code implements Chernozhukov et al.'s (2017) Double Machine Learning (DML) technique and instrumental variables (IV) regression to analyze heterogeneity in the parental health returns to children's education.\nThis implementation of the DML approach utilizes the partialling out concept based on the Frisch-Waugh-Lovell (FWL) theorem, also known as decomposition theorem.\nMachine Learning\nThe code allows to compare a range of different machine learning methods, including regularized regression (Lasso, Ridge, Elastic Net: glmnet), random forest (ranger), and gradient boosting (XGBoost).\nEconometrics\nAfter learning the nuisance parameters for bias reduction, this code implements IV estimation for causal inference.\nThe code allows to compute heteroskedasticity robust and clustered standard errors and additionally applies the wild cluster bootstrap procedure (Cameron et al. 2008).\nReferences\nCameron, A. C., J. B. Gelbach, and D. L. Miller. 2008. Bootstrap-Based Improvements for Inference with Clustered Errors. The Review of Economics and Statistics, 90(3), 414-427.\nChen, T., and C. Guestrin. 2016. XGBoost: A Scalable Tree Boosting System. 22nd SIGKDD Conference on Knowledge Discovery and Data Mining.\nChernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and J. Robins. 2018. Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1), C1-C68.\nFriedman, J., T. Hastie, and R. Tibshirani. 2008. Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22.\nFrisch, R., and F. V. Waugh. 1933. Partial time regressions as compared with individual trends. Econometrica, 1, 387-401.\nWright, M. N., and A. Ziegler. 2017. ranger: A fast implementation of random forests for high dimensional data in C++ and R. Journal of Statistical Software, 77, 1-17.\n""], 'url_profile': 'https://github.com/jeverding', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Jan 13, 2020', 'R', 'MIT license', 'Updated Jan 30, 2020', '1', 'R', 'Updated May 14, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 31, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Instagram_TaskA\nRegression model to predict the level of advertisement, nudity, human visibility and human focus based on the images only.\n'], 'url_profile': 'https://github.com/KinanZ', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Jan 13, 2020', 'R', 'MIT license', 'Updated Jan 30, 2020', '1', 'R', 'Updated May 14, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 31, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['StockX - Predict Resell Price of a Shoe\nWould you spend $3,000 on a shoe? Some people do! That price is not the retail price, but the resell after market value. I got this idea after speaking with a friend who does this for a liivng. He buys (he has a friend that works a shoe store and holds a pair for him, so he has the inside scoop. Most people don\'t!) very limited released shoes or a shoes he thinks will resell very high. He reads a bunch or shoe blogs and articles that are up with the current trends to get insights on which shoe to try and get. Below are a few of those blogs if you are interested.\nSneaker Blogs:\n\nNice Kicks\nHigh Snobiety\nFlight Club\n\nThe popular buy and sell site stockx is where my data comes from. You can check out the site here. This is by far the most popular site for this buy and resell market. Early investors included rapper Eminem and model Karlie Kloss, but the site was founded by Dan Gilbert, the owner of the Cleveland Cavaliers. Just to give you an idea of how much this industry makes forbes estimated all sales of aftermarket reached 2 billion last year.\nNote: This work originated as a Capstone project for the Data Science Immersive curriculum at Galvanize in December 2019. The efforts are continuing.\nBackground\nI\'ve seen a few other models developed for this, but everyything has been very broad, and the data has been very limited. The original thought was trying to predict the price for any shoe on the site, but due to my data collection restraints I narrowed it down to just two types of brands. Addias and Nike. Within those two brands, there are two sub-brands called ""Yeezy"" and ""Off-White"". My dataset contains only those two types of shoes.\nThere are so many styles and types that it would be very difficult to try and predict the price for all shoes. The biggest benefit of a model that predicts properly would be useful for ""Sneaker Heads"" to see what they could expect on their investment on these high valued items.\nProcedures followed\nThis project uses data gathered from the stockx website here. Each row of data is a shoe that was sold from the years 2017 to early 2019 and the features that are the type of shoe, size, color, the state which it was sold to, and the sale price.\nThe features are consist of:\n\nOrder Date\nBrand\nShoe\nSale Price\nRetail Price\nRelease Date\nShoe Size\nBuyer Region\nOrder year\nOrder month\nOrder day\nRelease year\nRelease month\nRelease day\naverage_sale_price\nshoe_premium\n\n\n Figure 1: Workflow and Tools\n\nData Cleaning:\nMy data cleaning process wasn\'t too bad. There were a few columns that had characters mixed with numbers so I had to deal with that small issues. There were also some data types that were object that needed to be swichted to datetime and then split on that date to get month ordered, day ordered, and year ordered.\nFeature engineering\nAfter trying to run a regression on the entire dataset my model was not very intiative. So after thinking it over, I wanted to simplify my model. My entire dataset consisted of about 50 different unique shoes. Below are the different types of shoes that are for each brand. Even though off-white had more different styles, the yeezy brand and far more obervations. After filtering by brand and then shoe count, I took the shoe that had the highest count of every style and tried to predict that one shoe\'s resell price.\nBesides deciding to reduce my dataset to one shoe, I also calculated the mark up and the days the shoe was bought from the release date. My new dataframe can be seen below.\nModels Used\nLinear Regression, Lasso Regression ,and Random Forest Regession resulted in a MSE from 83.777, 83.690, 84.697.\n'], 'url_profile': 'https://github.com/brandonmojica', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Jan 13, 2020', 'R', 'MIT license', 'Updated Jan 30, 2020', '1', 'R', 'Updated May 14, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 31, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'Cincinnati, Ohio', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': [""Predicting-Landing-distance-of-Flights\nFederal Aviation Administration case study\nAnalysis of 950 commercial flights to draw insights on what factors affect the flight landing distance. This was done through data importing, dataset merging, treatment of missing values, exploratory data analysis (EDA), and data visualization.\nAlso, a predictive model (using Linear Regression) was built to predict landing distance so that risk of landing overrun can be reduced.\nTools Used:\nR\nDatasets used\nFAA1.csv and FAA2.csv datasets uploaded in this repository.\nVariables analyzed\nAircraft: The make of an aircraft (Boeing or Airbus).\nDuration (in minutes): Flight duration between taking off and landing. The duration of a normal flight should always be greater than 40min.\nNo_pasg: The number of passengers in a flight.\nSpeed_ground (in miles per hour): The ground speed of an aircraft when passing over the threshold of the runway. If its value is less than 30MPH or greater than 140MPH, then the landing would be considered as abnormal.\nSpeed_air (in miles per hour): The air speed of an aircraft when passing over the threshold of the runway. If its value is less than 30MPH or greater than 140MPH, then the landing would be considered as abnormal.\nHeight (in meters): The height of an aircraft when it is passing over the threshold of the runway. The landing aircraft is required to be at least 6 meters high at the threshold of the runway.\nPitch (in degrees): Pitch angle of an aircraft when it is passing over the threshold of the runway.\nDistance (in feet): The landing distance of an aircraft. More specifically, it refers to the distance between the threshold of the runway and the point where the aircraft can be fully stopped. The length of the airport runway is typically less than 6000 feet.\nKey Insights\nThere is a difference in factors and how they affect landing distance for the two different types of aircrafts\nWhereas for Airbus planes, factors affecting 'Landing Distance' the most tend to be 'Air speed', 'Ground Speed', and 'Height', for Boeing planes the variables are Air speed and Height (at a 95% confidence level).\nIn general, a unit change in 'Air speed' affected 'landing distance' 8 times more than a unit change in 'height'.\n""], 'url_profile': 'https://github.com/parthskuc', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Jan 13, 2020', 'R', 'MIT license', 'Updated Jan 30, 2020', '1', 'R', 'Updated May 14, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 31, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['EPL Predictions\nPredictions on English Premier League standings using a linear regression model based on historical dataset from seasons 2005-2006 to 2018-2019 (raw data can be found here).\nPrerequisites\nThe following are the python libraries used for this project:\nglob2==0.6\nkeras==2.2.3\nmatplotlib==2.2.2\nnumpy==1.14.3\npandas==0.23.0\nscikit-learn==0.19.1\nseaborn==0.8.1\nOverview\n\nraw_data - contains all raw csv files for EPL and EFL seasons\ndata - contains all preprocessed csv files for EPL seasons\nsrc - contains code for processing and analyzing results\n\nApproach\nIn this project, I implemented a linear regression model using the season aggregate statistics of a team performance from seasons 2005 - 2006 to 2018 - 2019, to predict the performances of the 20 EPL teams in the 2019 - 2020 season. The exploratory data analysis (EDA) on the dataset, data preprocessing, and the results of the regression model can be found in preprocessing.ipynb and models.ipynb\nResults\nThe implemented model is only able to provide realistic predictions for teams that have consistent performance from the previous year to the 19/20 season. For teams that are performing significantly different from the previous seasons, the model fails to provide useful predictions. This is likely due to the fact that the premier league is such a complex beast to model, aggregated statistics from the previous season alone is not going to provide enough information, and thus our model cannot capture the complexity of the problem. A larger set of more relevant features may help improve the performance of the model (e.g. amount spent in transfer market, quantified metric of manager competency, existence of star player in squad), but it is likely that a lot of useful information is lost by using aggregate statistics of the entire season. For future work, making predictions at the game-level instead of the season-level will likely yield better results.\n'], 'url_profile': 'https://github.com/jimmywoo1', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Jan 13, 2020', 'R', 'MIT license', 'Updated Jan 30, 2020', '1', 'R', 'Updated May 14, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 31, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Predicting-Profitability-MBA\nA regression analysis to predict the profit of new products and a market basket analysis of transactional data\nhttps://rpubs.com/FDonoso/624619\n'], 'url_profile': 'https://github.com/FerranDonoso', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Jan 13, 2020', 'R', 'MIT license', 'Updated Jan 30, 2020', '1', 'R', 'Updated May 14, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 31, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '231 contributions\n        in the last year', 'description': [""\nFacial_Keypoints\nIn this project we are predicting 15 key features on human faces. The dataset consists of 7039 images - all in size of 96x96 pixels with 30 coordinates of those features for each image (images are predictors).\n\nData\nFor only 4 of those features we have data in all observations and in the rest we've got around 2200 observations (as you can see below).\n\nData can be download from:\nhttps://www.kaggle.com/c/facial-keypoints-detection/data\nTransfer Learning\nWe trained the network on the whole data for those 4 features (8 coordinates) and then use the layers from it to train the target network with all features by adding few layers on top of the initial one. This solution outperforms all others that we've tried.\n\nResults\nRMSE (in pixels): 1.556\n\nIt works quite well but it is not perfect :)\n\n""], 'url_profile': 'https://github.com/blawok', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'Updated Jan 13, 2020', 'R', 'MIT license', 'Updated Jan 30, 2020', '1', 'R', 'Updated May 14, 2020', 'Python', 'Updated Jan 28, 2020', 'Jupyter Notebook', 'Updated Nov 5, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 31, 2020', 'R', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated May 9, 2020']}"
"{'location': 'London, United Kingdom', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['🍷 Predicting wine ratings with Tensorflow\nThis is a small project I did just so that I brush up\nmy data science skills.\nThe dataset\nThe dataset I used can be found here.\nIt does have some bottle necks caused by the collection of the data, it happens that\nthe author scraped only wines with a score higher than 80.\nGraphs\nThere are various graphs(in the graphs/ directory) which I plotted\nto help with my understanding of the dataset.\nSome of them are a work in progress and will be\ncleaned.\nInstallation\nUse the package manager pip to\ninstall the requirements of the project.\npip install -r requirements.txt\nNote that you will also need to put the path of your\nfolder to your dataset in an env variable called WINE_DATASET.\nPossible applications\nBecause of the nature of the data, I think even an ideal\nmodel would have restricted use cases in a real\nlife application(maybe sell it to a winery in France?).\nHowever, this kind of prediction based on online ratings is\nquite notorious in some industries(like the music industry).\nLicense\nMIT\n'], 'url_profile': 'https://github.com/lagercat', 'info_list': ['Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'R', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models. Moreover, you also got a brief introduction to data ethics. Remember that throughout your data work it is essential to consider personal privacy and the potential impacts of the data you have access to.\nRegression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preperation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\nEthics\nAside from regression, you also took a look at data privacy and ethics. You probably had already heard some of these ideas, but may have not been familiar with GDPR or privacy advocacy groups like the Electornic Frontier Foundation. The digital age has brought a slew of political and philosophical questions to the arena, and there are always fascinating (and disturbing) conversations to be had. Be sure to keep these and other issues at the forefront of your thought process, and not simply be dazzled by the power of machine learning algorithms. Ask yourself questions like, ""What is the algorithm being used for?"" or ""What are the ramifications or impact of this analysis/program/algorithm?"".\nWhen Einstein released his theory of relativity, its impact had tremendous benefit in advancing the field of physics yet the subsequent development of the Manhattan project was arguably a great detriment of humanity. To a similar vain, be thoughtful of which planes of thought you are operating on, and always be sure to include an ethical and philosophical perspective of the potential ramifications of your work.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'R', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'R', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'R', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'R', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets. Key takeaways include:\n\nThe Pearson Correlation (range: -1 -> 1) is a standard way to describe the correlation between two variables\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'R', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'Waterloo', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Logistic Regression on dataset for Heart disease probablity\nFrom the UCI dataset using R programming language building a simple model to predict the probablity of heart disease\n'], 'url_profile': 'https://github.com/dhavalthakur', 'info_list': ['Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'R', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'R', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'R', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/devil33-rgb', 'info_list': ['Python', 'MIT license', 'Updated Jun 28, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', '1', 'R', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 16, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Na-ve-Bayes-and-Logistic-Regression-for-Text-Classification\n'], 'url_profile': 'https://github.com/ksiddhartha1998', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 16, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nIntroduce Statsmodels for multiple regression\nPresent alternatives for running regression in Scikit Learn\n\nStatsmodels for multiple linear regression\nThis lecture will be more of a code-along, where we will walk through a multiple linear regression model using both Statsmodels and Scikit-Learn.\nRemember that we introduced single linear regression before, which is known as ordinary least squares. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(""auto-mpg.csv"") \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[""acceleration""]\nlogdisp = np.log(data[""displacement""])\nloghorse = np.log(data[""horsepower""])\nlogweight= np.log(data[""weight""])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[""acc""]= scaled_acc\ndata_fin[""disp""]= scaled_disp\ndata_fin[""horse""] = scaled_horse\ndata_fin[""weight""] = scaled_weight\ncyl_dummies = pd.get_dummies(data[""cylinders""], prefix=""cyl"")\nyr_dummies = pd.get_dummies(data[""model year""], prefix=""yr"")\norig_dummies = pd.get_dummies(data[""origin""], prefix=""orig"")\nmpg = data[""mpg""]\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 26 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_3     392 non-null uint8\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_70     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_1    392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(21)\nmemory usage: 23.4 KB\n\nThis was the data we had until now. As we want to focus on model interpretation and still don\'t want to have a massive model for now, let\'s only inlude ""acc"", ""horse"" and the three ""orig"" categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis= 1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_1\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n1\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n1\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n1\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n1\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n1\n0\n0\n\n\n\n\nA linear model using Statsmodels\nNow, let\'s use the statsmodels.api to run our ols on all our data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$, where, with $n$ predictors, X is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = ""mpg ~ acceleration+weight+orig_1+orig_2+orig_3""\nmodel = ols(formula= formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable ""mpg"" out of your data frame, and use the a ""+"".join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nOr even easier, simply use the .OLS-method from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors dataframe so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    16.1041     0.509    31.636  0.000    15.103    17.105\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1     4.6566     0.363    12.839  0.000     3.944     5.370\n\n\norig_2     5.0690     0.454    11.176  0.000     4.177     5.961\n\n\norig_3     6.3785     0.430    14.829  0.000     5.533     7.224\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.           2.18e+15\n\n\nInterpretation\nJust like for single multiple regression, the coefficients for our model should be interpreted as ""how does Y change for each additional unit X""? Do note that the fact that we transformed X, interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed X, the actual relationship is ""how does Y change for each additional unit X\'"", where X\' is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit learn\nYou can also repeat this process using Scikit-Learn. The code to do this can be found below. The Scikit-learn is generally known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit learn compared to Statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of Scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -0.71140721, -0.29903267,  1.01043987])\n\nThe intercept of the model is stored in the .intercept_-attribute.\n# intercept\nlinreg.intercept_\n21.472164286075383\n\nWhy are the coefficients different in scikit learn vs Statsmodels?\nYou might have noticed that running our regression in Scikit-learn and Statsmodels returned (partially) different parameter estimates. Let\'s put them side to side:\n\n\n\n\nStatsmodels\nScikit-learn\n\n\n\n\nintercept\n16.1041\n21.4722\n\n\nacceleration\n5.0494\n5.0494\n\n\nweight\n-5.8764\n-5.8764\n\n\norig_1\n4.6566\n-0.7114\n\n\norig_2\n5.0690\n-0.2990\n\n\norig_3\n6.3785\n1.0104\n\n\n\nThese models return equivalent results!\nWe\'ll use an example to illustrate this. Remember that minmax-scaling was used on acceleration, and standardization on log(weight).\nLet\'s assume a particular observation with a value of 0.5 for both acceleration and weight after transformation, and let\'s assume that the origin of the car = orig_3. The predicted value for mpg for this particular value will then be equal to:\n\n16.1041 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 6.3785 = 22.0691 according to the Statsmodels\n21.4722 + 5.0494 * 0.5+ (-5.8764) * 0.5 + 1.0104 = 22.0691 according to the Scikit-learn model\n\nThe eventual result is the same. The extimates for the categorical variables are the same ""up to a constant"", the difference between the categorical variables, in this case 5.3681, is added in the intercept!\nYou can make sure to get the same result in both Statsmodels and Scikit-learn, by dropping out one of the orig_-levels. This way, you\'re essentially forcing the coefficient of this level to be equal to zero, and the intercepts and the other coefficients will be the same.\nThis is how you do it in Scikit-learn:\npredictors = predictors.drop(""orig_3"",axis=1)\nlinreg.fit(predictors, y)\nlinreg.coef_\narray([ 5.04941007, -5.87640551, -1.72184708, -1.30947254])\n\nlinreg.intercept_\n22.482604160455665\n\nAnd Statsmodels:\npred_sum = ""+"".join(predictors.columns)\nformula = outcome + ""~"" + pred_sum\nmodel = ols(formula= formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 08 Nov 2018   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:56:09   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    22.4826     0.789    28.504  0.000    20.932    24.033\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_1    -1.7218     0.653    -2.638  0.009    -3.005    -0.438\n\n\norig_2    -1.3095     0.688    -1.903  0.058    -2.662     0.043\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               9.59\n\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in both Scikit-Learn and Statsmodels. Before we discuss the model metrics in detail, let\'s go ahead and try out this model on the Boston Housing Data Set!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 16, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 16, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 16, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '182 contributions\n        in the last year', 'description': ['Used Regression to predict the amount of water outflowing a Dam using data regarding the change in water level in the dam. The code is in Matlab.\nNote: All the files are well documented and commented wherever I felt necessary\nRun the file named WateroutflowingDam.m which does the following:\n\nLoads the dataset DamWaterLevelData.mat and visualises it\nFeature mapping for the Polynomial Regression\nLearning curve for the Regression\nGives Error on the Test set\nUses the Cross-Validation set for selecting ""best"" lambda\n\n'], 'url_profile': 'https://github.com/Adarsh-K', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 16, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 16, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 16, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 16, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nCovariance and Correlation\nWe start the section by covering covariance and correlation, both of which relate to how likely two variables are to change together. For example, with houses, it wouldn\'t be too surprising if the number of rooms and the price of a house was correlated (in general, more rooms == more expensive).\nStatistical Learning Theory\nWe then explore statistical learning theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'MATLAB', 'Updated Jan 16, 2021', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'Atlanta', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Survival Analysis using Cox Proportional Hazards Model for predicting time to pay an invoice by a customer based on past customer history and invoice information.\n'], 'url_profile': 'https://github.com/anubhav0723', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'Colombo, Sri Lanka', 'stats_list': [], 'contributions': '291 contributions\n        in the last year', 'description': ['Predicting-CO2-emission-using-Multiple-regression\nExercise done in lab session in the online course Coursera Machine Learning with Python\nData set\nFuelConsumption.csv\nPredicting the Co2 emission vs several independent variables\nThe independent variables are ENGINESIZE, CYLINDERS, FUELCONSUMPTION_COMB\n'], 'url_profile': 'https://github.com/AkshaanB', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Python', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020']}"
"{'location': 'Toronto, ON', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Machine-Learning\n'], 'url_profile': 'https://github.com/SmzAmir', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Nov 22, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021', 'HTML', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Weshall13', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Nov 22, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021', 'HTML', 'Updated Jan 14, 2020']}","{'location': 'Lahore, Pakistan', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hamzahsaleem', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Nov 22, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021', 'HTML', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kasiakry', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Nov 22, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021', 'HTML', 'Updated Jan 14, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Wine-Prediction\nPredicting the goodness of wine using Ridge Regression using LOOCV written from scratch. The data was scraped from WineEnthusiast during the week of June 15th, 2017.\n'], 'url_profile': 'https://github.com/ishanmahajan34', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Nov 22, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021', 'HTML', 'Updated Jan 14, 2020']}","{'location': 'Washington DC', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['Titanic_Survivor_Problem\nThe following code is the solution to the Titanic Survivor Problem. The predictions have been made using Logistic Regression and Decision Tree.\nThe data set that has been used for this project is the Kaggle Titanic dataset which can be accessed from this location -  https://www.kaggle.com/c/titanic/data\n'], 'url_profile': 'https://github.com/a-dwivedi', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Nov 22, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021', 'HTML', 'Updated Jan 14, 2020']}","{'location': 'West Lafayette', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shubhsharma1994', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Nov 22, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021', 'HTML', 'Updated Jan 14, 2020']}","{'location': 'Mountain View, California', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['AB-testing\nFor this project, I ran an A/B test run by an e-commerce website. My goal is to to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision. In the final part, I verify that the result I achieved in the A/B test can also be achieved by logistic regression.\n'], 'url_profile': 'https://github.com/ghahremani', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Nov 22, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021', 'HTML', 'Updated Jan 14, 2020']}","{'location': 'San Francisco Bay Area', 'stats_list': [], 'contributions': '189 contributions\n        in the last year', 'description': ['For my Insight Data Science Fellowship, I created a tool that fantasy baseball players can use to help build rosters on a daily basis. The app is built on ~400,000 player at-bats from the 2015-2019 baseball seasons and uses a linear regression model to make at-bat outcome predictions. This app was built in about 3.5 weeks.\n'], 'url_profile': 'https://github.com/benslack19', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Nov 22, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021', 'HTML', 'Updated Jan 14, 2020']}","{'location': 'Salt Lake City', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Minute-Maid-Sales-probability-prediction\nUsing Logisitic regression and Support Vector Machines, the probability of different cutomers buying the ""Minute maid"" brand over a competitor brand is predicted.\n'], 'url_profile': 'https://github.com/RamyaDeepthiAvula', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Nov 22, 2020', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', '1', 'Jupyter Notebook', 'Updated Feb 26, 2021', 'HTML', 'Updated Jan 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['candev_demo\nWe use the data from Linkedin to extract important skills or variables about hiring criminal position in RCMP based on PCA and Logistic Regression.\n'], 'url_profile': 'https://github.com/Mujingrui', 'info_list': ['R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 30, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'MATLAB', 'Updated Jan 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['Core-Week-6-IP\nApplying polynomial and logistic regression as well as feature engineering, cross validation, etc. to predict whether a football match will result in a win, lose, or draw.\n'], 'url_profile': 'https://github.com/KarenNgugi', 'info_list': ['R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 30, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'MATLAB', 'Updated Jan 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Salzburg , Austria', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Machine-Learning-Loans-Predictor\nMachine Learning Loans Predictor ,Cleaning Data and Classifying using KNN , Decision Tree, Support Vector Machine and Logistic Regression , F1 and Jaccard accuracy for evaulating results\nIntroduction\nThe two most critical questions in the lending industry are: 1) How risky is the borrower? 2) Given the borrower’s risk,\nshould we lend him/her? The answer to the first question determines the interest rate the borrower would have.\nInterest rate measures among other things (such as time value of money) the riskness of the borrower, i.e.\nthe riskier the borrower, the higher the interest rate. With interest rate in mind,\nwe can then determine if the borrower is eligible for the loan.\nData Section\nThis dataset is about past loans. The Loan_train.csv data set includes details of 346 customers whose loan are\nalready paid off or defaulted. It includes following fields:\n\n\n\nField\nDescription\n\n\n\n\nLoan_status\nWhether a loan is paid off on in collection\n\n\nPrincipal\nBasic principal loan amount\n\n\nTerms\nOrigination terms which can be weekly (7 days), biweekly, and monthly payoff schedule\n\n\nEffective_date\nWhen the loan got originated and took effects\n\n\nDue_date\nSince it’s one-time payoff schedule, each loan has one single due date\n\n\nAge\nAge of applicant\n\n\nEducation\nEducation of applicant\n\n\nGender\nThe gender of applicant\n\n\n\nData is then transformed into a dataframe using pandas , visualized and pre-processed using matplotlib , seaborn and one hot encoding technique to convert categorical varables to binary variables and append them to the feature Data Frame¶.\nMethodology Section\nThe Dataframe were splited to generate a training set and a test set to increase the accuracy , Scikitlearn library were used to classify the data using this techniques :\n\nK Nearest Neighbor(KNN)\nDecision Tree\nSupport Vector Machine\nLogistic Regression\n\nThe accuracy were then evaluated using the test set with accuracy_score, f1_score ,jaccard_similarity_score to find the best algorithm to predicit the loans status.\n'], 'url_profile': 'https://github.com/hazem-kamel', 'info_list': ['R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 30, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'MATLAB', 'Updated Jan 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Mooresville, North Carolina', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/varunt23', 'info_list': ['R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 30, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'MATLAB', 'Updated Jan 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Business insight report in R\nFinding business insight from auto_mpg.xls data by using decision tree and regression models, and report it by using markdown in R\n'], 'url_profile': 'https://github.com/piyanick', 'info_list': ['R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 30, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'MATLAB', 'Updated Jan 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'New York, USA', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TanviPareek', 'info_list': ['R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 30, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'MATLAB', 'Updated Jan 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/k-sapkota', 'info_list': ['R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 30, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'MATLAB', 'Updated Jan 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['Machine-Learning-Algorithms\nML problems solved using the given dataset.\n'], 'url_profile': 'https://github.com/Yathish27', 'info_list': ['R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 30, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'MATLAB', 'Updated Jan 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/leopold-git', 'info_list': ['R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 30, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'MATLAB', 'Updated Jan 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['dataScienceProjects\nLinear Regression/Cross Validation and Bias-Variance/ Logistic Regression/K Nearest Neighbors/ Decision Tree and Random Forests/ Support Vector Machines/ K Means and Clustering/ Principal Component Analysis/ Recommender System/ NLP/ Big Data and Spark with Python/ Neural Nets and Deep Learning\n'], 'url_profile': 'https://github.com/chohonest', 'info_list': ['R', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 30, 2020', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Updated Apr 8, 2020', '1', 'Python', 'Updated Oct 31, 2020', 'MATLAB', 'Updated Jan 13, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}"
"{'location': 'Boston, MA, USA', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Stock-Prediction-using-news-headlines\nAdjoint close value of stock is predicted using classification algorithms such as SVM, Naive Bayes, Logistic Regression and tuned model using Lasso Regression. Classification is done based on the weightage of each news item using prebuilt NLP algorithm where percentage of positivity, negativity, objection, neutrality of news content is extracted\n'], 'url_profile': 'https://github.com/anishnitin', 'info_list': ['Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', '3', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 14, 2020', '1', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', '2', 'C', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 19, 2020']}","{'location': 'Denver, CO', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/egeneroli', 'info_list': ['Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', '3', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 14, 2020', '1', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', '2', 'C', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 19, 2020']}","{'location': 'Austin, Texas', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': [""Anatomy of a Social CEO\nEngagement and Stock Price Analysis of CEOs on Twitter\n\nIntroduction\nTwitter is one of the largest microblogging social network of this decade and CEOs are increasingly flocking to this site. Who can blame them, given that this is where all the action happens - the latest news of the companies, marketing and leadership. Whether you are the CEO of a Fortune 100 company or a small business owner, your Tweets matter and it is an effective way to strengthen ties with the customers, build your brand image, network with influencers and crowdsource to learn and improve on products and services.\nIn this project, we analyze the tweets of various CEOs and try to ascertain the characteristics and tweeting styles that make them influential. We also look into customer engagement based on these tweets, and what topics help the company build their brand presence. We tie it all together by looking at whether these tweets offer any support for increased success in the company’s economic value.\nTechnology\nThe project was implemented in Python 3.7.3.\nPackages\n\ntweepy\nspacy\ngensim\nnltk\ntextblob\n\nApproach\n\nScraped Twitter accounts of 31 CEOs from different industries using the API. Also scraped daily stock data and financial statistics from Yahoo Finance for the companies whose CEOs we analyzed.\nSentiment Analysis on the tweets for each CEO\nTopic Modeling using Latent Dirichlet Allocation for each CEO to determine their tweeting styles and a breakdown of their common topics\nStock Price Analysis (Regression) using a combination of CEO attributes (age, compensation etc.), Tweet attributes (tweeting style/topics, sentiment scores, number of retweets and likes etc.) and Company attributes (company statistics, daily returns, volume of stock traded etc.)\nEngagement Analysis to determine what kind of topics garner more engagement from their customers using Logistic Regression\n\nGeneral Summary\n\nInsights and Recommendations\nSentiment Analysis and Topic Modeling\n\nThe sentiments revealed that most of the CEOs' tweets are positive or neutral. The tweets that were classified as negative were general condolences to global tragedies.\nA closer look revealed that the sentiment analyzer was unable to distinguish between genuine and sarcastic tweets. For example, John Legere (T-Mobile) has a good number of sarcastic tweets directed towards his competitors but they were classified as positive.\nTopic Modeling revealed different tweeting styles of CEOs and how they maintain their online persona in the eyes of their customers and follwers.\nFor example, Elon Musk has a very casual tweeting style and uses Twitter mainly as a marketing tool where he talks about his various products.  On the other hand, Tim Cook makes a lot of statements about current events and clarifies what Apple is doing to help out. We also saw that he tries to connects with his followers by retweeting their pictures and captions it with the very popular hashtag #shotoniphone\nSome of the CEOs focus on tweeting about their business, whereas some CEOs don't use Twitter as an advertising platform. Instead, they talk about their personal interests and their take on social and global issues. These CEOs are using social media to establish an emotional connection with their followers and who by association can also connect to their brand.\n\nStock Price and Engagement Analysis\n\nFrom our analysis, we saw that engagement and influence has an economic impact of the company. A single tweet has the potential to affect the stock market prices. The influential CEOs can leverage Twitter to turn the negative sentiments of the crowd to the positive ones and affect the market (Examples are given in the presentation).\nEngagement analysis showed that customers repond well when CEOs engage in conversations, apart from pushing their products or services. When CEOs use the platform to promote philanthropic efforts, impart life advice or share their personal interests, they tend to give the impression of being authentic and relatable, increasing their engagement.\n\nFuture Scope\n\nUsing Linear Regression is not ideal in this scenario but it was a good place to start to discern if there was relation between tweets sentiments and the stock price.\nExtend this by building more complex predictive models using Time Series, ARIMA etc and incorporate social network analysis of the twitter users for influence and reputation.\n\n""], 'url_profile': 'https://github.com/sahanasub', 'info_list': ['Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', '3', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 14, 2020', '1', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', '2', 'C', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['esn_eval\nThis repository works for simple tests about ESN (echo state network), including the accuracy of time-series prediction and chaotic system prediction.\nrelated work\npyESN: https://github.com/cknd/pyESN\nMackeyGlass: https://github.com/Dauriel/MackeyGlass\n'], 'url_profile': 'https://github.com/sunyuqi148', 'info_list': ['Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', '3', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 14, 2020', '1', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', '2', 'C', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/olumide2021', 'info_list': ['Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', '3', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 14, 2020', '1', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', '2', 'C', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 19, 2020']}","{'location': 'Liverpooooool, UK', 'stats_list': [], 'contributions': '2,639 contributions\n        in the last year', 'description': [""boston-housing-dataset\nThe Boston Housing Dataset, compiled by in Harrison and Rubinfeld in 1978.\n🚀 Getting Started\nUsing npm:\nnpm i -s boston-housing-dataset\nUsing yarn:\nyarn add boston-housing-dataset\n✍️ Usage\nimport BostonHousing from 'boston-housing-dataset';\n\nconsole.log(BostonHousing[Math.floor(Math.random() * BostonHousing.length)]);\n// { crim: 0.12083, zn: 0, indus: 2.89, chas: 0, nox: 0.445, rm: 8.069, age: 76, dis: 3.4952, rad: 2, tax: 276, ptratio: 18, b: 396.9, lstat: 4.21, medv: 38.7 }\n✌️ License\nMIT\n""], 'url_profile': 'https://github.com/cawfree', 'info_list': ['Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', '3', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 14, 2020', '1', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', '2', 'C', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 19, 2020']}","{'location': 'Earth, Antarctica', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['YarSU_ML_Course\nCompleted machine learning course in YarSU\n'], 'url_profile': 'https://github.com/leonel11', 'info_list': ['Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', '3', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 14, 2020', '1', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', '2', 'C', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 19, 2020']}","{'location': 'China-Chengdu', 'stats_list': [], 'contributions': '1,054 contributions\n        in the last year', 'description': ['Angin-Embedded\n\n该库有以下几个文件夹:\n\n1_demo:是最初的demo工程其目的是测试CubeMX能否针对L431正常生成工程, 代码仅有闪灯(PC13)功能\n2_regressor_test:这是第一个跑通的神经网络工程,  里面固化了一个单输入单输出的线性回归神经网络\n3_regressor_original:这是从CubeMX直接生成的工程, 里面配置了串口2和神经网络,用于和下面的对比\n4_regressor_original_comparison:与上一个工程进行对比, 之缺少了神经网络的功能\n5_migration_test 该工程是把第3项中的神经网络部分移植到第4项中的一个测试\n6_classifier_test:这是一个运行MNIST数据集的神经网络工程, 并已经进行了测试.\n7_LSTM_test:实现了LSTM\nh5:里面存放着Keras生成的.h5文件, 供CubeMX生成固化神经网络\nimg:存放README依赖的图片等\nvalidate_nn_6:YouTube的ST频道有个教程, 里面的工程就是这个\n\n运行环境:\n\nKeil 5.27\nCubeMX 5.50\n编译器:6.13.1(强烈建议使用6及以上版本的编译器,编译爽翻天)\nRCC: HSE, PLL×20, PLLCLK, HCLK==80MHz\nLite OS 0.3.0.0\n生成工程固件版本:STM32Cube FW_L4 V1.15.0\n\n神经网络最小功能测试:\n源码见2_regressor_test文件夹.\n开发板:小熊派L431RCT6 256KB 64KB 80MHz\n该工程实现了一个简单的神经网络样例, 连接串口二后输出神经网络的计算结果:\n(输入为随机数)\n\n该神经网络具有一个神经元, 单输入, 单输出.\n该神经元是由Keras训练得到的, 其参数为:\nweights: [[0.36967745]] biases: [2.001801]\n\n经验证input * weights + biases == output, 其计算结果准确.\nMNIST测试\n源码见6_classifier_test文件夹\n开发板:小熊派L431RCT6 256KB 64KB 80MHz\n该工程实现了一个简单的分类器,输入28*28像素的单个阿拉伯数字图片, 连接串口二后输出神经网络的识别概率:\n输入图片:\n\n该神经网络具有三层结构:\n\n第一层到第二层为784输入(为图片像素展成的一维数组), 32输出,\n第二层到第三层32输入, 10输出(为每个数字的概率,\n\n训练该网络的详细代码见keras_learn的readme,\n该库还写了一个简单的代码生成器, 用于生成单片机所需的输入参数.\n对于上面的图片, 单片机的输出的结果为:\n\n\n\n数字\n概率\n\n\n\n\n0\n0.011202\n\n\n1\n0.000288\n\n\n2\n0.003477\n\n\n3\n0.004369\n\n\n4\n0.000385\n\n\n5\n0.076738\n\n\n6\n0.897352\n\n\n7\n0.000079\n\n\n8\n0.005681\n\n\n9\n0.000429\n\n\n\n可以看出, 单片机可以很好的完成手写数字识别的功能\n其生成的网络占用空间为:\n\u200b\tFlash: 101.8 KByte\n\u200b\tRAM: 168 Byte\nLSTM测试:\n输入20个时间点, 每个时间点有单个参数, 即sin(),  预测输出20个时间点, 即cos()\n输入:\n0.0000, 0.0318, 0.0637, 0.0955, 0.1273, 0.1592, 0.1910, 0.2228, 0.2546, 0.2865, 0.3183, 0.3501, 0.3820, 0.4138, 0.4456, 0.4775, 0.5093, 0.5411, 0.5730, 0.6048\n输出:\n0.021437  0.522149  1.087820  0.992776  0.989927  1.002372  0.995649  0.973917  0.962171  0.957946  0.952693  0.945284  0.936054  0.924734  0.911672  0.897318  0.881727  0.865241  0.848155  0.830363\n有神经网络的工程修改了哪些文件?\n\n\\Inc新增了文件:\n\napp_x-cube-ai.h\nconstants_ai.h\n[network_name].h\n[network_name]_data.h\nRTE_Components.h//删除该文件无影响\n\n\n\\Src文件增加了app_x-cube-ai.c, [network_name].c,[network_name]_data.c\n\\Src中的stm32l4xx_hal_msp.c初始化了CRC\nmain()中调用了MX_CRC_Init()和MX_X_CUBE_AI_Init(), 并include了""app_x-cube-ai.h""\n\\Inc\\stm32l4xx_hal_conf.h 取消注释了#define HAL_CRC_MODULE_ENABLED模块\n新增了Middlewares文件夹及相关文件\n\n将神经网络迁移至无神经网络的keil工程:\n\n[神经网络]工程为CubeMX生成的带神经网络的工程\n\n\n[无神经网络]工程为CubeMX生成的不带神经网络的工程\n\n步骤:\n\n\n将[神经网络]工程的\\Inc文件夹下的:\n\napp_x-cube-ai.h\nconstants_ai.h\n[network_name].h\n[network_name]_data.h\n\n文件复制到[无神经网络]工程相同路径\n\n\n将[神经网络]工程的\\Src文件夹下的:\n\napp_x-cube-ai.c\n[network_name].c\n[network_name]_data.c\n\n文件复制到[无神经网络]工程相同路径\n\n\n新建文件夹\\Middlewares\\AI(用于放置AI接口), 将[神经网络]工程的Middlewares\\ST\\AI路径下的\n\nInc文件夹\nLib文件夹\n\n复制到[无神经网络]工程的\\Middlewares\\AI路径下\n\n\n检查[神经网络]工程下的\\Drivers\\STM32L4xx_HAL_Driver\\Src文件夹及\\Drivers\\STM32L4xx_HAL_Driver\\Src文件夹, 这两个文件夹应包含启用CRC必须的文件:\n\n\nstm32l4xx_hal_crc.h\n\n\nstm32l4xx_hal_crc_ex.h\n\n\nstm32l4xx_hal_crc.c\n\n\nstm32l4xx_hal_crc_ex.c\n\n\n\n\n\n以上文件移动完成, 接下来是Keil内部的工程的配置:\n\n\n\nAdd Existing Files to Group Application/User, 添加\\Src下的:\n\napp_x-cube-ai.c\n[network_name].c\n[network_name]_data.c\n\n注意:这里不要图省事全选添加, 否则会把不应该加进去的system_stm32l4xx.c文件也加到这个Group中(这个文件是在Drivers/CMSIS这个Group中的\n\n\nAdd Existing Files to Group Drivers/STM32xxx_HAL_Driver, 添加\\Drivers\\STM32L4xx_HAL_Driver\\Src文件夹下的\n\nstm32l4xx_hal_crc.c\nstm32l4xx_hal_crc_ex.c\n\n文件(在步骤4检查过)\n\n\nAdd Group, 并命名为lib(这个Group用于放置CubeMX编译好的神经网络静态链接库), Add Existing Files to Group lib, 添加\\Middlewares\\AI\\Lib文件夹下的NetworkRuntime500_CM4_Keil.lib\n\n\nOptions for Target \'[无神经网络]工程\',->C/C++选项卡,->Include Paths->添加../Middlewares/AI/Inc(在第3步新建的)\n\n\n\n以上Keil配置完成, 下面修改.c文件\n\n\n\n在stm32l4xx_hal_conf.h文件中取消#define HAL_CRC_MODULE_ENABLED  模块的注释\n\n\n在stm32xxxx_hal_msp.c文件中新增CRC的MSP初始化代码:\n/**\n* @brief CRC MSP Initialization\n* This function configures the hardware resources used in this example\n* @param hcrc: CRC handle pointer\n* @retval None\n*/\nvoid HAL_CRC_MspInit(CRC_HandleTypeDef* hcrc)\n{\n  if(hcrc->Instance==CRC)\n  {\n  /* USER CODE BEGIN CRC_MspInit 0 */\n\n  /* USER CODE END CRC_MspInit 0 */\n    /* Peripheral clock enable */\n    __HAL_RCC_CRC_CLK_ENABLE();\n  /* USER CODE BEGIN CRC_MspInit 1 */\n\n  /* USER CODE END CRC_MspInit 1 */\n  }\n\n}\n\n/**\n* @brief CRC MSP De-Initialization\n* This function freeze the hardware resources used in this example\n* @param hcrc: CRC handle pointer\n* @retval None\n*/\nvoid HAL_CRC_MspDeInit(CRC_HandleTypeDef* hcrc)\n{\n  if(hcrc->Instance==CRC)\n  {\n  /* USER CODE BEGIN CRC_MspDeInit 0 */\n\n  /* USER CODE END CRC_MspDeInit 0 */\n    /* Peripheral clock disable */\n    __HAL_RCC_CRC_CLK_DISABLE();\n  /* USER CODE BEGIN CRC_MspDeInit 1 */\n\n  /* USER CODE END CRC_MspDeInit 1 */\n  }\n\n}\n\n\n在main.c加入#include ""app_x-cube-ai.h""头文件\n\n\n在main.c中新增CRC的初始化代码:\n/* Private variables ---------------------------------------------------------*/\nCRC_HandleTypeDef hcrc;\nstatic void MX_CRC_Init(void);//记得声明\nmain(){...}\nstatic void MX_CRC_Init(void)\n{\n\n  /* USER CODE BEGIN CRC_Init 0 */\n\n  /* USER CODE END CRC_Init 0 */\n\n  /* USER CODE BEGIN CRC_Init 1 */\n\n  /* USER CODE END CRC_Init 1 */\n  hcrc.Instance = CRC;\n  hcrc.Init.DefaultPolynomialUse = DEFAULT_POLYNOMIAL_ENABLE;\n  hcrc.Init.DefaultInitValueUse = DEFAULT_INIT_VALUE_ENABLE;\n  hcrc.Init.InputDataInversionMode = CRC_INPUTDATA_INVERSION_NONE;\n  hcrc.Init.OutputDataInversionMode = CRC_OUTPUTDATA_INVERSION_DISABLE;\n  hcrc.InputDataFormat = CRC_INPUTDATA_FORMAT_BYTES;\n  if (HAL_CRC_Init(&hcrc) != HAL_OK)\n  {\n    Error_Handler();\n  }\n  /* USER CODE BEGIN CRC_Init 2 */\n\n  /* USER CODE END CRC_Init 2 */\n\n}\n\n\n在main中初始化CRC, 初始化神经网络, 在while(1)中执行神经网络:\nint main(void)\n{\n  HAL_Init();\n  SystemClock_Config();\n  MX_CRC_Init();//初始化CRC\n  MX_X_CUBE_AI_Init();//初始化神经网络\n  while (1)\n  {\n    MX_X_CUBE_AI_Process();//开始执行神经网络\n  }\n}\n\n\n迁移完成.\n\n\n将神经网络迁移至无神经网络的makefile工程\n已经实现, 过程有空再写\n将一个新的神经网络放到原来的工程中:\n只用执行上面的第1, 2步, 所以所有的.c和.h均需要移植.\n备忘录\n\n\nHAL_CRC_MspInit()函数的调用链:6_\nmain()==>MX_CRC_Init()==>HAL_CRC_Init()==>HAL_CRC_MspInit()[位于stm32xxx_HAL_msp.c]\n\n\nHAL_MspInit()只初始化RCC,其余外设的初始化都放在了各自的Msp函数中\n\n\n\n'], 'url_profile': 'https://github.com/wangzilinn', 'info_list': ['Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', '3', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 14, 2020', '1', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', '2', 'C', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/apriori-yarik', 'info_list': ['Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', '3', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 14, 2020', '1', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', '2', 'C', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 19, 2020']}","{'location': 'Dallas, Texas', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['AirBnB-Dataset-Analysis-using-R\nA showcase of data analytics techniques such as Regression and Clustering on AirBnB dataset to predict the best price possible and suggest some recommendations based on the predicted model\nThe repository consists of AirBnB dataset with one day transactions in Boston area\nThe dataset available is a preprocessed version with all the imputations done as per the requirement of the model\nThe code consists of a regression model with few evaluation techniques implemented to predict the best model\nThe ppt guides you through all the discussions and recommendations in the project\n'], 'url_profile': 'https://github.com/kaulakshay10', 'info_list': ['Jupyter Notebook', 'Updated Jan 14, 2020', 'R', 'Updated Jan 15, 2020', '3', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 14, 2020', '1', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 16, 2020', '2', 'C', 'Updated Jun 6, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Jan 19, 2020']}"
"{'location': 'Washington, D.C.', 'stats_list': [], 'contributions': '621 contributions\n        in the last year', 'description': ['Spoonacular Recipe Ad Targeting\nThis README.md lists project members, goals, responsibilities, and a summary of the files in the repository.\nProject File Summary\n\nREADME.md - a summary of all contents in this repository.\n/Data - all data called from the Spoonacular API saved out as .csv files.\n/Linear_Regression_Trial - Linear regression working code (not used for the final business recommendation).\n/Logistic_Regression_Final - Logistic regression code.\n/Project_Prompt - the prompt for this project.\n/Python_Files - .py files loaded / imported in the Jupyter Notebooks.\n\nProject Members\n\nAlex Cheng\nNimu Sidhu\n\nProject Scenario\nTotal U.S. spending on food advertising was $151 billion dollars in 2018. This was a 4.1% increase from 2017. According to the New York Times, a person living in a city today sees over 5,000 ads per day, so how can we target successful ad placement in a world where food related ads are everywhere? As a business case study to address this, we work as an ads strategy consultant to businesses selling products and services related to the food industry, such as: Williams-Sonoma, KitchenAid, Blue Apron, and Hello Fresh. They are targeting Spoonacular.com to run ads. Spoonacular is a popular recipe aggregating website and app for users to look up recipes by ingredient, by nutritional content, by price, and more. The Spoonacular API includes over 360,000 recipes as well as an open source recipe database. Our clients only want to spend money to run ads on webpages that they know people will visit a lot. In this case, without knowing how many people visited each recipe page on Spoonacular, we will use “Likes” as our proxy metric for web-traffic.\nProject Goals\n\nWe want to predict if a recipe will be  ""liked"" a lot, to understand where to run ads.\nWe want to build a classification model that will predict whether a recipe will be ""highly liked"" or not. This way, we will be able to determine where to run ads, when a new recipe is posted on Spoonacular.\n\nMethodology\n\nGenerate business application.\nFind and select a source of data to draw from and analyze (Spoonacular).\nIdentify predictor and target variables to focus on.\nGenerate criteria and call data sorted by popularity from the API.\nMerge all API calls into one raw dataframe, and export to CSV.\nClean data as needed.\nPerform Exploratory Data Analysis (EDA) to investigate the data.\nDetermine criteria for classification.\nGenerate logistic regression model based on criteria to fit the training data.\nFit additional models using Ridge and Lasso regularization to compare with ""vanilla"" logistic regression model.\nPerform hyperparameter tuning using RandomSearchCV / GridSearchCV.\nTest the optimized training model on the test data and evaluate score.\nPlot Receiver Operating Characteristic (ROC) curve and calculate AUC to evaluate model performance on test data.\nCreate a presentation to translate findings into actionable insights for the business application.\n\nProject Responsibilities\n\nAll project responsibilities were shared equally between Alex Cheng and Nimu Sidhu.\n\n'], 'url_profile': 'https://github.com/alexwcheng', 'info_list': ['Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': [""Project 2: Ames, IA Housing Data\nProblem Statement\nI am a acting as a consultant that just recently gained a client in Ames, Iowa. They have heard of my work and have asked me to help them predict housing prices in the area so that they can have a leg up on the market!\n\nExecutive Summary\nThe goal of this project is to take data and information collected from the Ames Assessor’s Office about residential units sold from 2006 - 2010 and build a linear regression model to predict unknown sales prices.\nThis data set includes 81 characteristics & 2051 observations (or rows of home sales). Within these data, the observations can be categorized as either: Continuous, Ordinal, or Categorical. From these charateristics it's important to select columns of interest and transform/manipulate the data so it can  be useful to create the productive model.\nI used a heatmap & correlation coeficients to help determine the which variables ultimately makes a impact on predicting sales prices. I used techniques like feature engineering and log transformations to create the model variables.\nThe goal is for our model to predict the price of a home with specific characteristics with little error. In order to know the performance of our model, it was evaluated against 2 different metrics. R-squared & RMSE (Root Mean Squared Error). R-squared is on a scale of 0 to 1, with 1 being the best score. RMSE measures how for our predictions are from the correct value, on average. The closer we get to 0 (No difference in value) the better.\n\nConclusions and Recommendations\nI generated several different models and measured them against the 2 metrics. Two examples of results I yielded from my models is as follows:\nModel 1: R-squared - 0.81; RMSE - $34,072. This model did not generalize well to unseen data.\nModel 2: R-squared - 0.91; RMSE - $21,166. Generalizes well to unseen data.\nI believe two next steps from these results include. More feature engineering and tuning of my model. Also, collect more Data! The data set I have based this model off of, is slightly outdated. Obtaining up to date data can potentially improve the model greatly.\nAdditional data collection\nThe data collected was from 2006 to 2010. We are now in the year 2019, so that's 9 years of additional data that can be collected. If this data is collected, our models can be modified and improved to include market changes and trends that have occurred during this time.\n\n""], 'url_profile': 'https://github.com/mayjordata', 'info_list': ['Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'Bangalore, Karnataka, India', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['predict_follower_count_vk\nA regression model to run and analyse a predictor model for follower counts on VK, a Russian social media app. Made as the final project for the infamous Stats 101 at Duke Kunshan University.\n'], 'url_profile': 'https://github.com/Aryan-Poonacha', 'info_list': ['Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['BAN673-Time-Series-Analytics-Project\nUtilized multiple time series forecasting methods (auto-regression/Holt-Winter’s/ARIMA Models) to forecast New Zealand tourist numbers and their broad purpose of visit 14 months into the future\nProject Summary:\nNew Zealand government’s problem statement to analytically budget their 2013 marketing spend of 100 million $ required their understanding of the trends and patterns both in the visitor numbers as well as their purpose of visit. This will help them obtain the forecasts and\nplan the monthly advertising budget and advertising content focus (Business, Education, Vacation etc.). There is data available monthly from Jan 2000 to Oct 2012.\nThey contracted ELSSA analytics to analyse the data and predict up to 14 months in future. Our objective was to analyse various time series methods and select the best method to forecast visitor arrivals, the number of visitors and their broad purpose of visit with the given\ndata. From this we could suggest which category of NZ Tourism advertisement could be allocated how much budget to run for each month of the year 2013 with what percentage of their marketing budget.\nThe broad approach here was to employ several time series forecasting methods on training and validation partitions and compare the accuracy error measures for their models. The best models were then extended to full historical data and checked for compared for accuracy with respect to full historical data. The best performing method was used to forecast the future data and allocate marketing budgets.\nThe exercise revealed that various time series, though related to visitor inflow exhibited varied trends and seasonal patterns when visualized. They also required different forecasting methods to efficiently make future predictions. The Arrival and Corporate time series\ndemanded the use of a different forecasting approach than those of Education and Vacation time series and concluded with prediction accuracies of varied metrics. Upon successful forecasting, interesting trends and contrasting seasonality were revealed for different visitor categories which helped optimize marketing spend and advertising budget allocation.\nIntroduction:\nNew Zealand government recently released data for NZ Air passengers from incoming and outgoing passengers. It also released other monthly data for Visitor Purposes. The Tourism department has decided to understand the trends and patterns in the visitor travel information in relation to their purpose. ELSSA analytics has been contracted to analyse the data and bring out trends, seasonality patterns in these using historical data and forecast for the period upto Dec 2013. At ELSSA Analytics, they specialize in time series forecasting using various methods like Smoothing, Regression, Auto-correlation and ARIMA models.\nELSSA relied on an Independent research that suggests that most people plan and book their vacations 2 months in advance, so that these advertisements will be suggested for the months 2 months prior to forecasted month. Example: January 2013 Advertisements will be run keeping in mind the March 2013 predictions. If predicted March visitor inflow is 20% of predicted annual inflow, then 20% marketing budget will be suggested to be allocated in January advertising campaigns.\nUsing multiple time series analytical techniques, we will suggest which category of NZ Tourism advertisement could be allocated how much budget to run for each month of the year 2013 and with how much allocation for different genres of advertisements whether Business, Education or Vacation (depending on predicted visitor inflow for that month as a percentage of the year).\nConclusion:\nIt is deducible that the months of December and January have maximum visitor inflow, possibly because it is summer time in New Zealand when it is winter time in other countries. However, this is mostly owing to the fact that tourism and vacationing visitors make up the bulk of the overall inflow.\nIt is noteworthy that proportionately, educational and business-oriented traffic peaks in the middle of the year when vacationers dip relatively. This is indicated by the June 2013 traffic forecast where Corporate and Educational visitors constitute 22.55% of the traffic and Vacation visitors are at their year minimum of 77.46%. This is also one of the least busy months in the year for New Zealand and is exact opposite of countries like USA, in the northern hemisphere.\n'], 'url_profile': 'https://github.com/snehaapotdar', 'info_list': ['Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['USA-housing-price-prediction-\nIn this case study we analyse house pricing data of USA and applied multi linear regression model. In this usecase we can understand that how backward elimination help to eliminate statistically less important variable.\n'], 'url_profile': 'https://github.com/Akhileshlekurwale', 'info_list': ['Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'Jalandhar, Punjab', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/khsprashanth', 'info_list': ['Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['PredictIng-Titanic-Survival-\nUsing cross validation techique on Logistic Regression , Random Forest, SVM,LDA to compara the performance of the models. Also used advance packages  like Numpy, Pandas, Matplotlib to predicting Titanic Survival\n'], 'url_profile': 'https://github.com/kingdukedav', 'info_list': ['Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'Dhaka', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Machine-Learning-Course\nDifferent Machine Learning Algorithms such as Regression, Classification and Clustering have been implemented here. Data preprocessing and other important steps also have been explained. All works are done using Spyder.\n'], 'url_profile': 'https://github.com/mdanisurrahmanrony', 'info_list': ['Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'San Antonio, Texas', 'stats_list': [], 'contributions': '190 contributions\n        in the last year', 'description': ['LOL-Champion-Choice-Research\nThis is a project I did back in Summer 2018. I downloaded datasets from Riot official websites. By using linear regression model on the dataset we try to analyze champion choices and its effect on winning.\nThis pretty much the first large scope project I did outside course works. It teaches me basic data analysis and data pre-processing, regression and linear programming techniques plus some basic knowledge of machine learning, including test training datasets, overfitting vs underfitting, etc.\nlinearReg.py is used to build the model. ""tester.py"" is used for testing prediction accuracy of the model. The model generated over 75% on predicting winning team based on champion choices and other in game statistics using a regression model.\n'], 'url_profile': 'https://github.com/sliu-trinity', 'info_list': ['Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'R', 'Updated Jan 13, 2020']}","{'location': 'San Francisco Bay Area', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Framingham\nPredicted whether a patient experiences CHD within 10 years of their first examination (accuracy 85%), identified risk factors and determined a charge co-payment for an insurance company using logistic regression. Identified risk factors.\n'], 'url_profile': 'https://github.com/SophiePlt', 'info_list': ['Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 13, 2020', 'R', 'Updated Jan 16, 2020', 'R', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 13, 2020', 'Apache-2.0 license', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 18, 2020', 'R', 'Updated Jan 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Sales-Performance-By-Region-And-Month\nWe will use Tableau to get a report about sales performance by region and month.  Also, we want to use linear regression and random forest to predict future sales  performance\n'], 'url_profile': 'https://github.com/SunnyJia1010', 'info_list': ['Jupyter Notebook', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 16, 2020', '4', 'Jupyter Notebook', 'Updated Feb 25, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020']}","{'location': 'Dhaka', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Machine-Learning-using-Jupyter-Notebook\nDifferent Machine Learning Algorithms such as Regression, Classification and Clustering have been implemented here. Data preprocessing, data visualization and other important steps also have been explained. All works are done using Jupyter Notebook.\n'], 'url_profile': 'https://github.com/mdanisurrahmanrony', 'info_list': ['Jupyter Notebook', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 16, 2020', '4', 'Jupyter Notebook', 'Updated Feb 25, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ShwethaGowriNagaraj', 'info_list': ['Jupyter Notebook', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 16, 2020', '4', 'Jupyter Notebook', 'Updated Feb 25, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': [""The choice of machine learning algorithm solely depends on the type of data. If you are given a data set which exhibits linearity, then linear regression would be the best algorithm to use. If you given to work on images, audios, then neural network would help you to build a robust model. If the data comprises of non linear interactions, then a boosting or bagging algorithm should be the choice. If the business requirement is to build a model which can be deployed, then we’ll use regression or a decision tree model (easy to interpret and explain) instead of black box algorithms like SVM, GBM etc. In short, there is no one master algorithm for all situations. We must be scrupulous enough to understand which algorithm to use.\nFeature Importance is the process of finding the most important feature to a target. Through PCA, the feature that contains the most information can be found, but feature importance concerns a feature’s impact on the target. A change in an ‘important’ feature will have a large effect on the y-variable, whereas a change in an ‘unimportant’ feature will have little to no effect on the y-variable.\nNNs\nhttps://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73\nGradient Descent(GD)\nDerivative of the sum is the sum of the derivatives. Therefore, in order to compute the derivatives of the loss, we need to go through each example of our dataset. It would be very inefficient to do it every iteration of the Gradient Descent, because each iteration of the algorithm only improves our loss by some small step.\nMini-batch Gradient\nWe approximate the derivative on some small mini-batch of the dataset and use that derivative to update the weights. Mini-batch isn’t guaranteed to take steps in optimal direction. In fact, it usually won’t.\nStochastic Gradient Descent(SGD)\nExtreme version of mini-batch gradient descent with batch size equals to 1.\nFix/change the learning rates\nhttps://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b\nAn overview of gradient descent optimization algorithms\nhttps://ruder.io/optimizing-gradient-descent/\nRMSprop\n\nAdaptation of rprop algorithm for mini-batch learning.\nSimilar to Adagrad - it radically diminishes learning rates.\nhttps://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a\n\nBoosting algorithms\nThey play a crucial role in dealing with bias variance trade-off. The selection of sample is made more intelligently. We subsequently give more and more weight to hard to classify observations.\nhttps://www.analyticsvidhya.com/blog/2015/09/complete-guide-boosting-methods/\nBagging algorithms\nOnly controls for high variance in a model. It is an approach where you take random samples of data, build learning algorithms and take simple means to find bagging probabilities.\nMachine Learning mostly have to deal with two Trade-offs\n\nBias-Variance Trade-offs\nPrecision-Recall Trade-offs\n\nBias\nLeads to a high error on training and test data. Low Bias, when predicted data points are close to the target. Also, the model suggests less assumptions about the form of the target function. High-Bias, when predicted data points are far from the target. Also, the model suggests more assumptions about the form of the target function.\nLow-bias ML algos:\n\nDecision Trees\nk-Nearest Neighbors\nSupport Vector Machines\n\nHigh-bias ML algos:\n\nLinear Regression\nLinear Discriminant Analysis\nLogistic Regression.\n\nVariance\nPerforms well on training data but has high error rates on test data. Low Variance: Data points are close to each as a result close to function. Also, the model Suggests small changes to the estimate of the target function with changes to the training. High Variance: Data points are spread and as a result far from the function. Suggests large changes to the estimate of the target function with changes to the training.\nLow-variance algos:\n\nLinear Regression\nLinear Discriminant Analysis\nLogistic\n\nHigh-variance algos:\n\nDecision Trees\nk-Nearest Neighbors\nSupport Vector Machines.\n\nParametric or linear machine learning algorithms often have a high bias but a low variance. Non-parametric or non-linear machine learning algorithms often have low bias but high variance.\nRead about PCA - https://towardsdatascience.com/understanding-pca-fae3e243731d\nRead about Neural Networks/Backpropagation - https://towardsdatascience.com/understanding-neural-networks-19020b758230\nRead about Binomial Distribution - https://towardsdatascience.com/fun-with-the-binomial-distribution-96a5ecabf65b\nRead about Random Forrest - https://towardsdatascience.com/understanding-random-forest-58381e0602d2\n\nsum of squares of regression\nhttps://365datascience.com/sum-squares/\nR-squared\nhttps://365datascience.com/r-squared/\nZero means our regression line explains none of the variability of the data, 1 would mean our model explains the entire variability of the data. It measures the goodness of fit of our model. The more factors we include in our regression, the higher the R-squared.\nAdjusted R-squared\nAlways smaller than the R-squared, as it penalizes excessive use of variables. We will consider adjusted R² as opposed to R² to evaluate model fit because R² increases irrespective of improvement in prediction accuracy as we add more variables. But, adjusted R² would only increase if an additional variable improves the accuracy of model, otherwise stays same.\nnaive bayes - bayes theorem formula - application\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/#feature-engineering\nbox plot - inter-quartile range -\nbias/variance - corrections.\nvariance - regularisation - L1, L2 - advantage/cons\nRidge regression, lasso regression - https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a\nlstm - forward/back propagation\nbi-directional rnn\ngru -\nprecision/recall - roc/area under - when to choose roc/recall/precision - not do precision when data is imbalanced, recall when less data available class's is req.(credit fraud/cancer detection) / F1 score\nROC - bank defaulters - check the threshold value for classifiation, with different threshold values, accuracy changes\nROC is a plot of signal (True Positive Rate) against noise (False Positive Rate). The model performance is determined by looking at the area under the ROC curve (or AUC). The best possible AUC is 1 while the worst is 0.5 (the 45 degrees random line). Any value less than 0.5 means we can simply do the exact opposite of what the model recommends to get the value back above 0.5.\n\nROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\n\ngradient descent - formmulas\nwhen to use different algos.\nRandom forrest when features are too much\nNaive bayes - features are indep, overfitting avoid\nP(A|B) = P(A ∩ B) / P(B)\nSimilarly, P(B|A) = P(A ∩ B) / P(A)\nIt follows that P(A ∩ B) = P(A|B) * P(B) = P(B|A) * P(A)\nThus, P(A|B) = P(B|A)*P(A) / P(B)\nP(A) is called Prior probability and P(B) is called Evidence.\nThe underlying assumption of these classifiers is that all the features used for classification are independent of each other, and hence the name Naive.\nLinear - features are 1000s, overfitting avoid, less computation needed\nisolation forest\n\nhttps://github.com/h2oai/h2o-tutorials/blob/master/tutorials/isolation-forest/isolation-forest.ipynb\nhttps://github.com/h2oai/h2o-tutorials/blob/master/tutorials/isolation-forest/interpreting_isolation-forest.ipynb\nFeature engg - https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\nPCA -\nEnsemble boosting\nRandom sampling with replacement\n\nA parsimonious model is a model that accomplishes a desired level of explanation or prediction with as few predictor variables as possible.\nidf(w) = log(total number of documents/number of documents containing word w)\ntf(w) = doc.count(w)/total words in doc\nTf-idf(w) = tf(w)*idf(w)\nConfusion Matrix: (TP + TN) / (TP + TN + FP + FN)\nRecall = TP / (TP + FN)\nRecall can be defined as the ratio of the total number of correctly classified positive examples divide to the total number of positive examples. High Recall indicates the class is correctly recognized (small number of FN). Recall is the ability of a classifier to find all positive instances. For each class, it is defined as the ratio of true positives to the sum of true positives and false negatives.\nPrecision = TP / (TP + FP)\nPrecision is the ability of a classifier not to label an instance positive that is actually negative. For each class, it is defined as the ratio of true positives to the sum of true and false positives.\nTo get the value of precision we divide the total number of correctly classified positive examples by the total number of predicted positive examples. High Precision indicates an example labeled as positive is indeed positive (small number of FP).\nHigh recall, low precision:This means that most of the positive examples are correctly recognized (low FN) but there are a lot of false positives.\nLow recall, high precision:This shows that we miss a lot of positive examples (high FN) but those we predict as positive are indeed positive (low FP)\nF-measure: (2 * Recall * Precision) / (Recall + Precision)\nSince we have two measures (Precision and Recall) it helps to have a measurement that represents both of them. We calculate an F-measure which uses Harmonic Mean in place of Arithmetic Mean as it punishes the extreme values more. The F-Measure will always be nearer to the smaller value of Precision or Recall. The F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0.\nPrincipal Component Analysis (PCA) - In the real world, we deal with multi-dimensional data. Thus, data visualization and computation become more challenging with the increase in dimensions. In such a scenario, we might have to reduce the dimensions to analyze and visualize the data easily. We do this by:\n\nRemoving irrelevant dimensions\nKeeping only the most relevant dimensions\nThis is where we use Principal Component Analysis (PCA).\nFinding a fresh collection of uncorrelated dimensions (orthogonal) and ranking them on the basis of variance are the goals of Principal Component Analysis.\nThe Mechanism of PCA:\nCompute the covariance matrix for data objects\nCompute the Eigen vectors and the Eigen values in a descending order\nTo get the new dimensions, select the initial N Eigen vectors\nFinally, change the initial n-dimensional data objects into N-dimensions\n\nDatasets: https://medium.com/towards-artificial-intelligence/best-datasets-for-machine-learning-data-science-computer-vision-nlp-ai-c9541058cf4f\n""], 'url_profile': 'https://github.com/Saket-Kr', 'info_list': ['Jupyter Notebook', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 16, 2020', '4', 'Jupyter Notebook', 'Updated Feb 25, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020']}","{'location': 'Mumbai , Maharashtra', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['PROJECT-NO.2\n'], 'url_profile': 'https://github.com/KritiUpadhyay777', 'info_list': ['Jupyter Notebook', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 16, 2020', '4', 'Jupyter Notebook', 'Updated Feb 25, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Prediction-of-the-presence-of-CKD-\nThe aim of the project is to find out the factors which help in identifying the patients with a high risk of CKD and predicting the presence of CKD in a person using the classification algorithm - logistic regression.\n'], 'url_profile': 'https://github.com/suriya3193', 'info_list': ['Jupyter Notebook', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 16, 2020', '4', 'Jupyter Notebook', 'Updated Feb 25, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020']}","{'location': 'West Lafayette, Indiana', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Estimate-Trending-Duration-of-YouTube-Videos\nBuilt a multivariate linear regression model to predict trending duration of a video on YouTube based on feature engineering, A/B testing and sentiment score generated from user comments (https://www.youtube.com/watch?v=oPr8Sx9L9Mg)\n                                                    Executive Summary\n\nThe content explosion in the media industry, and online space, in the past few years has attracted a huge number of investors. These days, investors and advertisers often sponsor channels and blogs. For instance, in social advertising, YouTube ads convert more customers than any other medium. There are two popular ways in which advertisements are shown:\n\nTargeting in-market audience based on browsing history\nProduct placement, where you place your products’ ads based on trending blogs and videos\nThe latter is widely being used on platforms like YouTube, with the rise of channel sponsoring, where ads are shown on particular videos or channels.\nTherefore, we have analyzed YouTube trending videos from 2017-2018 to derive insights and build a model which can predict how long a video will trend, considering variables such as likes, comments, dislikes, sentiment scores of comments, and video category. This can help us devise a marketing strategy to place our product ads on recommended trending videos.\n\nExploratory Data Analysis and Modelling\nWe built a regression model based on the YouTube data of day1 of trending videos which can predict how long a video can trend considering factors such as like to view ratio, dislike to view ratio, comment to view ratio, video category, sentiment score and number of views as our input variable. Before fitting the model, we did exploratory data analysis which showed that most of our input variables were right skewed. Hence, we preprocessed the data using logarithmic transformation, capping and flooring outlier treatment, and standardization to account for skewness in our data.  Initial analysis showed that if other parameters are constant, the median of trending duration is higher for Film & Animation, Music, People & Blogs, Comedy and Entertainment as compared to other video categories. We calculated sentiment score for each video - average of sentiment score of all comments for that video. Since the number of likes, and dislikes can be misleading as an input variable, we did feature engineering to convert these variables with respect to number of views as base. We generated dummy variables for categorical variable - video category, keeping Film & Animation as base.\nConclusions and Recommendations\nFilm and Animation category videos are likely to trend for the longest duration as compared to other video categories, provided other parameters are kept constant. Hence, we recommend potential advertisers/channel sponsors to place their product ads on Film and Animation category videos. Moreover, advertisers should invest in videos having comments with more negative sentiments to reach out to a larger audience. Based on the raw data, we can easily find the top ten trending channels from Film and Animation category videos where advertisers can invest. Our model can be improved further using more predictor variables and advanced machine learning techniques.\n'], 'url_profile': 'https://github.com/kumar394', 'info_list': ['Jupyter Notebook', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 16, 2020', '4', 'Jupyter Notebook', 'Updated Feb 25, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Text-classification-News-Article\nBuild a classification model to determine whether the data is correctly classified using Logistic regression and Support Vector Machine (SVM). Used the functionalities of NLP such as stemming, lemmatization.\nDataset used in this project:https://www.kaggle.com/rmisra/news-category-dataset\n'], 'url_profile': 'https://github.com/Parikshit-07', 'info_list': ['Jupyter Notebook', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 16, 2020', '4', 'Jupyter Notebook', 'Updated Feb 25, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Tian99Yu', 'info_list': ['Jupyter Notebook', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 16, 2020', '4', 'Jupyter Notebook', 'Updated Feb 25, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Board-Game-Review-Prediction\nUsing a linear regression model to predict the average review a board game will receive based on characteristics such as minimum and maximum number of players, playing time, complexity, etc.\n'], 'url_profile': 'https://github.com/ngp111', 'info_list': ['Jupyter Notebook', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'R', 'Updated Jan 16, 2020', '4', 'Jupyter Notebook', 'Updated Feb 25, 2021', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'R', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 19, 2020']}"
"{'location': 'Dhaka', 'stats_list': [], 'contributions': '216 contributions\n        in the last year', 'description': ['Machine-Learning-Step-by-Step\nDifferent Machine Learning Algorithms such as Regression, Classification and Clustering have been implemented here. Data preprocessing and other important steps also have been explained. All works are done using Spyder.\n'], 'url_profile': 'https://github.com/mdanisurrahmanrony', 'info_list': ['Python', 'Updated Jan 17, 2020', '2', 'Updated Feb 6, 2021', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 13, 2020', '2', 'Python', 'GPL-3.0 license', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Python', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Dec 18, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'bangalore', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['knn-algorithm\n##K Nearest Neighbors with Python|ML##\nHow It Works ?\nK-Nearest Neighbors is one of the most basic yet essential classification algorithms in Machine Learning. It belongs to the supervised learning domain and finds intense application in pattern recognition, data mining and intrusion detection. ->The K-Nearest Neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems.\n->The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other. KNN captures the idea of similarity (sometimes called distance, proximity, or closeness) with some mathematics we might have learned in our childhood— calculating the distance between points on a graph. There are other ways of calculating distance, and one way might be preferable depending on the problem we are solving. However, the straight-line distance (also called the Euclidean distance) is a popular and familiar choice. ->It is widely disposable in real-life scenarios since it is non-parametric, meaning, it does not make any underlying assumptions about the distribution of data (as opposed to other algorithms such as GMM, which assume a Gaussian distribution of the given data).\nPre-requisites: Numpy, Pandas, matplotlib, sklearn\n->We’ve been given a random data set with one feature as the target classes. We’ll try to use KNN to create a model that directly predicts a class for a new data point based off of the features.\n'], 'url_profile': 'https://github.com/Pranithayadav', 'info_list': ['Python', 'Updated Jan 17, 2020', '2', 'Updated Feb 6, 2021', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 13, 2020', '2', 'Python', 'GPL-3.0 license', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Python', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Dec 18, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Egypt', 'stats_list': [], 'contributions': '623 contributions\n        in the last year', 'description': ['DataCamp-Projects\nProjects I have created while learning on datacamp.com\n'], 'url_profile': 'https://github.com/saraheisa', 'info_list': ['Python', 'Updated Jan 17, 2020', '2', 'Updated Feb 6, 2021', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 13, 2020', '2', 'Python', 'GPL-3.0 license', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Python', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Dec 18, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Yelp_Business_Success_Rate_Prediction_Based_On_Reviews\nThis repo contains code for restuarant recommendation system for users based upon business rating value.\nData\nhttps://www.yelp.com/dataset/download\n'], 'url_profile': 'https://github.com/patel577', 'info_list': ['Python', 'Updated Jan 17, 2020', '2', 'Updated Feb 6, 2021', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 13, 2020', '2', 'Python', 'GPL-3.0 license', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Python', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Dec 18, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Nirmal', 'stats_list': [], 'contributions': '737 contributions\n        in the last year', 'description': ['Try every ML algorithm\nWe as a Data Scientist are very lazy in trying out every different algorithm on a given dataset.\nThis web interface provides you a convinent way of switching between algorithms are seeing there results. With this now, you can apply machine learning models without writing a Single Piece of code.\nThe web application is written in streamlit.\nLink to the web app is here\nHow it works\n\nOpen the App\nUpload the dataset*\nInspect your data, if you wish\nSelect the features\nSelect the label\nMake a Train Test split\nSelect an appropriate algorithm\nStart tweaking the hyperparameters\n\n\nThis app expects a preprocessed dataset with all the NaN, Null values handled properly, One Hot encoded, and scaled\n\nA Demo\n\nRequired Modules\n\nPandas\nStreamlit\nScikit-learn\n\nGetting a copy of this repo\nClone the repository before running any commands\n$ git clone https://github.com/AnuragAnalog/Try-every-ML-algorithm.git\n$ cd Try-every-ML-algorithm\nInstallation\nRun the below command to install all the dependencies in your local machine to run the py script.\n$ sudo pip3 install -r requirements.txt\nRunning the app\n$ streamlit run app.py\nAlgorithms\n\n\nRegression\n\nLinear Regression\nK Nearest Neighbours\nDecision Trees\nRandom Forest\nAda Boost\nGradient Boosting\n\n\n\nClassification\n\nLogistic Regression\nK Nearest Neighbours\nDecision Trees\nRandom Forest\nAda Boost\nGradient Boosting\n\n\n\nWant to contribute? then fork, develop, and create a pull-request\nFuture Work\n\n Add an option to show the code which implements the above selected algorithm with the corresponding hyperparameters.\n Added code for One Hotencoding.\n Add more algorithms.\n Add some functionality for preprocessing data too.\n\n'], 'url_profile': 'https://github.com/AnuragAnalog', 'info_list': ['Python', 'Updated Jan 17, 2020', '2', 'Updated Feb 6, 2021', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 13, 2020', '2', 'Python', 'GPL-3.0 license', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Python', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Dec 18, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Minneapolis, MN', 'stats_list': [], 'contributions': '979 contributions\n        in the last year', 'description': [""A Look at Global Weather Patterns\n\nUsing gmaps to display a heatmap with locations of hotels based on my weather parameter inputs\nBackground\nIn this repo, I gathered global data on current weather conditions thru API calls on OpenWeather and then ran a series of linear regressions, looking for weather patterns.  And then for fun, I set up my own parameters for what I want my vacation weather to be, and then created a list of potential hotels and plotted them using gmaps.\nIn a subsequent repo, I put all of the linear regression graphs and written analysis into a webpage.  You can find that repo here.\nShould you choose to clone this repo to your desktop and follow along, some things to note:\n\nYou'll need two API keys, one for Open Weather Map, and one for Google\n\nDON'T FORGET TO ADD A GITIGNORE\nLinks below to where you can get your own API keys\nTo more easily follow along with my code, I placed both API keys inside a file called config.py, which is located in the WeatherPy folder and on the same level as the WeatherPy and VacationPy jupyter notebooks\nI titled both keys g_key & weather_api_key\n\n\nIn the `WeatherPy Jupyter Notebook', notice lines 23 and 24 in cell 1\n\nUnder a free subscription to Open Weather, you get 60 API calls/minute\nSo in order to not get a temporary block from the service, you'll need to add the line of code time.sleep(1) in cell 3\nWith my code, you'll get ~600 cities, so it will take roughly 10 miniutes.  To avoid the wait, you can alter the latitudes and longitudes in cell 1 and play around more quickly with the data until you're ready to run the full kit-and-caboodle\n\n\nInformation on the cities.csv located in the WeatherPy folder\n\nThe csv is created in the fifth cell, after the api calls have been performed\nYou can copy the csv into your own cloned repo and it will just be overwritten when you run your own api calls\nI created the csv so that I could create a Pandas dataframe and run linear regression on the data\n\n\nThe difference between WeatherPy and VacationPy:\n\nI created VacationPy so that I could alter my parameters based on vacation criteria that I wanted\nYou'll need to run WeatherPy first seeing as VacationPy utilizes the cities.csv created there\nYou can run the code and put in your own weather criteria so as to create your own list of hotels for vacation!\n\n\n\nHelpful Links\nhttps://openweathermap.org/api\nhttps://developers.google.com/places/web-service/get-api-key\n""], 'url_profile': 'https://github.com/VallieTracy', 'info_list': ['Python', 'Updated Jan 17, 2020', '2', 'Updated Feb 6, 2021', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 13, 2020', '2', 'Python', 'GPL-3.0 license', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Python', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Dec 18, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Ontario Canada', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': [""cannibalization-predictor\nThis model compares the revenues of various products before and after a new product launch and uses a PCA Algorithm to assess how much other products in a company's portfolio suffered as a result of the new product's new presence on shelf. The algorithm sums the effect across the specified period of inquiry. For example, if you were curious about the anticipated 12 week cannibalization, the effect would be averaged for that period and give a probability that trend would conitinue for another 12 weeks.\nThis calculation is typically carried out across many years so it is recommended you analyize at least 1 year out so you can get a 2 year prediction coefficient as an output.\n""], 'url_profile': 'https://github.com/ibiggy9', 'info_list': ['Python', 'Updated Jan 17, 2020', '2', 'Updated Feb 6, 2021', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 13, 2020', '2', 'Python', 'GPL-3.0 license', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Python', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Dec 18, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Deep_learning_Projets\n4 Keras projects done in AI\nBinary Classification Sonar Project 1 for the Navy: Mines vs. Rocks\nMulti-Class Classification Iris Flowers Project 2 for Mothers who love Gardening and Flowers: Identifying Flower Types\nRegression Housing Pricing Project 3 for Fathers who want to buy a House: Predicting Housing Prices\nDropout Regularization Sonar Project 4 for the Navy: Mines vs. Rocks\n'], 'url_profile': 'https://github.com/mehdiraaza', 'info_list': ['Python', 'Updated Jan 17, 2020', '2', 'Updated Feb 6, 2021', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 13, 2020', '2', 'Python', 'GPL-3.0 license', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Python', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Dec 18, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/javierpe27', 'info_list': ['Python', 'Updated Jan 17, 2020', '2', 'Updated Feb 6, 2021', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 13, 2020', '2', 'Python', 'GPL-3.0 license', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Python', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Dec 18, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sapienhwaker', 'info_list': ['Python', 'Updated Jan 17, 2020', '2', 'Updated Feb 6, 2021', 'Jupyter Notebook', 'Unlicense license', 'Updated Feb 2, 2020', 'Python', 'Updated Jan 13, 2020', '2', 'Python', 'GPL-3.0 license', 'Updated Sep 21, 2020', 'Jupyter Notebook', 'Updated May 26, 2020', 'Python', 'Updated Dec 7, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Dec 18, 2020', 'Python', 'Updated Jan 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '203 contributions\n        in the last year', 'description': ['Deep-Learning-KERAS\nHow neural networks feed data forward through the network. The gradient descent algorithm and how variables are optimized with respect to a defined function. Backpropagation and how neural networks learn and update their weights and biases. The vanishing gradient problem. Activation Functions. Deep learning libraries namely, Keras, PyTorch, and TensorFlow. Building a regression model using the Keras library. Building a classification model using the Keras library. The difference between the shallow and deep neural networks. Convolutional networks and how to build them using the Keras library. Recurrent Neural Networks. Autoencoders and how to build them using the Keras library.\n1. Artificial Neural Networks\n\nForward Propagation:  I built a neural network from scratch and code how it performs predictions using forward propagation.\n\n2. Shallow Models with Keras\n\n\nRegression: Making Regression Neural Model using Keras on a sample data.\n\n\nClassification: Making a Classification Neural Model using Keras and predicing data on MNIST database. The MNIST database, short for Modified National Institute of Standards and Technology database, is a large database of handwritten digits that is commonly used for training various image processing systems.he database is also widely used for training and testing in the field of machine learning. The MNIST database contains 60,000 training images and 10,000 testing images of digits written by high school students and employees of the United States Census Bureau.\n\n\n3. Convolutional Neural Network (CNN)\n\nConvolutional Neural Network with One & Two Convolutional and Pooling Layers using MNIST database.\n\n'], 'url_profile': 'https://github.com/worldofsaad', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Jan 19, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 14, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/VHimmatramka', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Jan 19, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 14, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '196 contributions\n        in the last year', 'description': ['California-Housing-Price-Prediction\nStep1: Import all libraries\nStep2: Load the data\nStep2.1: Read the “housing.csv” file from the folder into the program\nStep2.2: Print first few rows of this data\nStep2.3: Extract input (X) and output (y) data from the dataset\nStep3: Handle missing values: Fill the missing values with the mean of the respective column\nStep4: Encode categorical data: Convert categorical column in the dataset to numerical data\nStep5: Split the dataset: Split the data into 80% training dataset and 20% test dataset\nStep6: Standardize data: Standardize training and test datasets\nTask1: Perform Linear Regression\nTask1.1: Perform Linear Regression on training data\nTask1.2: Predict output for test dataset using the fitted model\nTask1.3: Print root mean squared error (RMSE) from Linear Regression\nTask2: Perform Decision Tree Regression\nTask2.1: Perform Decision Tree Regression on training data\nTask2.2: Predict output for test dataset using the fitted model\nTask2.3: Print root mean squared error from Decision Tree Regression\nTask3: Perform Random Forest Regression\nTask3.1: Perform Random Forest Regression on training data\nTask3.2: Predict output for test dataset using the fitted model\nTask3.3: Print root mean squared error from Random Forest Regression\nTask4: Bonus exercise:  Perform Linear Regression with one independent variable\nTask4.1: Extract just the median_income column from the independent variables (from X_train and X_test)\nTask4.2: Perform Linear Regression to predict housing values based on median_income\nTask4.3: Predict output for test dataset using the fitted model\nTask4.4: Plot the fitted model for training data as well as for test data to check if the fitted model satisfies the test data\nTask4.4.1: let us visualize the Training set\nTask4.4.2: let us visualize the Testing set\n'], 'url_profile': 'https://github.com/Satnam00', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Jan 19, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 14, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hankhowland', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Jan 19, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 14, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GentimisThanos', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Jan 19, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 14, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sairamgajavalli', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Jan 19, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 14, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': [""EPT-Hackathon-AI\nMy work during EPT's mini AI hackathon the goal was to create a regression model to help hotel pricing decision making using GridSearchCV along woth CatBoostRegressor I managed to get 4'th place\n""], 'url_profile': 'https://github.com/azayz', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Jan 19, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 14, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020']}","{'location': 'Columbus, OH', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Portfolio-Analysis-and-Optimization\nWe are trying to evaluate whether a portfolio constructed of companies regarded as sector leaders in the S&P 500 and optimized through mean variance weights can outperform the broader market over a long term on risk adjusted returns. Our analysis also includes a regression analysis to test the characteristics of this portfolio.\n'], 'url_profile': 'https://github.com/kumar848', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Jan 19, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 14, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chandansingh2693', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Jan 19, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 14, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': [""Twitter Sentiment Analysis\nUsing Twiiter data to identify the positive and negative sentiments in making a better purchasing decision.\nProject Organization\nConcepts\nThe following approaches were taken in performing our sentiment analysis through NLP:\n\nData Assembly\nData Processing\nData Exploration or Visualization\nModel Building & Validation\n\nLet's understand the different data preprocessing activities:\n\nConvert text to lowercase – Allows us to deal with uniform case text.\nRemove numbers – Numbers usually do not carry any importance in sentiment analysis\nRemove punctuation – For bag of words based sentiment analysis punctuation does not add value.\nRemove stop words – Stop words are common words found in a language and they are mainly neutral.\nStrip white space – Eliminate extra white spaces.\nTokenization - The process of segmenting running text into words and sentences.\nLemmatisation – Transform to dictionary base form i.e., “produce” & “produced” become “produce”\nStemming – Transforms to root word. Stemming uses an algorithm that removes common word endings.\n\nExploratory Analysis\nI used the following methods to visualize our the tweet field.\n\nWordCloud: An image that visualizes the unique words extracted from our clean tweet\nggplot:\nFrequency Distribution plot: Plots the real frequency of occurrence of text\nCount plot: Plots our categorical data in terms of bars.\n\nTrain-Test Split\nI used Scikit-learn's function train_test_split(). You need to pass basically 3 parameters features, target, and test_set size. Additionally, I have used random_state to select records randomly.\nFeature Extraction\nThe features were extracted using two models:\n\nBag of Words: A numerical statistic that is intended to reflect the importance of a word in a document from a collection or corpus.\nTF-IDF Features: A numerical statistic that is intended to reflect the importance of a word in a document from a collection or corpus.\n\nModel Building\nI used the following classifiers to build the models:\n\nRandom Forest\nLogistic Regression\nNaive Bayes\nGradient Boosting\n\nEvaluation\nI used the following metrics to evaluate the performance of our models:\n\nAccuracy Score: The functions that computes the count of correct predictions.\nConfusion Matrix: A table used to describe the classifier on a set of test data for which the true values are known.\nClassification Report: Measures the quality of our predictions through a classification algorithm through Precision and Recall\n\n""], 'url_profile': 'https://github.com/ktdawood', 'info_list': ['Jupyter Notebook', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'R', 'Updated Jan 19, 2020', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Jan 14, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Jupyter Notebook', 'Updated Sep 25, 2020']}"
"{'location': 'Dhaka, Bangladesh.', 'stats_list': [], 'contributions': '241 contributions\n        in the last year', 'description': ['Welcome to machine learning!\nMachine learning (ML) is the study of computer algorithms that improve automatically through experience. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as ""training data"", in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks.\nMachine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. In its application across business problems, machine learning is also referred to as predictive analytics.\nTopics\n1. Data preprocessing\n2. Regression\nSimple Linear Regression\nMultiple Linear Regression\nPolynomial Regression\nSupport Vector for Regression (SVR)\nDecision Tree Regression\nRandom Forest Regression\n3. Classification\nLogistic Regression\nK-Nearest Neighbors (K-NN)\nSupport Vector Machine (SVM)\nKernel SVM\nNaive Bayes\nDecision Tree Classification\nRandom Forest Classification\n4. Clustering\nHierarchical Clustering\nK-means Clustering\n5. Association Rule Learning\nApriori\nEclat\n6. Reinforcement Learning\nUpper Confidence Bound\nThompson Sampling\n7. Natural Language Processing\n8. Deep Learning\nArtificial-Neural-Networks-(ANN)\nConvolutional-Neural-Networks-(CNN)\n9. Dimensionality-Reduction\nKernel-PCA\nLinear-Discriminant-Analysis\nPrincipal-Component-Analysis\n10. Model-Selection-and-Boosting\nModel Selection\nXGBoost\n'], 'url_profile': 'https://github.com/shomnathsomu', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated Dec 13, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Aug 29, 2020', 'HTML', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'Boston, MA, USA', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Telecom-Subscriber-Churn-Prediction\nProject done on determination of subscriber leaving a telecom subscription. This project is a 2 split scenario where churn prediction is done based on only cost data (conventional telecom) and prediction based on services provided by the telecom service (such as streaming, internet etc). Classification is done using logistic regression, decision trees and random forest and all 3 models are evaluated to find best performing model using confusion matrix, accuracy, precision, recall and ROC curves\n'], 'url_profile': 'https://github.com/anishnitin', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated Dec 13, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Aug 29, 2020', 'HTML', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'Toronto', 'stats_list': [], 'contributions': '463 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wmalisch', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated Dec 13, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Aug 29, 2020', 'HTML', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'Regina SK', 'stats_list': [], 'contributions': '237 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AdarshKoppManjunath', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated Dec 13, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Aug 29, 2020', 'HTML', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'London, Ontario, Canada', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': [""ProsperLoan Analysis, Bad Loan and Borrower APR\nby Peter Wu\nIntroduction\nThe anlaysis aimed to explore and explain the potential variables affecting the bad loan ratio, i.e., what are the factors determining the loan completion status of each borrower. On the other hand, would borrower APR be affected due to the company's concern on bad loan, and what influences the borrower APR.\nFirst a exploratory analysis on the dataset highlight few key variables which potentially have major imapcts on a loan, and these variables are analyzed further statistically in a explanatory anlaysis.\nA detailed presentation of findings is produced at the end, with data visualization from Pandas, Matplotlib, and seaborn.\nDataset\nThe dataset contains 113,937 loans with 81 variables on each loan,\nincluding loan amount, borrower rate, current loan status, borrower income, and many others.\nThe dataset can be found in a\nrepository for Amazon's AWS here,\nwith feature documentation available here.\nThe goal of this analysis was set to determine two things:  first, which features are best at predicting the LoanStatus; second, which field best determine the BorrowerAPR   and  `BorrowerRate?\nSummary of Findings\nIn the exploration, several relationships were observed for the two pivot variables (LoanStatus and BorrowerAPR/BorrowerRate):\n\n\nThere is a moderate negative raltionship between the two credit score bounds and  BorrowerAPR/BorrowerRate.\n\n\nProsperScore seemed to be negatively related to BorrowerAPR/BorrowerRate.\n\n\nIn high ProsperScore, the BorrowerAPR/BorrowerRate decreases as borrowers' CreditScoreRangeLower increases.\n\n\nThe Defaulted, Cancelled, Chargedoff and Past Due LoanStatus, which all are bad loans, have a lower median CreditScore Bounds than the Current and InProgress LoanStatus, which are good loans (or about to be good loans). This might reveal that good debts came from a high credit score borrower on average.\n\n\nIf a loan was borrowed in a Term of 36, then it is more likely to go bad, if the borrower's CreditScore is below 700.\n\n\nIf a borrower has a high ProsperScore of 11.0 and a high CreditScoreRangeLower above 700, then his/her loan is almost guaranteed to not go bad.\n\n\nMedian and low ProsperScore (< 9.0) in the Term 36 coontributed most to bad loans.\n\n\nOutside the main exploration, I had two interesting findings:\n\nThe two pairs of variables: BorrowerAPR and BorrowerRate, CreditScoreRangeLower and CreditScoreRangeUpper are almost perfectly related to each other.\nThere is also an interplay betweeen the two pivot variables,  although their relationships were not intended to be analyzed initially.\n\nKey Insights for Presentation\nFor the presentation, I will focus my analysis on how Credit Score, ProsperScore, and  Term,  affect the pivot variables.\nI will start with univariate analysis, looking at the distrbution of the three pivot variables: LoanStatus, BorrowerAPR, and BorrowerRate. Then shift to the bivaraite relationships between pivot variables and their potential determinants: Credit Score, ProsperScore, and  Term. Lastly, the multivariate section will dive deep into the interplays of each three variables.\n""], 'url_profile': 'https://github.com/Peta0228', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated Dec 13, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Aug 29, 2020', 'HTML', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'Ghana', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Predicting-Stroke\nThis work was to bring forth the factors influencing stroke. As such, the approach chosen was to develop a predictive model using the provided train and test dataset. The programming or statistical package employed here was Python using data wrangling and visualization libraries Numpy, Pandas, Seaborn and Matplotlib.pyplot. In modeling, classifiers chosen were Decision Tree, Random Forest, Logistics Regression and XGBoost. Finally, the Maximum Hard Voting Ensemble Model was chosen to encompass base classifiers. Feature importance was used alongside to visualize pressing factors of stroke for each base classifier.\n'], 'url_profile': 'https://github.com/ioakowuah', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated Dec 13, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Aug 29, 2020', 'HTML', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['Sales_prediction\nSales forecasting is the process of estimating future sales. Accurate sales forecasts enable companies to make informed business decisions and predict short-term and long-term performance. Companies can base their forecasts on past sales data, industry-wide comparisons, and economic trends.  The dataset that we used in this problem is ""Advertising"" dataset. It consists of five columns, out of which three columns namely TV, radio and newspaper are considered as input data and the last column i.e., sales is considered to be target values.  In this problem, we made use of linear regression to predict the rate of sales.\n'], 'url_profile': 'https://github.com/harika1101', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated Dec 13, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Aug 29, 2020', 'HTML', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}","{'location': 'Springfield, Illinois', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Sentimental-Analysis-of-Amazon-Alexa-products-NLP-Voice-Recognition-Products-Using-Apache-Spark\nThis study tends to analyze Amazon’s Alexa products; to predict feedback views on the Amazon Alexa products using text, variation, and feedbacks. Train machine learning models for sentiment analysis and analyze customer reviews by showing distributions of positive reviews as well as negative reviews. Also, it will display some exploratory data analysis that will reveal distributions of unique variations contained in the dataset. Models to be used will be Text classification, Random Forest and Gradient Boosting  Train a number of different models on the training set and evaluate each one on the validation set to see which model generalizes best to the unseen data. The test set will be used to provide an unbiased evaluation of the tuned model. The test set is used only once to evaluate the final model. Upon which decision can be made to choose which model provides optimal feedbacks. Models employed were weighted Loss Logistic Regression upon up-sampling the imbalanced dataset.   Second and third models employed are random forest and GBT upon splitting the DataFrame into testing and training data set and up-sampling the training the data set and fitting the pipeline on the up-sampled data set.\nData Link: https://www.kaggle.com/sid321axn/amazon-alexa-reviews\n'], 'url_profile': 'https://github.com/Jukonu', 'info_list': ['Python', 'Updated Apr 30, 2020', 'R', 'Updated Dec 13, 2020', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 14, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Aug 29, 2020', 'HTML', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020']}",,
