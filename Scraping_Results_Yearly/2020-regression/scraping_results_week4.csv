"{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['my.lm\nCode and functions associated with the my.lm() regression function.\nFor more information, see the associated Medium post at https://medium.com/@dyudkin/custom-r-regression-functions-you-might-find-useful-8f58d610f41\n'], 'url_profile': 'https://github.com/dyudkin', 'info_list': ['9', 'R', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '2', 'Python', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 10, 2020', 'Python', 'Updated Feb 3, 2020', '3', 'Python', 'MIT license', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['9', 'R', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '2', 'Python', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 10, 2020', 'Python', 'Updated Feb 3, 2020', '3', 'Python', 'MIT license', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['9', 'R', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '2', 'Python', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 10, 2020', 'Python', 'Updated Feb 3, 2020', '3', 'Python', 'MIT license', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['9', 'R', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '2', 'Python', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 10, 2020', 'Python', 'Updated Feb 3, 2020', '3', 'Python', 'MIT license', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'Iran, Rasht', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sajjad-n', 'info_list': ['9', 'R', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '2', 'Python', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 10, 2020', 'Python', 'Updated Feb 3, 2020', '3', 'Python', 'MIT license', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saheelb', 'info_list': ['9', 'R', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '2', 'Python', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 10, 2020', 'Python', 'Updated Feb 3, 2020', '3', 'Python', 'MIT license', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'Guinée, Conakry, GN', 'stats_list': [], 'contributions': '700 contributions\n        in the last year', 'description': ['Model-regression-linear-from-scratch\nImplement model regression linear simple and multiple form scratch and compare it the sklearn model\nContexts\nL\'idee de ce projet est d\'aide les gens qui veulent en savoir plus sur les maths (et les algorithms) qui se cachent derriere la regression linear.\nPermettre de comprendre un peu plus en profondeur la therorie derriere le machie learning.\nDans ce jupyter nous allons commencer avec la Regression Linear, un algorithme beaucoup plus simple.\n\nExplications\nLa regression linear est etablie par une equation : y = a*X + b\n\n\nL\'objectif du model sera de trouver les bons parameters pour : a , b\n\n\n\nOn a donc besoin d\'une fonction de cout (ou fonction erreur) Pour nous permettre d\'evaluer le model et par la suite apprendre sur les parameters. Cette fonction est definie par : \n\n\nExplication : supposons que notre model predit une valeur de y=600 alors qu\'il devrait predire 550 ?\nOn dit alors que notre model a fait une erreur de -50 (550-600) on peut donc parfois obtenir des erreurs negatives, pour palier a ce probleme... on a deux solutions :\n    Soit on prend la valeurs absolue de l\'erreur : |550-600|=50 (Celui est la procedure avec mean absolute error)\n    Soit on prend la racine carree du resultat soit (-50)**2 = 2500 (on a donc une erreur trop grande il faut donc le penaliser) mais avant on applique cela a tout notre dataset ainsi on la somme precedente.\n\nla representation de cette fonction est convexe (une fonction qui a un seul minima possible)\n\n\nCette fonction est donc un algorithm iterative, il faut applique des derives partielles et multiplier par une certain valeur (c\'est ce qu\'on appelle taux d\'apprentissage ou encore learning rate). \n\n\nIl faut donc trouver les gradients pour chaque parameters : a,b\n\n\nA savoir : Tout ce qu\'on fait en realite, c\'est de derive la fonction et calculer le nouveau parameter en retranchant a chaque fois l\'encienne valeur et comme la decente de gratient a un seul mina possible (une fonction convexe)... apres plus iteration on va donc tomber sur le minima (ou en tout etre proche).\nMais attention, vous remarquez qu\'on multiplie par le learning rate apres avoir calucle la somme des erreurs, cette valeur est importante... il faut savoir la choisir, en generale cette valeur est tres faible et varie entre 0.001 a 0.1.\nSi cette valeur est trop grande on risque de ne pas tomber sur le minima... on ferra des sauts sans jamais tomber decus, par contre si cette valeur est tres petite on risque de mettre bcp de temps avant d\'y arriver on aura donc besoin de beaucoup d\'iteration.\n\n\n\nSi vous avez encore des doutes ou que vous n\'avez pas tout compris : Je vous recomande de suivre cette \nvideo youtube qui l\'explique bien avant de continuer sur le code : \n\nGradient decente Thibault Neveu\nou encore sur wiki https://fr.wikipedia.org/wiki/Algorithme_du_gradient\nImport tools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nimport pandas as pd\nimport seaborn as sns\nCreate data\nnp.random.seed(0)\nx, y = make_regression(n_samples=180, n_features=1, noise=10)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\nx_train.shape\n(144, 1)\n\nx_test.shape\n(36, 1)\n\nprint(y_train.shape, y_test.shape)\n(144,) (36,)\n\nplt.scatter(x_train, y_train)\n<matplotlib.collections.PathCollection at 0x7f212439ab70>\n\n\nCreate model\nclass RegressionLinearCustom():\n    """"""\n        Implementation Linear Regression Model with gradient decente solution.\n        The gradient decente use partial derivate.\n        This model have two essentiel parameters :\n            - Learning rate : learning_rate (0.01 default)\n            - max interation : max_iter (1000 default)\n        The other parameters:\n            - coefs_ this is coeffients of parameters data\n            - intercept_ this is intercept of function, can edit it True or False\n            - error_gradient this is gradient parameters learning.\n    """"""\n    \n    def __init__(self, learning_rate=0.01, max_iter=1000):\n        """"""\n            Initializer parameters model\n        """"""\n        self.lrate=learning_rate\n        self.max_iter=max_iter\n        self.coefs_=None\n        self.intercept_= 0\n        self.error_gradient = None\n        self.is_fited = False\n        \n    def error_coefs(self, coefs,intercept_, xi, yi, m, lr):\n        """"""\n            Calcul coefs gradient:\n            -- Parameters :\n                - xi : value for one sample in data\n                - yi : real value prediction for this precedent sample\n                - m : dataset size\n                - lr : learning rate\n        """"""\n        return lr/m * (xi*(coefs*xi+intercept_-yi))\n    \n    def error_intercept(self,coefs, intercept_, xi, yi, m, lr):\n        """"""\n            Calcul intercept gradient\n            -- Parameters:\n                - xi : value for one sample in data\n                - yi : real value prediction for this precedent sample\n                - m : dataset size\n                - lr : learning rate\n        """"""\n        return lr/m * sum(coefs*xi+intercept_-yi)\n    \n    def cout_function(self, m, coefs, intercept_, xi, yi):\n        """"""\n            Gradient function\n            -- Parameters:\n                - xi : value for one sample in data\n                - yi : real value prediction for this precedent sample\n                - m : dataset size\n        """"""\n        return sum((yi - coefs*xi+intercept_)**2)/2*m\n    \n    def score(self, x, y):\n        """"""\n            Score function\n        """"""\n        return 1 - sum((y-self.predict(x))**2)/sum((y-y.mean())**2)\n        \n    \n    def fit(self,x, y):\n        """"""\n            Fit fuction, learning parameters\n            -- Parameters:\n                - x, sample data\n                - y, predict data\n        """"""\n        \n        if x.shape[0] != y.shape[0]:\n            return ValueError(""x and y must have same sample"")\n        \n        m = x.shape[0] # size du dataset\n        self.coefs_ = np.zeros(x.shape[1]) # nuers of features\n        error_intermed = None\n        for _ in range(self.max_iter):\n            for xi,yi in zip(x,y):\n                self.coefs_ -= self.error_coefs(self.coefs_,self.intercept_, xi, yi, m, self.lrate) \n                self.intercept_ -= self.error_intercept(self.coefs_, self.intercept_, xi, yi,m, self.lrate) \n                self.error_gradient = self.cout_function(m, self.coefs_, self.intercept_, xi, yi)\n         \n        self.is_fited = True\n        print(\'RegressionLinear(learning_rate={}, max_iter={})\'.format(self.lrate, self.max_iter))\n    \n    def predict(self, x):\n        """"""\n            Predict function : \n            -- Parameters:\n                - x, sample data what to predict\n        """"""\n        if not self.is_fited:\n            return ValueError(""model must fited after predict"")\n        if x.shape[1] != self.coefs_.shape[0]:\n            return ValueError(""the features of x do not have the same size as those to train"")\n        return (x*self.coefs_).sum(axis=1)+self.intercept_ # equation -- coefs*X + intercept\nlr = RegressionLinearCustom()\nlr.fit(x_train,y_train)\nRegressionLinear(learning_rate=0.01, max_iter=1000)\n\nlr.coefs_\narray([21.1303301])\n\nlr.intercept_\n-0.8352580818314677\n\nplt.scatter(x_train, y_train)\nplt.plot(x_train, lr.predict(x_train), c=\'red\', lw=3)\n[<matplotlib.lines.Line2D at 0x7f212433dc50>]\n\n\n Evalute custum model\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\nprint(\'MAE : \', mean_absolute_error(y_test, lr.predict(x_test)))\nprint(\'RMSE : \', np.sqrt(mean_squared_error(y_test, lr.predict(x_test))))\nprint(\'Median absolute error : \', median_absolute_error(y_test, lr.predict(x_test)))\nMAE :  7.877842240242018\nRMSE :  10.042785587416502\nMedian absolute error :  6.54594841083626\n\nprint(\'Score Custum model : \', lr.score(x_test, y_test))\nScore Custum model :  0.7612679859914026\n\nComparaison avec Sklearn\nfrom sklearn.linear_model import LinearRegression\nlr2 = LinearRegression()\nlr2.fit(x_train,y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nlr2.coef_\narray([21.12867091])\n\nlr2.intercept_\n-0.8328942101095778\n\nplt.scatter(x_train,y_train)\nplt.plot(x_train, lr2.predict(x_train), c=\'red\', lw=3)\nplt.title(\'LinearRegression model\')\nText(0.5,1,\'LinearRegression model\')\n\n\nEvaluate Linear regression sklearn\n\nprint(\'MAE : \', mean_absolute_error(y_test, lr2.predict(x_test)))\nprint(\'RMSE : \', np.sqrt(mean_squared_error(y_test, lr2.predict(x_test))))\nprint(\'Median absolute error : \', median_absolute_error(y_test, lr2.predict(x_test)))\nMAE :  7.878092566619603\nRMSE :  10.043458219943728\nMedian absolute error :  6.544939758863786\n\nprint(\'Score sklearn model : \', lr2.score(x_test, y_test))\nScore sklearn model :  0.7612360059607552\n\nMultiple Linear regression\nnp.random.seed(0)\nx1, y1 = make_regression(n_samples=1280, n_features=3, noise=10)\nx1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.2)\nx1_train.shape\n(1024, 3)\n\ny1_train.shape\n(1024,)\n\nlr3 = RegressionLinearCustom(learning_rate=0.01)\nlr3.fit(x1_train, y1_train)\nRegressionLinear(learning_rate=0.01, max_iter=1000)\n\nlr3.coefs_\narray([64.59675767, 62.55187783, 60.5581771 ])\n\nlr3.intercept_\n-3.8782243776243024\n\n\n\n- Evaluate LinearRegressionCustum for multiple Regression\n\nprint(\'MAE : \', mean_absolute_error(y1_test, lr3.predict(x1_test)))\nprint(\'RMSE : \', np.sqrt(mean_squared_error(y1_test, lr3.predict(x1_test))))\nprint(\'Median absolute error : \', median_absolute_error(y1_test, lr3.predict(x1_test)))\nMAE :  9.187022392401918\nRMSE :  11.324822688558232\nMedian absolute error :  7.700921217861264\n\nprint(\'Score Custum model multiple regression : \', lr3.score(x1_test, y1_test))\nScore Custum model multiple regression :  0.9901428799842829\n\nSklearn\nfrom sklearn.linear_model import SGDRegressor\nsg = SGDRegressor()\nlrr3 = LinearRegression()\nlrr3.fit(x1_train, y1_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nsg.fit(x1_train, y1_train)\nSGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n             learning_rate=\'invscaling\', loss=\'squared_loss\', max_iter=1000,\n             n_iter_no_change=5, penalty=\'l2\', power_t=0.25, random_state=None,\n             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n             warm_start=False)\n\nlrr3.coef_\narray([66.6285208 , 64.15098078, 63.81493204])\n\nsg.coef_\narray([66.60881996, 64.15799953, 63.74762287])\n\nlrr3.intercept_\n0.4128450631445766\n\nsg.intercept_\narray([0.44550697])\n\n\n\n- Evaluate LinearRegression\n\nprint(\'MAE : \', mean_absolute_error(y1_test, lrr3.predict(x1_test)))\nprint(\'RMSE : \', np.sqrt(mean_squared_error(y1_test, lrr3.predict(x1_test))))\nprint(\'Median absolute error : \', median_absolute_error(y1_test, lrr3.predict(x1_test)))\nMAE :  7.9704692752093855\nRMSE :  9.851604662213807\nMedian absolute error :  6.889328330159444\n\nprint(\'Score sklearn model LinearRegression : \', lrr3.score(x1_test, y1_test))\nScore sklearn model LinearRegression :  0.9925406467656483\n\n\n- Evaluate SGDRegressor\n\nprint(\'MAE : \', mean_absolute_error(y1_test, sg.predict(x1_test)))\nprint(\'RMSE : \', np.sqrt(mean_squared_error(y1_test, sg.predict(x1_test))))\nprint(\'Median absolute error : \', median_absolute_error(y1_test, sg.predict(x1_test)))\nMAE :  7.972534909526331\nRMSE :  9.855358962547134\nMedian absolute error :  6.859896458867109\n\nprint(\'Score sklearn SGDRegressor : \', sg.score(x1_test, y1_test))\nScore sklearn SGDRegressor :  0.992534960384724\n\nPiste d\'amelioration\n- Ameliorer ce model pour prendre en compte la regression non-linear.\n- Jouer sur le learning rate ou le max_inter lorsque le model under-fit.\n- Ajouter un peu plus de performance sur modele, notament permettre de normalizer les donnees avant de faire le fit.\n\n#Credit :\nHarouna Diallo\n'], 'url_profile': 'https://github.com/hadpro24', 'info_list': ['9', 'R', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '2', 'Python', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 10, 2020', 'Python', 'Updated Feb 3, 2020', '3', 'Python', 'MIT license', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'Warangal, India', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Multivariate-Linear-Regression\nData Science course assignment 1\nTo do :\n\n Get file name as argument\n Regularization\n Divide data into training and test\n Determine accuracy of model (root mean square error)\n Observe variation of gradient Descent by varying alpha\n Cost function\n Gradient Descent\n Normalization\n Plotting cost vs iterations\n\n'], 'url_profile': 'https://github.com/mujtabali', 'info_list': ['9', 'R', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '2', 'Python', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 10, 2020', 'Python', 'Updated Feb 3, 2020', '3', 'Python', 'MIT license', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '128 contributions\n        in the last year', 'description': [""MAML and Reptile in PyTorch\nThis repository includes the Sine wave experiment with MAML and Reptile.\nRun\npython main.py --run=MAML\npython main.py --run=Reptile\nTested on\nPython 3.7.4\nPyTorch 1.3.1\nResults\nLoss after 30000 iterations\n---------------------------\nMAML: 0.058\nREPTILE: 0.048\nMAML:\n\nReptile:\n\nAdapted from\nJohn Schulman's GIST\n""], 'url_profile': 'https://github.com/JosephKJ', 'info_list': ['9', 'R', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '2', 'Python', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 10, 2020', 'Python', 'Updated Feb 3, 2020', '3', 'Python', 'MIT license', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 18, 2020']}","{'location': 'Global', 'stats_list': [], 'contributions': '278 contributions\n        in the last year', 'description': ['AI Sciences Course: Logistic Regression from Scratch to Advanced Level\nRequirements\n\nNo prior knowledge or experience needed. Only a passion to be successful!\nAdmin permissions to download files\n\nCourse description\n\nAre you ready to start your path to becoming a Machine Learning expert!\nAre you ready to train your machine like a father trains his son!\n\nA breakthrough in Machine Learning would be worth ten Microsofts."" -Bill Gates\nThere are lots of courses and lectures out there regarding logistic regression. This course is different!\nThis course is truly a step-by-step. In every new tutorial we build on what had already learned and move one extra step forward and then we assign you a small task that is solved in the beginning of next video.\nWe start by teaching the theoretical part of concept and then we implement everything as it is practically using python\nThis comprehensive course will be your guide to learning how to use the power of Python to train your machine such that your machine starts learning just like human and based on that learning, your machine starts making predictions as well!\nWe’ll be using python as programming language in this course which is the hottest language nowadays if we talk about machine leaning. Python will be taught from very basic level up to advanced level so that any machine learning concept can be implemented.\n\nWe’ll also learn various steps of data pre processing which allows us to make data ready for machine learning algorithms.\nWe’ll learn all general concepts of machine learning overall which will be followed by the implementation of one of the most important ML algorithm “Logistic regression”. Each and every concept of logistic regression will be taught theoretically and will be implemented using python.\nMachine learning has been ranked one of the hottest jobs on Glassdoor and the average salary of a machine learning engineer is over $110,000 in the United States according to Indeed! Machine Learning is a rewarding career that allows you to solve some of the world\'s most interesting problems!\nThis course is designed for both beginners with some programming experience or even those who know nothing about ML and Logistic Regression!\n\n\nThis comprehensive course is comparable to other Machine Learning courses that usually cost thousands of dollars, but now you can learn all that information at a fraction of the cost! With over 50 HD video lectures and detailed code notebooks for every lecture this is one of the most comprehensive course for Logistic regression and machine learning on Udemy!\n\nWe\'ll teach you how to program with Python, how to use it for data pre processing and logistic regression! Here a just a few of the topics we will be learning:\n\nProgramming with Python\nNumPy with Python for array handling\nUsing pandas Data Frames to handle Excel Files\nUse matplotlib for data visualizations\nData Pre processing\nMachine Learning concepts, including:\nTraining and testing sets\nModel training\nModel Validation\nLogistic regression with sk-learn\nLogistic regression from absolute scratch\nImplementing logistic regression on different data sets\nand much, much more!\n\nWho this course is for:\n\nThis course is for you if you want to learn how to program in Python for Machine Learning\nThis course is for you if you want to make a predictive analysis model\nThis course is for you if you are tired of Machine Learning courses that are too complicated and expensive\nThis course is for you if you want to learn Python by doing\nThis course is for someone who is absolute beginner and have very little idea of machine learning\nThis course is for someone who want to learn Logistic regression from zero to hero\n\n'], 'url_profile': 'https://github.com/AISPUBLISHING', 'info_list': ['9', 'R', 'Updated Aug 21, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', '2', 'Python', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Dec 10, 2020', 'Python', 'Updated Feb 3, 2020', '3', 'Python', 'MIT license', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Feb 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/suchayan01', 'info_list': ['Java', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Sep 9, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Regression\nRegression Examples\n'], 'url_profile': 'https://github.com/suhas2019', 'info_list': ['Java', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Sep 9, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Melbourne', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/raghuknsv', 'info_list': ['Java', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Sep 9, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Bangladesh', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tanveer234', 'info_list': ['Java', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Sep 9, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Greater San Diego, USA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sanskrut-01', 'info_list': ['Java', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Sep 9, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,029 contributions\n        in the last year', 'description': ['Regression\nMachine Learning\n'], 'url_profile': 'https://github.com/khatrironit', 'info_list': ['Java', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Sep 9, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Natsu20', 'info_list': ['Java', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Sep 9, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Introduction\nThe objective for this project is to perform optimization, tree-based modeling and prediction algorithms on the Ames Housing dataset. The dataset is randomly sampled into training (75%) and test (25%) data. We used generalized linear regression and random foresting to evaluate the relation between sale price and other dependent factors. Upon testing the correlation, we observed a high multicollinearity between dependent variables. For eliminating redundancy in our regression equation, we used the Lasso and Ridge optimization techniques to deal with the variables with high Variable Inflation Factor (more than 10). K-Fold Cross Validation is also used to optimize the random forest model. Using the regression hyper tuning parameters like RMSE, MAE, AIC and BIC, we have selected the best fit model to predict the final sale price.\nDeeper Analysis is provided in the word document and presentation.\n'], 'url_profile': 'https://github.com/nitikab23', 'info_list': ['Java', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Sep 9, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Hydrabad', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RushiB007', 'info_list': ['Java', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Sep 9, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}","{'location': 'Karachi', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['Regression\nLinear regression\nLinear regression with simple data generator with Numpy\nLinear regression using batch gradient descent\nLinear regression using batch gradient descent with simple data generator with Numpy\nLinear regression using  Mini-batch gradient descent\nLinear regression using  Mini-batch gradient descent with simple data generator with Numpy\nRegression using Stochastic Gradient Descent\nRegression using Stochastic Gradient Descent with simple data generator with Numpy\nPolynomial regression\nPolynomial regression with simple data generator with Numpy\nRegression with decision trees\nRegression decision trees with Iris Dataset\n'], 'url_profile': 'https://github.com/mnavaidd', 'info_list': ['Java', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'R', 'Updated Sep 9, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2020']}"
"{'location': 'Bangalore, Karnataka, India', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HemrajBhatt', 'info_list': ['1', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mdurular', 'info_list': ['1', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 24, 2020']}","{'location': 'Iran, Guilan', 'stats_list': [], 'contributions': '237 contributions\n        in the last year', 'description': [""Regression\nIn statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'predictors', 'covariates', or 'features'). The most common form of regression analysis is linear regression, in which a researcher finds the line (or a more complex linear function) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared distances between the true data and that line (or hyperplane).\n""], 'url_profile': 'https://github.com/M-Taghizadeh', 'info_list': ['1', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/waterhorse1', 'info_list': ['1', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 24, 2020']}","{'location': 'Tehran', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mohsenfallahnjd', 'info_list': ['1', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['This is a linear regression for a dataset on Boston housing prices.\n'], 'url_profile': 'https://github.com/pjain199', 'info_list': ['1', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 24, 2020']}","{'location': 'Gurgaon,IN', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression-Tutorial\nLinear Regression,Lasso Regression,Decision Tree Regression etc\n'], 'url_profile': 'https://github.com/Rinkeshai', 'info_list': ['1', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 24, 2020']}","{'location': 'KENYA', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': [""Supervised-Learning-using-Various-Regression-Techniques  REGRESSION TECHNIQUES:\nPredicting the Price of a House.\n   a) Defining the Question\n\nAs a Data Scientist, I have been recruited to work for Hass Consulting Company which is a real estate leader with over 25 years of experience. I have been tasked to study the factors that affect housing prices using the given information on real estate properties that was collected over the past few months. Later onwards, create a model that would allow the company to accurately predict the sale of prices upon being provided with the predictor variables.\n   b) Defining the Metric for Success\n\nThis project will be successful when:\n          1)We Identify the most crucial independent variables that affect house prices.\n\n          2)The Regression model achieves atleast 80% accuracy\n\n          3)Have the lowest RMSE score possible\n          \n\n   c) Understanding the context\n\nHassConsult offers investment-grade developments all over the City. The developments have been conceptualized by their development team to be high quality assets using a data-driven approach in planning and design, that gears property for high rental returns and an escalated rate of capital growth.\n   d)Experimental Design\n\nThe project was undertaken using the following design Datasets(Independent Project Week 7 - house_data.csv)\nPerforming EDA\nFeature Engineering\nMultiple Linear Regression\nQuantile regression\nRidge Regression\nLasso Regression\nElastic-Net Regression\nComputing the RMSE\nPlotting residual plots\nAssess their heteroscedasticity using Barlette's test\nChallenging the solution\nConclusion\n   e) Data Relevance\n\nThe relavance of our data will be answered by the following questions.\nHow accurate is the data at predicting the Price of a house?\nWas the dataset sufficient?\nWas the data biased?\nIs the data source a reliable source?\n""], 'url_profile': 'https://github.com/vivianusdjpy', 'info_list': ['1', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 24, 2020']}","{'location': 'Morocco', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['machineLearningSchoolarData\nA little test to get a model to anticipate new values based on preliminary data\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]) \nY = np.array([8, 6, 6, 6, 6, 5, 8, 9, 12, 16, 17, 17])\nAnd this is the model obtained after a lots of calculations to get the slope and y-intercept\n\n'], 'url_profile': 'https://github.com/farhatizakaria', 'info_list': ['1', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Dolley1997', 'info_list': ['1', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '2', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 24, 2020']}"
"{'location': 'Banglore', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogistic Regression\n'], 'url_profile': 'https://github.com/RanjithKumar05', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 25, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'HTML', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Dolley1997', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 25, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'HTML', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6,910 contributions\n        in the last year', 'description': [""skellam\nSkellam is a personal python project I have been working on which allows you to predict variables that belong to the Skellam distribution.\nThe Skellam distribution is a frequency distribution of the difference between two independent variates following the\nsame Poisson distribution (Irwin, 1937). One notable example that follows a Skellam distribution is the margin in a soccer game as this is the difference\nbetween two Poisson distributed variables, the home team's score and the away team's score.\nUsage\n# Create margin variable which follows the Skellam distribution\ndf['margin'] = df['Home Score'] - df['Away Score'] \n\n# Define model and provide independent variables, the intercept and the odds\n# of the home team, and also the dependent variable, the margin.\nmodel = SkellamRegression(df[['intercept', 'Home Odds']], df['margin'])\n\n# Initiate the training of the model\nmodel.train()\n\n# To produce metrics such as R2\nresults = model.model_performance()\nresults.r2()\nReferences\n\nIrwin, J. (1937). The Frequency Distribution of the Difference between Two Independent Variates following the same Poisson Distribution. Journal of the Royal Statistical Society, 100(3), 415-416. doi:10.2307/2980526\n\n""], 'url_profile': 'https://github.com/nathan-bennett', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 25, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'HTML', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020']}","{'location': 'San Francisco, California', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogistic Regression\n'], 'url_profile': 'https://github.com/vedantsahay19', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 25, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'HTML', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020']}","{'location': 'Gurgaon', 'stats_list': [], 'contributions': '267 contributions\n        in the last year', 'description': [""Assignment 3 - Logistic Regression\nIn this assignment, you'll be implementing a basic logistic regression model to solve a binary classification problem. This model will be used to fit the given 2D data that comprises of marks of some students. Aim here is to predict whether the student will get admission in a college based upon these marks or not. The fits would be visualized. So the goal here is to find the decision boundary that fits the data best.\nWorking files:\nYou will be restricted to work with 'model.py' and 'Assignment3-Logistic Regression.ipynb'. On running and working through the 'Assignment3-Linear Regression.ipynb', a file named 'hyper_param.json' would be generated.\nData:\nData is already stored in a csv file and provided to you. You are not allowed to change the CSV file in any manner.\nActual Work:\n\nComplete some functions related to logistic regression (in models.py)\n\ncomputing sigmoid - sigmoid\nloss and gradient - loss\npredicting values - assign_predictions\ncalculating accuracy - accuracy\ntraining the model - train\n\n\nTuning various parameters of the models in order to achieve the best results (in Assignment3-Logistic Regression.ipynb)\n\nSteps:\n\nOpen 'Assignment3-Logistic Regression.ipynb' via Jupyter Notebook.\nWork through the 'Assignment3-Logistic Regression.ipynb' and follow the instructions therein.\nYou need to submit the files: 'models.py', 'Assignment3-Logistic Regression.ipynb' and 'hyper_param.json'.\n\nNOTE: We will be testing your results with the help of 'model.py' and the hyperparameters in 'hyper_param.json'.\nHint File:\nThere is also a hint.pdf, that contains the mathematical workout for logistic\nregression's. Try to complete the assignment without looking at it, but do\nconsult it if you get stuck.\nAll the best.\n""], 'url_profile': 'https://github.com/shreyansh26', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 25, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'HTML', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020']}","{'location': 'mumbai,maharastra', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PATELVIMALV1', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 25, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'HTML', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Logistic-Regression\nThe classification goal is to predict whether the client will subscribe (1/0) to a term deposit (variable y).\nThe dataset comes from the UCI Machine Learning repository, and it is related to direct marketing campaigns (phone calls) of a Portuguese banking institution.\nLogistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.\nLogistic Regression Assumptions\n\n\nBinary logistic regression requires the dependent variable to be binary.\n\n\nFor a binary regression, the factor level 1 of the dependent variable should represent the desired outcome.\n\n\nOnly the meaningful variables should be included.\n\n\nThe independent variables should be independent of each other. That is, the model should have little or no multicollinearity.\n\n\nThe independent variables are linearly related to the log odds.\n\n\nLogistic regression requires quite large sample sizes.\n\n\nSMOTE algorithm\nSMOTE algorithm(Synthetic Minority Oversampling Technique) is used for the over sampling purpose.It creates synthetic (not duplicate) samples of the minority class. Hence making the minority class equal to the majority class. SMOTE does this by selecting similar records and altering that record one column at a time by a random amount within the difference to the neighbouring records.At a high level, SMOTE:\n\n\nWorks by creating synthetic samples from the minor class (no-subscription) instead of creating copies.\n\n\nRandomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observations.\n\n\nRecursive Feature Elimination\nRecursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features.\nROC Curve\nROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\n'], 'url_profile': 'https://github.com/Vishal1999-33', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 25, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'HTML', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '161 contributions\n        in the last year', 'description': ['Air_Quality_Index---Hyderabad\nWe are predicting Air Quality Index based on various parameters.\nThis will contain real data from 2013 to 2018\n'], 'url_profile': 'https://github.com/Padhysai', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 25, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'HTML', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020']}","{'location': 'Toronto, CA', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jeremytenjo', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 25, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'HTML', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Linear-Regression-R\nLinear Regression using R\n'], 'url_profile': 'https://github.com/VishalNayak-92', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Feb 25, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 22, 2020', '2', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'HTML', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Machine-Learning\nLinear Regression Project to predict final grades based on First and Second Period Grade, and other factors like absences, studytime and failures.\nFurthermore try to understand what has the most weight or influence to a students grades based on correlation.\n'], 'url_profile': 'https://github.com/joshuapacheco66', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Jan 24, 2020', 'C++', 'Updated Aug 15, 2020', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'HTML', 'Updated Jan 21, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear regression model\n'], 'url_profile': 'https://github.com/RanjithKumar05', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Jan 24, 2020', 'C++', 'Updated Aug 15, 2020', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'HTML', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Bank-Churn-Prediction\nEDA and  Logistic Regression\n'], 'url_profile': 'https://github.com/premsai-K', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Jan 24, 2020', 'C++', 'Updated Aug 15, 2020', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'HTML', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Dolley1997', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Jan 24, 2020', 'C++', 'Updated Aug 15, 2020', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'HTML', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['wind_gp_regression\nROS package for Gaussian process (GP)-based regression of the wind disturbance. The resulting GPs learn the corresponding disturbance forces to improve the tracking accuracy of the controller (NMPC). Three GPs namely gp_disturb_main_x, gp_disturb_main_y, and gp_disturb_main_z have to run simultaneously to perform the regression along the three body-axes. Besides, the disturbance forces are $F^{x_dist}$, $F^{y_dist}$, and $F^{z_dist}$ and they are computed in the body-frame of the aerial robot.\nNote: there are quite a few paramters that can be changed if needed. They are specified in the three launch files: gp_disturb_main_x.launch, gp_disturb_main_y.launch, and gp_disturb_main_z.launch .\nDependency\nlibgp: A Gaussian process regression library.\nThis GP regression package is utilized in the following work. Please don\'t forget to consider citing it if you use these codes in your work.\nPlain text:\nM. Mehndiratta and E. Kayacan, ""Gaussian Process-based Learning Control of Aerial Robots for Precise Visualization of Geological Outcrops,"" 2020 European Control Conference (ECC), Saint Petersburg, Russia, 2020, pp. 10-16.\n\nBibtex:\n@INPROCEEDINGS{9143655,\n  author={M. {Mehndiratta} and E. {Kayacan}},\n  booktitle={2020 European Control Conference (ECC)}, \n  title={Gaussian Process-based Learning Control of Aerial Robots for Precise Visualization of Geological Outcrops}, \n  year={2020},\n  volume={},\n  number={},\n  pages={10-16}\n}\n\n'], 'url_profile': 'https://github.com/mohit004', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Jan 24, 2020', 'C++', 'Updated Aug 15, 2020', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'HTML', 'Updated Jan 21, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '1,559 contributions\n        in the last year', 'description': ['STAT-361\nSTAT-361\nLinear Models\nProjects Listed:\n\nStatistical Analysis of Insurance Coverage Against Social Status (authors: Kevil Khadka, Jialin Xiang, Rafael Pereira)\nHollywoodMovies 2011 (authors: Kevil khadka, Santosh Jnawali, Rizon Giri)\n\nNotes:\n\nR Review by Dr.Weber\nScatterplot Matrix by Dr.Weber\nSimple Linear Regression by Dr.Weber\nMLR Coefficient Interpretation by Dr.Weber\n\n'], 'url_profile': 'https://github.com/kk289', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Jan 24, 2020', 'C++', 'Updated Aug 15, 2020', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'HTML', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['ML\nLinear Regression- multiple variables\n'], 'url_profile': 'https://github.com/Korlambhavya', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Jan 24, 2020', 'C++', 'Updated Aug 15, 2020', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'HTML', 'Updated Jan 21, 2020']}","{'location': 'Kyiv, Ukraine', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/etugoluk', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Jan 24, 2020', 'C++', 'Updated Aug 15, 2020', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'HTML', 'Updated Jan 21, 2020']}","{'location': 'nagpur', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SomyKamble', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Jan 24, 2020', 'C++', 'Updated Aug 15, 2020', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'HTML', 'Updated Jan 21, 2020']}","{'location': 'Anyang, Korea', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['logistic-regression_1\npractice, logistic regression\nA. ""cci.csv"" file\ncomorbidities and 1 year mortality in 100 patients\nB. Covariables\n\npatientID (char3): patients ID\nsex (char1): M0, F1\nagegroup (char2): 0 year = 0, 0-4 year = 1, 5-9 year = 2, 10-14 year = 3, ... over 85 = 18\nDM1 (char1): DM without complications, yes 1, no 0\nDM2 (char1): DM with complications, yes 1, no 0\nPAD (char1): peripheral arterial disease, yes 1, no 0\nCHF (char1): congestive heart failure, yes 1, no 0\nOMI (char1): old myocardial infarction, yes 1, no 0\nCOPD (char1): chronic obstructive pulmonary disease, yes 1, no 0\nliver1 (char1): liver disease, mild, yes 1, no 0\nliver2 (char1): liver disease, severe, yes 1, no 0\nCKD (char1): chronic kidney disease, yes 1, no 0\nCVD (char1) cerebrovascular disease, yes 1, no 0\nrheuma (char1): rheumatologic disease, yes 1, no 0\nulcer (char1): peptic ulcer disease, yes 1, no 0\ndementia (char1): dementia, yes 1, no 0\nmailg (char1): mailgnancy, yes 1, no 0\nmeta (char1): metastatic cancer, yes 1, no 0\nhemi (char1): hemiplegia, paraplegia, yes 1, no 0\nHIV (char1): AIDS, yes 1, no 0\ndeath_1yr (char1): death with in 1 year 1, alive over 1 year 0\n\nC. baseline characteristics table\nD. logistic regression analysis\n'], 'url_profile': 'https://github.com/pcsacred', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Jan 24, 2020', 'C++', 'Updated Aug 15, 2020', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'HTML', 'Updated Jan 21, 2020']}"
"{'location': 'London, UK', 'stats_list': [], 'contributions': '2,668 contributions\n        in the last year', 'description': [""Visual regression testing on the cheap\nI want to whack some visual regression tests on my frontend stuff without\nspending any money or bloating my project. This is my attempt at that.\nDependencies\n\nyarn\ndocker\ndocker-compose\n\nTry it\nyarn install\nyarn test\nOverview\nWe have the usual src and test directories. src contains a components\nsubdirectory, which holds an example component I've borrowed from\nmicrosoft/TypeScript-React-Starter.\nAnd test holds our logic for testing this example component. That is, testing\nthat its appearance doesn't change.\nThere's quite a lot going on in test, but at a high-level what we're doing is:\n\nServing a page from inside a docker container that will render the component on it's own.\nSpinning up Puppeteer inside a docker container and using\nit to screenshot the page we created in the previous step.\nUsing Jest,\nspecifically jest-image-snapshot,\nto compare the screenshots created in the previous step with any existing\nscreenshots.\n\n""], 'url_profile': 'https://github.com/jmackie', 'info_list': ['JavaScript', 'Updated Jun 2, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 24, 2020', '2', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Jan 21, 2020', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'HTML', 'Updated Jan 24, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'jaipur', 'stats_list': [], 'contributions': '269 contributions\n        in the last year', 'description': ['go-regression\nRegression in Go\n'], 'url_profile': 'https://github.com/its-ash', 'info_list': ['JavaScript', 'Updated Jun 2, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 24, 2020', '2', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Jan 21, 2020', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'HTML', 'Updated Jan 24, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Karachi,Pakistan', 'stats_list': [], 'contributions': '882 contributions\n        in the last year', 'description': ['Predicting-houses-prices-algorithm\nregression algorithm example which predicts houses prices\nstack\n\ntensorflow\nkeras\nphython\nboston housing data\nnumpy\n\n'], 'url_profile': 'https://github.com/mshahzaib101', 'info_list': ['JavaScript', 'Updated Jun 2, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 24, 2020', '2', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Jan 21, 2020', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'HTML', 'Updated Jan 24, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Technologiepark Zwijnaarde', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vstorme', 'info_list': ['JavaScript', 'Updated Jun 2, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 24, 2020', '2', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Jan 21, 2020', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'HTML', 'Updated Jan 24, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to data analysis and linear regression in R\nThis repository contains content for a 5-day course for new PHD students (and other interesting people), run within the School of Biological, Earth & Environmental Sciences (BEES) at the University of New South Wales.\nDetails for this session are as follows:\n\nDates:\n\nMonday 03 February to Tuesday 04 February (9.00am to 5.00pm) - Data Manipulation and Visualisation\nMonday 10 and Tuesday 11 February – Introduction to design and analysis + Introduction to linear modelling\n\n\nAudience: New HDR or Hons students in BEES\nVenue: BEES Teaching Lab 3, Ground Floor D26\nWhat to bring: your laptop\nPresenters\n\nDaniel Falster (BEES)\nWill Cornwell (BEES)\nDony Indiarto (BEES)\nEve Slavich (Stats Central)\nGordana Popovic (Stats Central)\n\n\n\nAims & Content\nDay 1 – Introduction to R (for new beginners) [ Will Cornwell ]\nGetting started with R\n\nIntroduction to Rstudio\nIntroduction to coding in R\nGetting data in and out of R - R objects and classes\nPackages\n\nDays 2-3 Project management, data manipulation & data visualisation [ Daniel Falster, Will Cornwell, Dony Indiarto ]\nTopics\n\nProjects: Organising and managing data - Reproducible research with Rmarkdown\nData manipulation & visualisation with the tidyverse\nData manipulation with the tidyverse\nData visualisation with ggplot\n\nLesson plan (Day 1)\n\n\n9:30 Intro (Dan)\n\n\n9:45 Getting organised: Projects, path names, folders (Dan)\n\n\n10:30 Rmd files (Dan)\n\n\n11:00 MORNING TEA\n\n\n11:15 Reading data with readr (Dan)\n\n\n11:45 Data manipulation with dplyr (Dan)\n\nfilter, select, mutate, rename, arrange, summarise,\npipes\n\n\n\n12:30 LUNCH\n\n\n13:30 Imagine your plot (Will)\n\n\n14:30 Intro to data visualisation with ggplot (Dony)\n\n\n15:15 AFTERNOON TEA\n\n\n15:30 Exercises\n\n\nLesson plan (Day 2)\n\n\n9:30 Tidy Data concept (Dony)\n\npivots\n\n\n\n10:00 Advanced data manipulation with dplyr  (Dan)\n\ngroup_by (summarise, mutate),\njoin\n\n\n\n11:00 MORNING TEA\n\n\n11:15 Advanced data visualisation with ggplot (Will)\n\n(extend plots from Day 1 in various ways)\nfacets\nstyles: themes, scales, labels, palettes\nmultiple plot layouts with patchwork\n\n\n\n12:30 LUNCH\n\n\n13:30 Data wrangling & visualisation challenge (Dan)\n\n\n15:15 AFTERNOON TEA\n\n\n15:30 Extensions\n\nggplot in talks (Rose O'Dea)\nggplot extensions (Will)\nReproducible research (Dan)\n\n\n\nDay 3-4 Introduction to design and analysis and  linear modelling [ Eve Slavich and Gordana Popovic]\nIntroduction to statistics\n\nWhich method do you use when? - Statistical inference\nTwo-sample t-test\n\nIntroduction to Experimental design\n\nSample sizes\nTreatments\n\nLinear regression\n\nLinear regression\nEquivalence of two-sample t and linear regression\n\nLinear models\n\nMultiple regression\nAnalysis of variance (and equivalence to multiple regression)\n\nWeirder linear models\n\nBlocked and paired designs - ANCOVA\nFactorial experiments\nInteractions in regression\n\nInstallation instructions\nThe course assumes you have the R software and the development environment RStudio installed on your computer.\nR can be downloaded here.\nThe Desktop version of RStudio can be downloaded here.\n""], 'url_profile': 'https://github.com/nicercode', 'info_list': ['JavaScript', 'Updated Jun 2, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 24, 2020', '2', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Jan 21, 2020', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'HTML', 'Updated Jan 24, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression-Models\nML Regression Models using Python\nThey are used to target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting\nThe various models included are:\nSimple Linear Regression\nMultiple Linear Regression\nPolynomial Regression\nSupport Vector Regression\nDecision Tree Regression\nRandom Forest Regression\n'], 'url_profile': 'https://github.com/SaiSiddhanthGujjari', 'info_list': ['JavaScript', 'Updated Jun 2, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 24, 2020', '2', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Jan 21, 2020', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'HTML', 'Updated Jan 24, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['pyarchical\nhierarchical regression models in python\n'], 'url_profile': 'https://github.com/Jorsorokin', 'info_list': ['JavaScript', 'Updated Jun 2, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 24, 2020', '2', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Jan 21, 2020', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'HTML', 'Updated Jan 24, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Seattle, Washington', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['LinearRegression_Predicting_BostonPropertyValue\nLinear regression on Boston Dataset\n'], 'url_profile': 'https://github.com/Purbaroy', 'info_list': ['JavaScript', 'Updated Jun 2, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 24, 2020', '2', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Jan 21, 2020', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'HTML', 'Updated Jan 24, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '186 contributions\n        in the last year', 'description': ['salary-prediction-LR\nsalary prediction using linear regression\nnormal equation method is used for curve fitting .\nbasics of linear algebra and multivariate calculus\n'], 'url_profile': 'https://github.com/C0deGeek007', 'info_list': ['JavaScript', 'Updated Jun 2, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 24, 2020', '2', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Jan 21, 2020', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'HTML', 'Updated Jan 24, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/iqbalt30', 'info_list': ['JavaScript', 'Updated Jun 2, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 24, 2020', '2', 'R', 'Updated Feb 4, 2020', 'Python', 'Updated Jan 21, 2020', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'HTML', 'Updated Jan 24, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['Minimal Theme\nDemo the Theme\nThis is the raw HTML and styles that are used for the minimal theme on GitHub Pages.\nSyntax highlighting is provided on GitHub Pages by Pygments.\nLicense\nThis work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.\n'], 'url_profile': 'https://github.com/maryclare', 'info_list': ['SAS', 'Updated Aug 20, 2020', 'R', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '2', 'Python', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Sep 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['house-pricing\nHouse Prices: Advanced Regression Techniques\n'], 'url_profile': 'https://github.com/20fguedes', 'info_list': ['SAS', 'Updated Aug 20, 2020', 'R', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '2', 'Python', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Sep 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Hasanraza786', 'info_list': ['SAS', 'Updated Aug 20, 2020', 'R', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '2', 'Python', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Sep 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['logistic_regression\nSpringboard Mini Project: Logistic Regression\n'], 'url_profile': 'https://github.com/scottpenn', 'info_list': ['SAS', 'Updated Aug 20, 2020', 'R', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '2', 'Python', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Sep 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/supremeleader3', 'info_list': ['SAS', 'Updated Aug 20, 2020', 'R', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '2', 'Python', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Sep 17, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Logistic-Regressor\nA barebones Logistic Regression Classifier. Classes are denoted by 0 and 1 respectively.\n'], 'url_profile': 'https://github.com/HKhawaja', 'info_list': ['SAS', 'Updated Aug 20, 2020', 'R', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '2', 'Python', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Sep 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '170 contributions\n        in the last year', 'description': ['linear_regression-\nlinear regression for time series\n'], 'url_profile': 'https://github.com/Anaisdg', 'info_list': ['SAS', 'Updated Aug 20, 2020', 'R', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '2', 'Python', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Sep 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['linear_regression\nSpringboard Mini Project: Linear Regression\n'], 'url_profile': 'https://github.com/scottpenn', 'info_list': ['SAS', 'Updated Aug 20, 2020', 'R', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '2', 'Python', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Sep 17, 2020']}","{'location': 'West Jakarta, DKI Jakarta', 'stats_list': [], 'contributions': '432 contributions\n        in the last year', 'description': ['Classification and Regression Tree Optimized with Artificial Bee Colony Algorithm\nMachine learning with Classification and Regression Trees (CART) and Artificial Bee Colony (ABC) Algorithm for credit scoring\non german credit data\nInstallation (On Windows)\nGo to the project directory and execute\n> install\nThis command will install all required python packages from requirements.txt\nExample Usage\n> python main.py -m cart-abc -v split -f german.txt -s 0.3\n\nAzhary Arliansyah\nReach me on Linkedin\n'], 'url_profile': 'https://github.com/azhry', 'info_list': ['SAS', 'Updated Aug 20, 2020', 'R', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '2', 'Python', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Sep 17, 2020']}","{'location': 'Copenhagen, Denmark', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': [""Salary Prediction\nRepository holds end-to-end regression project, with solution for deployment using PostgreSQL database and Airflow.\nProblem definition\nAn HR company wants to provide a professional advice on salary for its customers. It turns out to be a very valuable information for   both the candidates, that are not sure how much money they should aim for in negotiation process and for companies that don't want to   offer too low salary for an open position, which could discaurage talented candidates, but on the other hand they don't want to overpay  them, as unnecessarily high salaries mean smaller budget for other companies expenses.\nThe company is currently using a simple model, which is average salary per industry and job type, degree and industry, but is looking   for a more accurate solution, as suggesting too low or too high salary for a job position is leading to either not satisfied employees   leaving their employer or not satisfied companies, that eventually finds out that they are paying too much. Result of both cases is   negative review of the HR company, which logically leads to less customers.\nThe final product is expected to be a machine learning model delivered in a form, in which it can be both easily used to predict new   salaries but also can be easily maintained and retrained as the labour market changes and new data becomes available.\nDataset\nDataset - data.csv has 1 000 000 unique rows with 9 columns. Salary is in thousands dollars per year.\n\nExploration Data Analysis\nSalary_predictions-EDA.ipynb contains data exploration. Findings and plots are part of the project presetion. Here is also established a base line, which is the mean salary by job type, degree and industry with\nmean squared error of 743K and\nmean absolute error of 22K.\nThe new model needs to take into account also miles from metropolis and years of experience.\nEven though relationship between each of this numerical features and salary is linear,\nI chose to go for algorithms that can model also non linear relationships such as tree-based models: Decision Trees, Random Forest and Gradient Boosted Trees.\nThe challenge is not in feature selection in this case, but in generating new features, to help the algorithms to see the patterns among categories.\nFor this I used summary statistics of each group, where groups will be created by jobType, degree and industry.\nCompanyID was dropped as it doesn't imply and relationship with the target variable, and the model will be thisway generalized towards any company.\nRegression\nThe modeling part of the project is happening in Salary_predictions-MODELING.ipynb. The dataset notebook is pre-set to work with the data.csv, but for production there is a commented out part in cell with input parameters with details to download and save the data to a Postgresql database alongside with the model, categorical variables encoders, selected features and generated features.\nDeployment\nNootebook Salary_predictions-PREDICTING.ipynb is for predicting salaries of new jobs (if there are any) based on the train model.\nOutput of this notebook is table with predicted salaries. As it is meant for production where new jobs will be added on regular bases,\nonly jobs without predicted salary yet are scored and the ones that already have salaries predicted are not touched.\n\nIt is pre-set to work with unseen_data.csv, but there is again a commented out part of the code for downloading data from a PostgreSQL database.\nAlternatively it is possible to run Salary_predictions-PREDICTING.py from the command line, passing path to the data.\nIf the file is in the current folder run it witht he command:\n__python Salary_predictions-PREDICTING.py unseen_data.csv\nFile salary_predictions_DAG.py is for the deployment with Airflow server. It has task to execute the notebook Salary_predictions-PREDICTING.ipynb, that is scheduled to run every night.\nFile salary_predictions_helper.py contains all the classes and functions used in all three stages, EDA, modeling and predicting.\nThe notebook has a snippet of code, that checks whether there are any new jobs with missing salary at all and stop the execution of the code, if there are not any.\nPresentation\nProject presentation is in the attached powerpoint presentation Salary_predictions-PRESENTATION.pptx, or you can view it directly on github opening file Salary_predictions-PRESENTATION.pdf\n""], 'url_profile': 'https://github.com/IvanaHybenova', 'info_list': ['SAS', 'Updated Aug 20, 2020', 'R', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 26, 2020', '1', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '2', 'Python', 'Updated Jan 26, 2020', '2', 'Jupyter Notebook', 'Updated Sep 17, 2020']}"
"{'location': 'Boston', 'stats_list': [], 'contributions': '144 contributions\n        in the last year', 'description': ['#RandomForest Algorithm is used in the python file for stock prediction accuracy!!\nwhich in all cases is more than 80% which is usually considered good\nTime Series Exploratory Data Analysis (EDA)\nA time series is simply a series of data points ordered in time. In a time series, time is often the independent variable and the goal is usually to make a forecast for the future.\nHowever, there are other aspects that come into play when dealing with time series. Namely:\n\n\nIs it stationary? Stationarity is an important characteristic of time series. A time series is said to be\nstationary if its statistical properties do not change over time. In other words, it has\nconstant mean and variance, and covariance is independent of time. Often, stock prices are not a stationary process, since we might see a growing trend, or\nits volatility might increase over time (meaning that variance is changing). Ideally, we want to have a stationary time series for modelling. Of course, not all of them are stationary, but we can often make different transformations to make them stationary. Dickey-Fuller is the statistical test\nthat we run to determine if a time series is stationary or not.\n\n\nIs the target variable autocorrelated? Autocorrelation is the similarity between observations as a function of the\ntime lag between them\n\n\nIs there a seasonality? Seasonality refers to periodic fluctuations. For example, electricity consumption is high\nduring the day and low during night, or online sales increase during Christmas before slowing down again. seasonality can also be derived from an autocorrelation plot if it has a\nsinusoidal shape. Simply look at the period, and it gives the length of the season\n\n\nFirst, we import libraries that will be helpful throughout our analysis.\nThen, we import a dataset.\nTrends are easier to spot now. Notice how the 30-day and 90-day trend show a\ndownward curve at the end. This might mean that the stock is likely to go down in the\nfollowing days.\nExponential smoothing\nExponential smoothing uses a similar logic to moving average, but this time, a different\ndecreasing weight is assigned to each observations. In other words, less importance is\ngiven to observations as we move further from the present (very old observations become less important).\nDouble exponential smoothing is used when there is a trend in the time series. In that\ncase, we use this technique, which is simply a recursive use of exponential smoothing\ntwice.\nTriple exponential smoothing extends double exponential smoothing, by adding a seasonal smoothing\nfactor. Of course, this is useful if you notice seasonality in your time series.\n$\\alpha$ is a smoothing factor that takes values between 0 and 1. It determines how\nfast the weight decreases for previous observations.\n$\\beta$ is the trend smoothing factor, and it takes values between 0 and 1.\n$\\gamma$ is the seasonal smoothing factor and L is the length of the season.\nLet\'s do exponential smoothing and use 0.05 and 0.3 as values for the smoothing factor.\nHere we show an example of how well Scikit-learn and pandas integrate to do Machine Learning (Scikit-learn) from data (pandas).\nBut random forests are so cool. In opinion, the most generally successful algorithm in ML. Invented in the 1950s (as a decision tree algorithm) and optimized using ensembling in modern libraries like Scikit-Learn. Just be amazed ;-)\nPandas and matplotlib is used for the stock predication in the ""stock prediction.ipynb"" file\nStudy of the stationarity of three stocks from three different industries.\n\n     Ploting the closing prices, model them with either simple or double exponential smoothing, and test to see if they\'re stationary.\n\n     Using a random forest from Scikit-learn to predict stock prices for your three chosen stocks, and see how far into the future you can go.\n\n---""Stock prediction in python"" file contains the jupyter notebook file for the program\nYahoo Finance data is used for stock data extraction\n3 industries are used:-\nTesla - Car manufacturing Company\nBitcoin - Cryptocurrency\nReliance - conglomerate Company\n\nThe data files for stock containing the\nOpen, Close, High, Low, Adj Close are the columns\ntesla.csv\nreliance.csv\nbtc.csv\nThe accuracy predication for all the models is more than 70 percent!!\n'], 'url_profile': 'https://github.com/abhishekdabas31', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', '1', 'Python', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/collabTester18', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', '1', 'Python', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Jan 20, 2020']}","{'location': 'Bari, Italy', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['RegressioneMultipla\n'], 'url_profile': 'https://github.com/giuseppemaiorano', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', '1', 'Python', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '158 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ayjabri', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', '1', 'Python', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Jan 20, 2020']}","{'location': 'Arizona', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['machine-learning-challenge\nIn this Machine Learning challenge, I used the Kepler Exoplanet Data from: Kepler Exoplanet Data. I tried 7 different machine learning models and a few variations of different variables within those models, along with train & test samples, to try to predict the koi_disposition of whether the object is confirmed as a planet, a candidate for a planet, or false-positive for a planet. The machine learning models I chose plus the accuracy they returned were: \nThe best model is Random Forest Classifier in the ""machine learning files"" folder, titled ""ML Random Forest Classifier.ipynb.""\nThe file MachineLearningModelRatings.xlsx in the ""machine learning files"" folder includes a Pivot Chart with the bar graph displayed above.\nDetails: These independent variables gave the best results for the different combinations I tried: \nkoi_score \nkoi_period \nkoi_time0bk \nkoi_impact \nkoi_duration \nkoi_depth \nkoi_prad \nkoi_teq \nkoi_insol \nkoi_steff \nkoi_slogg \nkoi_srad \nra \ndec \nkoi_kepmag\nIf you would like to install these files, you will need Jupyter Notebook to open the\nipynb files, which contain all of the machine learning models in Python code.\n'], 'url_profile': 'https://github.com/dianess', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', '1', 'Python', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Bhagesh-Bhutani', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', '1', 'Python', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Jan 20, 2020']}","{'location': 'Mountain View, CA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Py/Invoke Regression Test and Search\nSetup\n\nClone py-invoke into repos/invoke.\nInstall python 3.8\n\n\nThis script was written to work on Windows, and will need to be modified slightly to work on Linux/OSX.\nIt also assumes Python 3.8, so may need changes to work with 3.7 and earlier.\n\nRegression Overview\nThis regression generates 26 tasks and attempts to invoke them with\ninvoke a b c d ... z`\n\nThis scenario exacerbates the invoke error. In order to detect that the invoke command has succeeded, each command logs its name (e.g. 'a') to an output file. For this test to succed, the last line in the file should be 'z'.\nThe test outputs the last line from the file for each run, and a pass '.' or fail 'F' flag. When failing, the output will look like:\n[0] c F\n[1] d F\n[2] q F\n[3] b F\n[4] l F\nFailures: 5/5\n\nWhen passing, all iterations will end with 'z' and have the pass flag '.'.\n[0] z .\n[1] z .\n[2] z .\n[3] z .\n[4] z .\nFailures: 0/5\n\nRunning the regression test for Invoke\nTo run the scan in two parts using pre-discovered commits, use the batch file:\nrun_regression.bat\n\nTo run the invoke-chain regression against the currently checked out commit:\npython tests\\test_invoke_chain.py\n\nRunning generic regression searches (like bisect)\nIf a test is failing at the most recent commit, and you want to know when it last worked, run the regress script\nwith --stop-on-pass. You can speed up the search by skipping commits, with --skip N:\npython regress.py --commit HEAD -n 100 --repo repos/invoke --command test\\REGRESSION_TEST.py --skip 10 --stop-on-pass\n\nOnce you have a last-known-good (LKG) and a previous failure (PRV), search one at a time between them and stop as soon\nas the test passes. Then, the culprit will be the commit immediately after the PASS.\npython regress.py --commit PRV -n 100 --repo repos/invoke --command test\\REGRESSION_TEST.py --stop-on-pass\n\n""], 'url_profile': 'https://github.com/cod3monk3y', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', '1', 'Python', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/AlgoML-Devs', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', '1', 'Python', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Jan 20, 2020']}","{'location': 'Kiev', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tsergien', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', '1', 'Python', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Jan 20, 2020']}","{'location': 'Kavala, Greece', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Crowdfunding Prediction in Python 🐍\nby using Linear Regression Model\n❓ About\n\nΑναγνώριση Προτύπων σε Python & Matlab\n\n🔨 Requirements\nPython IDE\nNumpy\nPandas\nSKlearn\n🔧 Install Comamands\npip install numpy\npip install pandas\npip install sklearn\n\n'], 'url_profile': 'https://github.com/Fotic', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Jupyter Notebook', 'Updated Feb 12, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', '1', 'Python', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 24, 2020', 'Python', 'Apache-2.0 license', 'Updated Jan 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '175 contributions\n        in the last year', 'description': ['Linear-Regression-Model\nMultiple Linear Regression model using Backward Elimination\n'], 'url_profile': 'https://github.com/Prerna99-star', 'info_list': ['Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'Shell', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Shell', 'Updated Jan 22, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Ploynomial-Regression-using-R\nPredicting salary but using polynomial regression\nThe CSV file must be included in the download folder or else, you can can give the URL of the data set.\nThe file is python notebook file, the language is R, so you will be required to setup an R environment before using it.\nHappy MAchine Learning!!\n'], 'url_profile': 'https://github.com/Premm98', 'info_list': ['Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'Shell', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Shell', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['ml_project\nBreast cancer project using Logistic Regression\n'], 'url_profile': 'https://github.com/rcuberrr', 'info_list': ['Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'Shell', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Shell', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Gerr4ard', 'info_list': ['Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'Shell', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Shell', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Vijayakeerthi', 'info_list': ['Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'Shell', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Shell', 'Updated Jan 22, 2020']}","{'location': 'Tempe, AZ', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['boost-test\nFiles for running boost regression tests\nTo run tests:\npython run_tests.py\n'], 'url_profile': 'https://github.com/jeremy7nelson', 'info_list': ['Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'Shell', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Shell', 'Updated Jan 22, 2020']}","{'location': 'Barcelona', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chus-chus', 'info_list': ['Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'Shell', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Shell', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['courses-causal-inference-with-r-modeling-with-regression\n'], 'url_profile': 'https://github.com/ModUDuke', 'info_list': ['Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'Shell', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Shell', 'Updated Jan 22, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '434 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SANJAY072000', 'info_list': ['Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'Shell', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Shell', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['Perf-regression\nrepo for all the CC regression suites\nCreate new branch for each product seperately\n'], 'url_profile': 'https://github.com/prasanth247', 'info_list': ['Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'Shell', 'Updated Apr 30, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Shell', 'Updated Jan 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '1,233 contributions\n        in the last year', 'description': ['linearregression\nLinear Regression in Machine Learning (OCTAVE)\n'], 'url_profile': 'https://github.com/rajkothari634', 'info_list': ['MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Python', 'Updated Jan 25, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'Stony Brook', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shubham12595', 'info_list': ['MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Python', 'Updated Jan 25, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Stock-Prediction\nPredicting Stock Price using Linear Regression\nRecently, a lot of interesting work has been done in the area of applying Machine\nLearning Algorithms for analyzing price patterns and predicting stock prices and index\nchanges. Most stock traders nowadays depend on Intelligent Trading Systems which help\nthem in predicting prices based on various situations and conditions, thereby helping\nthem in making instantaneous investment decisions.\nStock Prices are considered to be very dynamic and susceptible to quick changes because\nof the underlying nature of the financial domain and in part because of the mix of known\nparameters (Previous Days Closing Price, P/E Ratio etc.) and unknown factors (like\nElection Results, Rumors etc.)\nAn intelligent trader would predict the stock price and buy a stock before the price rises,\nor sell it before its value declines. Though it is very hard to replace the expertise that an\nexperienced trader has gained, an accurate prediction algorithm can directly result into\nhigh profits for investment firms, indicating a direct relationship between the accuracy of\nthe prediction algorithm and the profit made from using the algorithm.\n'], 'url_profile': 'https://github.com/ashdriod', 'info_list': ['MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Python', 'Updated Jan 25, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['DIT_forecasting\nInitial code for performing regression/forecasting models.\n'], 'url_profile': 'https://github.com/joshicha', 'info_list': ['MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Python', 'Updated Jan 25, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'Paris - France', 'stats_list': [], 'contributions': '547 contributions\n        in the last year', 'description': ['Optimization-for-Data-Science\nOptimization methods for lasso penalized logistic regression.\n'], 'url_profile': 'https://github.com/Niangmohamed', 'info_list': ['MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Python', 'Updated Jan 25, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '175 contributions\n        in the last year', 'description': ['Salar-vs-Experience-year-SLR\nSimple Linear Regression on given data\n'], 'url_profile': 'https://github.com/Prerna99-star', 'info_list': ['MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Python', 'Updated Jan 25, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rterkesli', 'info_list': ['MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Python', 'Updated Jan 25, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'Delhi,India', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/redrivals', 'info_list': ['MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Python', 'Updated Jan 25, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['This project provides an implementation of logistic regression algorithm. It supports an arbitary number of feature inputs and accepts un-encoded class labels.\n\nLogReg.py\n\n\nThe constructor __init__() takes parameter self that will be an instatnce of the LogReg class. The X and y parameters will store the training data and function, and NLL() calculates and returns the negative log-likelihood score for a proposed model.\n\n\nMethod predict_proba() calculates and returns 1D array with the predicted probabilities on a scale from 0 to 1 and where the number of entries equal to the number of rows in X.\n\n\nMethod predict() returns the predicted class with the threshold being equal to 0.5.\n\n\nMethod score() calculates and returns the training accuracy.\n\n\nMethod summary() prints out the message with number of observations, best coeffecient estimates, Log-Likelihood, and accuracy.\n\n\nMethod precision_recall() calculates and prints out the calculated precisions and recall to tell us more about the model.\n\n\n\nTesting.ipynb\n\nThis jupyter notebook provides some examples with three different datasets.\n\n\nClassificationPlotter.py\nThis python file helps to visualize the data points for the first dataset.\n'], 'url_profile': 'https://github.com/alisherAbdullaev', 'info_list': ['MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Python', 'Updated Jan 25, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}","{'location': 'France, Valbonne', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['SimpleLinearRegression\nLinear regression from stratch using only Numpy\nRequired libraries\n\nNumpy\nMatplotlib\nSci-kit learn\n\n'], 'url_profile': 'https://github.com/saundersp', 'info_list': ['MATLAB', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Python', 'Updated Jan 25, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Feb 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['regression_testing\nExamples and script for DeepSec regression testing\n'], 'url_profile': 'https://github.com/DeepSec-prover', 'info_list': ['OCaml', 'GPL-3.0 license', 'Updated Jul 6, 2020', 'Java', 'Updated Apr 23, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'Updated Jun 8, 2020', 'JavaScript', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/whennemuth', 'info_list': ['OCaml', 'GPL-3.0 license', 'Updated Jul 6, 2020', 'Java', 'Updated Apr 23, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'Updated Jun 8, 2020', 'JavaScript', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Turkey', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['RegressionTheoryAndMethods\nRegression Theory and Methods course materials\nBasically there are 3 Homeworks in this repository.\n'], 'url_profile': 'https://github.com/oltuluorcun', 'info_list': ['OCaml', 'GPL-3.0 license', 'Updated Jul 6, 2020', 'Java', 'Updated Apr 23, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'Updated Jun 8, 2020', 'JavaScript', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Richmond Va', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GoldinLocks', 'info_list': ['OCaml', 'GPL-3.0 license', 'Updated Jul 6, 2020', 'Java', 'Updated Apr 23, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'Updated Jun 8, 2020', 'JavaScript', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '520 contributions\n        in the last year', 'description': ['Machine-Learning-Bootcamp @ 42\nDay 0 & Day 1: Linear Regression\nDay 2: Logistic Regression\nUse of numpy, matrix / vector calculations\nUnderstand and code sklearn basic features\nRessource: https://www.coursera.org/learn/machine-learning (course by Andrew NG on Coursera)\n'], 'url_profile': 'https://github.com/Grim22', 'info_list': ['OCaml', 'GPL-3.0 license', 'Updated Jul 6, 2020', 'Java', 'Updated Apr 23, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'Updated Jun 8, 2020', 'JavaScript', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['cancelPayout_regressionTesting\nSteps 1) npm install\nSteps 2) go to RegressionTesting Directory and run mocha MainScript.js\nFor mocha reports\ninstall : npm i -g mochawesome\nmocha test.js --reporter mochawesome\n'], 'url_profile': 'https://github.com/abhi9098', 'info_list': ['OCaml', 'GPL-3.0 license', 'Updated Jul 6, 2020', 'Java', 'Updated Apr 23, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'Updated Jun 8, 2020', 'JavaScript', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Bari, Italy', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['RegressioneSempliceCase\n'], 'url_profile': 'https://github.com/giuseppemaiorano', 'info_list': ['OCaml', 'GPL-3.0 license', 'Updated Jul 6, 2020', 'Java', 'Updated Apr 23, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'Updated Jun 8, 2020', 'JavaScript', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Geneva, Switzerland', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Valink16', 'info_list': ['OCaml', 'GPL-3.0 license', 'Updated Jul 6, 2020', 'Java', 'Updated Apr 23, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'Updated Jun 8, 2020', 'JavaScript', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Decision-Tree-Regression-implementation-from-scratch\nI have implemented a decision tree regression algorithm on a univariate dataset, which contains 272 data points about the duration of the eruption and waiting time between eruptions for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA (https://www.yellowstonepark.com/thingsto-do/about-old-faithful), in the file named dataset.csv.\nThe algorithm was implemented using the pre-pruning rule, such that if a node has 𝑃 or fewer data points, it is converted into a terminal node. Using pre-pruning rule is one of the methods to prevent over-fitting.\nThe following indexing rule was used (since Python starts from index 0) and all data structures were made consistent with this:\nFor left node: 2parent + 1   (instead of 2parent)\nFor right node: 2parent + 2 (instead of 2 parent + 1)\nDuring the learning process, at each step we generate all possible split positions and then pick the best one based on the score function given below:\n\nBased on the selected split, then we generate the left node and the right node. The algorithm goes on until there are no nodes to split or we reach the P value for remaining nodes.\nThe decision tree is visualized for P=25 as shown below:\n\ny_predicted is calculated for X_test and the RMSE is calculated to compare y_predicted and y_test. The error was ~6.45 when P=25.\nThe algorithm was then run on varying P values and the corresponding RMSE were compared. We can see from the following graph that RMSE is minimal when P is between 30 and 40.\n\n'], 'url_profile': 'https://github.com/gizemtanriver', 'info_list': ['OCaml', 'GPL-3.0 license', 'Updated Jul 6, 2020', 'Java', 'Updated Apr 23, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'Updated Jun 8, 2020', 'JavaScript', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Albion, Mauritius', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/FANHATCHA', 'info_list': ['OCaml', 'GPL-3.0 license', 'Updated Jul 6, 2020', 'Java', 'Updated Apr 23, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Python', 'Updated Jun 8, 2020', 'JavaScript', 'Updated Oct 6, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated Jan 21, 2020']}"
"{'location': 'New York, NY', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Model Zoo\nLightweight, out-of-the-box probabilistic regression models in PyTorch.\nInstallation\ngit clone https://github.com/samuelstanton/model-zoo.git\ncd model-zoo\npip install -e .\n\nReference\n@misc{Model Zoo,\n  author = {Stanton, Samuel},\n  title = {Out-of-the-box Probabilistic Models in PyTorch},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/samuelstanton/model-zoo}},\n}\n\n'], 'url_profile': 'https://github.com/samuelstanton', 'info_list': ['Python', 'MIT license', 'Updated May 1, 2020', 'Java', 'Updated Jan 21, 2020', '2', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 22, 2020', '6', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Java', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'MATLAB', 'MIT license', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Lunavath', 'info_list': ['Python', 'MIT license', 'Updated May 1, 2020', 'Java', 'Updated Jan 21, 2020', '2', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 22, 2020', '6', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Java', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'MATLAB', 'MIT license', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Linear-Regression-from-scratch-sklearn-and-ANN\nIn this notebook, we implemented linear regression model from scratch. To ensure that the model works well, we used scikit learn library to implement the same model. We went ahead and built an artificial neural network using keras. The data that was used here is the famous diabetes data.\n'], 'url_profile': 'https://github.com/fneema', 'info_list': ['Python', 'MIT license', 'Updated May 1, 2020', 'Java', 'Updated Jan 21, 2020', '2', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 22, 2020', '6', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Java', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'MATLAB', 'MIT license', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sselvi', 'info_list': ['Python', 'MIT license', 'Updated May 1, 2020', 'Java', 'Updated Jan 21, 2020', '2', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 22, 2020', '6', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Java', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'MATLAB', 'MIT license', 'Updated Feb 6, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['Machine-Learning\nMy Machine Learning projects\nPython3\n'], 'url_profile': 'https://github.com/Vikhyat2603', 'info_list': ['Python', 'MIT license', 'Updated May 1, 2020', 'Java', 'Updated Jan 21, 2020', '2', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 22, 2020', '6', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Java', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'MATLAB', 'MIT license', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Data Set Information:\nThe dataset consists of feature vectors belonging to 12,330 sessions. The dataset was formed so that each session would belong to a different user in a 1-year period to avoid any tendency to a specific campaign, special day, user profile, or period.\nAttribute Information:\nThe dataset consists of 10 numerical and 8 categorical attributes. The \'Revenue\' attribute can be used as the class label.\n""Administrative"", ""Administrative Duration"", ""Informational"", ""Informational Duration"", ""Product Related"" and ""Product Related Duration"" represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another. The ""Bounce Rate"", ""Exit Rate"" and ""Page Value"" features represent the metrics measured by ""Google Analytics"" for each page in the e-commerce site. The value of ""Bounce Rate"" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave (""bounce"") without triggering any other requests to the analytics server during that session. The value of ""Exit Rate"" feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session. The ""Page Value"" feature represents the average value for a web page that a user visited before completing an e-commerce transaction. The ""Special Day"" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine\'s Day) in which the sessions are more likely to be finalized with transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8. The dataset also includes operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.\nReference: https://www.kaggle.com/roshansharma/online-shoppers-intention\n'], 'url_profile': 'https://github.com/pradeepmuni', 'info_list': ['Python', 'MIT license', 'Updated May 1, 2020', 'Java', 'Updated Jan 21, 2020', '2', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 22, 2020', '6', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Java', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'MATLAB', 'MIT license', 'Updated Feb 6, 2020']}","{'location': 'Indore', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sidv1905', 'info_list': ['Python', 'MIT license', 'Updated May 1, 2020', 'Java', 'Updated Jan 21, 2020', '2', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 22, 2020', '6', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Java', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'MATLAB', 'MIT license', 'Updated Feb 6, 2020']}","{'location': 'Brazil', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['regressionlab\nRegression trends (exponential, logarithm, polynomial and power) laboratory project.\nOriginal code from:\nhttps://stackoverflow.com/questions/17592139/trend-lines-regression-curve-fitting-java-library\nSee the following examples of regression trends.\nExcel: https://github.com/gjportella/regressionlab/blob/master/source/src/test/resources/excel/regression.xlsx\nJava: https://github.com/gjportella/regressionlab/blob/master/source/src/test/java/br/unb/cic/laico/regressionlab/AppTest.java\n'], 'url_profile': 'https://github.com/gjportella', 'info_list': ['Python', 'MIT license', 'Updated May 1, 2020', 'Java', 'Updated Jan 21, 2020', '2', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 22, 2020', '6', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Java', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'MATLAB', 'MIT license', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['AttritionPrediction\nThe project is about the attrition prediction using logistic regression.\nYou will require python 3.7 and flask api.\n'], 'url_profile': 'https://github.com/abhishek16101998', 'info_list': ['Python', 'MIT license', 'Updated May 1, 2020', 'Java', 'Updated Jan 21, 2020', '2', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 22, 2020', '6', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Java', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'MATLAB', 'MIT license', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NNK Graph\nMatlab source code for the paper: \nGraph Construction from Data using Non Negative Kernel regression (NNK Graphs).\nTo be presented at ICASSP 2020.\nCiting this work\n@article{shekkizhar2020graph,\n    title={Graph Construction from Data by Non-Negative Kernel regression},\n    author={Sarath Shekkizhar and Antonio Ortega},\n    year={2020},\n    booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, \n}\n\n@misc{shekkizhar2019graph,\n    title={Graph Construction from Data using Non Negative Kernel regression (NNK Graphs)},\n    author={Sarath Shekkizhar and Antonio Ortega},\n    year={2019},\n    eprint={1910.09383},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n\n'], 'url_profile': 'https://github.com/STAC-USC', 'info_list': ['Python', 'MIT license', 'Updated May 1, 2020', 'Java', 'Updated Jan 21, 2020', '2', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 22, 2020', '6', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Java', 'GPL-3.0 license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'MATLAB', 'MIT license', 'Updated Feb 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['RF-and-XGBoost_Comparison\nJupyter Notebook exploring the Random Forest and XGBoost regression algorithms. Includes data import, cleaning, transformation, and basic exploration prior to training/test splits and grid searches for hyperparameters of models.\nStill a work in a progress, the notebook builds two regression models (RF and XGBoost) attempting to predict the change per hour of San Francisco bikeshare stations. The idea is that the bikeshare company could use such a model to anticipate under/over supply of bikes at all their stations given the time of day and weather that day.\nWill be updated as the models are basic and additional refinement and/or data engineering possible.\n'], 'url_profile': 'https://github.com/russhowd', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Python', 'Updated Jan 20, 2020', 'R', 'Updated Apr 19, 2020', 'Python', 'Updated Jan 21, 2020', 'R', 'Updated Feb 18, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,024 contributions\n        in the last year', 'description': ['ESM 244 (Winter 2020) Lab 3\n\nBinary logistic regression\n(Re)-introduction to spatial data with sf\n\n'], 'url_profile': 'https://github.com/allisonhorst', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Python', 'Updated Jan 20, 2020', 'R', 'Updated Apr 19, 2020', 'Python', 'Updated Jan 21, 2020', 'R', 'Updated Feb 18, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rterkesli', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Python', 'Updated Jan 20, 2020', 'R', 'Updated Apr 19, 2020', 'Python', 'Updated Jan 21, 2020', 'R', 'Updated Feb 18, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'China Shanghai', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': [""Maximization-Of-Loan-Profit----Logistic-Regression\nKey Words:\nLogistic Regression / Customer Information / Return Matrix / Model Applying\nDescription:\nWhen customers went to a bank for loan business, bank will evalueate the payment ability of customers, which is the important reference for making decision that whether to lend money to customers. If the evalueation system shows that a customer having a good income condation, bank will lend money to her or him, otherwise the loan aplication will be reject. But sometimes the evalueation system could make some mistake, such as mark a high score for those haven't the payment ability, which would bring lost for bank. So in order to reduce the probability of these unexpectable consenqunse to zero limitly, bank must find a perfect and precise model, which is also our project goal. So we will create a better model using the customer loaning history, with the asistance from sklearn-package and python, to make the profit of bank loan business maximized.\n""], 'url_profile': 'https://github.com/StrangeData-v', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Python', 'Updated Jan 20, 2020', 'R', 'Updated Apr 19, 2020', 'Python', 'Updated Jan 21, 2020', 'R', 'Updated Feb 18, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Madrid, Spain', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['sorted_stratification\nWhen performing a grid search on sklearn, we have several mechanisms for classification problems to stratify the sample. However, there are not too many utilities for regression problems. That is why I have developed a method called Sorted stratification inspired by the following article link that allows combining this technique with the traditional K-fold method of sklearn.\nMethod definition\n""Let N denote the number of samples, y the target variable for the samples, and k the number of equally sized partitions we wish to create.\nWith sorted stratification, we first sort the samples based on their target variable, y. Then we step through each consecutive k samples in this order and randomly allocate exactly one of them to one of the partitions. We continue this floor(N/k) times, and the remaining mod(N,k) samples are randomly allocated to one of the k partitions.""\n'], 'url_profile': 'https://github.com/ARomoH', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Python', 'Updated Jan 20, 2020', 'R', 'Updated Apr 19, 2020', 'Python', 'Updated Jan 21, 2020', 'R', 'Updated Feb 18, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JasonChan07', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Python', 'Updated Jan 20, 2020', 'R', 'Updated Apr 19, 2020', 'Python', 'Updated Jan 21, 2020', 'R', 'Updated Feb 18, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['How to Make Movies (and Get Rich Trying!)\nPredicting box office revenue.\nFor this project, I scraped data from the IMDb pages for the top 100 grossing movies for each year from 2009 to 2018 and then analyzed things that might be predictive of box office revenue.\nFiles in this repository include:\nscraper.py: This file contains the code to scrape the IMDb pages for each movie for potential features.  After scraping is completed and the data collected is stored in a dataframe, it will save the dataframe to a picklefile called movie_dataframe.pkl in the present working directory.\nanalysis.py: This file performs analysis on the data and outputs visuals to represent the data analyzed.\n'], 'url_profile': 'https://github.com/Trento89', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Python', 'Updated Jan 20, 2020', 'R', 'Updated Apr 19, 2020', 'Python', 'Updated Jan 21, 2020', 'R', 'Updated Feb 18, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Washington, DC', 'stats_list': [], 'contributions': '135 contributions\n        in the last year', 'description': ['R_7_regression_models\nan array of regression models, distributions, and diagnostic techniques\n'], 'url_profile': 'https://github.com/dhruvsingh14', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Python', 'Updated Jan 20, 2020', 'R', 'Updated Apr 19, 2020', 'Python', 'Updated Jan 21, 2020', 'R', 'Updated Feb 18, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': [""ft_linear_regression\nThis project is the first data project of the 42 school's cursus.\nThe goal is to implement a program to predict the price of a used car depending on its mileage.\nThe values of the program are initialized to 0, and we have to train the program using a linear regression to predict the price of the car. In order to calculate the coefficients, we have to use a gradient descent algorithm. This repo contains two different programs : the first one processes the training and the second one estimates the price.\nRequirements\n\nmatplotlib.pyplot\nnumpy\npandas\nargparse\nprogressbar2\n\nUsage\nYou can run the training program with python3 train.py ./path_to_the_file. \nTo plot the data, the line and the evolution of the coefficients during the training, use --plot.\nThe coefficients will be stored in the config.json file which will be used by the prediction program.\nTo reset the program, you can use the flag --reset without specifying any other option.\nOnce the training is done, you can then run the prediction program with python3 predict.py.\nIt will ask you for the mileage of your car, and return its estimated value. To quit the program, just type quit or exit in the prompt.\nPlease note that the plotting option only works if you specify a path for the training.\n\n""], 'url_profile': 'https://github.com/gab959', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Python', 'Updated Jan 20, 2020', 'R', 'Updated Apr 19, 2020', 'Python', 'Updated Jan 21, 2020', 'R', 'Updated Feb 18, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Credit-Card-Fraud-Detection\nA logistic regression model to detect fraud in transaction data\nThe following techniques are used:\n\n\ncross-validation\n\n\nconfusion matrix\n\n\nundersampling\n\n\nSMOTE: Synthetic Minority Over-sampling Technique\n\n\n'], 'url_profile': 'https://github.com/yxzhu16', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 17, 2020', '1', 'Python', 'Updated Jan 20, 2020', 'R', 'Updated Apr 19, 2020', 'Python', 'Updated Jan 21, 2020', 'R', 'Updated Feb 18, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['pyregress\n'], 'url_profile': 'https://github.com/justbennet', 'info_list': ['Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', 'HTML', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Munch1987', 'info_list': ['Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', 'HTML', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'Delhi,India', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/redrivals', 'info_list': ['Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', 'HTML', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/adi19891', 'info_list': ['Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', 'HTML', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ssanti17', 'info_list': ['Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', 'HTML', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Student-Performanc-Machine-Learning\n'], 'url_profile': 'https://github.com/Wonder13oy', 'info_list': ['Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', 'HTML', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'Washington,DC', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['Cardiovascular Disease Analysis\nAnalyzed data using chi square test and logistic regression.\nBREIF DESCRIPTION:\nAnalyzed WHO MONICA(Multinational Monitoring of Trends and Determinants in Cardiovascular Disease) dataset to find demographic and physiological indicators associated with the outcome of mortality from cardiovascular disease.\nPerformed univariate and multivariate analysis using chi-square test to find the strategically significant variables and plotted graphs  to visualize the output that was generated.\nPerformed logistic regression to find the strength of the association between the predictor factors such as high cholesterol, smoking status, high blood pressure and previous myocardial infraction to predict the outcome(dead or live).\nTOP FINDINGS:\n\nHospitalization status was the strongest predictor of cardiovascular mortality.\nKNN method outperforms the multivariable logistic regression model, which produced McFadden R2 values in the moderate range of 55-65%.\nThe Decision Tree method produced the most accurate predictions of the outcome dead for men (77%), wheareas Random Forest produced the most accurate predicitons of the outcome dead for women (78.6%).\n\n'], 'url_profile': 'https://github.com/anweshatomar', 'info_list': ['Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', 'HTML', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '163 contributions\n        in the last year', 'description': ['ML_MODEL_SOP_FOREIGN_OWNERSHIP\nREGRESSION IN ML MODEL\nFINDING COEFFICIENTS FOR THREE EXTENT OF FOREIGN OWNERSHIP AND EXPORT SALES\nUsed net sales as a proxy for export sales\nMissing data found by taking mean approximation\nDependent variable is net sales\nIndependent variables are inventory, %of current assets, net profit, employee costs, % of net sales and total assets\nOLS regression of type multiple linear regression used :)\n'], 'url_profile': 'https://github.com/bhavyagera10', 'info_list': ['Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', 'HTML', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'Netherlands', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/girish208', 'info_list': ['Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', 'HTML', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chandan115325', 'info_list': ['Updated May 8, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', 'HTML', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated May 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['visual_lineup\nPOC for running visual regression tests with lineup ruby gem\n'], 'url_profile': 'https://github.com/cicloswgit', 'info_list': ['Ruby', 'Updated Jan 24, 2020', 'Gnuplot', 'Updated Feb 27, 2021', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 8, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020']}","{'location': 'Austin, TX, United States', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['SU2 Regression Tests\nThese regression tests are intended to benchmark SU2 behavior at various\ncommits.  The current continuous integration system only checks the\ntransient behavior.  Because of this, non-trivial changes to the\nnumerics can cause the CI tests to fail.  Nevertheless, changes to the\ntransient solution do not always produce significant changes in the\nconverged solution. These tests allow a closer look. Both the\nconvergence history and the converged solution are compared for a few\nsimple cases.\nNote that these tests are not intended to be a proper verification or\nvalidation of the SU2 code.  They are intended to show how the\nconvergence and converged solution differ between two versions of SU2,\nas a quick check.\n'], 'url_profile': 'https://github.com/clarkpede', 'info_list': ['Ruby', 'Updated Jan 24, 2020', 'Gnuplot', 'Updated Feb 27, 2021', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 8, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020']}","{'location': 'Dublin, Ireland', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['machine-Learning-model-evaluation\nPerformance evaluation of Regression and classification models using R\n'], 'url_profile': 'https://github.com/vinaysheelwagh', 'info_list': ['Ruby', 'Updated Jan 24, 2020', 'Gnuplot', 'Updated Feb 27, 2021', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 8, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DS-Sagar', 'info_list': ['Ruby', 'Updated Jan 24, 2020', 'Gnuplot', 'Updated Feb 27, 2021', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 8, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': [""Project Overview\nSpam Vs Non-Spam\nIn the project , we classify Spam mails in the given dataset.  This is achieved using Logistic Regression model . Initiallly a baseline model is created containing all the features. Using correlation matrix , the highly correlated features are removed and then a second model with lesser number of features is modelled. Similarly , Chi square and Annova model are used to furthure refine the features to achieve better accuracy. Finally PCA is applied to the dataset for feature extraction for better test accuracy.\nAbout the dataset:\n•\tNumber of Instances: 4601 (1813 Spam = 39.4%)\n•\tNumber of Attributes: 58 (57 continuous, 1 nominal class label)\n•\tAttribute Information:\no\tThe last column of 'spambase.data' denotes whether the e-mail was considered spam (1) or not (0)\n\no\t48 attributes are continuous real [0,100] numbers of type word freq WORD i.e. percentage of words in the e-mail that match WORD\n\no\t6 attributes are continuous real [0,100] numbers of type char freq CHAR i.e. percentage of characters in the e-mail that match CHAR\n\no\t1 attribute is continuous real [1,…] numbers of type capital run length average i.e. average length of uninterrupted sequences of capital letters\n\no\t1 attribute is continuous integer [1,…] numbers of type capital run length longest i.e. length of longest uninterrupted sequence of capital letters\n\no\t1 attribute is continuous integer [1,…] numbers of type capital run length total i.e. sum of length of uninterrupted sequences of capital letters in the email\n\no\t1 attribute is nominal {0,1} class of type spam i.e denotes whether the e-mail was considered spam (1) or not (0)\n\n•\tMissing Attribute Values: None\n•\tClass Distribution: Spam 1813 (39.4%) Non-Spam 2788 (60.6%)\nFollowing concepts have been implemented in this project:\n•\tLogistic Regression\n•\tCorrelation Matrix\n•\tClassification Report\n•\tConfusion Matrix\n•\tChi square test\n•\tAnnova model\n•\tPCA model\n•\tSelectKBest\n""], 'url_profile': 'https://github.com/nish700', 'info_list': ['Ruby', 'Updated Jan 24, 2020', 'Gnuplot', 'Updated Feb 27, 2021', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 8, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['regone\nCode for project one, scenario one in SF2930 Regression Analysis at KTH Royal Institute of Technology\nRunning the project\n\nDownload the bodyfatmen.csv file from the course page, and place it in a data folder in the parent directory.\nImport devtools and run devtools::install_github(""itslwg/regone"").\nRun regone::RunProject().\n\n'], 'url_profile': 'https://github.com/itslwg', 'info_list': ['Ruby', 'Updated Jan 24, 2020', 'Gnuplot', 'Updated Feb 27, 2021', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 8, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020']}","{'location': 'Virginia Beach', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/reegak', 'info_list': ['Ruby', 'Updated Jan 24, 2020', 'Gnuplot', 'Updated Feb 27, 2021', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 8, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020']}","{'location': 'Bellevue, Washington', 'stats_list': [], 'contributions': '160 contributions\n        in the last year', 'description': ['Regression-BostonHousing\nUsing Boston housing dataset to find the statistically significant predictors using regression and model selection methods\n'], 'url_profile': 'https://github.com/PrachiSablani', 'info_list': ['Ruby', 'Updated Jan 24, 2020', 'Gnuplot', 'Updated Feb 27, 2021', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 8, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""quickregress\nPolynomial regression for the lazy\nquickregress is a minimalist wrapper for sklearn's polynomial and linear regression functionality, intended to reduce the amount of effort needed for simple regression operations.\nquickregress provides one function: regress(x, y, degree). regress returns a RegressionResult, which has the following methods:\npredict(x) returns the model's predictions for a list of x values.\nformula(digits=6, latex=False) returns the model's formula as a string. digits changes the number of significant digits, and latex outputs a LaTeX-friendly string (for use with Jupyter and the like).\n""], 'url_profile': 'https://github.com/hexaguin', 'info_list': ['Ruby', 'Updated Jan 24, 2020', 'Gnuplot', 'Updated Feb 27, 2021', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 8, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prateekesh1995', 'info_list': ['Ruby', 'Updated Jan 24, 2020', 'Gnuplot', 'Updated Feb 27, 2021', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 8, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 24, 2020', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020']}"
"{'location': 'Bengaluru, Karnataka', 'stats_list': [], 'contributions': '174 contributions\n        in the last year', 'description': ['Gradient-Descent-For-Logistic-Regression-From-Scratch\nImplementation of Logistic Regression from scratch using Gradient Descent.\n'], 'url_profile': 'https://github.com/Reactor11', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', '1', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Via Lactea', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['How to implement Logistic Regression in Python from scratch?\nExample of the Logistic Regression class, written from scratch using Gradient Descent algorithm.\nThis is a training example which could help understand more how logistic regression works.\nCode\nimport numpy as np\n\n\nclass LogisticRegression:\n    def __init__(self, learning_rate=0.01, num_iter=100, fit_intercept=True, verbose=False):\n        self.learning_rate = learning_rate  # learning_rate of the algorithm\n        self.num_iter = num_iter  #  number of iterations of the gradient descent\n        self.fit_intercept = fit_intercept  # boolean indicating whether we`re adding base X0 feature vector or not\n        self.verbose = verbose  \n\n    def _add_intercept(self, X):\n        intercept = np.ones((X.shape[0], 1))  #  creating X0 features vector(M x 1)\n        return np.concatenate((intercept, X), axis=1)  # concatenating X0 features vector with our features making intercept\n\n    def _sigmoid(self, z):\n        \'\'\'Defines our ""logit"" function based on which we make predictions\n           parameters:\n              z - product of the our features with weights\n           return:\n              probability of the attachment to class\n        \'\'\'\n\n        return 1 / (1 + np.exp(-z))\n\n    def _loss(self, h, y):\n        \'\'\'\n        Functions have parameters or weights and we want to find the best values for them.\n        To start we pick random values and we need a way to measure how well the algorithm performs using those random weights.\n        That measure is computed using the loss function\n        \'\'\'\n\n        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n\n    def train(self, X, y):\n        \'\'\'\n        Function for training the algorithm.\n            parameters:\n              X - input data matrix (all our features without target variable)\n              y - target variable vector (1/0)\n            \n            return:\n              None\n        \'\'\'\n\n        if self.fit_intercept:\n            X = self._add_intercept(X)  # X will get a result with ""zero"" feature\n\n        self._weights = np.zeros(X.shape[1])  #  inicializing our weights vector filled with zeros\n        \n        for i in range(self.num_iter):  # implementing Gradient Descent algorithm\n            z = np.dot(X, self._weights)  #  calculate the product of the weights and predictor matrix\n            h = self._sigmoid(z)\n            gradient = np.dot(X.T, (h - y)) / y.size\n            self._weights -= self.learning_rate * gradient\n            \n            if (self.verbose == True and i % 10000 == 0):\n                z = np.dot(X, self._weights)\n                h = self._sigmoid(z)\n                print(f\'loss: {self._loss(h, y)} \\t\')\n\n    def predict_prob(self, X):  \n        if self.fit_intercept:\n            X = self._add_intercept(X)\n    \n        return self._sigmoid(np.dot(X, self._weights))\n    \n    def predict(self, X, threshold):\n        return self.predict_prob(X) >= threshold\nHow to use\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_breast_cancer\n\n\nX, y = load_breast_cancer(return_X_y=True)  # load the dataset\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)  # split the dataset\n\n# Create an instance of the LogisticRegression class\n\nlogit = LogisticRegression(learning_rate=0.1, num_iter=200000)  # you can play with hyperparameters to understand how learning rate works.\n\n# Train the model\n\nlogit.train(X_train, y_train)\n\n# Normalize output generated by sigmoid function\n\ny_pred = [int(round(x)) for x in logit.predict_prob(X_test).flatten()]  # you can do it much sipler it`s just me\n\n# look at the score\n\nscore = accuracy_score(y_test, y_pred)  # 0.956 -> 95.6 %.\n'], 'url_profile': 'https://github.com/m4qo5', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', '1', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['Linear_reg\nLinear Regression model written in python for a single feature.\n'], 'url_profile': 'https://github.com/MehwishTariq', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', '1', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Project: To Predict the Concrete Strength\nSummary:\n1.Performed Basic Data Analysis\n2.Exploratory Data Analysis\n>Univariate Analysis\n>Bivariate Analysis\n3.Feature Engineering\n4.Fitting Model On:\n>Lasso\n>Ridge\n>DecisionTree\n>RandomForest\n>XGBoost\n5.Hyperparameter Model Tuning\nDataset & Notebook attached.\n'], 'url_profile': 'https://github.com/mohanakotkar24', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', '1', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Blumenau', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mateusmb', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', '1', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jeromerufin', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', '1', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Islamabad ', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Price-Prediction-Regression-Model\nThis is a tickets pricing monitoring system. It scrapes tickets pricing data periodically and stores it in a database. Ticket pricing changes based on demand and time, and there can be significant difference in price. We are creating this product mainly with ourselves in mind. Users can set up alarms using an email, choosing an origin and destination (cities), time (date and hour range picker) choosing a price reduction over mean price, etc.\n'], 'url_profile': 'https://github.com/B-Chaudhry', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', '1', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rterkesli', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', '1', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Via Lactea', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['How to implement Linear Regression in Python from scratch?\nExample of the Linear Regression class, written from scratch.\nThis is a training example which could help understand more how linear regression works.\nCode\nclass LinearRegression:\n    def __init__(self, learing_rate=0.01, n_iter=10000):\n        self.learing_rate = learing_rate  # learning rate alpha hyperparemeter\n        self.n_iter = n_iter  # number of iterations to minimize the cost function\n\n    def train(self, X, y):\n        self._n_samples = len(y)  # size of the dataset (rows), in formulas m.\n        self.X = np.hstack((np.ones(\n            (self._n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))  # scaled features, added ""zero"" feature vector with all values = 1\n        self.n_features = np.size(X, 1)  # total number of the features\n        self.y = y[:, np.newaxis]  # creating a target variable vector\n        self.params = np.zeros((self.n_features + 1, 1))  # setting weights to zeroes as start values\n\n        for _ in range(self.n_iter):  # using Gradient descent, simultaneously updating weights\n            self.params = self.params - (self.learing_rate/self._n_samples) * \\  \n            self.X.T @ (self.X @ self.params - self.y)\n\n        self.intercept_ = self.params[0]  # bias\n        self.weights = self.params[1:]  # our weights for features\n\n    def score(self, X=None, y=None):\n        if X is None:\n            X = self.X\n        else:\n            n_samples = np.size(X, 0)\n            X = np.hstack((np.ones(\n                (n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))\n        if y is None:\n            y = self.y\n        else:\n            y = y[:, np.newaxis]\n        y_pred = X @ self.params\n        score = 1 - (((y - y_pred)**2).sum() / ((y - y.mean())**2).sum())  # Coefficient of Determination\n        return score\n\n    def predict(self, X):\n        n_samples = np.size(X, 0)\n        y = np.hstack((np.ones((n_samples, 1)), (X-np.mean(X, 0)) \\\n                            / np.std(X, 0))) @ self.params\n        return y\nHow to use\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_boston\n\n\nX, y = load_boston(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nregressor = LinearRegression(learing_rate=0.03, n_iter=3000)  # you can tune hyperparameters\nregressor.train(X, y)\nregressor.score()  # 75%, like in scikit-learn library\n'], 'url_profile': 'https://github.com/m4qo5', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', '1', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['windPrediction\nWind power hour-ahead prediction\nWind energy output is predicted using climate data and yearly wind energy production data of 2015. Wind energy output is predicted and accuracy and RMSE is compared using different regression models including random forest regression, gradient boosted tree regression, adaboost regression and isolation forest regression. Achieved 96% accuracy with Recurrent Neural Networks for the prediction.\nSystem Overview\nWe used public data sets for  30-year climate data for wind speed, vapor pressure, precipitation, etc., from the Gridded Agro-Meteorological Data in Europe portal, and the EMHIRES dataset of European wind power generation dataset derived from meteorological sources. The frequency of the obtained datasets were different as they were from different sources, but both used the NUTS-2 geographical classification system which was used to link the data sets. The data was processed to obtain aggregate wind energy and climate data for individual days, using the time period of 2006-2015 for our analysis.\nWind power generation is considered as the ‘label’ and other data as ‘features’ for regression. Developed correlation matrix for the data and applied different regression models to predict wind energy output. Further to improve the accuracy of the predictions, Recurrent Neural Networks were used to predict wind energy output. Recurrent Neural Networks create connections to form  directed circle and is a type of Artificial Neural Network.\n'], 'url_profile': 'https://github.com/vikas3v', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', '1', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', '1', 'Python', 'MIT license', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}"
"{'location': 'Chennai', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Titanic-SAS\nLogistic Regression Applied on the Categorical Variable using SAS Technology.\n'], 'url_profile': 'https://github.com/prateekesh1995', 'info_list': ['SAS', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 20, 2020', 'Updated Oct 31, 2020', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 20, 2020', '4', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'HTML', 'Updated Jan 24, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'Indore', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sidv1905', 'info_list': ['SAS', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 20, 2020', 'Updated Oct 31, 2020', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 20, 2020', '4', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'HTML', 'Updated Jan 24, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': [""Bayesian Zero- and- One Inflated Beta Regression in Stan with Causal\nMediation\nLexi Rene\nPurpose\nThis code implements a Bayesian approach to causal mediation analysis,\nusing a zero-one inflated beta regression model in STAN. To illustrate\nthe method, the JOBS II data is used. This dataset can be found in the R\npackage\nmediation.\nThe JOBS II study was a randomized field experiment that investigated\nthe efficacy of a job training intervention on unemployed workers. In\nthis dataset there are 899 observations, containing no missing values.\nThe potential outcome variable, depress2, is a continuous measure of\ndepressive symptoms post-treatment (participation in job-skills\nworkshops). The mediator variable, job_seek, is a continuous measure\nof job search self-efficacy. The treatment variable, treat is an\nindicator variable for whether the participant was randomly selected for\nthe JOBS II training program.\nPre-process data\nFor pre-processing of the data, the predictors are scaled using the R\nfunction scale. The scaled matrices are designed to include a column\nfor the intercept. Both the mediator and the outcome variables are\nscaled to be between 0 and 1.\nlibrary(rstan)\nrstan_options(auto_write = TRUE)\nlibrary(mediation)\nlibrary(ggplot2)\nlibrary(tictoc)\nlibrary(bayesplot)\nlibrary(kableExtra)\nlibrary(ggpubr)\n# library(shinystan)\n\ndata(jobs); invisible(names(jobs))\n\nnormalize <- function(x){ \n  return((x- min(x)) /(max(x)-min(x)))\n}\nround_df <- function(x, digits) {\n    # round all numeric variables\n    # x: data frame \n    # digits: number of digits to round\n    numeric_columns <- sapply(x, class) == 'numeric'\n    x[numeric_columns] <-  round(x[numeric_columns], digits)\n    x\n}\nnum_seed = 1810201\n\n## create data \nscaled_z <- scale(jobs[,c('econ_hard','sex','age')])\ntrt <- jobs$treat\n\n## scale mediate and outcome\ny <- normalize(jobs$depress2)\nmed <- normalize(jobs$job_seek)\n\nqplot(y, geom = 'histogram', binwidth = .025, ylab = 'Frequency', xlab = 'Measure of Depression', fill=I('white'), col=I('blue')) + theme_bw() + theme(panel.grid.minor = element_blank())\n\nqplot(med, geom = 'histogram', binwidth = .025, ylab = 'Frequency', xlab = 'Measure of Confidence/Self- Efficacy in Job Search', fill=I('white'), col=I('blue')) + theme_bw() + theme(panel.grid.minor = element_blank())\n\nCausal Mediation Analysis\nPotential/ Counterfactual Framework\nUsing the potential (counterfactual) outcome framework, the causal\neffect of the job training program can be defined as the difference\nbetween two potential outcomes. One potential outcome is realized if the\nsubject participates in the training program and the other potential\noutcome is realized if the subject does not participate.\nSuppose we use \nto represent the measure of depression for the\n\nsubject, , the\nmeasure of confidence/self efficacy in the job search for the\n\nsubject, and ,\nthe binary indicator variable for the\n\nsubject’s treatment/participation in the JOBS II training program;\n takes on the\nvalues of \n(participation in the training) or \n(otherwise). The depression level of subject\n is independent of\nsubject  (). In\naddition, since the treatment in the JOBS II study is randomized,\n is statistically\nindependent of the potential outcomes; we can write this as\n.\nThe observed value for the depression level can be denoted by\n, where , which can result in two potential values. For\nexample,\n would be the observed depression level for subject\n, if subject\n actually participated\nin the training program; in this case, the unobserved outcome for\nsubject  is the level of\ndepression if they did not participate in the training program. We will\nuse  to represent the potential level of depression that\nwould result under the treatment status\n for subject\n. In addition, in causal\nmediation analysis, the potential outcome also depends on the mediator.\nIn the context of this study, this implies that the level of job search\nself-efficacy can be affected by participation in the program, which can\nbe represented by\n; which also has two potential values\n and\n. The potential mediator value of subject\n are independent of the\ntreatment status for subject  (). Therefore, we will update the potential outcome to be\ndenoted as  and also note that the potential outcome for\nsubject  is independent\nof both the treatment status and the mediator value of subject\n ().\nCausal Mediation Effects\nThe statistical independence between the treatment and the potential\noutcome allows us to compute the average causal effect as the observed\nmean difference between the treatment and control group:\n\nUnder the counterfactual/potential outcome framework, only one potential\noutcome of  is observed. Let\n be a vector of\nbaseline covariate for each subject\n and  be\nthe support of the distribution of\n; in addition,\nthe support of \nis\n. To identify the effects of treatment and mediation, we\nassume sequential ignorability, as per Imai et al, by assuming the\nfollowing two statements of conditional independence hold: \n\nwhere  and  for , and, all  and .\nThese ignorability assumptions are made sequentially. The first part of\nthe assumption assumes that given the observed confounders, prior to\ntreatment, the treatment assignment is ignorable. In terms of\nstatistical independence, the observed pre-treatment is independent of\nthe potential outcomes and potential mediators. The second part of\nsequential ignorability states that the mediator is ignorable given the\nobserved treatment and pre-treatment confounders; meaning that the\npotential outcome and mediator are unconfounded on the past observations\nand confounders.\nThe indirect effect of the treatment on the outcome, through the\nmediating variable is defined as the causal mediation effect (Imai et\nal., 2010), for :\n\nThe following definitions are defined as the effect of the treatment\n, on the outcome, through the mediation variable:\nThe average causal mediation effect is defined by:\n\nThe direct effect is defined by:\n\nThe average direct effect is defined by:\n\nThe total effect is defined by:\n\nLastly, the average total effect is defined by:\n\nUnder the assumptions from\n and\n, Imai et\nal.\xa0then showed that the distribution of the potential outcomes is\nnonparametrically identified:\n\nThis result allows us to estimate the potential outcome and mediators\nthat we are unable to observe.\nStatistical Model\nDensities\nThe density of a random variable\n with a beta\ndistribution, where , can be reparametrized (Ferrari & Cribari-Neto 2004) to\nbe defined as:\n\n Note for :\n denotes the gamma function, , and ;\n is a\nprecision parameter, where for a fixed\n, there is an\ninverse relationship between\n and\n.\nUsing  we\nfurther assume the following regression models for both the response\nvariables, depress2, ,\nand job_seek, , to\nfollow a zero-one inflated beta (ZOIB) distribution, as they lie within\nthe bounds [0,1]. The cumulative distribution function of the random\nvariable  under a ZOIB\ndistribution is:\n\nWhere the density of ,is\ndefined as:\n\n\nhere \nis the probability that the response is equal to zero,\n is\nthe probability that the response is equal to one, given the probability\nthat the response is not equal to zero,\n is the\nexpected value of the beta distribution,\n is the\nprecision of the beta distribution, and\n and\n are shape parameters;\n and  (Ferrari & Cribari-Neto 2004). Ultimately,  and .\nThe  moment for the density\n of\n and its’ variance can\nbe written as:\n\nBayesian approach to Causal Mediation Analysis\nBayesian Approach\n where we assume\n\nwhere the link function \nis a logit link function;\n,\nis the inverse of the link function that map values to a proportion\nbetween 0 and 1. \nis the Multivariate Normal distribution, and\n represents distribution of the\n\ncoefficients, excluding the intercept; the intercept of every\n\ncoefficient is assigned a uniform prior distribution.\nFor the mediator model, \nis a matrix containing the intercept, the baseline covariates,\n, and the treatment\nvariable . For the\noutcome model,  is a\nmatrix containing the intercept, the baseline covariates,\n, the treatment variable\n, and the mediator\nvariable  under a\nspecified treatment.\nProcedure\n\nAssign the prior covariance matrix per parameter for the\n\ncoefficients.\nUsing these priors and the observed data, fit the models for the\nmediator and the outcome. Simulate the model parameters to obtain\nthe estimated coefficients for each sampling distribution:\n and\n.\nFor each chain of the simulation: create a new dataset, of size\n, by taking a\nrandom sample, of the rows, with replacement,from the original data.\nThe probability of selection for each row is given by\n; .\nDuplicate the dataset created in step 3. In one of the duplicated\ndatasets, set the treatment variable to\n for the entire\ndataset. In the other duplicated dataset, set the treatment variable\nto .\nUsing the duplicated datasets created in step 4, simulate new values\nof the mediator for each subject. Simulate these new mediator values\nusing the estimated parameters from step 2. This step will result in\ntwo variables, the mediator under treatment\n,\n(), and the\nmediator under treatment\n,\n().\nUsing the duplicated datasets from step 4 and the new simulated\nmediator values from step 5, create a total of four datasets. Half\nof these datasets will have the treatment variable set to\n for the entire\ndataset and the other half will have the treatment variable set to\n for the entire\ndataset. For the half of the datasets that have treatment set to\n, set the mediator\nvalue for one entire dataset to\n and for the\nother dataset set the mediator value to\n. Do the same for the second half of the datasets that have\nthe treatment set to .\nSimulate new values of the outcome for each dataset created in step\n6. Simulate these new outcome values using the estimated parameters\nfrom step 2.\nUse the simulated outcome values from step 7 to compute the average\ndirect, indirect, and total effect between the two outcome\npredictions under each treatment status.\n\nThe steps above will be programmed in STAN.\nStore data and input in a list to send to STAN\nThe STAN model accepts the following values stored in a list:\n* n - the total number of observations\n\n* np - the total number of predictors,excluding the intercept and the treatment\n\n* sim - the total number of iterations per chain\n    \n* y - the outcome variable scaled between 0 and 1; vector\n\n* m - the mediator variable scaled between 0 and 1; vector\n\n* a - the treatment variable; vector\n\n* z - the data matrix of scaled predictors\n\n* alpha_cov_m - the covariance for the normal prior set on alpha; used to model m\n\n* gamma_cov_m -  the covariance for the normal prior set on gamma; used to model m\n\n* mu_cov_m -  the covariance for the normal prior set on mu; used to model m\n\n* phi_cov_m -  the covariance for the normal prior set on phi; used to model m\n\n* alpha_cov_y - the covariance for the normal prior set on alpha; used to model y\n\n* gamma_cov_y -  the covariance for the normal prior set on gamma; used to model y\n\n* mu_cov_y -  the covariance for the normal prior set on mu; used to model y\n\n* phi_cov_y -  the covariance for the normal prior set on phi; used to model y\n\njobs_data <-\n  list(n = nrow(scaled_z),\n       np = ncol(scaled_z),\n       sim = 1000,\n       y = y,\n       m = med,\n       a = trt,\n       z = scaled_z,    \n       ## cov_m: prior for coefficients of the mediator model; include treatment, do NOT include the intercept or mediator\n       alpha_cov_m = diag(5, ncol(scaled_z)+1), ## == np + 1\n       gamma_cov_m = diag(5, ncol(scaled_z)+1),\n       mu_cov_m = diag(5, ncol(scaled_z)+1),\n       phi_cov_m = diag(5, ncol(scaled_z)+1),\n       ## cov_y: prior for coefficients of the outcome model; include the mediator and treatment, do not include the intercept\n       alpha_cov_y = diag(5, ncol(scaled_z)+2),  ## == np + 2\n       gamma_cov_y = diag(5, ncol(scaled_z)+2),\n       mu_cov_y = diag(5, ncol(scaled_z)+2),\n       phi_cov_y = diag(5, ncol(scaled_z)+2)\n  )\nStan Model\nThis model will return:\n* all_params_y - alpha, gamma, p, q for the outcome model (1:iterations,1:n,1:4)\n\n* all_params_m - alpha, gamma, p, q for the mediator model (1:iterations,1:n,1:4)\n\n* coef_mediator -  alpha, gamma, mu, phi;  coefficients for the mediator model (1:iterations,1:np,1:4)\n\n* coef_outcome -  alpha, gamma, mu, phi; coefficients for the outcome model (1:iterations,1:np+1,1:4)\n\n* tau - total effect (length = total iterations)\n\n* delta - causal effect (1:iterations, 2) where [a = 0, a = 1]\n\n* zeta - direct effect (1:iterations, 2) where [a = 0, a = 1]\n\n* pred_m - generated quantities, prediction of the mediator (1:iterations, 1:sim, 2) where [a = 0, a = 1]\n\n* pred_y - generated quantities, prediction of the outcome (1:iterations, 1:sim, 4) where [y0m0, y0m1, y1m1, y1m0]\n\nNote: pred_y has the columns to represent .\nWe can fit the model in Stan with the following code .\n## S4 class stanmodel 'bayes_zoib' coded as follows:\n## functions{\n##   matrix calc_zoib_par(matrix x_f,matrix coef_f){\n##     vector[rows(x_f)] p_f;\n##     vector[rows(x_f)] q_f;\n##     matrix[rows(x_f), cols(coef_f)] x_theta;\n##     matrix[rows(x_f), cols(coef_f)] params_hold;\n##     matrix[rows(x_f), 2] new_alpha_gamma;\n##     matrix[rows(x_f), 2] p_and_q;\n##       \n##     x_theta =  x_f * coef_f;   \n##       x_theta[,1] = inv_logit(x_theta[,1]);\n##       x_theta[,2] = inv_logit(x_theta[,2]);\n##       x_theta[,3] = inv_logit(x_theta[,3]);\n##       x_theta[,4] = exp(x_theta[,4]);\n##     p_f = x_theta[,3] .* x_theta[,4];\n##     q_f = x_theta[,4] - p_f;\n##   \n##     p_and_q = append_col(p_f,q_f);\n##     new_alpha_gamma = append_col(x_theta[,1], x_theta[,2]);\n##     params_hold = append_col(new_alpha_gamma, p_and_q); \n##   return params_hold ;\n##   } \n## \n##   matrix calc_pred(matrix param_pred, int num_trt_splits){\n##       matrix[(cols(param_pred)-num_trt_splits), rows(param_pred)] wt;\n##       int sim = rows(param_pred);\n##       int d = 1;\n##       int i = 1;\n##       \n##       while( d < cols(param_pred)-3){\n##         wt[d,] = to_row_vector(param_pred[,i]);\n##         wt[d+1,] = to_row_vector((rep_vector(1,sim)-param_pred[,i]) .* param_pred[,i+1]);\n##         wt[d+2,] = to_row_vector((rep_vector(1,sim)-param_pred[,i]) .* (rep_vector(1,sim)-param_pred[,i+1]));\n##         d += 3;\n##         i += 4;\n##       }\n##   return wt;\n##   } \n## } \n## data{\n##   int n;\n##   int np; // number of parameters excluding intercept and treatment\n##   int sim;\n##   vector<lower=0, upper=1>[n] y;\n##   vector<lower=0, upper=1>[n] m;\n##   vector[n] a; //treatment variable\n##   matrix[n, np] z;\n##   matrix[np+1, np+1] alpha_cov_m;\n##   matrix[np+1, np+1] gamma_cov_m;\n##   matrix[np+1, np+1] mu_cov_m;\n##   matrix[np+1, np+1] phi_cov_m;\n##   matrix[np+2, np+2] alpha_cov_y;\n##   matrix[np+2, np+2] gamma_cov_y;\n##   matrix[np+2, np+2] mu_cov_y;\n##   matrix[np+2, np+2] phi_cov_y;\n## }\n## transformed data{\n##   matrix[n, np+2] x; //ncol(z), trt, int\n##   matrix[n, np+3] x_out;\n##   x = append_col(append_col(rep_vector(1,n),z), a); //insert col for intercept of 1s\n##   x_out = append_col(x, m);\n## }\n## parameters{\n##   matrix[np+2, 4] coef_mediator;\n##   matrix[np+3, 4] coef_outcome;\n## }\n## transformed parameters{\n##   matrix[n, 4] all_params_m;\n##   matrix[n, 4] all_params_y;\n##   all_params_m = calc_zoib_par(x, coef_mediator);\n##   all_params_y = calc_zoib_par(x_out, coef_outcome);\n## }\n## model{\n##   // coefficients for mediator model; does not include the mediator\n##   coef_mediator[2:,1] ~ multi_normal(rep_vector(0,cols(x)-1), alpha_cov_m);\n##   coef_mediator[2:,2] ~ multi_normal(rep_vector(0,cols(x)-1), gamma_cov_m);\n##   coef_mediator[2:,3] ~ multi_normal(rep_vector(0,cols(x)-1), mu_cov_m);\n##   coef_mediator[2:,4] ~ multi_normal(rep_vector(0,cols(x)-1), phi_cov_m);\n##   \n##   // coefficients for outcome model; includes the mediator\n##   coef_outcome[2:,1] ~ multi_normal(rep_vector(0,cols(x_out)-1), alpha_cov_y);\n##   coef_outcome[2:,2] ~ multi_normal(rep_vector(0,cols(x_out)-1), gamma_cov_y);\n##   coef_outcome[2:,3] ~ multi_normal(rep_vector(0,cols(x_out)-1), mu_cov_y);\n##   coef_outcome[2:,4] ~ multi_normal(rep_vector(0,cols(x_out)-1), phi_cov_y);\n## \n##   // zero one inflated beta likelihood\n##     for (i in 1:n) {\n##       if (y[i] == 0) {\n##         target += log(all_params_y[i,1]) ;\n##       } else if (y[i] == 1) {\n##         target += log1m(all_params_y[i,1]) + log(all_params_y[i,2]);\n##       } else {\n##         target += log1m(all_params_y[i,1]) + log1m(all_params_y[i,2]) + beta_lpdf(y[i] | all_params_y[i,3], all_params_y[i,4]);\n##       }\n##     \n##       if (m[i] == 0) {\n##         target += log(all_params_m[i,1]);\n##       } else if (m[i] == 1) {\n##         target += log1m(all_params_m[i,1]) + log(all_params_m[i,2]);\n##       } else {\n##         target += log1m(all_params_m[i,1]) + log1m(all_params_m[i,2]) + beta_lpdf(m[i] | all_params_m[i,3], all_params_m[i,4]);\n##       }\n##     }\n## }\n## generated quantities{\n##   real tau;\n##   vector[2] delta;\n##   vector[2] zeta;\n##   matrix[sim, 2] pred_m;\n##   matrix[sim, 4] pred_y;\n##   {\n##     int index;\n##     matrix[sim, 16] param_pred_y;\n##     matrix[sim, 8] param_pred_m;\n##     vector[rows(z)] wt;\n##     matrix[6, sim] wt_m; // three possible outcomes for density per trt\n##     matrix[12, sim] wt_y;\n##     matrix[sim, np+2] X_sample;\n##     matrix[sim, np+2] X_m0;\n##     matrix[sim, np+2] X_m1;\n##     matrix[sim, np+3] X_y0_m0;\n##     matrix[sim, np+3] X_y0_m1;\n##     matrix[sim, np+3] X_y1_m0;\n##     matrix[sim, np+3] X_y1_m1;\n##     \n##       wt = dirichlet_rng(rep_vector(1, rows(z))); \n##       for (j in 1:sim){\n##         index = categorical_rng(wt);\n##         X_sample[j,:] = x[index,:];\n##       }\n## \n##       X_m0 = X_sample;\n##       X_m0[:,cols(x)] = rep_vector(0, sim); // cols(x) == index for last col of x, which is treat\n## \n##       X_m1 = X_sample;\n##       X_m1[:,cols(x)] = rep_vector(1, sim);\n##       \n##       // calculate new alpha, gamma, mu, phi\n##       param_pred_m[,1:4] = calc_zoib_par(X_m0, coef_mediator);\n##       param_pred_m[,5:8] = calc_zoib_par(X_m1, coef_mediator);\n##     \n##       wt_m = calc_pred(param_pred_m, 2);\n## \n##       for(k in 1:sim){\n##         int index_wtm0 = categorical_rng(wt_m[1:3,k]);\n##         int index_wtm1 = categorical_rng(wt_m[4:6,k]);\n##         \n##         if (index_wtm0 == 1){ pred_m[k,1] = 0;}\n##           else if (index_wtm0 == 2){pred_m[k,1] = 1;}\n##             else if (index_wtm0 == 3){pred_m[k,1] = beta_rng(param_pred_m[k,3],param_pred_m[k,4]);}\n##             \n##         if (index_wtm1 == 1){ pred_m[k,2] = 0;}\n##           else if (index_wtm1 == 2){pred_m[k,2] = 1;}\n##             else if (index_wtm1 == 3){pred_m[k,2] = beta_rng(param_pred_m[k,7],param_pred_m[k,8]);}\n##       }\n##       \n##       X_y0_m0 = append_col(X_m0, pred_m[,1]);\n##       X_y0_m1 = append_col(X_m0, pred_m[,2]);\n##       X_y1_m1 = append_col(X_m1, pred_m[,2]);\n##       X_y1_m0 = append_col(X_m1, pred_m[,1]);\n##       \n##       //coef_mediator is np+2 x 4. it includes the mediator\n##       param_pred_y[,1:4] = calc_zoib_par(X_y0_m0, coef_outcome);\n##       param_pred_y[,5:8] = calc_zoib_par(X_y0_m1, coef_outcome);\n##       param_pred_y[,9:12] = calc_zoib_par(X_y1_m1, coef_outcome);\n##       param_pred_y[,13:16] = calc_zoib_par(X_y1_m0, coef_outcome);\n##       \n##       wt_y = calc_pred(param_pred_y, 4);\n##       \n##       for(h in 1:sim){\n##           int index_y0m0 = categorical_rng(wt_y[1:3,h]);\n##           int index_y0m1 = categorical_rng(wt_y[4:6,h]);\n##           int index_y1m1 = categorical_rng(wt_y[7:9,h]);\n##           int index_y1m0 = categorical_rng(wt_y[10:12,h]);\n## \n##           if (index_y0m0 == 1){pred_y[h,1] = 0;}\n##           else if (index_y0m0 == 2){pred_y[h,1] = 1;}\n##           else if (index_y0m0 == 3){pred_y[h,1] = beta_rng(param_pred_y[h,3],param_pred_y[h,4]);}\n## \n##           if (index_y0m1 == 1){ pred_y[h,2] = 0;}\n##           else if (index_y0m1 == 2){pred_y[h,2] = 1;}\n##           else if (index_y0m1 == 3){pred_y[h,2] = beta_rng(param_pred_y[h,7],param_pred_y[h,8]);}\n## \n##           if (index_y1m1 == 1){ pred_y[h,3] = 0;}\n##           else if (index_y1m1 == 2){pred_y[h,3] = 1;}\n##           else if (index_y1m1 == 3){pred_y[h,3] = beta_rng(param_pred_y[h,11],param_pred_y[h,12]);}\n## \n##           if (index_y1m0 == 1){ pred_y[h,4] = 0;}\n##           else if (index_y1m0 == 2){pred_y[h,4] = 1;}\n##           else if (index_y1m0 == 3){pred_y[h,4] = beta_rng(param_pred_y[h,15],param_pred_y[h,16]);}\n##       }\n##       \n##       delta[1] = mean(pred_y[:,2]) - mean(pred_y[:,1]);\n##       delta[2] = mean(pred_y[:,3]) - mean(pred_y[:,4]);\n##       zeta[1] = mean(pred_y[:,4]) - mean(pred_y[:,1]);\n##       zeta[2] = mean(pred_y[:,3]) - mean(pred_y[:,2]);\n##       tau = mean(pred_y[:,3]) - mean(pred_y[:,1]);\n## \n##   }  // end of local variables\n## }  //end of generated quantities\n## \n## \n\nAssessing STAN output\nWhen assessing STAN output, one of the things that you want to check is\nwhether the chains are converging, and that they are converging to the\nsame area. Some of the recommended convergence checks include monitoring\nthe potential scale reduction (PSR) factor,  statistic,\nand using visual checks, e.g.\xa0traceplots.  evaluates\nthe mixing of the chains by comparing the variation between the chains\nto the variation within the chains. “The condition of  being\n‘near’ 1 depends on the problem at hand, but we generally have been\nsatisfied with setting 1.1 as a threshold” (Gelman et al., 2004).\nAdditionally, since the Markov Chain Monte Carlo (MCMC) does not return\nindependent draws, the simulations within each chain will show some\nlevel of autocorrelation. This autocorrelation increases the uncertainty\nof the estimation of posterior quantities. The amount by which this\nautocorrelation increases in estimates can be measured by the effective\nsample size (ESS),\n,\nwhich should be large so that it can provide a measure of precision;\n\nis the ‘effective number of independent simulation draws’.\n## $coef_mediator\n## [1] 4000    5    4\n## \n## $coef_outcome\n## [1] 4000    6    4\n## \n## $all_params_m\n## [1] 4000  899    4\n## \n## $all_params_y\n## [1] 4000  899    4\n## \n## $tau\n## [1] 4000\n## \n## $delta\n## [1] 4000    2\n## \n## $zeta\n## [1] 4000    2\n## \n## $pred_m\n## [1] 4000 1000    2\n## \n## $pred_y\n## [1] 4000 1000    4\n## \n## $lp__\n## [1] 4000\n\nAssess Rhat and Effective Sample Size\n\nTable 1\n\n\n\n\n\nMin.\n\n\n1st Qu.\n\n\nMedian\n\n\nMean\n\n\n3rd Qu.\n\n\nMax.\n\n\n\n\n\n\nr_hat\n\n\n0.9990\n\n\n0.9996\n\n\n0.9999\n\n\n0.9999\n\n\n1.0002\n\n\n1.0031\n\n\n\n\nn_eff\n\n\n1549\n\n\n3822\n\n\n4003\n\n\n4167\n\n\n4401\n\n\n7930\n\n\n\n\nAssess Traceplots\n\nSummarizing the Posteriors\n\n\n\nTable 3\n\n\n\nparam\n\n\nspecified_param\n\n\noverall_param\n\n\nmean\n\n\nse_mean\n\n\nsd\n\n\n2.5%\n\n\n97.5%\n\n\nn_eff\n\n\nRhat\n\n\n\n\n\n\ncoef_mediator\n\n\nalpha\n\n\ncoef_mediator[1,1]\n\n\n-9.0283\n\n\n0.0404\n\n\n1.8501\n\n\n-12.9109\n\n\n-5.8674\n\n\n2099.987\n\n\n1.0003\n\n\n\n\ngamma\n\n\ncoef_mediator[1,2]\n\n\n-2.0751\n\n\n0.0034\n\n\n0.1825\n\n\n-2.4442\n\n\n-1.7406\n\n\n2938.431\n\n\n1.0006\n\n\n\n\nmu\n\n\ncoef_mediator[1,3]\n\n\n0.9333\n\n\n0.0009\n\n\n0.0475\n\n\n0.8394\n\n\n1.0269\n\n\n2664.567\n\n\n0.9994\n\n\n\n\nphi\n\n\ncoef_mediator[1,4]\n\n\n1.9967\n\n\n0.0016\n\n\n0.0829\n\n\n1.8382\n\n\n2.1584\n\n\n2648.302\n\n\n1.0005\n\n\n\n\nalpha\n\n\ncoef_mediator[2,1]\n\n\n-0.8211\n\n\n0.0134\n\n\n0.8095\n\n\n-2.5619\n\n\n0.6632\n\n\n3656.823\n\n\n1.0004\n\n\n\n\ngamma\n\n\ncoef_mediator[2,2]\n\n\n0.2259\n\n\n0.0013\n\n\n0.0964\n\n\n0.0414\n\n\n0.4129\n\n\n5331.649\n\n\n0.9996\n\n\n\n\nmu\n\n\ncoef_mediator[2,3]\n\n\n0.0203\n\n\n0.0004\n\n\n0.0290\n\n\n-0.0375\n\n\n0.0778\n\n\n4847.485\n\n\n0.9996\n\n\n\n\nphi\n\n\ncoef_mediator[2,4]\n\n\n-0.0101\n\n\n0.0008\n\n\n0.0490\n\n\n-0.1075\n\n\n0.0857\n\n\n4258.027\n\n\n0.9993\n\n\n\n\nalpha\n\n\ncoef_mediator[3,1]\n\n\n-0.1564\n\n\n0.0110\n\n\n0.8025\n\n\n-1.7402\n\n\n1.4096\n\n\n5367.523\n\n\n1.0001\n\n\n\n\ngamma\n\n\ncoef_mediator[3,2]\n\n\n0.1103\n\n\n0.0013\n\n\n0.0981\n\n\n-0.0807\n\n\n0.3030\n\n\n5316.086\n\n\n0.9994\n\n\n\n\nmu\n\n\ncoef_mediator[3,3]\n\n\n-0.0405\n\n\n0.0004\n\n\n0.0286\n\n\n-0.0941\n\n\n0.0174\n\n\n5054.996\n\n\n0.9999\n\n\n\n\nphi\n\n\ncoef_mediator[3,4]\n\n\n-0.1246\n\n\n0.0007\n\n\n0.0497\n\n\n-0.2222\n\n\n-0.0256\n\n\n5578.842\n\n\n0.9998\n\n\n\n\nalpha\n\n\ncoef_mediator[4,1]\n\n\n0.3644\n\n\n0.0107\n\n\n0.6905\n\n\n-1.0056\n\n\n1.7135\n\n\n4156.059\n\n\n1.0000\n\n\n\n\ngamma\n\n\ncoef_mediator[4,2]\n\n\n-0.0762\n\n\n0.0014\n\n\n0.0971\n\n\n-0.2712\n\n\n0.1121\n\n\n4534.600\n\n\n0.9998\n\n\n\n\nmu\n\n\ncoef_mediator[4,3]\n\n\n0.0885\n\n\n0.0004\n\n\n0.0279\n\n\n0.0352\n\n\n0.1428\n\n\n5137.867\n\n\n1.0005\n\n\n\n\nphi\n\n\ncoef_mediator[4,4]\n\n\n0.1236\n\n\n0.0007\n\n\n0.0502\n\n\n0.0241\n\n\n0.2230\n\n\n5143.150\n\n\n0.9999\n\n\n\n\nalpha\n\n\ncoef_mediator[5,1]\n\n\n1.7391\n\n\n0.0319\n\n\n1.6061\n\n\n-1.1382\n\n\n5.2488\n\n\n2539.395\n\n\n1.0000\n\n\n\n\ngamma\n\n\ncoef_mediator[5,2]\n\n\n0.3692\n\n\n0.0039\n\n\n0.2157\n\n\n-0.0367\n\n\n0.7931\n\n\n3009.033\n\n\n1.0006\n\n\n\n\nmu\n\n\ncoef_mediator[5,3]\n\n\n0.0166\n\n\n0.0011\n\n\n0.0595\n\n\n-0.0960\n\n\n0.1365\n\n\n2748.876\n\n\n0.9995\n\n\n\n\nphi\n\n\ncoef_mediator[5,4]\n\n\n-0.1101\n\n\n0.0019\n\n\n0.1010\n\n\n-0.3107\n\n\n0.0836\n\n\n2797.864\n\n\n1.0004\n\n\n\n\ncoef_outcome\n\n\nalpha\n\n\ncoef_outcome[1,1]\n\n\n-5.1793\n\n\n0.0125\n\n\n0.6274\n\n\n-6.4370\n\n\n-3.9643\n\n\n2505.241\n\n\n1.0010\n\n\n\n\ngamma\n\n\ncoef_outcome[1,2]\n\n\n-10.2122\n\n\n0.0786\n\n\n3.0949\n\n\n-16.9426\n\n\n-4.9023\n\n\n1549.436\n\n\n1.0021\n\n\n\n\nmu\n\n\ncoef_outcome[1,3]\n\n\n-0.4482\n\n\n0.0033\n\n\n0.1397\n\n\n-0.7158\n\n\n-0.1719\n\n\n1771.825\n\n\n1.0007\n\n\n\n\nphi\n\n\ncoef_outcome[1,4]\n\n\n1.7950\n\n\n0.0053\n\n\n0.2319\n\n\n1.3421\n\n\n2.2390\n\n\n1899.775\n\n\n1.0001\n\n\n\n\nalpha\n\n\ncoef_outcome[2,1]\n\n\n-0.3451\n\n\n0.0017\n\n\n0.1152\n\n\n-0.5736\n\n\n-0.1238\n\n\n4546.028\n\n\n1.0004\n\n\n\n\ngamma\n\n\ncoef_outcome[2,2]\n\n\n2.1103\n\n\n0.0276\n\n\n1.3485\n\n\n-0.2034\n\n\n5.1299\n\n\n2390.713\n\n\n1.0005\n\n\n\n\nmu\n\n\ncoef_outcome[2,3]\n\n\n0.2058\n\n\n0.0004\n\n\n0.0313\n\n\n0.1429\n\n\n0.2678\n\n\n5204.225\n\n\n0.9997\n\n\n\n\nphi\n\n\ncoef_outcome[2,4]\n\n\n-0.1287\n\n\n0.0007\n\n\n0.0503\n\n\n-0.2294\n\n\n-0.0332\n\n\n4805.560\n\n\n0.9994\n\n\n\n\nalpha\n\n\ncoef_outcome[3,1]\n\n\n-0.1146\n\n\n0.0015\n\n\n0.1086\n\n\n-0.3275\n\n\n0.1012\n\n\n4942.761\n\n\n1.0005\n\n\n\n\ngamma\n\n\ncoef_outcome[3,2]\n\n\n1.5823\n\n\n0.0250\n\n\n1.5155\n\n\n-0.9051\n\n\n5.0046\n\n\n3668.561\n\n\n1.0017\n\n\n\n\nmu\n\n\ncoef_outcome[3,3]\n\n\n0.0760\n\n\n0.0005\n\n\n0.0302\n\n\n0.0164\n\n\n0.1331\n\n\n4304.896\n\n\n1.0000\n\n\n\n\nphi\n\n\ncoef_outcome[3,4]\n\n\n-0.0964\n\n\n0.0008\n\n\n0.0485\n\n\n-0.1915\n\n\n-0.0001\n\n\n4091.754\n\n\n0.9999\n\n\n\n\nalpha\n\n\ncoef_outcome[4,1]\n\n\n0.1276\n\n\n0.0013\n\n\n0.1060\n\n\n-0.0850\n\n\n0.3314\n\n\n6175.524\n\n\n0.9992\n\n\n\n\ngamma\n\n\ncoef_outcome[4,2]\n\n\n-0.2904\n\n\n0.0163\n\n\n1.1389\n\n\n-2.7103\n\n\n1.8449\n\n\n4866.476\n\n\n1.0000\n\n\n\n\nmu\n\n\ncoef_outcome[4,3]\n\n\n0.0239\n\n\n0.0005\n\n\n0.0317\n\n\n-0.0394\n\n\n0.0855\n\n\n4424.241\n\n\n0.9996\n\n\n\n\nphi\n\n\ncoef_outcome[4,4]\n\n\n0.0000\n\n\n0.0008\n\n\n0.0507\n\n\n-0.1006\n\n\n0.0961\n\n\n4180.269\n\n\n1.0000\n\n\n\n\nalpha\n\n\ncoef_outcome[5,1]\n\n\n-0.1226\n\n\n0.0032\n\n\n0.2280\n\n\n-0.5560\n\n\n0.3308\n\n\n5094.499\n\n\n1.0004\n\n\n\n\ngamma\n\n\ncoef_outcome[5,2]\n\n\n-1.7735\n\n\n0.0233\n\n\n1.6789\n\n\n-5.2106\n\n\n1.4620\n\n\n5203.669\n\n\n0.9994\n\n\n\n\nmu\n\n\ncoef_outcome[5,3]\n\n\n-0.0656\n\n\n0.0009\n\n\n0.0651\n\n\n-0.1952\n\n\n0.0608\n\n\n4742.327\n\n\n0.9997\n\n\n\n\nphi\n\n\ncoef_outcome[5,4]\n\n\n-0.0424\n\n\n0.0014\n\n\n0.1013\n\n\n-0.2423\n\n\n0.1504\n\n\n5022.409\n\n\n0.9993\n\n\n\n\nalpha\n\n\ncoef_outcome[6,1]\n\n\n3.8905\n\n\n0.0145\n\n\n0.7257\n\n\n2.5129\n\n\n5.3496\n\n\n2506.562\n\n\n1.0008\n\n\n\n\ngamma\n\n\ncoef_outcome[6,2]\n\n\n-0.4225\n\n\n0.0313\n\n\n2.0384\n\n\n-4.4837\n\n\n3.4893\n\n\n4246.164\n\n\n0.9999\n\n\n\n\nmu\n\n\ncoef_outcome[6,3]\n\n\n-1.1088\n\n\n0.0043\n\n\n0.1792\n\n\n-1.4624\n\n\n-0.7587\n\n\n1726.288\n\n\n1.0009\n\n\n\n\nphi\n\n\ncoef_outcome[6,4]\n\n\n0.2271\n\n\n0.0067\n\n\n0.2967\n\n\n-0.3532\n\n\n0.8238\n\n\n1952.139\n\n\n1.0000\n\n\n\n\ntau\n\n\ncausal effects\n\n\ntau\n\n\n-0.0120\n\n\n0.0002\n\n\n0.0129\n\n\n-0.0373\n\n\n0.0133\n\n\n4549.963\n\n\n0.9995\n\n\n\n\ndelta\n\n\ndelta[1]\n\n\n-0.0033\n\n\n0.0001\n\n\n0.0081\n\n\n-0.0193\n\n\n0.0122\n\n\n3537.918\n\n\n0.9994\n\n\n\n\ndelta[2]\n\n\n-0.0030\n\n\n0.0001\n\n\n0.0076\n\n\n-0.0176\n\n\n0.0120\n\n\n3763.264\n\n\n1.0015\n\n\n\n\nzeta\n\n\nzeta[1]\n\n\n-0.0089\n\n\n0.0002\n\n\n0.0127\n\n\n-0.0338\n\n\n0.0162\n\n\n4630.450\n\n\n1.0000\n\n\n\n\nzeta[2]\n\n\n-0.0086\n\n\n0.0002\n\n\n0.0126\n\n\n-0.0332\n\n\n0.0163\n\n\n4619.492\n\n\n0.9998\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nFerrari, S., and Cribari-Neto, F. (2004). “Beta regression for\nmodelling rates and proportions,” Journal of Applied Statistics,\n31(7), 799-815\nGelman, A., Carlin, J.B., Stern, H.S., and Rubin, D.B. (2004).\nBayesian data analysis (2nd ed.)\nImai, K., Keele, L., and Tingley, D. (2010),“A General Approach to\nCausal Mediation Analysis,”Psychological Methods, 15(4), 309–334\n\n""], 'url_profile': 'https://github.com/renethestudent', 'info_list': ['SAS', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 20, 2020', 'Updated Oct 31, 2020', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 20, 2020', '4', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'HTML', 'Updated Jan 24, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['SPACancer\nR package for Poisson regression of background mutation rate.\nInstallation\nInstall directly from github with the R package devtools:\ninstall.packages(""devtools"")\nlibrary(devtools)\ninstall_github(""huangweiqing1993/SPACancer"")\nSmall usage example\nlibrary(SPACancer)\n\n# Load the example dataset:\ndata(mutation)\n\n# Fit a mixed-effects Poisson regression model\nfit <- ProBMR(m, M, x)\n\n# Get background mutation rates\nbmr <- fit$bkgd2$bmr\n\n# Single gene analysis\nconvolution_test(bmr, n)\n'], 'url_profile': 'https://github.com/huangweiqing1993', 'info_list': ['SAS', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 20, 2020', 'Updated Oct 31, 2020', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 20, 2020', '4', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'HTML', 'Updated Jan 24, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'Frankfurt am Main', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Logistic_Regression_to_Tableau\nProduction code of Logistic Regression in Tableau with TabPy\nIt includes the production code Python -> Tableau. One particular challenge was making the normalization (MinMaxNormalization) work in production.\n\n'], 'url_profile': 'https://github.com/FrancoSwiss', 'info_list': ['SAS', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 20, 2020', 'Updated Oct 31, 2020', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 20, 2020', '4', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'HTML', 'Updated Jan 24, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'new delhi', 'stats_list': [], 'contributions': '261 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Niketkumardheeryan', 'info_list': ['SAS', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 20, 2020', 'Updated Oct 31, 2020', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 20, 2020', '4', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'HTML', 'Updated Jan 24, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'Casablanca', 'stats_list': [], 'contributions': '328 contributions\n        in the last year', 'description': [""Build an iOS Application to Predict Air Pollution Using a Random Forest Regressor\n\nUsing Swift, Flask, and sklearn to predict pollution levels on\xa0iOS\n\nI'll create a model that can predict the level of small particles on a given day in the city of London and also create a small API that will be consumed by an iOS application.\n\nTrain the model\nMacOS, Linux & Windows:\npython API/train.py\nUsage example\nDownload a large dataset of PM2.5 particles from your hometown for example. (In this repo I used data for London).\nClean up the data and add it to the API folder, make sure to use the column names that I have in my own csv file or you can change the script to fit your csv file in train.py.\nThen you can call use the train.py to train the model and save it to the current directory.\nFinally, you can predict the PM2.5 particle level by running the Flask API or you can call the prediction function in predict.py file.\nRun the API\nMacOS, Linux & Windows:\npython API/app.py\nAPI call example\nhttp://127.0.0.1:5000/{day}{month}{year}{hour}\n\nJanuary 18, 2020 at 23:00\nhttp://127.0.0.1:5000/1801202023\n\nAbout me\nOmar MHAIMDAT:\nLinkedin: Linkedin profile\nEmail: omarmhaimdat@gmail.com\nDistributed under the MIT license. See LICENSE for more information.\nOther projects\nFace detection and recognition with CoreML and\xa0ARKit\n\n\nGithub Repo\n\n\nMedium article with full tutorial\n\n\nCreating a License Plate Reading iOS Application Using OCR Technologies and CoreData\n\n\nGithub Repo\n\n\nMedium article with full tutorial\n\n\nUsing Core ML and Natural Language for Sentiment Analysis on iOS\n\n\nGithub Repo\n\n\nMedium article with full tutorial\n\n\nBuilding a Sound Classification iOS Application using AI\n\n\nGithub Repo\n\n\nMedium article with full tutorial\n\n\n""], 'url_profile': 'https://github.com/omarmhaimdat', 'info_list': ['SAS', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 20, 2020', 'Updated Oct 31, 2020', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 20, 2020', '4', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'HTML', 'Updated Jan 24, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'Cambridge, MA', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jlkruguer', 'info_list': ['SAS', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 20, 2020', 'Updated Oct 31, 2020', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 20, 2020', '4', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'HTML', 'Updated Jan 24, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['HW2: Function and Regression\nFunction practice and exploring regression with Iris data, and more GitHub practice\nPull request due by Wednesday at midnight, comments due by 12 noon Thursday.\nR Task\n\nClone and create your own branch off of Regression-repo(instructions below if you need) and review code in\niris_regression_stub.R  \n\n\nOur goal in this homework is to demonstrate that the regression line is indeed the best fitting line through the data. One way to do that is to draw arbitrary lines through the data to show that the error is bigger. To do that develop your own regression script starting from the stub.\n\nStart by writing down the tasks you need (you will include this in your markdown as an outline). Write code for each step, then make sure each step works.\nPlease read in the data from iris.csv (just to make sure you remember how to do this).\nIn class we computed the regression errors as error <- yhat - y. Compute the sum of squared errors, save it as object sse.\nPlot the regression as in the stub, but add the sse= on the plot in addition to the regression slope and intercept.\nAfter you get all of your code working, wrap it into your custom plot function that will work on any specified slope and intercept (allow users to specify slope and intercept as arguments). Feel free to pretty it up as you wish, but make sure the plot has at least the same information elements as in the stub + sse.\nDemonstrate that your function works by running it on 5 arbitrary slopes and intercepts (you can make these up by any means).\nProve that you know how to write a function. Write a new function from scratch to do anything you wish and demonstrate it.\n\n\n\nSubmission\n\n\nWrite up and push back up to GitHub:\n\nR script (.R) that contains all of your functions\nRmarkdown (.Rmd) file and its web page (.html) file that documents your solution to (2.1-2.6). Be sure to document how you developed the code and how you demonstrated that the linear regression line is the best-fit line.\nRmarkdown and html files that introduces your new function (2.7) to the world. Be sure to include an example of its use.\n\n\n\nGenerate a pull request for your work on the GitHub website to complete submission.\n\n\nGo to your classmates submissions and provide at least 2 comments on a classmateʻs work. (See Submission below for step-by-step instructions)\n\n\n\nHelpful Notes on Rmarkdown and Git/GitHub\n\nRmarkdown\nRmarkdown basics\nGenerating html with Rmarkdown\nFrom within R (make sure the .Rmd file is in your R working directory):\nrequire(rmarkdown)\n\nrender(""markdown_example.Rmd"", output_format=""html_document"") # to html\n# render(""markdown_example.Rmd"", output_format=""word_document"")  # to word doc\n# render(""markdown_example.Rmd"", output_format=""pdf_document"")  # to pdf\n# render(""markdown_example.Rmd"", output_format=""md_document"")  # to markdown\n\nWorking with GitHub:\nCloning the repository. From your Rclass directory in your CMD or Terminal window:\n git clone https://github.com/Rbootcamp-UHM/Regression-repo.git\n\nCheck the status of your branches:\ngit branch \n\nMaking your own branch (give it your own name) and check it out (this means Git will start tracking the branch):\ngit branch my-awesome-branch\ngit checkout my-awesome-branch\n\nPushing your new files or changes back up to the repository.\nCheck status first:\ngit status\n\nYou should get a message that you have changes not staged for commit on your branch.\nAdd the new files on the docket to push up to your branch on github:\ngit add myscript.R    \n\nWrite a commit message:\ngit commit -m ""Add script to do xx""\n\nFinally push up the changes to your own branch. Donʻt worry it will not change the ""master"":\ngit push origin my-awesome-branch\n\nEvery time you made edits you have to do all three:\ngit add myscript.R    \ngit commit -m ""edits to myscript.R""\ngit push origin my-awesome-branch\n\nIf any files in your branch have been changed on the server (through the browser or someone else changed them), you will need to update your local branch before pushing your new files up to the repository:\nCheck the status of your branch (and confirm which one youʻre on):\ngit branch \n\nIf you have new changes on the remote my-awesome-branch and need to update your local files:\ngit pull origin my-awesome-branch\nCheck if all is up to date:\ngit status\n\nIf everyting is up to date it will say Your branch is up-to-date with \'origin/master\'. nothing to commit, working tree clean\nBrowsing branches\nList all branches. Checkout a cool looking branch. Check which branch youʻre on:\ngit branch -a\ngit checkout someones-cool-branch\ngit branch\n\nTo go back to your own branch, or to master:\ngit checkout my-awesome-branch\ngit branch\ngit checkout master\ngit branch\n\nSubmission\nGenerate a pull request to submit your branch for review. Do this on the GitHub website.\nProvide comments (at least 2) on your classmates work. Do this through the GitHub website. They have a very nice interface.\n\nClick on Pull requests along the upper menu\nClick on one of the pull requests (branches)\nClick on Files changed along the second upper menu\nHighlight the rows you want to comment on\nYou can checkout their branch to run the code or markdown on your own computer\n\nVideo tutorial on how to comment on other peopleʻs code in a Pull Request\n'], 'url_profile': 'https://github.com/Rbootcamp-UHM', 'info_list': ['SAS', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 20, 2020', 'Updated Oct 31, 2020', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 20, 2020', '4', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'HTML', 'Updated Jan 24, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['logistic-regression\n'], 'url_profile': 'https://github.com/pradheeshub', 'info_list': ['SAS', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 20, 2020', 'Updated Oct 31, 2020', 'R', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Updated Jan 20, 2020', '4', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'HTML', 'Updated Jan 24, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 28, 2020', 'MATLAB', 'Updated Jan 20, 2020']}"
"{'location': 'Bangalore', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Linear_regression\n'], 'url_profile': 'https://github.com/Tejawiniav', 'info_list': ['Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'C++', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 22, 2020', 'R', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['linear-regression\n'], 'url_profile': 'https://github.com/MaryamSyed', 'info_list': ['Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'C++', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 22, 2020', 'R', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '219 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jjaysuriya5', 'info_list': ['Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'C++', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 22, 2020', 'R', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sultanmurat97', 'info_list': ['Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'C++', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 22, 2020', 'R', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Meta_Regression\n'], 'url_profile': 'https://github.com/waterhorse1', 'info_list': ['Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'C++', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 22, 2020', 'R', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['LinearRegression\nCode:\n1)Linear_model_4_Advertising.ipynb file can be loaded in jupyter notebook and executed directly.Dataset that i am  referring to can be downloaded from https://www.kaggle.com/zebashaikh/linear-regression-on-advertising-dataset\nIt covers Simple and mulitple linear regression concept using sklearn\n2) Regression on Boston housing dataset. Dataset is present in sklearn.dataset\n'], 'url_profile': 'https://github.com/zeba-hub', 'info_list': ['Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'C++', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 22, 2020', 'R', 'Updated Jun 3, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/namanmangla', 'info_list': ['Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'C++', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 22, 2020', 'R', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['linearRegression\nProblem statement:\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n1.Which variables are significant in predicting the price of a car\n2.How well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\nour goal is to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/Nayanakulkarni09', 'info_list': ['Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'C++', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 22, 2020', 'R', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '818 contributions\n        in the last year', 'description': ['Polynomial-regression\nUsando o conjunto de dados do aerogerador (variável de entrada: velocidade do vento – m/s, variável de saída: potência gerada – kWatts), determine os modelos de regressão polinomial (graus 2, 3, 4 e 5) com parâmetros estimados pelo método dos mínimos quadrados. Avalie a qualidade de cada modelo pela métrica R2 e R2aj.\n'], 'url_profile': 'https://github.com/Gabriel3421', 'info_list': ['Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'C++', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 22, 2020', 'R', 'Updated Jun 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Regression-analysis\n'], 'url_profile': 'https://github.com/abhi285', 'info_list': ['Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'C++', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 22, 2020', 'R', 'Updated Jun 3, 2020']}"
"{'location': 'Netherlands', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['Getting Started\nIn the project root directory, run:\n$ conda env create --prefix ./env --file env.yaml \n\nThen activate the env:\n$ conda activate ./env\n\n'], 'url_profile': 'https://github.com/dipanjank', 'info_list': ['Jupyter Notebook', 'Updated Jan 28, 2020', 'Updated Jan 23, 2020', 'Python', 'Updated Feb 3, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 23, 2020', 'TeX', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Zeenyx_Regression\n'], 'url_profile': 'https://github.com/nagarjuna2789', 'info_list': ['Jupyter Notebook', 'Updated Jan 28, 2020', 'Updated Jan 23, 2020', 'Python', 'Updated Feb 3, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 23, 2020', 'TeX', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 20, 2020']}","{'location': 'Palo Alto', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Themancrazy', 'info_list': ['Jupyter Notebook', 'Updated Jan 28, 2020', 'Updated Jan 23, 2020', 'Python', 'Updated Feb 3, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 23, 2020', 'TeX', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 20, 2020']}","{'location': 'Nairobi, Kenya', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gathoni', 'info_list': ['Jupyter Notebook', 'Updated Jan 28, 2020', 'Updated Jan 23, 2020', 'Python', 'Updated Feb 3, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 23, 2020', 'TeX', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 20, 2020']}","{'location': 'Tamil Nadu, India', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Overview\nThis is the code for this blog post.\nDependencies\nscikit-learn\npandas\nmatplotlib\n'], 'url_profile': 'https://github.com/niranjanbsubramanian', 'info_list': ['Jupyter Notebook', 'Updated Jan 28, 2020', 'Updated Jan 23, 2020', 'Python', 'Updated Feb 3, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 23, 2020', 'TeX', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 20, 2020']}","{'location': 'Denver. CO', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Icvsmig', 'info_list': ['Jupyter Notebook', 'Updated Jan 28, 2020', 'Updated Jan 23, 2020', 'Python', 'Updated Feb 3, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 23, 2020', 'TeX', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['regression_app\nThis shiny app is in development to assist with quantifying minimum conductance (gmin).\n'], 'url_profile': 'https://github.com/jeffdudek', 'info_list': ['Jupyter Notebook', 'Updated Jan 28, 2020', 'Updated Jan 23, 2020', 'Python', 'Updated Feb 3, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 23, 2020', 'TeX', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 20, 2020']}","{'location': 'Munich (DE)', 'stats_list': [], 'contributions': '1,896 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/stefanocoretta', 'info_list': ['Jupyter Notebook', 'Updated Jan 28, 2020', 'Updated Jan 23, 2020', 'Python', 'Updated Feb 3, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 23, 2020', 'TeX', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/micheal632', 'info_list': ['Jupyter Notebook', 'Updated Jan 28, 2020', 'Updated Jan 23, 2020', 'Python', 'Updated Feb 3, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 23, 2020', 'TeX', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Correlation_Regression\n'], 'url_profile': 'https://github.com/BernardKBLee', 'info_list': ['Jupyter Notebook', 'Updated Jan 28, 2020', 'Updated Jan 23, 2020', 'Python', 'Updated Feb 3, 2020', 'Updated Jan 21, 2020', 'Python', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 23, 2020', 'TeX', 'Updated Aug 7, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'HTML', 'Updated Jan 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MariFeldhaus', 'info_list': ['R', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 31, 2020', 'Updated Jan 21, 2020', 'Shell', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/muskanlalit18', 'info_list': ['R', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 31, 2020', 'Updated Jan 21, 2020', 'Shell', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Johannesburg', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/codeART96', 'info_list': ['R', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 31, 2020', 'Updated Jan 21, 2020', 'Shell', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/banims', 'info_list': ['R', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 31, 2020', 'Updated Jan 21, 2020', 'Shell', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['An example of computing and visualizing the best fit for a line using linear regression in python.\n'], 'url_profile': 'https://github.com/cneville', 'info_list': ['R', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 31, 2020', 'Updated Jan 21, 2020', 'Shell', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Kigali, Rwanda', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Linear_Regression\nLinear regression is a linear approach to model the relationship between a dependent variable (target variable) and one (simple regression) or more (multiple regression) independent variables. Python has different libraries that allow us to plot a data set and analyze the relation between variables. In case we observe a linear trend, we can calculate the line that better fits our data and make predictions using this line.\nThe dataset used in this article was obtained in Kaggle. Kaggle is an online community of data scientists and machine learners where it can be found a wide variety of datasets. The dataset contains 1338 observations (rows) and 7 features (columns). The dataset contains 4 numerical features (age, bmi, children and expenses) and 3 nominal features (sex, smoker and region).\n'], 'url_profile': 'https://github.com/Safa1Mohamed', 'info_list': ['R', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 31, 2020', 'Updated Jan 21, 2020', 'Shell', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['ML-Regression\nThis folder contains different types of regression techniques on a single dataset and their accuracy.\n'], 'url_profile': 'https://github.com/tejaskochar', 'info_list': ['R', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 31, 2020', 'Updated Jan 21, 2020', 'Shell', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kajal200031', 'info_list': ['R', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 31, 2020', 'Updated Jan 21, 2020', 'Shell', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prasanth247', 'info_list': ['R', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 31, 2020', 'Updated Jan 21, 2020', 'Shell', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/keithleung95', 'info_list': ['R', 'Updated Jan 23, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 31, 2020', 'Updated Jan 21, 2020', 'Shell', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}"
"{'location': 'Greater San Diego, USA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sanskrut-01', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Kiev', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tsergien', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': [""Regression__Analysis\nThis is the program from the course ASDS04 - Introduction to Data science with Python.\n#Goriber_Try\nThis program is about Linear regression with 2 popular libraries on python. First library is ScikitLearn. We called Linear_model from scikitlearn by importing this,\nfrom sklearn import linear_model\nBy importing this, we have used Linear_model's Ordinary least squares features. To study more of it, visit this\nhttps://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares\nHere we have used LinearRegression(), fit(), coef_, intercept_ functions. You will find details of this functions from this URL\nhttps://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares\nNext we have used Statsmodel library with api methods as\nimport statsmodels.api as sm\nIf someone face the problem on importing the library, it means you don't have this in Anaconda (Mine wasn't). Then you need to download and install the library by writing this in CMD / Powershell, the command is -\nconda install -c conda-forge statsmodels\nThis program has used add__constant(), OLS(), fit(), summary() - these methods. To study in details, visit the URL-\nhttps://www.statsmodels.org/stable/gettingstarted.html\nRun only the portions of 1st block and 2nd block by pressing F9 button. What is 1st block and 2nd block, see the code comment please.\nAlso check the comments inside the program to understand why here is used multiple regression and how to analyze single regression.\nHope it helps, happy pythonning\n""], 'url_profile': 'https://github.com/Mobinul341', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': [""SymNet\n\nSymNet is a deep learning pipeline with a focus on simplicity. Functionality is available through command-line options or as an API. The focus is\non simplicity and getting quick results.\nAPI Usage\nNumeric data\nThe symnet.py file shows how to use the API for multi-class classification\non a tabular (CSV) dataset. Start by creating a model:\nmodel = NumericModel(csv_path, n_classes=3, label_column='target', task='classification')\n\nThen, you can call fit and predict on the model, or find the loss and accuracy using\nthe score method.\nImage data\nImage classifiers inherit from AbstractImageClassificationModel. Currently,\nonly ResNet is implemented. See symnet.py for example usage. Like\nall models, you can call fit, predict, and score.\nCLI Usage\nYou can use the symnet.py file to run classification on a tabular dataset. The available options are:\n\n--task: One of 'classification' and 'regression'\n--dataset: The CSV dataset.\n--data-type: As of now, only 'numeric' and 'image' are supported.\n--labels: The CSV column with labels\n--num-classes: Number of classes (for classification)\n--activation: The activation to use. Any of ('relu', 'elu', 'selu', 'sigmoid', 'softmax', 'linear', 'sbaf', 'arelu', 'softplus)\n--no-header: Indicates that the CSV does not have a header row\n--batch-size: The batch size to use\n--train-split: The training data subset split size\n--epochs: The number of epochs\n--no-balance: Do not rebalance classes in classification problems\n--no-augment: For image datasets, do not augment the data\n\nDocker\nThe Dockerfile in symnet-docker sets up a minimal Debian Stretch system with Python 3.7 and\nrequired packages installed. Run it with\ndocker run -it -v [host-src:]/symnet symnet /bin/bash\n\nThis starts up an interactive terminal, mounts a volume at /symnet in the container,\nand runs a Bash shell. You can change the command run in the\nlast argument.\nTodo\n\n  Add DenseNet architecture\n  Add support for text datasets\n  Add support for image segmentation tasks\n  Resize and normalize images\n  For images, use LipschitzLR scheduler\n\nCite our work\nSymNet uses the LipschitzLR learning rate policy: arXiv:1902.07399\nBibTeX entry:\n@article{yedida2019novel,\n  title={A novel adaptive learning rate scheduler for deep neural networks},\n  author={Yedida, Rahul and Saha, Snehanshu},\n  journal={arXiv preprint arXiv:1902.07399},\n  year={2019}\n}\n\nSymNet also implements the SBAF and A-ReLU activation functions: arXiv:1906.01975.\nIf you use these, please cite:\n@article{saha2019evolution,\n  title={Evolution of Novel Activation Functions in Neural Network Training with Applications to Classification of Exoplanets},\n  author={Saha, Snehanshu and Nagaraj, Nithin and Mathur, Archana and Yedida, Rahul},\n  journal={arXiv preprint arXiv:1906.01975},\n  year={2019}\n}\n\n""], 'url_profile': 'https://github.com/tej-prash', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Logistic-Regression\nPython based Code\nLast checked on uploaded date\n'], 'url_profile': 'https://github.com/abubakaar', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Regression-Model\nThis repository contains Linear Regression and Multi linear Regression model\n'], 'url_profile': 'https://github.com/muneebahmad174', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/marellapudihimamsu', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'Seattle, WA', 'stats_list': [], 'contributions': '513 contributions\n        in the last year', 'description': [""Logistic-Regression\nThis is an implementation of Logistic Regression using Newton's method. The implementation also includes the Sigma function, Loss function, Gradient, and Hessian.\nGoogle Colab URL: https://colab.research.google.com/drive/1ArjzMP3KNqeNn3IMxeAErznxn4CBCqCq?authuser=1\n""], 'url_profile': 'https://github.com/vcoz17', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/terao0724', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}","{'location': 'banglore', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['LOGISTIC-REGRESSION\n'], 'url_profile': 'https://github.com/naveenajaykumar', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 24, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Feb 7, 2020', 'Python', 'Updated Jan 21, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Linear-Regression\nData Science Practise\nThis practise has been done with the following instruction:\nhttps://towardsdatascience.com/a-beginners-guide-to-linear-regression-in-python-with-scikit-learn-83a8f7ae2b4f\n'], 'url_profile': 'https://github.com/Sulagna-Chowdhury', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'Updated Jan 23, 2020', '1', 'HTML', 'MIT license', 'Updated Apr 19, 2020', 'HTML', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '818 contributions\n        in the last year', 'description': ['ELM-regression\nDetermine um modelo de regressão usando rede neural Extreme Learning Machine (ELM) para o conjunto de dados aerogerador.dat (variável de entrada: velocidade do vento, variável de saída: potência gerada). Avalie a qualidade do modelo pela métrica R2 para diferentes quantidades de neurônios ocultos.\n'], 'url_profile': 'https://github.com/Gabriel3421', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'Updated Jan 23, 2020', '1', 'HTML', 'MIT license', 'Updated Apr 19, 2020', 'HTML', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['multivariate-regression\n'], 'url_profile': 'https://github.com/MaryamSyed', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'Updated Jan 23, 2020', '1', 'HTML', 'MIT license', 'Updated Apr 19, 2020', 'HTML', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '301 contributions\n        in the last year', 'description': ['linear_regression\nExercice de régression linéaire avec TensorFlow.js , p5.js et Vue.js\n'], 'url_profile': 'https://github.com/vysahjk', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'Updated Jan 23, 2020', '1', 'HTML', 'MIT license', 'Updated Apr 19, 2020', 'HTML', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': [""Linear-Regression\nGR5205 at Columbia University\nIt's a linear regression course from an applied angle. In this repository, I upload notes on important topics and a few important assignments I've done. Logic behind each of the assignment is about how to test an assumption through simulations with little deduction involved.\nTopics for the course\n\nDifferent objective functions to achieve linear regression.\nInference and prediction on coefficients and differents between the two.\nEstimator evaluation. Diagnoses.\nFour assumptions doing linear regression but what should we do if the assumptions are broken.\nSimultaneous inference including Bonferroni, Hack and traditional Pointwise. ANOVA is mentioned for nest model.\nDealing with high demensional data with variable selection (AIC, BIC, Forward/Backward selection) and cross validation.\nDirected Acycli Graphs (DAGs).\n\nTopics for the assignments\n\nTwo samples about simple linear regression.\nBootstrap & Categorical data (Logistics Regression) with pulsar star prediction.\nWeighted Linear Square (Individual vs Aggregated)\nModeling with DAG\nHigh demensional regression and Instrumental Variables\n\n""], 'url_profile': 'https://github.com/xinyi0351', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'Updated Jan 23, 2020', '1', 'HTML', 'MIT license', 'Updated Apr 19, 2020', 'HTML', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yfirdaws', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'Updated Jan 23, 2020', '1', 'HTML', 'MIT license', 'Updated Apr 19, 2020', 'HTML', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020']}","{'location': 'Kolkata, India', 'stats_list': [], 'contributions': '153 contributions\n        in the last year', 'description': ['Machine Learning Algorithms\nAll basic machine learning algorithm like KMeans, Linear regression, Logistic regression, Hebbs neural network, KNN, Decision Tree, Naive Bayes, Adaline, SVM implemented in python and scikit-learn\n'], 'url_profile': 'https://github.com/gobinda168', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'Updated Jan 23, 2020', '1', 'HTML', 'MIT license', 'Updated Apr 19, 2020', 'HTML', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Context:\nThis dataset contains tree observations from four areas of the Roosevelt National Forest in Colorado. All observations are cartographic variables (no remote sensing) from 30 meter x 30 meter sections of forest. There are over half a million measurements total!\nContent:\nThis dataset includes information on tree type, shadow coverage, distance to nearby landmarks (roads etcetera), soil type, and local topography. And the Project objective is to build a model that predicts what types of trees grow in an area based on the surrounding characteristics\nAcknowledgement:\nThis dataset is part of the UCI Machine Learning Repository, and the original source can be found here. The original database owners are Jock A. Blackard, Dr. Denis J. Dean, and Dr. Charles W. Anderson of the Remote Sensing and GIS Program at Colorado State University.\nData_Dictionary\nElevation = Elevation in meters.\nAspect = Aspect in degrees azimuth.\nSlope = Slope in degrees.\nHorizontal_Distance_To_Hydrology = Horizontal distance to nearest surface water features.\nVertical_Distance_To_Hydrology = Vertical distance to nearest surface water features.\nHorizontal_Distance_To_Roadways = Horizontal distance to nearest roadway.\nHillshade_9am = Hill shade index at 9am, summer solstice. Value out of 255.\nHillshade_Noon = Hill shade index at noon, summer solstice. Value out of 255.\nHillshade_3pm = Hill shade index at 3pm, summer solstice. Value out of 255.\nHorizontal_Distance_To_Fire_Point = sHorizontal distance to nearest wildfire ignition points.\nWilderness_Area1 = Rawah Wilderness Area\nWilderness_Area2 = Neota Wilderness Area\nWilderness_Area3 = Comanche Peak Wilderness Area\nWilderness_Area4 = Cache la Poudre Wilderness Area\nSoil_Type1 to Soil_Type39\nCover_Type - Forest Cover Type designation(Target Variable), with the following key:\nSpruce/Fir\nLodgepole Pine\nPonderosa Pine\nCottonwood/Willow\nAspen\nDouglas-fir\nKrummholz\nReference: https://www.kaggle.com/uciml/forest-cover-type-dataset\n'], 'url_profile': 'https://github.com/pradeepmuni', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'Updated Jan 23, 2020', '1', 'HTML', 'MIT license', 'Updated Apr 19, 2020', 'HTML', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Dataset : AirBnB Newyork Price prediction\nContext\nSince 2008, guests and hosts have used Airbnb to expand on traveling possibilities and present more unique, personalized way of experiencing the world. This dataset describes the listing activity and metrics in NYC, NY for 2019.\nContent\nThis data file includes all needed information to find out more about hosts, geographical availability, necessary metrics to make predictions and draw conclusions.\nAcknowledgements\nThis public dataset is part of Airbnb, and the original source can be found on this website.\nInspiration\nWhat can we learn about different hosts and areas?\nWhat can we learn from predictions? (ex: locations, prices, reviews, etc)\nWhich hosts are the busiest and why?\nIs there any noticeable difference of traffic among different areas and what could be the reason for it?\nRegression Problem: Predict the price of the airbnb listing based on various features in the dataset\n'], 'url_profile': 'https://github.com/pradeepmuni', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'Updated Jan 23, 2020', '1', 'HTML', 'MIT license', 'Updated Apr 19, 2020', 'HTML', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Simple Linear Regression\nThis repository is used to predict the salary of the employees with their experience.\n#100_Days_Of_ML\nThis is my first day and Iam learning about Simple Linear Reggression.\nLearn About !\n\nNeed to learn about DataFrames in pandas\nNeed to learn about StandardScaler\n\nSciKit-Learn\n\nIt uses ""sklearn.linear_model"" to generate the model\n\n from sklearn.linear_model import LinearRegression\n regressor = LinearRegression()\n regressor.fit(X_train, Y_train)\n\nLicense\nMIT\nFree Software, Hell Yeah!\n'], 'url_profile': 'https://github.com/Mohendran', 'info_list': ['Python', 'Updated Jan 20, 2020', 'Python', 'MIT license', 'Updated Jan 22, 2020', 'Updated Jan 23, 2020', '1', 'HTML', 'MIT license', 'Updated Apr 19, 2020', 'HTML', 'Updated Jan 29, 2021', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020']}"
"{'location': 'Chennai', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SaurabhCegian', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'R', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': [""Logistics-Regression-in-Python\nIn this dataset, we are determining whether person's income is less than 50k or more than 50k based on different parameters\n""], 'url_profile': 'https://github.com/anujjohri', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'R', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['multi-linear-regression\n'], 'url_profile': 'https://github.com/monika2612', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'R', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nagarjuna2789', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'R', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['SummarisingTempQuantileRegression\nSummarising Changes In Air Temperature Over Iran By Quantile Regression And Clustering\n'], 'url_profile': 'https://github.com/shirazipooya', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'R', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'Poland, Warsaw', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': [""Project description\n\nLearn best Linear Regression model on housing data using RFECV feature selection.  \nModel 1: default features \nModel 2: with polynomial features (degree 2)\nFeature visualizations - examples\n\n\n\n\nCorelation heatmap - only for numeric variables\n\nModel 1\nRFECV result\nOptimal number of features: 13\n\n\nModel summary\n\nRegression diagnostics - on test set\n\n\n\n\n\n\nGraph shows that error increases with increasing price. There is heteroscedascity. This leads to hypotesis that accepted form of the model isn't linear. Adding polynomial features should lead to model improvment. We can expect that price of expensive apartments depends on other factors then the price of cheap apartments.   \nModel 2\nRFECV result\nOptimal number of features: 118\n\n\nModel summary\n\nRegression diagnostics - on test set\n\n\n\n\n\n \nGraph shows model improvment compared to model 1 without polynomial features. \nModel comparision\n\n\n\n\nnames\nMSE\nR2\nn_features\n\n\n\n\nmodel 1 train\n4.87e+10\n0.636\n13\n\n\nmodel 1 test\n4.89e+10\n0.648\n13\n\n\nmodel 2 train\n3.61e+10\n0.730\n119\n\n\nmodel 2 test\n3.70e+10\n0.734\n119\n\n\n\n""], 'url_profile': 'https://github.com/mateusz-g94', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'R', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saninmersion', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'R', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['simpleLinerRegressionExercise-1\nPractice\n'], 'url_profile': 'https://github.com/cristian-nicp', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'R', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'Netherlands', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/girish208', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'R', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['Univariate-Linear-Regression\n'], 'url_profile': 'https://github.com/stelios357', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'R', 'MIT license', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 28, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}"
"{'location': 'Seattle, Washington', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Implementation of Bootstrapping Technique\nIntroduction\nBootstrapping is a non-parametric technique that does not require distributional assumptions (such as normally distributed errors),and can provide more accurate inferences when the data are not well behaved or when the sample size is small. Bootstrapping uses the sample data to estimate relevant characteristics of the population.The sampling distribution of a statistic is then constructed empirically by resampling from the sample. The goal of this study is to examine whether a bootstrap regression gives similar parameters (beta estimate and standard errors) as compared to a simple linear regression using the UCLA Graduate Dataset.\nWe used the open source UCLA Graduate Dataset which was available on Kaggle for this study. For this study, we implemented two steps- 1) Perform a linear regression and a Bootstrap regression on the same dataset 2) Compare the model parameters in the two cases\n'], 'url_profile': 'https://github.com/Ray-305', 'info_list': ['Updated Jan 24, 2020', 'Updated Jan 22, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 20, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Logistic-Regression-using-R\n'], 'url_profile': 'https://github.com/VishalNayak-92', 'info_list': ['Updated Jan 24, 2020', 'Updated Jan 22, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Drug-Overdose-Death-Regressions\nThis project uses R measure the likelihood of someone overdosing using panel data from Indiana Counties.\n'], 'url_profile': 'https://github.com/ryafranc', 'info_list': ['Updated Jan 24, 2020', 'Updated Jan 22, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 20, 2020']}","{'location': 'Indore ,India', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Logistic_regression_prediction\n'], 'url_profile': 'https://github.com/shubham22121998', 'info_list': ['Updated Jan 24, 2020', 'Updated Jan 22, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['simple-linear-regression\n'], 'url_profile': 'https://github.com/monika2612', 'info_list': ['Updated Jan 24, 2020', 'Updated Jan 22, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['World_Happiness_Regression\nUses data related to economic freedom to predict individual happiness by nation\n'], 'url_profile': 'https://github.com/Thybony', 'info_list': ['Updated Jan 24, 2020', 'Updated Jan 22, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 20, 2020']}","{'location': 'Orlando, FL', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['DataPrep_LinearRegression\n'], 'url_profile': 'https://github.com/sbarter510', 'info_list': ['Updated Jan 24, 2020', 'Updated Jan 22, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 20, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alexsinha', 'info_list': ['Updated Jan 24, 2020', 'Updated Jan 22, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 20, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Linear-Regression-notebook\nRegularised Linear Regression and gradient descent from scratch using numpy\nThis repo contains a Jupyter Notebook where I build a linear regression model along with the relevent maths from first principles, along with implementing my own gradient descent algorithm using numpy.\n'], 'url_profile': 'https://github.com/tomukmatthews', 'info_list': ['Updated Jan 24, 2020', 'Updated Jan 22, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 20, 2020']}","{'location': 'Greater San Diego, USA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sanskrut-01', 'info_list': ['Updated Jan 24, 2020', 'Updated Jan 22, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Python', 'Updated Jan 20, 2020']}"
"{'location': 'Bengaluru', 'stats_list': [], 'contributions': '291 contributions\n        in the last year', 'description': ['Simple_Linear_Regression\nGiven a data set called Salary_Data, a model is devised to predict the salary of an employee of a particular company by using Simple Linear Regression\n'], 'url_profile': 'https://github.com/Monalika-P', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 20, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pawanwhig', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 20, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'Santiago, Chile', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juanmadepalacios', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 20, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Zeenyx_Regression_KS\n'], 'url_profile': 'https://github.com/nagarjuna2789', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 20, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['Multi-Variate-Linear-Regression\n'], 'url_profile': 'https://github.com/stelios357', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 20, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DeltaOptimist', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 20, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/surya111111', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 20, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'Seattle, WA', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anil-joshi', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 20, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'Heidelberg', 'stats_list': [], 'contributions': '346 contributions\n        in the last year', 'description': [""This project was completed as a part of Udacity's Data Science Nanodegrees and consists of the following three parts:\nPart I - Probability\nPart II - A/B Test\nPart III - Regression\nIt analyses conversion of users on a website, sets and tests a hypothesis, visualizes the results with matplotlib and uses statsmodels and scipy to evaluate the results.\nFinally, by using statsmodels we fit a regression model to see if there is a significant difference in conversion based on which page customers receive.\nLibraries used:\npandas\nnumpy\nrandom\nmatplotlib.pyplot\nstatsmodels.api\n""], 'url_profile': 'https://github.com/blazova', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 20, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}","{'location': 'Bari, Italy', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['Regressione-Logistica-TUMORI-\n'], 'url_profile': 'https://github.com/giuseppemaiorano', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 24, 2020', 'R', 'Updated Jan 25, 2020', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'R', 'Updated Jan 20, 2020', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anagha5393', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prathamgarg-27', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Siriratkant', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'Bari, Italy', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['Regressione-Logistica-TUMORI-\n'], 'url_profile': 'https://github.com/giuseppemaiorano', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['KNN-DecisionTrees-Regression\n'], 'url_profile': 'https://github.com/ritadi', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/namanmangla', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['tensorflow-Regression-form-scratch\n'], 'url_profile': 'https://github.com/asifmohammed033', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['ML_LinearRegression\nLinear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting. Different regression models differ based on – the kind of relationship between dependent and independent variables, they are considering and the number of independent variables being used.\nWe use the equation of line to compute values of m and c and plot a graph to represnt the linear regression.\nAt the end we calculate the RMS error value.\n'], 'url_profile': 'https://github.com/varvaderishikesh', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Salary-Analysis-Linear-Regression\n'], 'url_profile': 'https://github.com/Wonder13oy', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/minaee', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 22, 2020', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Feb 25, 2020', 'Python', 'Updated Jan 21, 2020']}"
"{'location': 'Hangzhou', 'stats_list': [], 'contributions': '153 contributions\n        in the last year', 'description': ['linear_regression_project\nIndividual project from the course ""Foundations of Data Analytics and Machine Learning"". Topics covered in this project including: using linear regression to train a model from the dataset; Compair the efficiency of using different linear regression procedures.\nInstructions\n\nopen .ipynb file in Github/JuputerNotebook/GoogleColaboratory\n\n'], 'url_profile': 'https://github.com/YixiaoHong', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'R', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'Evry', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['Opti_LassoRegression\n'], 'url_profile': 'https://github.com/odeliaG', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'R', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'Hong Kong', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Regression-Boston-Data\nGetting started with non linear regression to predict house prices in boston based on various features.\n'], 'url_profile': 'https://github.com/SameerSinghDudi', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'R', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'Argentina', 'stats_list': [], 'contributions': '285 contributions\n        in the last year', 'description': [""ml-regression-multivariate-linear\n\n\nMultivariate linear regression. [edited for Angular 2+ usage]\nInstallation\nnpm install --save ngx-regression-multivariate-linear\nAPI ( See ml-regression-multivariate-linear )\nnew MLR(x, y[, options])\nArguments\n\nx: Matrix containing the inputs\ny: Matrix containing the outputs\n\nOptions\n\nintercept: boolean indicating if intercept terms should be computed (default: true)\nstatistics: boolean for calculating and returning regression statistics (default: true)\n\nUsage\nimport MLR from 'ngx-regression-multivariate-linear';\n\nconst x = [[0, 0], [1, 2], [2, 3], [3, 4]];\n// Y0 = X0 * 2, Y1 = X1 * 2, Y2 = X0 + X1\nconst y = [[0, 0, 0], [2, 4, 3], [4, 6, 5], [6, 8, 7]];\nconst mlr = new MLR(x, y);\nconsole.log(mlr.predict([3, 3]));\n// [6, 6, 6]\nLicense\nMIT\n""], 'url_profile': 'https://github.com/alexlndn', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'R', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chris-payne', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'R', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['-Regression-in-deep-learning\nRegression to Boston House Prices in deep learning in keras library\n'], 'url_profile': 'https://github.com/mahamad5ahmad', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'R', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['HousingPrices_LogisticRegression\nIn this project I will show the whole process from data cleansing to deploy a logist regression model in python.\nThe dataset is from kaggle (Ames Iowa).\nOutliers are not removed, the Machine Learning version will be uploaded soon\n'], 'url_profile': 'https://github.com/gmsaez', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'R', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['simple-linear-regression-mpg\n'], 'url_profile': 'https://github.com/learn-co-curriculum', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'R', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Linear-Regression-Diamonds\nLinear Regression tools in R to make predictions on diamond prices based on color, cut, clarity and carat of diamond\n'], 'url_profile': 'https://github.com/artimpatel00', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'R', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['ts_regression_forecasting\na simple implementation of regression forecasting methods for predicting time series\n'], 'url_profile': 'https://github.com/hatellezp', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'R', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 22, 2020']}"
"{'location': 'Evry', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['Opti_LassoRegression\n'], 'url_profile': 'https://github.com/odeliaG', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 25, 2020']}","{'location': 'Hong Kong', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Regression-Boston-Data\nGetting started with non linear regression to predict house prices in boston based on various features.\n'], 'url_profile': 'https://github.com/SameerSinghDudi', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 25, 2020']}","{'location': 'Argentina', 'stats_list': [], 'contributions': '285 contributions\n        in the last year', 'description': [""ml-regression-multivariate-linear\n\n\nMultivariate linear regression. [edited for Angular 2+ usage]\nInstallation\nnpm install --save ngx-regression-multivariate-linear\nAPI ( See ml-regression-multivariate-linear )\nnew MLR(x, y[, options])\nArguments\n\nx: Matrix containing the inputs\ny: Matrix containing the outputs\n\nOptions\n\nintercept: boolean indicating if intercept terms should be computed (default: true)\nstatistics: boolean for calculating and returning regression statistics (default: true)\n\nUsage\nimport MLR from 'ngx-regression-multivariate-linear';\n\nconst x = [[0, 0], [1, 2], [2, 3], [3, 4]];\n// Y0 = X0 * 2, Y1 = X1 * 2, Y2 = X0 + X1\nconst y = [[0, 0, 0], [2, 4, 3], [4, 6, 5], [6, 8, 7]];\nconst mlr = new MLR(x, y);\nconsole.log(mlr.predict([3, 3]));\n// [6, 6, 6]\nLicense\nMIT\n""], 'url_profile': 'https://github.com/alexlndn', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chris-payne', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zzhKev1n', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ron-Rocks', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['logistic-linear-regression\n'], 'url_profile': 'https://github.com/monika2612', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 25, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/satyan-dot', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 25, 2020']}","{'location': 'Nairobi, Kenya', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': [""House Price Prediction - Regression Analysis\n1. Defining the Question\na) Specifying the Question\nAs a Data Scientist, you work for Hass Consulting Company which is a real estate leader with over 25 years of experience. You have been tasked to study the factors that affect housing prices using the given information on real estate properties that was collected over the past few months.\nLater onwards, create a model that would allow the company to accurately predict the sale of prices upon being provided with the predictor variables.\nb) Defining the Metric for Success\nA model that accurately predicts the sale of prices upon being provided with the predictor variables.\nRegression techniques\nMultiple Linear Regression\nQuantile Regression\nRidge Regression\nLasso Regression\nElastic Net Regression\nc) Understanding the context\nTasked to study the factors that affect housing prices using the given information on real estate properties collected over the past few months. Later to, create a model that would allow the company to accurately predict the sale of prices upon being provided with the predictor variables.\nDataset Glossary\nId\nprice - Price of the house\nbedrooms - Number of Bedrooms\nbathrooms - Number of Bathrooms\nsqft_living - Square feet area of living area\nsqft_lot - Square feet area of parking Layout\nfloors - Number of Floors\nwaterfront - Whether waterfront is there or not\nview - Number of Views\ngrade - Grades\nsqft_above\nsqft_basement - Square feet area off basement\nyr_built - Year the house is built\nyr_renovated - Year the house is renovated\nzipcode - zipcode os the house\nlat : Latitude of the house\nlon : Longitude of the house\nsqft_living15\nsqft_lot15\nd) Recording the Experimental Design\nExplore given dataset\nDefine the appropriateness of the available data to answer the given question.\nFind and deal with outliers, anomalies, and missing data within the dataset.\nPerform univariate, bivariate and multivariate analysis recording your observations.\nPerforming regression analysis.\nIncorporate categorical independent variables into your models.\nCheck for multicollinearity\nProvide a recommendation based on your analysis.\nCreate residual plots for your models, and assess heteroskedasticity using Barlett's test.\nChallenge your solution by providing insights on how you can make improvements in model improvement.\n""], 'url_profile': 'https://github.com/gathoni', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '178 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/calzzone', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'MIT license', 'Updated Jan 22, 2020', 'R', 'Updated Mar 4, 2020', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 23, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Updated Jan 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '178 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/calzzone', 'info_list': ['Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '132 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shikhnu', 'info_list': ['Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\n'], 'url_profile': 'https://github.com/ikimran26', 'info_list': ['Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['house_price_regression\n'], 'url_profile': 'https://github.com/shoubhikchakraborty', 'info_list': ['Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'Belgrade, Serbia', 'stats_list': [], 'contributions': '379 contributions\n        in the last year', 'description': ['Webstorm jsdoc promise regression\nWithout a hint:\n\nWith a hint:\n\n'], 'url_profile': 'https://github.com/panta82', 'info_list': ['Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'Mexico City', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/CarlosDNieto', 'info_list': ['Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '200 contributions\n        in the last year', 'description': [""\n\ncrds-components\nThis project contains custom web components developed for Crossroads' Church. See StencilJS for information on the framework.\nThe following components are made available via this library. Please see the README file for each, for more information on usage, available props, etc.\n\n<crds-heart-button />\n<crds-modal />\n<crds-shared-footer />\n<crds-shared-header />\n<crds-snail-trail />\n<crds-subscribe />\n<crds-site-happenings />\n<life-stages />\n<greeting-component />\n\nEnvironments\nYou need to export the following variables when building or running the project...\n\n\n\nVariable\nDescription\n\n\n\n\nCONTENTFUL_ACCESS_TOKEN\nAccess token for the Contentful Delivery API\n\n\nCONTENTFUL_SPACE_ID\nContentful Space ID\n\n\nCONTENTFUL_ENV\nContentful environment (defaults to master)\n\n\nCRDS_INTERACTIONS_ENDPOINT\nService endpoint\n\n\nCRDS_GQL_ENDPOINT\nGraphQL endpoint\n\n\nENV_SUBDOMAIN\nSubdomain and cookie prefix for mp tokens.\n\n\nCRDS_GQL_ENDPOINT\nGraphQL Gateway Endpoint\n\n\nOKTA_CLIENT_ID\nOkta client id\n\n\nOKTA_OAUTH_BASE_URL\nCrds Okta base url\n\n\nCRDS_GATEWAY_SERVER_ENDPOINT\nCrds old gateway URL until we move to okta\n\n\n\nYou need to export the following variables when running tests on the project...\n\n\n\nVariable\nDescription\n\n\n\n\nTEST_SITE_USER_PW\nTest user's password\n\n\nTEST_CRDS_LOGIN_ENDPOINT\nGateway login endpoint\n\n\n\nLicense\nThis project is licensed under the 3-Clause BSD License.\n""], 'url_profile': 'https://github.com/J-Byron', 'info_list': ['Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['FEV-Regression_Analysis\nForced Expiratory Volume (FEV) is an index of pulmonary function that measures the volume of air expelled after 1 second of constant effort.\nThis repo aims to predict the FEV given certain variables such as age, gender, height, weight, and makes use of several regression techniques to find the best model to predict FEV.\nTechniques\n\nExploratory Data Analytics\nStandardization\nInterpretation of Coefficient\nT-test, F-test, ANOVA\nResidual Plots\nInteraction Terms\nBox-Cox Transformation\nBox-Tidwell Transformation\nMulticollinearity (Correlation Matrix, Variance Inflation Factor, Condition Number)\n\n'], 'url_profile': 'https://github.com/bensjx', 'info_list': ['Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'Stony Brook', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shubham12595', 'info_list': ['Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'Karachi,Pakistan', 'stats_list': [], 'contributions': '325 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alihussainia', 'info_list': ['Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'JavaScript', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'TypeScript', 'MIT license', 'Updated Jan 21, 2020', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['linearRegressionR\nLinear regression & multiple lineal regression examples with R. Simple & easy to follow , even if you are a begginer.\n'], 'url_profile': 'https://github.com/DavidPalomeque', 'info_list': ['R', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', 'R', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'HTML', 'Updated Jan 24, 2020', 'PowerShell', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'TypeScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'SAS', 'Updated Jan 23, 2020']}","{'location': 'Santiago, Chile', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Juanmadepalacios', 'info_list': ['R', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', 'R', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'HTML', 'Updated Jan 24, 2020', 'PowerShell', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'TypeScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'SAS', 'Updated Jan 23, 2020']}","{'location': 'Boston', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Red-wine-quality_regression\nIntroduction\n\nThe dataset consists of red variants of the Portuguese ""Vinho Verde"" wine.\xa0\xa0The physicochemical (inputs) and sensory (the output) variables are available for us to assess the quality metrics of the variant of wine.\n\nThese datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).\n\nInput variables (based on physicochemical tests)\\: fixed acidity\xa0, volatile acidity, citric acid, residual sugar, chlorides,\xa0free sulphur dioxide, total sulphur dioxide, density, pH, sulphates, alcohol Output variable (based on sensory data)\\:\xa0quality (score between 0 and 10)\xa0\n\nGoal\n\nTo find a regression model of wine quality with the various physicochemical variables\n\nThree regression techniques were applied, under a computationally efficient procedure that performs       simultaneous variable and model selection and that is guided by the sensitivity analysis\n\n\n\nThe business objective is to support the wine expert evaluations and ultimately improve the quality\n\nFigure out the significant physiochemical variables that affect the quality of the red wine\n\nFind out the correlation (positive & negative) between quality of wine & physiochemical variables \n\nObtain predictions from the fitted generalized linear model for most significant physiochemical variables \n\n'], 'url_profile': 'https://github.com/ashwink9', 'info_list': ['R', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', 'R', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'HTML', 'Updated Jan 24, 2020', 'PowerShell', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'TypeScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'SAS', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Use a logistic regression model to find the factors that were most important in predicting survival on the Titanic.\n\nLoad the train and test datasets.\nGet basic descriptive statistics for the training data and check for missing and incorrect values. Replace missing values if this makes sense to do.\nWhat are the factors that predict survival? (i.e. which variables significantly predict survival, p < 0.05)\nCreate a classification report and confusion matrix of predicted and observed values. What is the accuracy, sensitivity and specificity of the model on the (a) training and    (b) test data?\nPlot a Receiver Operating Characteristic (ROC) curve on the test data.\nWhat is overdispersion?\n\n'], 'url_profile': 'https://github.com/keanubowker8', 'info_list': ['R', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', 'R', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'HTML', 'Updated Jan 24, 2020', 'PowerShell', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'TypeScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'SAS', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ChenXiaoZhan', 'info_list': ['R', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', 'R', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'HTML', 'Updated Jan 24, 2020', 'PowerShell', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'TypeScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'SAS', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mizzouliger', 'info_list': ['R', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', 'R', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'HTML', 'Updated Jan 24, 2020', 'PowerShell', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'TypeScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'SAS', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '845 contributions\n        in the last year', 'description': ['""# Reggie-s-Linear-Regression""\n'], 'url_profile': 'https://github.com/zongpoljkk', 'info_list': ['R', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', 'R', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'HTML', 'Updated Jan 24, 2020', 'PowerShell', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'TypeScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'SAS', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': [""\n\n\nA progressive Node.js framework for building efficient and scalable server-side applications, heavily inspired by Angular.\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescription\nNest framework TypeScript starter repository.\nInstallation\n$ npm install\nRunning the app\n# development\n$ npm run start\n\n# watch mode\n$ npm run start:dev\n\n# production mode\n$ npm run start:prod\nTest\n# unit tests\n$ npm run test\n\n# e2e tests\n$ npm run test:e2e\n\n# test coverage\n$ npm run test:cov\nSupport\nNest is an MIT-licensed open source project. It can grow thanks to the sponsors and support by the amazing backers. If you'd like to join them, please read more here.\nStay in touch\n\nAuthor - Kamil Myśliwiec\nWebsite - https://nestjs.com\nTwitter - @nestframework\n\nLicense\nNest is MIT licensed.\n""], 'url_profile': 'https://github.com/AcidSlide', 'info_list': ['R', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', 'R', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'HTML', 'Updated Jan 24, 2020', 'PowerShell', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'TypeScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'SAS', 'Updated Jan 23, 2020']}","{'location': 'Austin, Texas', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['housing_prices_regression\nAn exercise in using linear regression to predict the sale price of a house.\n'], 'url_profile': 'https://github.com/madxdimac', 'info_list': ['R', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', 'R', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'HTML', 'Updated Jan 24, 2020', 'PowerShell', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'TypeScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'SAS', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rraasch', 'info_list': ['R', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 26, 2020', 'R', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'HTML', 'Updated Jan 24, 2020', 'PowerShell', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'TypeScript', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'SAS', 'Updated Jan 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Zhenia-Magic', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 24, 2020', 'Java', 'Updated Jan 26, 2020', '4', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 22, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['Multiple Linear Regression\nPredicting Home Price\n#Load the .Rdata\nlibrary(car)\n## Loading required package: carData\n\nload(""~/OneDrive - MNSCU/myGithub/Statistics/Regression_models/Multiple_Linear_Regression/Multiple-Linear-Regression/Data/mult.Rdata"")\nload(""~/OneDrive - MNSCU/myGithub/Statistics/Regression_models/Multiple_Linear_Regression/Multiple-Linear-Regression/Data/Regression.Rdata"")\nData\nThe data for these sales comes from the official public records of home\nsales in the King County area, Washington State. The data set contains\n21,606 homes that sold between May 2014 and May 2015. Load the train and\ntest data sets. The description of all variables is in the data section.\nsetwd(""~/OneDrive - MNSCU/myGithub/Statistics/Regression_models/Multiple_Linear_Regression/Multiple-Linear-Regression/Data"")\nKingTest = read.csv(file = ""King County Homes (test).csv"")\nKing = read.csv(""King County Homes (train).csv"")\nsetwd(""~/OneDrive - MNSCU/myGithub/Statistics/Regression_models/Multiple_Linear_Regression/Multiple-Linear-Regression"")\nstr(King)\n## \'data.frame\':    16187 obs. of  21 variables:\n##  $ ID           : int  1 2 3 4 5 6 7 8 9 10 ...\n##  $ price        : int  221900 538000 180000 510000 1230000 257500 291850 229500 323000 662500 ...\n##  $ bedrooms     : int  3 3 2 3 4 3 3 3 3 3 ...\n##  $ bathrooms    : num  1 2.25 1 2 4.5 2.25 1.5 1 2.5 2.5 ...\n##  $ sqft_living  : int  1180 2570 770 1680 5420 1715 1060 1780 1890 3560 ...\n##  $ sqft_lot     : int  5650 7242 10000 8080 101930 6819 9711 7470 6560 9796 ...\n##  $ floors       : num  1 2 1 1 1 2 1 1 2 1 ...\n##  $ waterfront   : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ view         : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ condition    : int  3 3 3 3 3 3 3 3 3 3 ...\n##  $ grade        : int  7 7 6 8 11 7 7 7 7 8 ...\n##  $ sqft_above   : int  1180 2170 770 1680 3890 1715 1060 1050 1890 1860 ...\n##  $ sqft_basement: int  0 400 0 0 1530 0 0 730 0 1700 ...\n##  $ yr_built     : int  1955 1951 1933 1987 2001 1995 1963 1960 2003 1965 ...\n##  $ yr_renovated : int  0 1991 0 0 0 0 0 0 0 0 ...\n##  $ renovated    : int  0 1 0 0 0 0 0 0 0 0 ...\n##  $ zipcode      : int  98178 98125 98028 98074 98053 98003 98198 98146 98038 98007 ...\n##  $ lat          : num  47.5 47.7 47.7 47.6 47.7 ...\n##  $ long         : num  -122 -122 -122 -122 -122 ...\n##  $ sqft_living15: int  1340 1690 2720 1800 4760 2238 1650 1780 2390 2210 ...\n##  $ sqft_lot15   : int  5650 7639 8062 7503 101930 6819 9711 8113 7570 8925 ...\n\nsummary(King)\n##        ID            price            bedrooms        bathrooms    \n##  Min.   :    1   Min.   :  75000   Min.   : 0.000   Min.   :0.000  \n##  1st Qu.: 4048   1st Qu.: 324624   1st Qu.: 3.000   1st Qu.:1.750  \n##  Median : 8094   Median : 451000   Median : 3.000   Median :2.250  \n##  Mean   : 8094   Mean   : 542802   Mean   : 3.374   Mean   :2.117  \n##  3rd Qu.:12140   3rd Qu.: 648876   3rd Qu.: 4.000   3rd Qu.:2.500  \n##  Max.   :16187   Max.   :7700000   Max.   :33.000   Max.   :8.000  \n##   sqft_living       sqft_lot           floors        waterfront      \n##  Min.   :  370   Min.   :    520   Min.   :1.000   Min.   :0.000000  \n##  1st Qu.: 1420   1st Qu.:   5040   1st Qu.:1.000   1st Qu.:0.000000  \n##  Median : 1920   Median :   7600   Median :1.500   Median :0.000000  \n##  Mean   : 2084   Mean   :  15069   Mean   :1.495   Mean   :0.007599  \n##  3rd Qu.: 2550   3rd Qu.:  10696   3rd Qu.:2.000   3rd Qu.:0.000000  \n##  Max.   :12050   Max.   :1651359   Max.   :3.500   Max.   :1.000000  \n##       view          condition         grade          sqft_above  \n##  Min.   :0.0000   Min.   :1.000   Min.   : 3.000   Min.   : 370  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.: 7.000   1st Qu.:1190  \n##  Median :0.0000   Median :3.000   Median : 7.000   Median :1560  \n##  Mean   :0.2435   Mean   :3.409   Mean   : 7.661   Mean   :1791  \n##  3rd Qu.:0.0000   3rd Qu.:4.000   3rd Qu.: 8.000   3rd Qu.:2220  \n##  Max.   :4.0000   Max.   :5.000   Max.   :13.000   Max.   :8860  \n##  sqft_basement       yr_built     yr_renovated       renovated     \n##  Min.   :   0.0   Min.   :1900   Min.   :   0.00   Min.   :0.0000  \n##  1st Qu.:   0.0   1st Qu.:1951   1st Qu.:   0.00   1st Qu.:0.0000  \n##  Median :   0.0   Median :1975   Median :   0.00   Median :0.0000  \n##  Mean   : 292.9   Mean   :1971   Mean   :  83.24   Mean   :0.0417  \n##  3rd Qu.: 570.0   3rd Qu.:1997   3rd Qu.:   0.00   3rd Qu.:0.0000  \n##  Max.   :4820.0   Max.   :2015   Max.   :2015.00   Max.   :1.0000  \n##     zipcode           lat             long        sqft_living15 \n##  Min.   :98001   Min.   :47.16   Min.   :-122.5   Min.   : 399  \n##  1st Qu.:98033   1st Qu.:47.47   1st Qu.:-122.3   1st Qu.:1490  \n##  Median :98065   Median :47.57   Median :-122.2   Median :1840  \n##  Mean   :98078   Mean   :47.56   Mean   :-122.2   Mean   :1990  \n##  3rd Qu.:98118   3rd Qu.:47.68   3rd Qu.:-122.1   3rd Qu.:2370  \n##  Max.   :98199   Max.   :47.78   Max.   :-121.3   Max.   :6210  \n##    sqft_lot15    \n##  Min.   :   651  \n##  1st Qu.:  5100  \n##  Median :  7601  \n##  Mean   : 12678  \n##  3rd Qu.: 10080  \n##  Max.   :871200\n\nExplore factor and ordinal variables\n#Factor variables: \n#waterfront (dummy), renovated(dummy), zipcode\n#Ordinal variables:\n#view (0-4), condition (1-5), grade (1-13), \nKing$waterfront = as.factor(King$waterfront)\nKing$renovated= as.factor(King$renovated)\nKing$zipcode = as.factor(King$zipcode)\nsummary(King)\n##        ID            price            bedrooms        bathrooms    \n##  Min.   :    1   Min.   :  75000   Min.   : 0.000   Min.   :0.000  \n##  1st Qu.: 4048   1st Qu.: 324624   1st Qu.: 3.000   1st Qu.:1.750  \n##  Median : 8094   Median : 451000   Median : 3.000   Median :2.250  \n##  Mean   : 8094   Mean   : 542802   Mean   : 3.374   Mean   :2.117  \n##  3rd Qu.:12140   3rd Qu.: 648876   3rd Qu.: 4.000   3rd Qu.:2.500  \n##  Max.   :16187   Max.   :7700000   Max.   :33.000   Max.   :8.000  \n##                                                                    \n##   sqft_living       sqft_lot           floors      waterfront\n##  Min.   :  370   Min.   :    520   Min.   :1.000   0:16064   \n##  1st Qu.: 1420   1st Qu.:   5040   1st Qu.:1.000   1:  123   \n##  Median : 1920   Median :   7600   Median :1.500             \n##  Mean   : 2084   Mean   :  15069   Mean   :1.495             \n##  3rd Qu.: 2550   3rd Qu.:  10696   3rd Qu.:2.000             \n##  Max.   :12050   Max.   :1651359   Max.   :3.500             \n##                                                              \n##       view          condition         grade          sqft_above  \n##  Min.   :0.0000   Min.   :1.000   Min.   : 3.000   Min.   : 370  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.: 7.000   1st Qu.:1190  \n##  Median :0.0000   Median :3.000   Median : 7.000   Median :1560  \n##  Mean   :0.2435   Mean   :3.409   Mean   : 7.661   Mean   :1791  \n##  3rd Qu.:0.0000   3rd Qu.:4.000   3rd Qu.: 8.000   3rd Qu.:2220  \n##  Max.   :4.0000   Max.   :5.000   Max.   :13.000   Max.   :8860  \n##                                                                  \n##  sqft_basement       yr_built     yr_renovated     renovated\n##  Min.   :   0.0   Min.   :1900   Min.   :   0.00   0:15512  \n##  1st Qu.:   0.0   1st Qu.:1951   1st Qu.:   0.00   1:  675  \n##  Median :   0.0   Median :1975   Median :   0.00            \n##  Mean   : 292.9   Mean   :1971   Mean   :  83.24            \n##  3rd Qu.: 570.0   3rd Qu.:1997   3rd Qu.:   0.00            \n##  Max.   :4820.0   Max.   :2015   Max.   :2015.00            \n##                                                             \n##     zipcode           lat             long        sqft_living15 \n##  98103  :  457   Min.   :47.16   Min.   :-122.5   Min.   : 399  \n##  98052  :  446   1st Qu.:47.47   1st Qu.:-122.3   1st Qu.:1490  \n##  98115  :  442   Median :47.57   Median :-122.2   Median :1840  \n##  98038  :  437   Mean   :47.56   Mean   :-122.2   Mean   :1990  \n##  98117  :  432   3rd Qu.:47.68   3rd Qu.:-122.1   3rd Qu.:2370  \n##  98034  :  403   Max.   :47.78   Max.   :-121.3   Max.   :6210  \n##  (Other):13570                                                  \n##    sqft_lot15    \n##  Min.   :   651  \n##  1st Qu.:  5100  \n##  Median :  7601  \n##  Mean   : 12678  \n##  3rd Qu.: 10080  \n##  Max.   :871200  \n## \n\nFirst, we are going to fit a base model and discuss any deficiencies.\nnames(King)\n##  [1] ""ID""            ""price""         ""bedrooms""      ""bathrooms""    \n##  [5] ""sqft_living""   ""sqft_lot""      ""floors""        ""waterfront""   \n##  [9] ""view""          ""condition""     ""grade""         ""sqft_above""   \n## [13] ""sqft_basement"" ""yr_built""      ""yr_renovated""  ""renovated""    \n## [17] ""zipcode""       ""lat""           ""long""          ""sqft_living15""\n## [21] ""sqft_lot15""\n\n#remove ID variable to only have y and x\'s df\nKing_clean = King[,-1]\nFit a full model:\nlm1.king = lm(price~., data = King_clean)\n#summary(lm1.king)\nComparative boxplots to examine the relationship between\ncategorical/ordinal predictors and a numeric response (Price)\npar(mfrow=c(2,2))\nboxplot(price~waterfront, data = King_clean, main = ""Price by Waterfront"")\nboxplot(price~renovated, data = King_clean, main = ""Price by Renovated"")\nboxplot(price~zipcode, data = King_clean, main = ""Price by Zipcode"")\n\nThe relationship of predictor & response is only looked at on individual\nbasis, not in the lm1.king model. We do not know how those predictors\nwill be behaving together in that model.\nnames(King_clean)\n##  [1] ""price""         ""bedrooms""      ""bathrooms""     ""sqft_living""  \n##  [5] ""sqft_lot""      ""floors""        ""waterfront""    ""view""         \n##  [9] ""condition""     ""grade""         ""sqft_above""    ""sqft_basement""\n## [13] ""yr_built""      ""yr_renovated""  ""renovated""     ""zipcode""      \n## [17] ""lat""           ""long""          ""sqft_living15"" ""sqft_lot15""\n\nLet’s plot the model to look at residuals.\npar(mfrow=c(2,2))\nplot(lm1.king)  \n Residuals\nvs.\xa0Fitted: we are looking whether there is a constant variance and\nwhether there is a curvature my model is missing. We can see that the\ndata points are not distributed with approximately similar vertical\ndistances - the model is not homoskedastic. There also seems to be some\ncurvature present that this model is not addressing.\nNormal Quantile plot shows that the data does not follow the normal\ndistribution.\nResiduals vs Leverage: data points to the very right may have big\nleverage and be “pulling” (affecting) the rest of the model. Some log\ntransformations could maybe fix this issue.\nWe are running vif(lm1.king) function from the car package to check the\nvariance inflation factor; the value larger than 10 would indicate that\nour model is facing multicollinearity concerns. We are getting an error\nmessage talking about aliased coefficients in the model. It means that\nthere is perfect multicollinearity present. Perfect multicollinearity\nmeans that some predictors are perfectly correlated with one another (+1\nor -1). These might be caused by sqft_above and sqft_basement\npredictors as they are perfect opposites of each other.\nExtracting fitted/predicted values and then plot actual vs.\xa0predicted.\ny = King_clean$price\nyhat = predict(lm1.king, data = King_clean)\nehat = resid(lm1.king)\nehat = y - yhat\nWe will not plot Actual vs.\xa0Predicted and Residuals vs.\xa0Predicted Price\nplots. The trendscat() function will add +/- SD bounds on the graph.\npar(mfrow=c(1,2))\ntrendscat(y,yhat, xlab = ""Actual Price"", ylab = ""Predicted Prince"")\nabline(0,1, lwd = 2, col = ""red"")\ntrendscat(y, ehat, xlab = ""Predicted Price"", ylab = ""Residuals"")\nabline(h = 0, lwd = 2, col = ""red"")\n The current\nmodel does not predict the price so well.\nBackward Elimination\nWe are going to use stepwise reduciton model to simplify it.\n#Backward Elimination\nback.king = step(lm1.king, direction = ""backward"")\n## Start:  AIC=388648.5\n## price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + \n##     waterfront + view + condition + grade + sqft_above + sqft_basement + \n##     yr_built + yr_renovated + renovated + zipcode + lat + long + \n##     sqft_living15 + sqft_lot15\n## \n## \n## Step:  AIC=388648.5\n## price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + \n##     waterfront + view + condition + grade + sqft_above + yr_built + \n##     yr_renovated + renovated + zipcode + lat + long + sqft_living15 + \n##     sqft_lot15\n## \n##                 Df  Sum of Sq        RSS    AIC\n## <none>                        4.2842e+14 388649\n## - sqft_lot15     1 5.5725e+10 4.2848e+14 388649\n## - long           1 1.4681e+11 4.2857e+14 388652\n## - lat            1 1.7972e+11 4.2860e+14 388653\n## - sqft_living15  1 2.1684e+11 4.2864e+14 388655\n## - sqft_lot       1 9.4192e+11 4.2937e+14 388682\n## - bathrooms      1 1.1296e+12 4.2955e+14 388689\n## - renovated      1 1.1571e+12 4.2958e+14 388690\n## - yr_renovated   1 1.1736e+12 4.2960e+14 388691\n## - yr_built       1 1.9413e+12 4.3036e+14 388720\n## - floors         1 3.8396e+12 4.3226e+14 388791\n## - condition      1 3.8549e+12 4.3228e+14 388792\n## - bedrooms       1 5.7550e+12 4.3418e+14 388863\n## - sqft_above     1 9.7169e+12 4.3814e+14 389010\n## - grade          1 1.8781e+13 4.4720e+14 389341\n## - view           1 2.2792e+13 4.5121e+14 389486\n## - sqft_living    1 2.6473e+13 4.5490e+14 389617\n## - waterfront     1 3.4761e+13 4.6318e+14 389909\n## - zipcode       69 2.4915e+14 6.7758e+14 395931\n\n…\nWe can get the analysis of variance table, ANOVA, with anova() function\nanova(back.king)\n## Analysis of Variance Table\n## \n## Response: price\n##                  Df     Sum Sq    Mean Sq    F value    Pr(>F)    \n## bedrooms          1 2.1422e+14 2.1422e+14  8050.4824 < 2.2e-16 ***\n## bathrooms         1 4.0514e+14 4.0514e+14 15225.1973 < 2.2e-16 ***\n## sqft_living       1 5.0402e+14 5.0402e+14 18941.0663 < 2.2e-16 ***\n## sqft_lot          1 2.9937e+12 2.9937e+12   112.5012 < 2.2e-16 ***\n## floors            1 2.0976e+08 2.0976e+08     0.0079  0.929254    \n## waterfront        1 6.5385e+13 6.5385e+13  2457.1476 < 2.2e-16 ***\n## view              1 4.7859e+13 4.7859e+13  1798.5325 < 2.2e-16 ***\n## condition         1 1.4589e+13 1.4589e+13   548.2352 < 2.2e-16 ***\n## grade             1 8.0755e+13 8.0755e+13  3034.7308 < 2.2e-16 ***\n## sqft_above        1 1.5661e+12 1.5661e+12    58.8540 1.795e-14 ***\n## yr_built          1 1.0238e+14 1.0238e+14  3847.5034 < 2.2e-16 ***\n## yr_renovated      1 1.7628e+11 1.7628e+11     6.6247  0.010066 *  \n## renovated         1 1.3663e+12 1.3663e+12    51.3444 8.086e-13 ***\n## zipcode          69 3.4194e+14 4.9557e+12   186.2335 < 2.2e-16 ***\n## lat               1 2.1458e+11 2.1458e+11     8.0637  0.004522 ** \n## long              1 1.6104e+11 1.6104e+11     6.0520  0.013901 *  \n## sqft_living15     1 2.0721e+11 2.0721e+11     7.7867  0.005269 ** \n## sqft_lot15        1 5.5725e+10 5.5725e+10     2.0941  0.147887    \n## Residuals     16100 4.2842e+14 2.6610e+10                         \n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n\n…\nLet’s check what the stepwise selction eliminated:\nback.king$anova\n##              Step Df Deviance Resid. Df   Resid. Dev      AIC\n## 1                 NA       NA     16100 4.284232e+14 388648.5\n## 2 - sqft_basement  0        0     16100 4.284232e+14 388648.5\n\n sqft_basement - which makes senese at it is the opposite of\nsqft_above and we only need one in the model.\nMixed model selection\nThe mixed stepwise selection method may work better (as it uses both\nforward and backward propagation)\nmixed.king = step(lm1.king)\n## Start:  AIC=388648.5\n## price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + \n##     waterfront + view + condition + grade + sqft_above + sqft_basement + \n##     yr_built + yr_renovated + renovated + zipcode + lat + long + \n##     sqft_living15 + sqft_lot15\n## \n## \n## Step:  AIC=388648.5\n## price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + \n##     waterfront + view + condition + grade + sqft_above + yr_built + \n##     yr_renovated + renovated + zipcode + lat + long + sqft_living15 + \n##     sqft_lot15\n## \n##                 Df  Sum of Sq        RSS    AIC\n## <none>                        4.2842e+14 388649\n## - sqft_lot15     1 5.5725e+10 4.2848e+14 388649\n## - long           1 1.4681e+11 4.2857e+14 388652\n## - lat            1 1.7972e+11 4.2860e+14 388653\n## - sqft_living15  1 2.1684e+11 4.2864e+14 388655\n## - sqft_lot       1 9.4192e+11 4.2937e+14 388682\n## - bathrooms      1 1.1296e+12 4.2955e+14 388689\n## - renovated      1 1.1571e+12 4.2958e+14 388690\n## - yr_renovated   1 1.1736e+12 4.2960e+14 388691\n## - yr_built       1 1.9413e+12 4.3036e+14 388720\n## - floors         1 3.8396e+12 4.3226e+14 388791\n## - condition      1 3.8549e+12 4.3228e+14 388792\n## - bedrooms       1 5.7550e+12 4.3418e+14 388863\n## - sqft_above     1 9.7169e+12 4.3814e+14 389010\n## - grade          1 1.8781e+13 4.4720e+14 389341\n## - view           1 2.2792e+13 4.5121e+14 389486\n## - sqft_living    1 2.6473e+13 4.5490e+14 389617\n## - waterfront     1 3.4761e+13 4.6318e+14 389909\n## - zipcode       69 2.4915e+14 6.7758e+14 395931\n\n… discussion\nKept predictors:\nanova(mixed.king)\n## Analysis of Variance Table\n## \n## Response: price\n##                  Df     Sum Sq    Mean Sq    F value    Pr(>F)    \n## bedrooms          1 2.1422e+14 2.1422e+14  8050.4824 < 2.2e-16 ***\n## bathrooms         1 4.0514e+14 4.0514e+14 15225.1973 < 2.2e-16 ***\n## sqft_living       1 5.0402e+14 5.0402e+14 18941.0663 < 2.2e-16 ***\n## sqft_lot          1 2.9937e+12 2.9937e+12   112.5012 < 2.2e-16 ***\n## floors            1 2.0976e+08 2.0976e+08     0.0079  0.929254    \n## waterfront        1 6.5385e+13 6.5385e+13  2457.1476 < 2.2e-16 ***\n## view              1 4.7859e+13 4.7859e+13  1798.5325 < 2.2e-16 ***\n## condition         1 1.4589e+13 1.4589e+13   548.2352 < 2.2e-16 ***\n## grade             1 8.0755e+13 8.0755e+13  3034.7308 < 2.2e-16 ***\n## sqft_above        1 1.5661e+12 1.5661e+12    58.8540 1.795e-14 ***\n## yr_built          1 1.0238e+14 1.0238e+14  3847.5034 < 2.2e-16 ***\n## yr_renovated      1 1.7628e+11 1.7628e+11     6.6247  0.010066 *  \n## renovated         1 1.3663e+12 1.3663e+12    51.3444 8.086e-13 ***\n## zipcode          69 3.4194e+14 4.9557e+12   186.2335 < 2.2e-16 ***\n## lat               1 2.1458e+11 2.1458e+11     8.0637  0.004522 ** \n## long              1 1.6104e+11 1.6104e+11     6.0520  0.013901 *  \n## sqft_living15     1 2.0721e+11 2.0721e+11     7.7867  0.005269 ** \n## sqft_lot15        1 5.5725e+10 5.5725e+10     2.0941  0.147887    \n## Residuals     16100 4.2842e+14 2.6610e+10                         \n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n\nRemoved predictors:\nmixed.king$anova\n##              Step Df Deviance Resid. Df   Resid. Dev      AIC\n## 1                 NA       NA     16100 4.284232e+14 388648.5\n## 2 - sqft_basement  0        0     16100 4.284232e+14 388648.5\n\nSame as for the backward mode: sqft_basement was eliminated.\nFinal model: mixed.king\nWhen sqft_basement was elminated, VIF() funciton can be run to test for\nmulticollinearity in the model.\nVIF(mixed.king)\n##                         \n##                         \n## Variance Inflation Factor Table\n##                         \n## \n\n##                    Variable          VIF  Rsquared\n## bedrooms           bedrooms     1.690266 0.4083772\n## bathrooms         bathrooms     3.480442 0.7126802\n## sqft_living     sqft_living     8.914384 0.8878217\n## sqft_lot           sqft_lot     2.071909 0.5173532\n## floors               floors     2.433612 0.5890882\n## waterfront1     waterfront1     1.258666 0.2055079\n## view                   view     1.519539 0.3419055\n## condition         condition     1.323002 0.2441432\n## grade                 grade     3.839353 0.7395395\n## sqft_above       sqft_above     7.533362 0.8672571\n## yr_built           yr_built     3.068826 0.6741425\n## yr_renovated   yr_renovated 15739.198945 0.9999365\n## renovated1       renovated1 15738.553581 0.9999365\n## zipcode98002   zipcode98002     1.556592 0.3575710\n## zipcode98003   zipcode98003     1.779080 0.4379118\n## zipcode98004   zipcode98004     6.997089 0.8570834\n## zipcode98005   zipcode98005     4.209910 0.7624653\n## zipcode98006   zipcode98006     8.063479 0.8759841\n## zipcode98007   zipcode98007     3.729782 0.7318878\n## zipcode98008   zipcode98008     6.920922 0.8555106\n## zipcode98010   zipcode98010     1.981216 0.4952595\n## zipcode98011   zipcode98011     8.069462 0.8760760\n## zipcode98014   zipcode98014     5.777032 0.8269007\n## zipcode98019   zipcode98019     8.960995 0.8884053\n## zipcode98022   zipcode98022     3.345039 0.7010498\n## zipcode98023   zipcode98023     2.633465 0.6202722\n## zipcode98024   zipcode98024     2.923819 0.6579816\n## zipcode98027   zipcode98027     7.137229 0.8598896\n## zipcode98028   zipcode98028    10.483304 0.9046102\n## zipcode98029   zipcode98029     7.124652 0.8596423\n## zipcode98030   zipcode98030     2.025456 0.5062840\n## zipcode98031   zipcode98031     2.330589 0.5709238\n## zipcode98032   zipcode98032     1.396090 0.2837137\n## zipcode98033   zipcode98033    11.635081 0.9140530\n## zipcode98034   zipcode98034    16.828665 0.9405776\n## zipcode98038   zipcode98038     5.921232 0.8311162\n## zipcode98039   zipcode98039     1.918358 0.4787210\n## zipcode98040   zipcode98040     4.721235 0.7881910\n## zipcode98042   zipcode98042     3.844786 0.7399075\n## zipcode98045   zipcode98045     7.154190 0.8602218\n## zipcode98052   zipcode98052    16.880384 0.9407596\n## zipcode98053   zipcode98053    13.084376 0.9235730\n## zipcode98055   zipcode98055     2.820488 0.6454514\n## zipcode98056   zipcode98056     4.949837 0.7979731\n## zipcode98058   zipcode98058     4.206137 0.7622522\n## zipcode98059   zipcode98059     5.545621 0.8196775\n## zipcode98065   zipcode98065     9.460152 0.8942935\n## zipcode98070   zipcode98070     2.025089 0.5061945\n## zipcode98072   zipcode98072    11.099780 0.9099081\n## zipcode98074   zipcode98074    11.609273 0.9138620\n## zipcode98075   zipcode98075     8.815166 0.8865591\n## zipcode98077   zipcode98077     8.195338 0.8779794\n## zipcode98092   zipcode98092     2.309706 0.5670444\n## zipcode98102   zipcode98102     3.080538 0.6753814\n## zipcode98103   zipcode98103    15.458394 0.9353102\n## zipcode98105   zipcode98105     6.166516 0.8378339\n## zipcode98106   zipcode98106     4.617078 0.7834128\n## zipcode98107   zipcode98107     7.224987 0.8615914\n## zipcode98108   zipcode98108     2.984023 0.6648819\n## zipcode98109   zipcode98109     3.165991 0.6841432\n## zipcode98112   zipcode98112     6.294816 0.8411391\n## zipcode98115   zipcode98115    15.398335 0.9350579\n## zipcode98116   zipcode98116     5.837734 0.8287006\n## zipcode98117   zipcode98117    15.441216 0.9352383\n## zipcode98118   zipcode98118     6.706141 0.8508830\n## zipcode98119   zipcode98119     4.635252 0.7842620\n## zipcode98122   zipcode98122     5.766719 0.8265912\n## zipcode98125   zipcode98125    12.776798 0.9217331\n## zipcode98126   zipcode98126     5.218797 0.8083850\n## zipcode98133   zipcode98133    16.119812 0.9379645\n## zipcode98136   zipcode98136     4.194164 0.7615735\n## zipcode98144   zipcode98144     6.241967 0.8397941\n## zipcode98146   zipcode98146     3.604442 0.7225645\n## zipcode98148   zipcode98148     1.324215 0.2448358\n## zipcode98155   zipcode98155    15.675171 0.9362048\n## zipcode98166   zipcode98166     2.570938 0.6110369\n## zipcode98168   zipcode98168     3.202008 0.6876959\n## zipcode98177   zipcode98177     9.684121 0.8967382\n## zipcode98178   zipcode98178     3.299705 0.6969426\n## zipcode98188   zipcode98188     1.787260 0.4404844\n## zipcode98198   zipcode98198     2.097949 0.5233440\n## zipcode98199   zipcode98199     7.627816 0.8689009\n## lat                     lat    64.810702 0.9845704\n## long                   long    37.432724 0.9732854\n## sqft_living15 sqft_living15     3.272798 0.6944511\n## sqft_lot15       sqft_lot15     2.219395 0.5494268\n\nThere is multicollinearity present (especially concerned with\nyr_renovated and renovated).\nUsing cross-validation methods to estimate the prediction error of this model using split-sample, k-fold, and the .632 bootstrap approaches\nSplit-sample approach\nCreate a validation set: split training set onto the training and\nvalidation (70/30)\nsize = n; number of rows in the training data set\nn = nrow(King_clean)\nCV = sample(c(""Train"", ""Valid""), size = n, replace = T, prob = c(.70, .30))\nking.lm2 = lm(price~., data = King_clean[CV == ""Train"",])\n#summary(king.lm2)\nMixed model stepwise: use the final model above\nmixed.king2 = step(king.lm2)\n## Start:  AIC=274208.3\n## price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + \n##     waterfront + view + condition + grade + sqft_above + sqft_basement + \n##     yr_built + yr_renovated + renovated + zipcode + lat + long + \n##     sqft_living15 + sqft_lot15\n## \n## \n## Step:  AIC=274208.3\n## price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + \n##     waterfront + view + condition + grade + sqft_above + yr_built + \n##     yr_renovated + renovated + zipcode + lat + long + sqft_living15 + \n##     sqft_lot15\n## \n##                 Df  Sum of Sq        RSS    AIC\n## - sqft_lot15     1 7.2808e+08 3.0312e+14 274206\n## - sqft_living15  1 2.3503e+10 3.0315e+14 274207\n## <none>                        3.0312e+14 274208\n## - long           1 6.3552e+10 3.0319e+14 274209\n## - lat            1 8.3965e+10 3.0321e+14 274209\n## - sqft_lot       1 4.1259e+11 3.0354e+14 274222\n## - renovated      1 6.1111e+11 3.0373e+14 274229\n## - yr_renovated   1 6.1979e+11 3.0374e+14 274230\n## - bathrooms      1 9.2237e+11 3.0405e+14 274241\n## - yr_built       1 1.3140e+12 3.0444e+14 274256\n## - floors         1 2.6943e+12 3.0582e+14 274307\n## - condition      1 2.7518e+12 3.0588e+14 274309\n## - bedrooms       1 4.0870e+12 3.0721e+14 274359\n## - sqft_above     1 6.6772e+12 3.0980e+14 274455\n## - grade          1 1.3858e+13 3.1698e+14 274717\n## - view           1 1.7027e+13 3.2015e+14 274830\n## - waterfront     1 1.8316e+13 3.2144e+14 274876\n## - sqft_living    1 2.0063e+13 3.2319e+14 274938\n## - zipcode       69 1.8495e+14 4.8807e+14 279508\n## \n## Step:  AIC=274206.3\n## price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + \n##     waterfront + view + condition + grade + sqft_above + yr_built + \n##     yr_renovated + renovated + zipcode + lat + long + sqft_living15\n## \n##                 Df  Sum of Sq        RSS    AIC\n## - sqft_living15  1 2.2998e+10 3.0315e+14 274205\n## <none>                        3.0312e+14 274206\n## - long           1 6.4692e+10 3.0319e+14 274207\n## - lat            1 8.4029e+10 3.0321e+14 274207\n## - sqft_lot       1 5.9165e+11 3.0372e+14 274227\n## - renovated      1 6.1068e+11 3.0373e+14 274227\n## - yr_renovated   1 6.1936e+11 3.0374e+14 274228\n## - bathrooms      1 9.2469e+11 3.0405e+14 274239\n## - yr_built       1 1.3145e+12 3.0444e+14 274254\n## - floors         1 2.6943e+12 3.0582e+14 274305\n## - condition      1 2.7511e+12 3.0588e+14 274307\n## - bedrooms       1 4.0866e+12 3.0721e+14 274357\n## - sqft_above     1 6.6768e+12 3.0980e+14 274453\n## - grade          1 1.3861e+13 3.1699e+14 274715\n## - view           1 1.7027e+13 3.2015e+14 274828\n## - waterfront     1 1.8318e+13 3.2144e+14 274874\n## - sqft_living    1 2.0066e+13 3.2319e+14 274936\n## - zipcode       69 1.8519e+14 4.8832e+14 279512\n## \n## Step:  AIC=274205.2\n## price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + \n##     waterfront + view + condition + grade + sqft_above + yr_built + \n##     yr_renovated + renovated + zipcode + lat + long\n## \n##                Df  Sum of Sq        RSS    AIC\n## <none>                       3.0315e+14 274205\n## - long          1 6.5497e+10 3.0321e+14 274206\n## - lat           1 8.4188e+10 3.0323e+14 274206\n## - sqft_lot      1 5.9026e+11 3.0374e+14 274225\n## - renovated     1 6.0651e+11 3.0375e+14 274226\n## - yr_renovated  1 6.1503e+11 3.0376e+14 274226\n## - bathrooms     1 9.1631e+11 3.0406e+14 274238\n## - yr_built      1 1.3379e+12 3.0448e+14 274253\n## - condition     1 2.7300e+12 3.0588e+14 274306\n## - floors        1 2.7301e+12 3.0588e+14 274306\n## - bedrooms      1 4.1035e+12 3.0725e+14 274357\n## - sqft_above    1 6.8570e+12 3.1000e+14 274459\n## - grade         1 1.5070e+13 3.1822e+14 274757\n## - view          1 1.7849e+13 3.2100e+14 274856\n## - waterfront    1 1.8296e+13 3.2144e+14 274872\n## - sqft_living   1 2.1139e+13 3.2429e+14 274973\n## - zipcode      69 1.8597e+14 4.8912e+14 279529\n\nmixed.king2$anova\n##              Step Df    Deviance Resid. Df   Resid. Dev      AIC\n## 1                 NA          NA     11330 3.031233e+14 274208.3\n## 2 - sqft_basement  0           0     11330 3.031233e+14 274208.3\n## 3    - sqft_lot15  1   728081847     11331 3.031241e+14 274206.3\n## 4 - sqft_living15  1 22997573467     11332 3.031471e+14 274205.2\n\nStepwise removed sqft_basement predictor again (sometimes removing\nsqft_lot15 as well).\nmixed.king2 to use to validate:\ny = King_clean$price[CV == ""Valid""]\nypred = predict(mixed.king2, newdata = King_clean[CV==""Valid"",])\nresults = PredAcc(y,ypred)\n## RMSEP\n## ================\n## 163639.1 \n## \n## MAE\n## ================\n## 99089.49 \n## \n## MAPE\n## ================\n## 20.18926\n\nRMSEP = 161013.1 MAE = 97808.4 MAPE = 20.40%\nK-fold\nRebuild the model:\nkfold.results.full = kfold.MLR(king.lm1, k=10)\n## Warning in predict.lm(fit2, newdata = data[folds == i, ]): prediction from\n## a rank-deficient fit may be misleading\n\n## Warning in predict.lm(fit2, newdata = data[folds == i, ]): prediction from\n## a rank-deficient fit may be misleading\n\n## Warning in predict.lm(fit2, newdata = data[folds == i, ]): prediction from\n## a rank-deficient fit may be misleading\n\n## Warning in predict.lm(fit2, newdata = data[folds == i, ]): prediction from\n## a rank-deficient fit may be misleading\n\n## Warning in predict.lm(fit2, newdata = data[folds == i, ]): prediction from\n## a rank-deficient fit may be misleading\n\n## Warning in predict.lm(fit2, newdata = data[folds == i, ]): prediction from\n## a rank-deficient fit may be misleading\n\n## Warning in predict.lm(fit2, newdata = data[folds == i, ]): prediction from\n## a rank-deficient fit may be misleading\n\n## Warning in predict.lm(fit2, newdata = data[folds == i, ]): prediction from\n## a rank-deficient fit may be misleading\n\n## Warning in predict.lm(fit2, newdata = data[folds == i, ]): prediction from\n## a rank-deficient fit may be misleading\n\n## Warning in predict.lm(fit2, newdata = data[folds == i, ]): prediction from\n## a rank-deficient fit may be misleading\n\nkfold.results.full\n##      RMSEP     MAE      MAPE\n## 1 164455.1 97759.3 0.1997352\n\nWe cannot use the full basic model as there are at least two predictors\nthat are perfectly correlated; rank deficient means that at least one\ncolumn depends on other column.\nBelow, we are using the simplified model by stepwise selection:\nkfold.results.step = kfold.MLR(mixed.king1, k=10)\nkfold.results.step\n##      RMSEP      MAE      MAPE\n## 1 164305.8 97727.05 0.1996578\n\nRMSE is better for the step model than for the full model.\n.632 Bootstrap\nboot.results.full\n##      RMSEP      MAE     MAPE\n## 1 165291.8 97830.44 0.199713\n\nboot.results.step = bootols.cv(mixed.king1, B=100)\n## RMSEP\n## ===============\n## 163958.6 \n## \n## MAE\n## ===============\n## 97599.85 \n## \n## MAPE\n## ===============\n## 0.1995386\n\nboot.results.step\n##      RMSEP      MAE      MAPE\n## 1 163958.6 97599.85 0.1995386\n\nboot.results.step had error rates smaller MAE (97719.86) and MAPE\n(19.93%). RMSE (164952) was a bit higher than that of the full model\n(163795.6).\nCompare the prediction error metrics:\nSplit-sample: RMSE = 161013.1 MAE = 97808.4 MAPE = 20.40\nK-fold: RMSE = 164542.1 MAE = 97687.6 MAPE = 19.95%\nBoot strap: RMSE = 164952 MAE = 97719.85 MAPE = 19.93%\nThese will serve to compare the predicted accuracy of the model in which\nwe are going to address the mentioned defficiencies.\nPART 2\nCheck for 0s or negative values\nking.lm1 = lm(price~., data = King_clean)\nsummary(king.lm1)\n## \n## Call:\n## lm(formula = price ~ ., data = King_clean)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1124012   -70633     -466    62572  4418822 \n## \n## Coefficients: (1 not defined because of singularities)\n##                 Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   -2.459e+07  7.498e+06  -3.279 0.001043 ** \n## bedrooms      -2.596e+04  1.765e+03 -14.706  < 2e-16 ***\n## bathrooms      2.022e+04  3.104e+03   6.515 7.47e-11 ***\n## sqft_living    1.311e+02  4.155e+00  31.541  < 2e-16 ***\n## sqft_lot       2.593e-01  4.358e-02   5.950 2.74e-09 ***\n## floors        -4.435e+04  3.692e+03 -12.012  < 2e-16 ***\n## waterfront1    5.987e+05  1.656e+04  36.143  < 2e-16 ***\n## view           5.906e+04  2.018e+03  29.266  < 2e-16 ***\n## condition      2.727e+04  2.266e+03  12.036  < 2e-16 ***\n## grade          5.648e+04  2.126e+03  26.567  < 2e-16 ***\n## sqft_above     8.105e+01  4.241e+00  19.109  < 2e-16 ***\n## sqft_basement         NA         NA      NA       NA    \n## yr_built      -6.508e+02  7.620e+01  -8.541  < 2e-16 ***\n## yr_renovated   2.677e+03  4.031e+02   6.641 3.21e-11 ***\n## renovated1    -5.306e+06  8.046e+05  -6.594 4.41e-11 ***\n## zipcode98002   3.763e+04  1.740e+04   2.163 0.030559 *  \n## zipcode98003  -2.297e+04  1.522e+04  -1.509 0.131336    \n## zipcode98004   7.296e+05  2.761e+04  26.423  < 2e-16 ***\n## zipcode98005   2.561e+05  2.959e+04   8.655  < 2e-16 ***\n## zipcode98006   2.304e+05  2.433e+04   9.471  < 2e-16 ***\n## zipcode98007   2.079e+05  3.056e+04   6.804 1.05e-11 ***\n## zipcode98008   2.123e+05  2.907e+04   7.304 2.93e-13 ***\n## zipcode98010   1.043e+05  2.606e+04   4.002 6.30e-05 ***\n## zipcode98011   4.858e+04  3.764e+04   1.291 0.196867    \n## zipcode98014   7.535e+04  4.144e+04   1.818 0.069081 .  \n## zipcode98019   5.665e+04  4.145e+04   1.367 0.171764    \n## zipcode98022   4.451e+04  2.294e+04   1.941 0.052309 .  \n## zipcode98023  -4.331e+04  1.415e+04  -3.061 0.002207 ** \n## zipcode98024   1.440e+05  3.734e+04   3.858 0.000115 ***\n## zipcode98027   1.586e+05  2.515e+04   6.307 2.91e-10 ***\n## zipcode98028   4.009e+04  3.669e+04   1.093 0.274503    \n## zipcode98029   2.032e+05  2.885e+04   7.043 1.96e-12 ***\n## zipcode98030   7.758e+03  1.660e+04   0.467 0.640263    \n## zipcode98031   8.761e+03  1.738e+04   0.504 0.614177    \n## zipcode98032  -7.291e+03  2.060e+04  -0.354 0.723411    \n## zipcode98033   3.013e+05  3.151e+04   9.562  < 2e-16 ***\n## zipcode98034   1.321e+05  3.376e+04   3.913 9.16e-05 ***\n## zipcode98038   5.099e+04  1.925e+04   2.649 0.008091 ** \n## zipcode98039   1.239e+06  3.770e+04  32.879  < 2e-16 ***\n## zipcode98040   4.664e+05  2.450e+04  19.034  < 2e-16 ***\n## zipcode98042   1.791e+04  1.623e+04   1.103 0.269951    \n## zipcode98045   1.379e+05  3.615e+04   3.814 0.000137 ***\n## zipcode98052   1.797e+05  3.218e+04   5.583 2.40e-08 ***\n## zipcode98053   1.544e+05  3.473e+04   4.446 8.81e-06 ***\n## zipcode98055   3.497e+04  1.940e+04   1.803 0.071455 .  \n## zipcode98056   7.238e+04  2.119e+04   3.416 0.000636 ***\n## zipcode98058   1.925e+04  1.850e+04   1.041 0.297961    \n## zipcode98059   6.595e+04  2.079e+04   3.173 0.001514 ** \n## zipcode98065   9.321e+04  3.276e+04   2.845 0.004451 ** \n## zipcode98070  -6.463e+04  2.414e+04  -2.677 0.007434 ** \n## zipcode98072   7.629e+04  3.766e+04   2.026 0.042801 *  \n## zipcode98074   1.423e+05  3.069e+04   4.638 3.54e-06 ***\n## zipcode98075   1.453e+05  2.956e+04   4.915 8.98e-07 ***\n## zipcode98077   6.125e+04  3.923e+04   1.561 0.118440    \n## zipcode98092  -2.225e+04  1.524e+04  -1.460 0.144308    \n## zipcode98102   4.492e+05  3.229e+04  13.911  < 2e-16 ***\n## zipcode98103   2.664e+05  3.043e+04   8.754  < 2e-16 ***\n## zipcode98105   4.127e+05  3.123e+04  13.214  < 2e-16 ***\n## zipcode98106   1.110e+05  2.261e+04   4.911 9.15e-07 ***\n## zipcode98107   2.729e+05  3.135e+04   8.703  < 2e-16 ***\n## zipcode98108   8.321e+04  2.530e+04   3.289 0.001008 ** \n## zipcode98109   4.491e+05  3.233e+04  13.889  < 2e-16 ***\n## zipcode98112   5.743e+05  2.856e+04  20.106  < 2e-16 ***\n## zipcode98115   2.659e+05  3.087e+04   8.613  < 2e-16 ***\n## zipcode98116   2.322e+05  2.512e+04   9.243  < 2e-16 ***\n## zipcode98117   2.373e+05  3.126e+04   7.592 3.31e-14 ***\n## zipcode98118   1.371e+05  2.196e+04   6.242 4.42e-10 ***\n## zipcode98119   4.235e+05  3.058e+04  13.849  < 2e-16 ***\n## zipcode98122   2.727e+05  2.727e+04  10.000  < 2e-16 ***\n## zipcode98125   1.266e+05  3.333e+04   3.798 0.000146 ***\n## zipcode98126   1.420e+05  2.304e+04   6.164 7.24e-10 ***\n## zipcode98133   8.428e+04  3.440e+04   2.450 0.014291 *  \n## zipcode98136   1.938e+05  2.360e+04   8.212 2.35e-16 ***\n## zipcode98144   2.283e+05  2.520e+04   9.060  < 2e-16 ***\n## zipcode98146   6.979e+04  2.107e+04   3.312 0.000927 ***\n## zipcode98148   4.653e+04  2.834e+04   1.642 0.100614    \n## zipcode98155   6.006e+04  3.581e+04   1.677 0.093543 .  \n## zipcode98166   1.865e+04  1.945e+04   0.959 0.337491    \n## zipcode98168   4.651e+04  2.018e+04   2.305 0.021195 *  \n## zipcode98177   1.281e+05  3.577e+04   3.581 0.000343 ***\n## zipcode98178   1.560e+04  2.103e+04   0.742 0.458127    \n## zipcode98188   2.183e+04  2.156e+04   1.013 0.311205    \n## zipcode98198  -1.695e+04  1.626e+04  -1.043 0.297182    \n## zipcode98199   3.024e+05  2.979e+04  10.151  < 2e-16 ***\n## lat            1.941e+05  7.469e+04   2.599 0.009364 ** \n## long          -1.322e+05  5.628e+04  -2.349 0.018846 *  \n## sqft_living15  9.624e+00  3.372e+00   2.855 0.004314 ** \n## sqft_lot15    -1.003e-01  6.933e-02  -1.447 0.147887    \n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## Residual standard error: 163100 on 16100 degrees of freedom\n## Multiple R-squared:  0.8063, Adjusted R-squared:  0.8052 \n## F-statistic: 779.1 on 86 and 16100 DF,  p-value: < 2.2e-16\n\nCheck for skewness:\npairs.plus2(King_clean[,c(1:6,8:14,17:20)])\n\nTransforming Home Price\nStatplot() function provided in the .Rdata file can check for the\ndistribution of the specified predictor or response.\n#Statplot(King_clean$price)\nIt is very right skewed.\nLet’s try logging the price and see.\n#Statplot(log(King_clean$price))\nUsing log transformation on the response (price) fixes the normality\nconcern. Now, let’s see if that was the “best” transformation:\nmyBC(King_clean$price)\n\n## bcPower Transformation to Normality \n##   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n## y   -0.2323       -0.23      -0.2549      -0.2097\n## \n## Likelihood ratio test that transformation parameter is equal to 0\n##  (log transformation)\n##                            LRT df       pval\n## LR test, lambda = (0) 416.0213  1 < 2.22e-16\n## \n## Likelihood ratio test that no transformation is needed\n##                         LRT df       pval\n## LR test, lambda = (1) 13569  1 < 2.22e-16\n\nSince there “best transformaiton” is very complicated and logging the\nreponse helped fix the skewness (normality concern) and it’s\ninterpretable, we are leaving it. So, the response is being logged.\nIdentify predictors with very skewed distributions to see if they need\ntransformations:\nsummary(King_clean)\n##      price            bedrooms        bathrooms      sqft_living   \n##  Min.   :  75000   Min.   : 0.000   Min.   :0.000   Min.   :  370  \n##  1st Qu.: 324624   1st Qu.: 3.000   1st Qu.:1.750   1st Qu.: 1420  \n##  Median : 451000   Median : 3.000   Median :2.250   Median : 1920  \n##  Mean   : 542802   Mean   : 3.374   Mean   :2.117   Mean   : 2084  \n##  3rd Qu.: 648876   3rd Qu.: 4.000   3rd Qu.:2.500   3rd Qu.: 2550  \n##  Max.   :7700000   Max.   :33.000   Max.   :8.000   Max.   :12050  \n##                                                                    \n##     sqft_lot           floors      waterfront      view       \n##  Min.   :    520   Min.   :1.000   0:16064    Min.   :0.0000  \n##  1st Qu.:   5040   1st Qu.:1.000   1:  123    1st Qu.:0.0000  \n##  Median :   7600   Median :1.500              Median :0.0000  \n##  Mean   :  15069   Mean   :1.495              Mean   :0.2435  \n##  3rd Qu.:  10696   3rd Qu.:2.000              3rd Qu.:0.0000  \n##  Max.   :1651359   Max.   :3.500              Max.   :4.0000  \n##                                                               \n##    condition         grade          sqft_above   sqft_basement   \n##  Min.   :1.000   Min.   : 3.000   Min.   : 370   Min.   :   0.0  \n##  1st Qu.:3.000   1st Qu.: 7.000   1st Qu.:1190   1st Qu.:   0.0  \n##  Median :3.000   Median : 7.000   Median :1560   Median :   0.0  \n##  Mean   :3.409   Mean   : 7.661   Mean   :1791   Mean   : 292.9  \n##  3rd Qu.:4.000   3rd Qu.: 8.000   3rd Qu.:2220   3rd Qu.: 570.0  \n##  Max.   :5.000   Max.   :13.000   Max.   :8860   Max.   :4820.0  \n##                                                                  \n##     yr_built     yr_renovated     renovated    zipcode     \n##  Min.   :1900   Min.   :   0.00   0:15512   98103  :  457  \n##  1st Qu.:1951   1st Qu.:   0.00   1:  675   98052  :  446  \n##  Median :1975   Median :   0.00             98115  :  442  \n##  Mean   :1971   Mean   :  83.24             98038  :  437  \n##  3rd Qu.:1997   3rd Qu.:   0.00             98117  :  432  \n##  Max.   :2015   Max.   :2015.00             98034  :  403  \n##                                             (Other):13570  \n##       lat             long        sqft_living15    sqft_lot15    \n##  Min.   :47.16   Min.   :-122.5   Min.   : 399   Min.   :   651  \n##  1st Qu.:47.47   1st Qu.:-122.3   1st Qu.:1490   1st Qu.:  5100  \n##  Median :47.57   Median :-122.2   Median :1840   Median :  7601  \n##  Mean   :47.56   Mean   :-122.2   Mean   :1990   Mean   : 12678  \n##  3rd Qu.:47.68   3rd Qu.:-122.1   3rd Qu.:2370   3rd Qu.: 10080  \n##  Max.   :47.78   Max.   :-121.3   Max.   :6210   Max.   :871200  \n## \n\nLongitude has negative values; yr_renovated, view, bathrooms, nedrooms\nhave 0 values. Sqft_basement has also 0 values but it was removed from\nthe model so we disregard it.\nCheck “best” labmda for very skewed predictors.\nmyBC(King_clean$bedrooms+1)\n\n## bcPower Transformation to Normality \n##   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n## y    0.3124        0.33       0.2699       0.3549\n## \n## Likelihood ratio test that transformation parameter is equal to 0\n##  (log transformation)\n##                            LRT df       pval\n## LR test, lambda = (0) 199.1334  1 < 2.22e-16\n## \n## Likelihood ratio test that no transformation is needed\n##                            LRT df       pval\n## LR test, lambda = (1) 1256.046  1 < 2.22e-16\n\n#labmda = 0.3 for bedrooms\nmyBC(King_clean$bathrooms+1)\n\n## bcPower Transformation to Normality \n##   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n## y    0.3617        0.33        0.309       0.4144\n## \n## Likelihood ratio test that transformation parameter is equal to 0\n##  (log transformation)\n##                            LRT df       pval\n## LR test, lambda = (0) 176.7871  1 < 2.22e-16\n## \n## Likelihood ratio test that no transformation is needed\n##                            LRT df       pval\n## LR test, lambda = (1) 587.5319  1 < 2.22e-16\n\n# lambda = 0.4 for bathrooms\nmyBC(King_clean$sqft_living)\n\n## bcPower Transformation to Normality \n##   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n## y    0.0337        0.03       0.0037       0.0638\n## \n## Likelihood ratio test that transformation parameter is equal to 0\n##  (log transformation)\n##                          LRT df    pval\n## LR test, lambda = (0) 4.8448  1 0.02773\n## \n## Likelihood ratio test that no transformation is needed\n##                            LRT df       pval\n## LR test, lambda = (1) 4056.177  1 < 2.22e-16\n\n#log(sqft_living)\nmyBC(King_clean$sqft_lot)\n\n## bcPower Transformation to Normality \n##   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n## y   -0.1903       -0.19      -0.2011      -0.1796\n## \n## Likelihood ratio test that transformation parameter is equal to 0\n##  (log transformation)\n##                            LRT df       pval\n## LR test, lambda = (0) 1283.967  1 < 2.22e-16\n## \n## Likelihood ratio test that no transformation is needed\n##                            LRT df       pval\n## LR test, lambda = (1) 58572.63  1 < 2.22e-16\n\n#lambda = -.20\nmyBC(King_clean$floors+1)\n\n## bcPower Transformation to Normality \n##   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n## y   -1.4927       -1.49      -1.5949      -1.3905\n## \n## Likelihood ratio test that transformation parameter is equal to 0\n##  (log transformation)\n##                            LRT df       pval\n## LR test, lambda = (0) 907.1114  1 < 2.22e-16\n## \n## Likelihood ratio test that no transformation is needed\n##                            LRT df       pval\n## LR test, lambda = (1) 2725.805  1 < 2.22e-16\n\n#lambda = -.40\nmyBC(King_clean$view+1)\n\n## bcPower Transformation to Normality \n##   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n## y   -8.2943       -8.29      -8.4234      -8.1652\n## \n## Likelihood ratio test that transformation parameter is equal to 0\n##  (log transformation)\n##                            LRT df       pval\n## LR test, lambda = (0) 42757.23  1 < 2.22e-16\n## \n## Likelihood ratio test that no transformation is needed\n##                         LRT df       pval\n## LR test, lambda = (1) 63236  1 < 2.22e-16\n\n#lambda = -2\nmyBC(King_clean$condition)\n\n## bcPower Transformation to Normality \n##   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n## y   -0.2835       -0.28      -0.3274      -0.2396\n## \n## Likelihood ratio test that transformation parameter is equal to 0\n##  (log transformation)\n##                            LRT df       pval\n## LR test, lambda = (0) 146.3281  1 < 2.22e-16\n## \n## Likelihood ratio test that no transformation is needed\n##                            LRT df       pval\n## LR test, lambda = (1) 2307.607  1 < 2.22e-16\n\n#lambda = -.3\nmyBC(King_clean$grade)\n\n## bcPower Transformation to Normality \n##   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n## y    -0.295       -0.33      -0.3654      -0.2246\n## \n## Likelihood ratio test that transformation parameter is equal to 0\n##  (log transformation)\n##                            LRT df       pval\n## LR test, lambda = (0) 66.31839  1 3.3307e-16\n## \n## Likelihood ratio test that no transformation is needed\n##                            LRT df       pval\n## LR test, lambda = (1) 1219.105  1 < 2.22e-16\n\n#lambda = -.3\nmyBC(King_clean$sqft_above)\n\n## bcPower Transformation to Normality \n##   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n## y   -0.2316       -0.23      -0.2635      -0.1996\n## \n## Likelihood ratio test that transformation parameter is equal to 0\n##  (log transformation)\n##                           LRT df       pval\n## LR test, lambda = (0) 201.947  1 < 2.22e-16\n## \n## Likelihood ratio test that no transformation is needed\n##                            LRT df       pval\n## LR test, lambda = (1) 5791.132  1 < 2.22e-16\n\n#lambda = -.2\nmyBC(King_clean$yr_built)\n\n## bcPower Transformation to Normality \n##   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n## y   17.0767       17.08      15.9918      18.1617\n## \n## Likelihood ratio test that transformation parameter is equal to 0\n##  (log transformation)\n##                            LRT df       pval\n## LR test, lambda = (0) 997.5099  1 < 2.22e-16\n## \n## Likelihood ratio test that no transformation is needed\n##                            LRT df       pval\n## LR test, lambda = (1) 881.9361  1 < 2.22e-16\n\n#lambda = 2\nmyBC(King_clean$yr_renovated+1)\n\n## bcPower Transformation to Normality \n##   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n## y   -3.1556       -3.16      -3.2042       -3.107\n## \n## Likelihood ratio test that transformation parameter is equal to 0\n##  (log transformation)\n##                            LRT df       pval\n## LR test, lambda = (0) 70486.34  1 < 2.22e-16\n## \n## Likelihood ratio test that no transformation is needed\n##                            LRT df       pval\n## LR test, lambda = (1) 240579.9  1 < 2.22e-16\n\n#lambda = -2\nmyBC(King_clean$lat)\n\n## bcPower Transformation to Normality \n##   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n## y   90.4167       90.42      84.8155      96.0178\n## \n## Likelihood ratio test that transformation parameter is equal to 0\n##  (log transformation)\n##                            LRT df       pval\n## LR test, lambda = (0) 1040.881  1 < 2.22e-16\n## \n## Likelihood ratio test that no transformation is needed\n##                            LRT df       pval\n## LR test, lambda = (1) 1017.597  1 < 2.22e-16\n\n#lambda = 2\nPredictors to be transformed. When transformed, check with Statplot()\nfunction.\nApplying transformaitons and to the copy of King_clean data frame,\ncalled King_Trans\n#Copy of the training set\nKing_Trans = King_clean[,-12] #removing the sqft_basement\nKing_Trans$bedrooms = yjPower(King_Trans$bedrooms, 0.30)\nKing_Trans$bathrooms = yjPower(King_Trans$bathrooms, 0.40)\nKing_Trans$sqft_living = bcPower(King_Trans$sqft_living, 0)\nKing_Trans$sqft_lot = bcPower(King_Trans$sqft_lot, -0.20)\nKing_Trans$floors = yjPower(King_Trans$floors, -.40)\nKing_Trans$view = yjPower(King_Trans$view, -2)\n#King_Trans$condition = bcPower(King_Trans$condition, -0.30)\nKing_Trans$grade = bcPower(King_Trans$grade, -0.30)\nKing_Trans$sqft_above = bcPower(King_Trans$sqft_above, -0.20)\n#King_Trans$yr_built = bcPower(King_Trans$yr_built, 2)\n#King_Trans$yr_renovated = yjPower(King_Trans$yr_renovated, 2)\n#King_Trans$lat = bcPower(King_Trans$lat, 2)\ntrans.lm1 = lm(price~., data = King_Trans)\npar(mfrow=c(2,2))\nplot(trans.lm1)\n This is\nnot that much better at all. Let’s fit a model with the price (response)\ntransformed as well.\nKing_Trans$price =  log(King_Trans$price)\n#Statplot(King_Trans$price)\nThe reposnse is a go. It looks way better.\nloghp.trans.lm1 = lm(price~., data = King_Trans)\npar(mfrow=c(2,2))\nplot(loghp.trans.lm1)\n The\nresidual vs.\xa0Fitted plot looks way better. Now, we have constant\nvariance!\nLet’s address some multicolinearity by checking the variance inflation\nfactor, vif(). Anything VIF > 10 would be a concern of\nmulticollinearity.\nVIF(loghp.trans.lm1)\n##                         \n##                         \n## Variance Inflation Factor Table\n##                         \n## \n\n##                    Variable          VIF  Rsquared\n## bedrooms           bedrooms     1.937847 0.4839633\n## bathrooms         bathrooms     3.578169 0.7205275\n## sqft_living     sqft_living     8.434902 0.8814450\n## sqft_lot           sqft_lot     3.538076 0.7173605\n## floors               floors     2.903179 0.6555499\n## waterfront1     waterfront1     1.152343 0.1322025\n## view                   view     1.364004 0.2668644\n## condition         condition     1.328540 0.2472940\n## grade                 grade     3.557411 0.7188967\n## sqft_above       sqft_above     7.511987 0.8668794\n## yr_built           yr_built     3.388619 0.7048945\n## yr_renovated   yr_renovated 15735.967747 0.9999365\n## renovated1       renovated1 15735.469334 0.9999364\n## zipcode98002   zipcode98002     1.567010 0.3618419\n## zipcode98003   zipcode98003     1.780454 0.4383456\n## zipcode98004   zipcode98004     7.006130 0.8572678\n## zipcode98005   zipcode98005     4.214118 0.7627024\n## zipcode98006   zipcode98006     8.081790 0.8762650\n## zipcode98007   zipcode98007     3.737463 0.7324388\n## zipcode98008   zipcode98008     6.947113 0.8560553\n## zipcode98010   zipcode98010     1.982598 0.4956112\n## zipcode98011   zipcode98011     8.070581 0.8760932\n## zipcode98014   zipcode98014     5.782192 0.8270552\n## zipcode98019   zipcode98019     8.965380 0.8884598\n## zipcode98022   zipcode98022     3.362682 0.7026183\n## zipcode98023   zipcode98023     2.633006 0.6202060\n## zipcode98024   zipcode98024     2.925797 0.6582128\n## zipcode98027   zipcode98027     7.171037 0.8605502\n## zipcode98028   zipcode98028    10.483701 0.9046138\n## zipcode98029   zipcode98029     7.183985 0.8608015\n## zipcode98030   zipcode98030     2.030158 0.5074275\n## zipcode98031   zipcode98031     2.335063 0.5717460\n## zipcode98032   zipcode98032     1.397243 0.2843050\n## zipcode98033   zipcode98033    11.647157 0.9141421\n## zipcode98034   zipcode98034    16.837198 0.9406077\n## zipcode98038   zipcode98038     5.963245 0.8323061\n## zipcode98039   zipcode98039     1.917054 0.4783663\n## zipcode98040   zipcode98040     4.728191 0.7885026\n## zipcode98042   zipcode98042     3.861567 0.7410378\n## zipcode98045   zipcode98045     7.173988 0.8606075\n## zipcode98052   zipcode98052    16.904909 0.9408456\n## zipcode98053   zipcode98053    13.097322 0.9236485\n## zipcode98055   zipcode98055     2.832367 0.6469385\n## zipcode98056   zipcode98056     4.963067 0.7985117\n## zipcode98058   zipcode98058     4.217859 0.7629129\n## zipcode98059   zipcode98059     5.557886 0.8200755\n## zipcode98065   zipcode98065     9.539046 0.8951677\n## zipcode98070   zipcode98070     2.043840 0.5107249\n## zipcode98072   zipcode98072    11.100324 0.9099125\n## zipcode98074   zipcode98074    11.635746 0.9140579\n## zipcode98075   zipcode98075     8.843616 0.8869241\n## zipcode98077   zipcode98077     8.197378 0.8780098\n## zipcode98092   zipcode98092     2.316304 0.5682778\n## zipcode98102   zipcode98102     3.117266 0.6792061\n## zipcode98103   zipcode98103    15.559387 0.9357301\n## zipcode98105   zipcode98105     6.211573 0.8390102\n## zipcode98106   zipcode98106     4.644234 0.7846792\n## zipcode98107   zipcode98107     7.268063 0.8624118\n## zipcode98108   zipcode98108     3.001316 0.6668128\n## zipcode98109   zipcode98109     3.193773 0.6868907\n## zipcode98112   zipcode98112     6.354390 0.8426285\n## zipcode98115   zipcode98115    15.466846 0.9353456\n## zipcode98116   zipcode98116     5.872240 0.8297072\n## zipcode98117   zipcode98117    15.497498 0.9354735\n## zipcode98118   zipcode98118     6.766200 0.8522065\n## zipcode98119   zipcode98119     4.680743 0.7863587\n## zipcode98122   zipcode98122     5.849041 0.8290318\n## zipcode98125   zipcode98125    12.783770 0.9217758\n## zipcode98126   zipcode98126     5.245581 0.8093633\n## zipcode98133   zipcode98133    16.126151 0.9379889\n## zipcode98136   zipcode98136     4.221783 0.7631332\n## zipcode98144   zipcode98144     6.334283 0.8421289\n## zipcode98146   zipcode98146     3.605089 0.7226143\n## zipcode98148   zipcode98148     1.324461 0.2449756\n## zipcode98155   zipcode98155    15.677152 0.9362129\n## zipcode98166   zipcode98166     2.572621 0.6112913\n## zipcode98168   zipcode98168     3.203865 0.6878770\n## zipcode98177   zipcode98177     9.682148 0.8967171\n## zipcode98178   zipcode98178     3.314617 0.6983060\n## zipcode98188   zipcode98188     1.787719 0.4406280\n## zipcode98198   zipcode98198     2.100374 0.5238943\n## zipcode98199   zipcode98199     7.653278 0.8693370\n## lat                     lat    64.825159 0.9845739\n## long                   long    37.805441 0.9735488\n## sqft_living15 sqft_living15     3.134022 0.6809212\n## sqft_lot15       sqft_lot15     1.673941 0.4026075\n\nrenovated & yr_renovated look like there might be some\nmulticollinearity. Let’s test a model without one:\nrenovated is a dummy variable telling us whether a house has been\nrenovated at all or not; yr_renovated tells us the year a house has\nbeen last renovated (0 means that it has not been). Yr_renovated has an\nodd notation so I am choosing to keep renovated (dummy variable)\nnames(King_Trans)\n##  [1] ""price""         ""bedrooms""      ""bathrooms""     ""sqft_living""  \n##  [5] ""sqft_lot""      ""floors""        ""waterfront""    ""view""         \n##  [9] ""condition""     ""grade""         ""sqft_above""    ""yr_built""     \n## [13] ""yr_renovated""  ""renovated""     ""zipcode""       ""lat""          \n## [17] ""long""          ""sqft_living15"" ""sqft_lot15""\n\nloghp.trans.lm2 = lm(price~., data = King_Trans[,-13])\npar(mfrow=c(2,2))\n#plot(loghp.trans.lm2)\n#summary(trans.lm3)\nvif(loghp.trans.lm2)\n##                       GVIF Df GVIF^(1/(2*Df))\n## bedrooms          1.936392  1        1.391543\n## bathrooms         3.572290  1        1.890050\n## sqft_living       8.434893  1        2.904289\n## sqft_lot          3.537861  1        1.880920\n## floors            2.901816  1        1.703472\n## waterfront        1.150497  1        1.072612\n## view              1.363815  1        1.167825\n## condition         1.322920  1        1.150183\n## grade             3.552190  1        1.884725\n## sqft_above        7.506130  1        2.739732\n## yr_built          3.388440  1        1.840772\n## renovated         1.175514  1        1.084211\n## zipcode       10286.494158 69        1.069238\n## lat              64.815149  1        8.050786\n## long             37.805369  1        6.148607\n## sqft_living15     3.132520  1        1.769893\n## sqft_lot15        1.673933  1        1.293806\n\nNow, the model looks way better.\nlatitude and longitude –> but it makes practical sense to leave them\nbe in the model since latitude or longitude by themsleves would not give\nas all the informaiton needed (when they are both present in the model).\nzipcode: we have 70 different zipcodes.\nstr(King_Trans$zipcode)\n##  Factor w/ 70 levels ""98001"",""98002"",..: 67 56 17 38 30 3 69 61 24 7 ...\n\ntable(King_Trans$zipcode)\n## \n## 98001 98002 98003 98004 98005 98006 98007 98008 98010 98011 98014 98019 \n##   273   138   207   248   129   371   107   221    78   153    90   140 \n## 98022 98023 98024 98027 98028 98029 98030 98031 98032 98033 98034 98038 \n##   171   358    56   306   210   231   198   208    88   318   403   437 \n## 98039 98040 98042 98045 98052 98053 98055 98056 98058 98059 98065 98070 \n##    36   212   398   147   446   294   202   299   334   349   238    93 \n## 98072 98074 98075 98077 98092 98102 98103 98105 98106 98107 98108 98109 \n##   211   335   273   143   269    79   457   170   244   198   125    81 \n## 98112 98115 98116 98117 98118 98119 98122 98125 98126 98133 98136 98144 \n##   208   442   250   432   379   133   209   312   266   371   203   266 \n## 98146 98148 98155 98166 98168 98177 98178 98188 98198 98199 \n##   219    44   332   183   212   204   201   103   214   232\n\nIt makes sense for some zipcodes to be correlated with others. Even\nthough each full zip code represents its own area, the zip code prefix\nrepresents a region in a given state (the are many zip codes with\nsimilar prefixes: 980, 981)\nShould we include? Since we already have a latitude and longitude? Let’s\nsee how the stepwise selection sees it.\nPredictive performance of loghp.trans.lm2\nloghp.trans.lm2.step$anova\n##         Step Df    Deviance Resid. Df Resid. Dev       AIC\n## 1            NA          NA     16101   537.2240 -54953.62\n## 2   - floors  1 0.004404823     16102   537.2284 -54955.48\n## 3 - yr_built  1 0.009013392     16103   537.2374 -54957.21\n\nTwo of the above predictors were removed from the final model.\nLet’s check the metrics of this model vs.\xa0the one that did not have the\nresponse logged.\ny=King_clean$price\nyloghat = fitted(loghp.trans.lm2.step)\nyexp = exp(yloghat)\nRMSElog = sqrt(mean((y-yexp)^2))\nMAPElog = mean(abs(y-yexp)/y)\nMAElog = mean(abs(y-yexp))\n\nypred = fitted(trans.lm1)\nRMSEorig = sqrt(mean((y-ypred)^2))\nMAPEorig = mean(abs(y-ypred)/y)\nMAEorig = mean(abs(y-ypred))\n\nRMSElog \n## [1] 135020.5\n\nRMSEorig\n## [1] 184032.6\n\nMAPElog\n## [1] 0.1361329\n\nMAPEorig\n## [1] 0.2212034\n\n#these need to be times by 100 to be in %\nMAElog\n## [1] 74450.44\n\nMAEorig\n## [1] 105298.4\n\nThe metrics are much much better for the model loghp.trans.lm2.step than\ntrans.lm1 It accounts for multicollinearity, it has constant variance,\nand is fixed for normality (it has applied transformations of predictors\nand logged response). It is also simpler because of the stepwise\nselection method used that removed both floors and yr_renovated from\nthe model. It was important to also know when not to use the Box-Cox\ntransformations on the predictors when we were looking for “optimal”\nlambdas - it was trying to every time suggest some lambda\ntransformation, but with many model-checking (RMSE, MAE, MAPE) back and\nforth, the metrics were doing better only if the predictors which had\nmost skewness were transformed.\nThe final model here seems very good. Now, we are going to\ncross-validate it and see how it predicts.\nCross validate using split-sample approach\nCV = sample(c(""Train"", ""Valid""), size = n, replace = T, prob = c(0.70 , 0.30))\ncv.loghp.trans.lm2.step = lm(price~., data = King_Trans[CV == ""Train"",-13])\ncv.loghp.trans.lm2.step = step(cv.loghp.trans.lm2.step)\n## Start:  AIC=-38365.24\n## price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + \n##     waterfront + view + condition + grade + sqft_above + yr_built + \n##     renovated + zipcode + lat + long + sqft_living15 + sqft_lot15\n## \n##                 Df Sum of Sq    RSS    AIC\n## - yr_built       1      0.00 368.87 -38367\n## - floors         1      0.00 368.87 -38367\n## <none>                       368.87 -38365\n## - sqft_lot15     1      0.12 368.99 -38364\n## - lat            1      0.85 369.72 -38341\n## - long           1      0.86 369.73 -38341\n## - bedrooms       1      1.36 370.23 -38326\n## - renovated      1      2.13 371.00 -38302\n## - bathrooms      1      2.76 371.63 -38283\n## - sqft_above     1      3.94 372.81 -38248\n## - condition      1      9.97 378.84 -38067\n## - sqft_lot       1     12.84 381.71 -37982\n## - sqft_living15  1     13.88 382.75 -37951\n## - view           1     15.85 384.72 -37893\n## - sqft_living    1     20.94 389.81 -37745\n## - waterfront     1     22.58 391.45 -37698\n## - grade          1     29.82 398.69 -37491\n## - zipcode       69    360.98 729.85 -30813\n## \n## Step:  AIC=-38367.22\n## price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + \n##     waterfront + view + condition + grade + sqft_above + renovated + \n##     zipcode + lat + long + sqft_living15 + sqft_lot15\n## \n##                 Df Sum of Sq    RSS    AIC\n## - floors         1      0.00 368.87 -38369\n## <none>                       368.87 -38367\n## - sqft_lot15     1      0.12 368.99 -38366\n## - lat            1      0.85 369.72 -38343\n## - long           1      0.86 369.73 -38343\n## - bedrooms       1      1.38 370.25 -38327\n## - renovated      1      2.31 371.18 -38299\n## - bathrooms      1      3.10 371.97 -38275\n## - sqft_above     1      3.94 372.81 -38250\n## - condition      1     11.03 379.90 -38037\n## - sqft_living15  1     13.89 382.77 -37953\n## - sqft_lot       1     14.20 383.08 -37943\n## - view           1     15.90 384.78 -37893\n## - sqft_living    1     20.97 389.84 -37746\n## - waterfront     1     22.58 391.45 -37700\n## - grade          1     32.94 401.81 -37405\n## - zipcode       69    417.32 786.19 -29977\n## \n## Step:  AIC=-38369.18\n## price ~ bedrooms + bathrooms + sqft_living + sqft_lot + waterfront + \n##     view + condition + grade + sqft_above + renovated + zipcode + \n##     lat + long + sqft_living15 + sqft_lot15\n## \n##                 Df Sum of Sq    RSS    AIC\n## <none>                       368.87 -38369\n## - sqft_lot15     1      0.12 369.00 -38367\n## - lat            1      0.84 369.72 -38345\n## - long           1      0.86 369.73 -38345\n## - bedrooms       1      1.38 370.25 -38329\n## - renovated      1      2.31 371.18 -38301\n## - bathrooms      1      3.29 372.16 -38271\n## - sqft_above     1      5.74 374.61 -38197\n## - condition      1     11.12 379.99 -38037\n## - sqft_living15  1     13.90 382.77 -37954\n## - view           1     15.90 384.78 -37895\n## - sqft_lot       1     17.25 386.12 -37856\n## - sqft_living    1     22.37 391.24 -37708\n## - waterfront     1     22.62 391.49 -37700\n## - grade          1     33.02 401.89 -37405\n## - zipcode       69    417.60 786.48 -29975\n\ncv.loghp.trans.lm2.step$anova\n##         Step Df     Deviance Resid. Df Resid. Dev       AIC\n## 1            NA           NA     11184   368.8707 -38365.24\n## 2 - yr_built  1 0.0008605464     11185   368.8715 -38367.22\n## 3   - floors  1 0.0013082581     11186   368.8728 -38369.18\n\nsummary(cv.loghp.trans.lm2.step)\n## \n## Call:\n## lm(formula = price ~ bedrooms + bathrooms + sqft_living + sqft_lot + \n##     waterfront + view + condition + grade + sqft_above + renovated + \n##     zipcode + lat + long + sqft_living15 + sqft_lot15, data = King_Trans[CV == \n##     ""Train"", -13])\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.23910 -0.09818  0.00322  0.10148  0.99289 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   -6.756e+01  1.016e+01  -6.648 3.11e-11 ***\n## bedrooms      -4.725e-02  7.302e-03  -6.471 1.02e-10 ***\n## bathrooms      7.700e-02  7.708e-03   9.990  < 2e-16 ***\n## sqft_living    2.933e-01  1.126e-02  26.044  < 2e-16 ***\n## sqft_lot       4.419e-01  1.932e-02  22.872  < 2e-16 ***\n## waterfront1    5.428e-01  2.073e-02  26.191  < 2e-16 ***\n## view           3.290e-01  1.498e-02  21.961  < 2e-16 ***\n## condition      5.247e-02  2.858e-03  18.361  < 2e-16 ***\n## grade          1.194e+00  3.772e-02  31.643  < 2e-16 ***\n## sqft_above     5.328e-01  4.038e-02  13.195  < 2e-16 ***\n## renovated1     7.366e-02  8.806e-03   8.365  < 2e-16 ***\n## zipcode98002   4.178e-02  2.400e-02   1.741 0.081688 .  \n## zipcode98003   4.047e-03  2.052e-02   0.197 0.843665    \n## zipcode98004   1.019e+00  3.669e-02  27.770  < 2e-16 ***\n## zipcode98005   6.179e-01  3.976e-02  15.542  < 2e-16 ***\n## zipcode98006   5.855e-01  3.260e-02  17.962  < 2e-16 ***\n## zipcode98007   5.575e-01  4.098e-02  13.604  < 2e-16 ***\n## zipcode98008   5.771e-01  3.896e-02  14.812  < 2e-16 ***\n## zipcode98010   3.184e-01  3.541e-02   8.992  < 2e-16 ***\n## zipcode98011   2.431e-01  5.044e-02   4.819 1.46e-06 ***\n## zipcode98014   2.747e-01  5.572e-02   4.930 8.32e-07 ***\n## zipcode98019   2.265e-01  5.587e-02   4.055 5.06e-05 ***\n## zipcode98022   2.035e-01  3.102e-02   6.562 5.54e-11 ***\n## zipcode98023  -6.267e-02  1.866e-02  -3.359 0.000784 ***\n## zipcode98024   4.089e-01  4.967e-02   8.233  < 2e-16 ***\n## zipcode98027   5.015e-01  3.377e-02  14.848  < 2e-16 ***\n## zipcode98028   2.194e-01  4.906e-02   4.472 7.81e-06 ***\n## zipcode98029   6.000e-01  3.878e-02  15.471  < 2e-16 ***\n## zipcode98030   5.166e-02  2.221e-02   2.327 0.020006 *  \n## zipcode98031   4.734e-02  2.323e-02   2.038 0.041588 *  \n## zipcode98032  -3.677e-02  2.666e-02  -1.379 0.167976    \n## zipcode98033   6.460e-01  4.215e-02  15.328  < 2e-16 ***\n## zipcode98034   3.759e-01  4.503e-02   8.347  < 2e-16 ***\n## zipcode98038   2.221e-01  2.596e-02   8.556  < 2e-16 ***\n## zipcode98039   1.164e+00  5.193e-02  22.419  < 2e-16 ***\n## zipcode98040   7.696e-01  3.253e-02  23.663  < 2e-16 ***\n## zipcode98042   8.204e-02  2.187e-02   3.751 0.000177 ***\n## zipcode98045   4.485e-01  4.871e-02   9.209  < 2e-16 ***\n## zipcode98052   5.091e-01  4.321e-02  11.781  < 2e-16 ***\n## zipcode98053   4.950e-01  4.665e-02  10.611  < 2e-16 ***\n## zipcode98055   1.015e-01  2.597e-02   3.907 9.41e-05 ***\n## zipcode98056   2.653e-01  2.839e-02   9.343  < 2e-16 ***\n## zipcode98058   1.457e-01  2.478e-02   5.879 4.24e-09 ***\n## zipcode98059   3.061e-01  2.790e-02  10.972  < 2e-16 ***\n## zipcode98065   4.592e-01  4.426e-02  10.374  < 2e-16 ***\n## zipcode98070   1.732e-01  3.192e-02   5.427 5.85e-08 ***\n## zipcode98072   2.813e-01  5.056e-02   5.563 2.71e-08 ***\n## zipcode98074   4.857e-01  4.115e-02  11.803  < 2e-16 ***\n## zipcode98075   5.165e-01  3.981e-02  12.973  < 2e-16 ***\n## zipcode98077   2.986e-01  5.230e-02   5.709 1.17e-08 ***\n## zipcode98092   6.759e-02  2.029e-02   3.331 0.000869 ***\n## zipcode98102   8.413e-01  4.344e-02  19.366  < 2e-16 ***\n## zipcode98103   6.942e-01  4.040e-02  17.184  < 2e-16 ***\n## zipcode98105   8.295e-01  4.138e-02  20.046  < 2e-16 ***\n## zipcode98106   2.671e-01  3.043e-02   8.779  < 2e-16 ***\n## zipcode98107   7.068e-01  4.143e-02  17.062  < 2e-16 ***\n## zipcode98108   2.860e-01  3.349e-02   8.538  < 2e-16 ***\n## zipcode98109   8.780e-01  4.301e-02  20.414  < 2e-16 ***\n## zipcode98112   9.295e-01  3.777e-02  24.609  < 2e-16 ***\n## zipcode98115   6.661e-01  4.100e-02  16.246  < 2e-16 ***\n## zipcode98116   6.505e-01  3.331e-02  19.529  < 2e-16 ***\n## zipcode98117   6.523e-01  4.150e-02  15.719  < 2e-16 ***\n## zipcode98118   4.017e-01  2.906e-02  13.824  < 2e-16 ***\n## zipcode98119   8.527e-01  4.040e-02  21.104  < 2e-16 ***\n## zipcode98122   6.992e-01  3.604e-02  19.398  < 2e-16 ***\n## zipcode98125   3.841e-01  4.442e-02   8.648  < 2e-16 ***\n## zipcode98126   4.458e-01  3.062e-02  14.561  < 2e-16 ***\n## zipcode98133   2.639e-01  4.577e-02   5.766 8.32e-09 ***\n## zipcode98136   5.882e-01  3.141e-02  18.724  < 2e-16 ***\n## zipcode98144   6.036e-01  3.351e-02  18.012  < 2e-16 ***\n## zipcode98146   1.788e-01  2.811e-02   6.361 2.08e-10 ***\n## zipcode98148   9.232e-02  3.670e-02   2.516 0.011885 *  \n## zipcode98155   2.026e-01  4.774e-02   4.244 2.21e-05 ***\n## zipcode98166   2.173e-01  2.546e-02   8.535  < 2e-16 ***\n## zipcode98168   2.061e-03  2.653e-02   0.078 0.938098    \n## zipcode98177   3.570e-01  4.756e-02   7.507 6.49e-14 ***\n## zipcode98178   9.862e-02  2.800e-02   3.522 0.000430 ***\n## zipcode98188   4.159e-02  2.870e-02   1.449 0.147366    \n## zipcode98198   3.906e-02  2.161e-02   1.808 0.070695 .  \n## zipcode98199   6.890e-01  3.960e-02  17.400  < 2e-16 ***\n## lat            5.061e-01  9.999e-02   5.062 4.22e-07 ***\n## long          -3.913e-01  7.672e-02  -5.101 3.44e-07 ***\n## sqft_living15  8.996e-05  4.382e-06  20.531  < 2e-16 ***\n## sqft_lot15     1.583e-07  8.128e-08   1.947 0.051567 .  \n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n## \n## Residual standard error: 0.1816 on 11186 degrees of freedom\n## Multiple R-squared:  0.8827, Adjusted R-squared:  0.8819 \n## F-statistic:  1014 on 83 and 11186 DF,  p-value: < 2.2e-16\n\nvif(cv.loghp.trans.lm2.step)\n##                      GVIF Df GVIF^(1/(2*Df))\n## bedrooms         1.929009  1        1.388888\n## bathrooms        3.069417  1        1.751975\n## sqft_living      7.851119  1        2.801985\n## sqft_lot         2.636904  1        1.623855\n## waterfront       1.162935  1        1.078395\n## view             1.376410  1        1.173205\n## condition        1.169294  1        1.081339\n## grade            3.231778  1        1.797715\n## sqft_above       5.223772  1        2.285557\n## renovated        1.069849  1        1.034335\n## zipcode       7707.746629 69        1.067004\n## lat             65.310495  1        8.081491\n## long            39.303428  1        6.269245\n## sqft_living15    3.137570  1        1.771319\n## sqft_lot15       1.638166  1        1.279909\n\nVIF, as discussed above, remained pretty much the same for the same\nvariables after taking out the renovated dummy variable.\nBecasue of random sample splitting, when doing stepwise methods,\nsometimes sqft_lot15, yr_built and floors are removed or just\nyr_built and floors or sometimes just floors. Let’s see how well the\nmodel predicts:\ny = King_Trans$price[CV == ""Valid""]\ny = exp(y)\nypred = predict(cv.loghp.trans.lm2.step, newdata = King_Trans[CV==""Valid"",])\nresults = PredAcc(y,ypred)\n## RMSEP\n## ================\n## 650978.7 \n## \n## MAE\n## ================\n## 539391.3 \n## \n## MAPE\n## ================\n## 99.99684\n\n###Cross validate using k-fold ###\nkfold.MLR.log(loghp.trans.lm2.step, k=10)\n##      RMSEP    MAE      MAPE\n## 1 656688.5 542789 0.9999686\n\n###Cross validate using .632 bootstrat approach ###\nbootlog.cv(loghp.trans.lm2.step, B=100)\n## RMSEP\n## ===============\n## 135590 \n## \n## MAE\n## ===============\n## 74840.63 \n## \n## MAPE\n## ===============\n## 13.69302\n\n##    RMSEP      MAE     MAPE\n## 1 135590 74840.63 13.69302\n\nThe best predictions had the .632 bootstrap of the transformed model\n(logged response with predictors transformed the way it was described\nabove).\nThe final model from problem 1 where we did not do any modifications or\nacocunt for defficiencies: Boot strap: RMSE = 164,952 MAE = 97,719.85\nMAPE = 19.93%\nTHe final model after transformations Boot strap: RMSE = 135,906.9 MAE =\n74893.33 MAPE = 13.7%\n#Review the model\nanova(loghp.trans.lm2.step)\n## Analysis of Variance Table\n## \n## Response: price\n##                  Df  Sum Sq Mean Sq    F value    Pr(>F)    \n## bedrooms          1  566.76  566.76 16987.9885 < 2.2e-16 ***\n## bathrooms         1  793.44  793.44 23782.2268 < 2.2e-16 ***\n## sqft_living       1  765.43  765.43 22942.8460 < 2.2e-16 ***\n## sqft_lot          1   36.70   36.70  1099.8979 < 2.2e-16 ***\n## waterfront        1   59.84   59.84  1793.5815 < 2.2e-16 ***\n## view              1  103.95  103.95  3115.7664 < 2.2e-16 ***\n## condition         1   31.96   31.96   957.9526 < 2.2e-16 ***\n## grade             1  290.86  290.86  8718.0875 < 2.2e-16 ***\n## sqft_above        1    5.65    5.65   169.4247 < 2.2e-16 ***\n## renovated         1   27.64   27.64   828.3564 < 2.2e-16 ***\n## zipcode          69 1275.89   18.49   554.2500 < 2.2e-16 ***\n## lat               1    1.64    1.64    49.1346 2.485e-12 ***\n## long              1    1.25    1.25    37.5418 9.156e-10 ***\n## sqft_living15     1   19.73   19.73   591.4616 < 2.2e-16 ***\n## sqft_lot15        1    0.13    0.13     4.0315   0.04468 *  \n## Residuals     16103  537.24    0.03                         \n## ---\n## Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\n\n#The following predictors were removed\nloghp.trans.lm2.step$anova\n##         Step Df    Deviance Resid. Df Resid. Dev       AIC\n## 1            NA          NA     16101   537.2240 -54953.62\n## 2   - floors  1 0.004404823     16102   537.2284 -54955.48\n## 3 - yr_built  1 0.009013392     16103   537.2374 -54957.21\n\nThe model with transformations and predictors that were removed in the\nstepwise selection (and sqft_basement and renovated removed because of\nmulticollinearity) predicted the best.\nLet’s predict using the test set right now.\nFirst, we need to apply the same chages and transformations we did to\nthe training set to the test set.\nKingTest$waterfront = as.factor(KingTest$waterfront)\nKingTest$renovated= as.factor(KingTest$renovated)\nKingTest$zipcode = as.factor(KingTest$zipcode)\nDo the same transformations to the test set as we’ve done to the\ntraining set:\nnames(KingTest)\n##  [1] ""ID""            ""bedrooms""      ""bathrooms""     ""sqft_living""  \n##  [5] ""sqft_lot""      ""floors""        ""waterfront""    ""view""         \n##  [9] ""condition""     ""grade""         ""sqft_above""    ""sqft_basement""\n## [13] ""yr_built""      ""yr_renovated""  ""renovated""     ""zipcode""      \n## [17] ""lat""           ""long""          ""sqft_living15"" ""sqft_lot15""\n\n#Transformations:\n#Copy of the training set\nKing_Test = KingTest[,-1] #removin ID\nKing_Test = KingTest[,-12] #removing the sqft_basement\nKing_Test$bedrooms = yjPower(King_Test$bedrooms, 0.30)\nKing_Test$bathrooms = yjPower(King_Test$bathrooms, 0.40)\nKing_Test$sqft_living = bcPower(King_Test$sqft_living, 0)\nKing_Test$sqft_lot = bcPower(King_Test$sqft_lot, -0.20)\nKing_Test$floors = yjPower(King_Test$floors, -.40)\nKing_Test$view = yjPower(King_Test$view, -2)\n#King_Trans$condition = bcPower(King_Trans$condition, -0.30)\nKing_Test$grade = bcPower(King_Test$grade, -0.30)\nKing_Test$sqft_above = bcPower(King_Test$sqft_above, -0.20)\n#King_Trans$yr_built = bcPower(King_Trans$yr_built, 2)\n#King_Test$yr_renovated = yjPower(King_Test$yr_renovated, 2)\n#King_Trans$lat = bcPower(King_Trans$lat, 2)\nPredict and Write to .csv\nmypred = predict(loghp.trans.lm2.step,newdata=King_Test)\n#The response is logged. Make sure to convert it back to USD!\nmypred.dollars = exp(mypred)\n\n#Save it as a data frame that also contains ID of each individual home\nsubmission = data.frame(ID=KingTest$ID,ypred=mypred.dollars)\n#Write to .csv file\nwrite.csv(submission,file=""Predictions.csv"")\n'], 'url_profile': 'https://github.com/mikiwieczorek', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 24, 2020', 'Java', 'Updated Jan 26, 2020', '4', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 22, 2020']}","{'location': 'Nairobi,Kenya', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': [""Supervised-Learning (Python Programming)\nAs a football analyst in a company i was requested to analyse - Mchezopesa Ltd football team and tasked to accomplish the task below.\nA prediction result of a game between team 1 and team 2, based on who's home and who's away.\nAnd whether or not the game is friendly (include rank in your training).\nb.) Metric of Success\nThe metric of success will be accomplished when i am able to give adequate result of the game between the two teams based on who is home and away.Furthermore show if the scores are for the friendly or real match. and the scores by each team.\nc.) Understanding the context\nThe new model for calculating the FIFA/Coca-Cola World Ranking (FWR) was developed over two years during which time a large number of different algorithms was tested and extensively discussed. Throughout this review and consultation process, the main aim was to identify an algorithm that is not only intuitive, easy to understand and improves overall accuracy of the formula, but also addresses feedback received about the previous model and provides fair and equal opportunities for all teams across all confederations to ascend the FWR.\nd.) Record the Experiment Design\nData Cleaning\nExploratory Data Analysis\nPerform your EDA\nPerform any necessary feature engineering\nCheck of multicollinearity\nStart building the model\nCross-validate the model\nCompute RMSE\nCreate residual plots for your models, and assess their heteroscedasticity using Bartlett’s test\n""], 'url_profile': 'https://github.com/deelilah', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 24, 2020', 'Java', 'Updated Jan 26, 2020', '4', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Hass consult housing data analysis\n\nThis notebook contains analysis and predictions made from housing features to predict the price of housing\n\n'], 'url_profile': 'https://github.com/WambuguGichuki', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 24, 2020', 'Java', 'Updated Jan 26, 2020', '4', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['boston_housing_regression\nboston_housing_regression ,Learn and Enjoy\nIf you find any mistakes or disagree with any of the explanations, please do not hesitate to submit an issue. I welcome any feedback, positive or negative!\n'], 'url_profile': 'https://github.com/AidinZe', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 24, 2020', 'Java', 'Updated Jan 26, 2020', '4', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 22, 2020']}","{'location': 'Belgium', 'stats_list': [], 'contributions': '1,228 contributions\n        in the last year', 'description': ['ft_linear_regression\nft_linear_regression project of school 42\n\nUsage\ncd src\npython cli.py --help\n\nTrain model\npython cli.py train\n\nOutput model cost\npython cli.py cost\n\nPlot model and dataset\npython cli.py plot\n\nNotebook\nCheck out the jupyter notebook where I try to explain the algorithm here.\n'], 'url_profile': 'https://github.com/cacharle', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 24, 2020', 'Java', 'Updated Jan 26, 2020', '4', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ChenXiaoZhan', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 24, 2020', 'Java', 'Updated Jan 26, 2020', '4', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 22, 2020']}","{'location': 'woking England', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Psteely', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 24, 2020', 'Java', 'Updated Jan 26, 2020', '4', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 22, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Boston_House_Price_Prediction\nThis project is on prediciting house price of Boston city using supervised machine learning algorithms. In this we used three models Multiple Linear Regression, Decision Tree and Random Forest and finally choose the best one.\n'], 'url_profile': 'https://github.com/amitmittal1005', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 24, 2020', 'Java', 'Updated Jan 26, 2020', '4', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ilyas-Malik', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'HTML', 'Updated Jan 24, 2020', 'Java', 'Updated Jan 26, 2020', '4', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Iris Dataset Classifier API\nThis project was written in Python 3.8.1 but should be compatible with any Python 3 version.\nThis project depends on the following python packages, make sure you have them installed in your Python interpreter.\n\nsklearn\njoblib\njson\nflask\nflask_restful\nrequests\n\nHow to run the API\n1. Set up a local server running the API\na. Clone the iris-api repository to your computer\nb. Navigate to the repo in a terminal\nc. Run the api/iris_api.py file in a terminal (python3 api/iris_api.py)\n2. Call the API\nThe API can be accessed in two ways (from a seperate terminal):\n1. Using an HTTP POST request to http://0.0.0.0/80 (painful)\nThe request should contain a JSON object in the following format, ensuring your mime-type is \'application/json\'\n\n{\n    ""0"": {""sepalLength"": 5.0, ""sepalWidth"": 3.6, ""petalLength"": 1.4, ""petalWidth"": 0.2},\n    ""1"": {""sepalLength"": 4.2, ""sepalWidth"": 3.9, ""petalLength"": 1.0, ""petalWidth"": 0.4},\n    ""2"": {""sepalLength"": 6.0, ""sepalWidth"": 2.9, ""petalLength"": 1.6, ""petalWidth"": 0.2,}\n    ...\n}\n\nThe response will include a JSON file in the following format with the model\'s prediction:\n\n{\n    ""0"": 0\n    ""1"": 2\n    ""2"": 1\n    ...\n}\n\n2. Using the get_prediction python method from a script (easy)\nSimply import the method using (you will likely have to add the \'api\' directory to your sys.path, see iris_api_tests.py)\n\nfrom get_prediction import get_prediction\n\nThe method takes one argument, a python list containing the data for prediction:\n\n[\n    [5.0, 3.6, 1.4, 0.2],\n    ...\n]\n\nand returns a python list of the model predictions from the API:\n\n[\n    0,\n    ...\n]\n\nRun the iris_api_tests.py script to see both behaviors in action.\nFollow-up Questions\n\nThis dataset was obviously quite small, in the product you will be working with much\nmore data. How would you scale your training pipeline and/or model to handle datasets\nwhich do not easily fit into system memory?\n\nI think there are a number of ways to handle this fundamental Big Data problem, in the system I\nbuilt for this particular problem I did all of the training on my computer, then sent the trained\nmodel to the server (really just a different local directory given I\'m also hosting the server locally)\nto serve predictions, so to handle the scaling problem I\'d have to implement a pretty simple\nbatching algorithm where I\'m pulling small chunks of data from the database for training and sort of\niterating through the desired amount of batches.\nAs for a more expensive option you could distribute the training load amongst one or more\nGPUs, increasing both training speed and RAM space.\n\nDescribe your optimal versioning strategy for APIs which expose machine learning\nmodels. How does training the model on new data fit into versioning strategy? List the\npros and cons of your described strategy in detail.\n\nI think the key to a successful versioning strategy is consistency amongst existing capabilities.\nWhat I mean by that is the methods and structures that your users have been working with should\nremain functional as new features become available. For example, if I wanted to improve the\nfunctionality of the get_prediction method to where the user can specify which pre-trained sklearn\nmodel they want to use, I would instead just write a whole new method, say get_prediction_with_model(model, data).\nOf course this approach could end up in some level of redundancy and duplication, but as a programmer I\'d\nrather not have my existing code broken by new features that I may or may not be interested in.\nAs for training new models on more data I would just make sure that again previous versions of models\nremain available, so users have proper time to adjust. In summary:\nPros: API remains backwards compatible as new versions are released, allowing users to choose whether\nto implement new capabilities.\nCons: Source code could become crowded and redundant and API becomes resistant to fundamental/large-scale\nimprovements.\n\nDescribe your choice of model and how it fits the problem. List benefits and drawbacks\nof this type of model used in the way you have chosen and where there may be scaling\nissues as a system like this grows in size or complexity.\n\nI chose to use logistic regression in sklearn to fit this particular problem. I chose logistic regression\nbecause it\'s an easy-to-apply algorithm and the iris dataset is famously easy to make a classifier for. I\nwanted to make sure I wasn\'t spending hours on a super complex algorithm when I could be focusing on the API development, something that I had to learn some new frameworks for and I knew might take a while.\nThe pros of how I went about deploying the logistic regression model is that I used sklearn\'s joblib to encode the\nmodel to a file for later use, which makes transitioning to other sklearn models and using them in the API\nquite simple. Additionally logistic regression takes relatively few iterations to converge which is always nice.\nThe obvious con to this approach though is the API is limited exclusively to sklearn\nmodels, which definitely isn\'t the most popular library if you want to move toward more advanced machine\nlearning algorithms. If I were to push this API to its full potential it would certainly include the\nability to use different algorithms from different libraries.\n'], 'url_profile': 'https://github.com/kleinsasser', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'JavaScript', 'Updated Jan 24, 2020', '1', 'R', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Jan 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Applied-Stastistics-for-Analytics-Project\nSentiment Analysis and Multiple Linear Regression for womens' ecommerce clothing reviews.\n""], 'url_profile': 'https://github.com/lee-moche', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'JavaScript', 'Updated Jan 24, 2020', '1', 'R', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Jan 24, 2020']}","{'location': 'Nantes, France', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['visualization\nSample of a JS visualization from an histogram and a linear regression\nLanguages\n\nHTML/CSS\nJavascript\nD3.JS library\n\n'], 'url_profile': 'https://github.com/julienlr', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'JavaScript', 'Updated Jan 24, 2020', '1', 'R', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Jan 24, 2020']}","{'location': 'Beijing, China', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Fractional-power interaction regression (FPIR) for regression y ~ x1 + x2 + x1^M * x2^N\nThis package estimates the optimum values of exponents (M and N) in FPIR y ~ x1 + x2 + x1^M * x2^N, return model parameters such as regression coefficients, sum of squares of the interaction term, R square, AIC.\n'], 'url_profile': 'https://github.com/Xinhai-Li', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'JavaScript', 'Updated Jan 24, 2020', '1', 'R', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Jan 24, 2020']}","{'location': 'Tunis', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wassimgt', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'JavaScript', 'Updated Jan 24, 2020', '1', 'R', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Jan 24, 2020']}","{'location': 'Pune, India', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Board-Game-Review-Prediction\nPerforming a linear regression analysis by predicting the average reviews on a board game\n'], 'url_profile': 'https://github.com/sau1994', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'JavaScript', 'Updated Jan 24, 2020', '1', 'R', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Jan 24, 2020']}","{'location': 'chennai', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AshwinBalakrishna-official', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'JavaScript', 'Updated Jan 24, 2020', '1', 'R', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Jan 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MandeepSingh47', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'JavaScript', 'Updated Jan 24, 2020', '1', 'R', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Jan 24, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['Samples\nRandom ML snippets tried overtime for practice.\nSo far\n\nNTLK\nRegression Models\nREGEX\n\n'], 'url_profile': 'https://github.com/Siraz22', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'JavaScript', 'Updated Jan 24, 2020', '1', 'R', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Jan 24, 2020']}","{'location': 'Gibraltar', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['UbeeqoAutomationPack\nThis project runs Ubeeqo regression tests on WebApp | Android App | iOS App tests using           - Selenide     - BackstopJS\n'], 'url_profile': 'https://github.com/fabiocaso', 'info_list': ['Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'JavaScript', 'Updated Jan 24, 2020', '1', 'R', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Jan 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': [""Data-Mining\nA regression model that predicts the song_rating attribute from the other song's attributes.\n""], 'url_profile': 'https://github.com/abirbhy', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NYC-Airbnb-dataset\nPredicting hotel prices using Linear Regression model on a Kaggle dataset of NYC Airbnb data\n'], 'url_profile': 'https://github.com/neelabhpaul', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 20, 2020']}","{'location': 'Brasília, Brazil', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['comparing-ML-models\nIn this exercise notebook, I apply the following algorithms, and find the best one for this specific dataset by accuracy evaluation methods.\n\nK Nearest Neighbor (KNN)\nDecision Tree\nSupport Vector Machine (SVM)\nLogistic Regression\n\nThe dataset is about loans. The Loan_train.csv data set includes details of 346 customers whose loan are already paid off or defaulted. More info about it in the notebook.\n'], 'url_profile': 'https://github.com/mendelson', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': ['titanic_dataset_LogisticRegression_model\n\n\nImplementation of titanic dataset from Kaggle to checking the accuracy of Logistic regression model.\nSteps which are required to implement the model :-\n\nData Collection\nData Analysis\nData Wrangling\nTrain & Test\nAccuracy Check\n\n'], 'url_profile': 'https://github.com/rs301378', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 20, 2020']}","{'location': 'Salt Lake City', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vilandao', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '363 contributions\n        in the last year', 'description': ['Logistic Regression Model\nImporting the model\nOur model is a class packaged in the LogisticRegression.py file for convenience purposes.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom LogisticRegression import LogisticRegression\n%matplotlib inline\nplt.rcParams[""figure.figsize""] = (20,10)\n\nExperiments to help understand the model\nIn these experiments we will use two-feature binary class datasets and try to find the optimal wieights for our classifier by manipulating the no. of iterations and the learning rate.\nFirst Experiment: Fitting the Model to a Reduced Iris Dataset\nWe Will start with a simple experiment. We are using a subset of the iris dataset iris-reduced.csv, we are using two classes and two feautres. The two classes are highly separated, this allows us to understabd more about the how the model works.\nProvided the learning rate is $\\eta$ and the number of iterations is $\\tau$\nHigh # of iterations, low learning rate\n\n$\\tau$ = 150,000\n$\\eta$ = 1e-5\n\nData=np.loadtxt(\'iris-reduced.csv\')\nX=Data[:,1:3]\ny=Data[:,4]\nclassifier = LogisticRegression(X,y)\nf = classifier.train(150000, 1e-5)\n@ no of iterations = 150000 and learning rate = 1e-05 \n final W = [[-0.07708747]\n [-0.41811067]\n [ 0.56088464]] \n final cost = [0.34484796]\n\n\nWe notice that, despite the high number of iterations, the learning gradient is not steep enough. this suggest that our learning rate was too low.\nReducing # of iterations and increasing the learning rate\n\n$\\tau$ = 50,000\n$\\eta$ = 1e-3\n\nclassifier = LogisticRegression(X,y)\nf = classifier.train(50000, 1e-3)\n@ no of iterations = 50000 and learning rate = 0.001 \n final W = [[-0.46054026]\n [-2.23480103]\n [ 2.66065312]] \n final cost = [0.01812753]\n\n\nWe notice that with this particular dataset, due to how widely separated the classes are, we can use very high learning rates and very low number of iterations.\nTrying a low # of itertions and a very high learning rate\n\n$\\tau$ = 1000\n$\\eta$ = 0.5\n\nclassifier = LogisticRegression(X,y)\nf = classifier.train(1000, 0.5, draw_history = False)\n@ no of iterations = 1000 and learning rate = 0.5 \n final W = [[-0.77434097]\n [-3.62275983]\n [ 4.33845848]] \n final cost = [0.00230502]\n\n\n\n$\\pagebreak$\nSecond Experiment: Fitting the to Model animesh-agrwal\'s Student Exams Dataset\nWe will use a dataset from animesh-agrwal\'s github originally used in a course on logistic regression he made, the file attaches is animesh-agarwal.csv. the dataset is not completely linearly separable, meaning our decision boundary will end up misclassifying some points regardless of how much we optimize.\nA too high learning rate\n\n$\\tau$ = 10,000\n$\\eta$ = .01\n\ndataxy = np.loadtxt(\'animesh-agarwal.csv\', dtype=float ,delimiter=\',\')\nX = dataxy[:,(0,1)]\ny = dataxy[:,2]\nclassifier = LogisticRegression(X,y)\nf = classifier.train(10000, .01)\n@ no of iterations = 10000 and learning rate = 0.01 \n final W = [[-7.65900397]\n [ 0.41024768]\n [-0.05324509]] \n final cost = [4.60514431]\n\n\nThe error curve above shows that the learning rate is way too high, we will modify our parameters accordingly.\nEffects of the Learning Rate and No. of Iterations on accuracy\nLearning Rate vs. Accuracy\nWe begin by trying differen learning rates at 100,000 iterations. We will plot the result on a semilog scale to find the optimal learning rate.\ndict = {}\nlearning_rates = [1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1]\nfor i in learning_rates:\n    classifier = LogisticRegression(X,y)\n    f = classifier.train(100000, i, results = False)\n    y_predict = classifier.predict(X)\n    Accuracy = 100*np.sum(y == y_predict)/len(y)\n    dict[i] = Accuracy\nf, ax = plt.subplots()\nax.set(xscale=\'log\',title=\'Learning Rate vs Accuracy\', xlabel=\'Learning Rate\', ylabel=\'Accuracy\')\nsns.lineplot(list(dict.keys()),list(dict.values()), label = ""learning rate vs accuracy, @ 100,000 iterations"", ax = ax)\nax.legend(loc=\'lower center\', bbox_to_anchor=(.5, -.3), ncol=1)\n<matplotlib.legend.Legend at 0x7fe933ecbf50>\n\n\nNo. of Iterations vs. Accuracy\nwe will take the best learning rate from above and try it with different numbers of itration, increasing iterations with every experiment.\ndict = {}\nfor i in range(10000,200001, 50000):\n    classifier = LogisticRegression(X,y)\n    f = classifier.train(i, 1e-3, results = False)\n    y_predict = classifier.predict(X)\n    Accuracy = 100*np.sum(y == y_predict)/len(y)\n    dict[i] = Accuracy\nf, ax = plt.subplots()\nax.set(title=\'No. of iterations vs Accuracy\', xlabel=\'No. of iterations\', ylabel=\'Accuracy\')\nsns.lineplot(list(dict.keys()),list(dict.values()), label = ""No. of iterations, @ 1e-3 learning rate"", ax = ax)\nax.legend(loc=\'lower center\', bbox_to_anchor=(.5, -.3), ncol=1)\n<matplotlib.legend.Legend at 0x7fe93359f410>\n\n\nMeasuring discrimination with near optimal parameters\nBased on the above, we will try to look at the decision line at:\n\n$\\tau$ = 100,000 and $\\eta$ = .001\n$\\tau$ = 250,000 and $\\eta$ = .001\n\nclassifier = LogisticRegression(X,y)\nf = classifier.train(100000, .001)\n\ny_predict = classifier.predict(X)\nAccuracy = 100*np.sum(y == y_predict)/len(y)\nprint(""\\n"")\nprint(f""Points on the correct side of the decision boundary = {Accuracy}%"")\n@ no of iterations = 100000 and learning rate = 0.001 \n final W = [[-4.81180027]\n [ 0.04528064]\n [ 0.03819149]] \n final cost = [0.38737536]\n\n\nPoints on the correct side of the decision boundary = 91.0%\n\n\n\n$\\tau$ = 250,000\n$\\eta$ = 0.001\n\nWe think these parameters are close to optimal. The decision boundary classifies 92% of the points correctly, that is fair given that the classes are not linearly separable.\nclassifier = LogisticRegression(X,y)\nf = classifier.train(250000, .001, draw_history = False)\n\ny_predict = classifier.predict(X)\nAccuracy = 100*np.sum(y == y_predict)/len(y)\nprint(""\\n"")\nprint(f""Points on the correct side of the decision boundary = {Accuracy}%"")\n@ no of iterations = 250000 and learning rate = 0.001 \n final W = [[-8.42279005]\n [ 0.07308283]\n [ 0.06668396]] \n final cost = [0.29757435]\n\n\nPoints on the correct side of the decision boundary = 92.0%\n\n\n\n$\\pagebreak$\nTesting the model prediction capabilities\nIn this section we will use larger and more complex binary class dataset and divide them into training and test data to test our predictions.\nOne of the restrictions of this model is that the labels have to be (0,1) encoded, for that we will use the LabelEncoder function from scikit learn.\nWe will also use train_test_split function from scikit.\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nWe will also define a function to draw a confusion matrix.\nimport scikitplot as skplt\ndef confusion(y_test, y_predict):\n    fig, ax = plt.subplots()\n\n    skplt.metrics.plot_confusion_matrix(\n        y_test, \n        y_predict,\n        ax=ax)\n\n    b, t = plt.ylim()\n    b += 0.5\n    t -= 0.5\n    plt.ylim(b, t)\n    plt.show() \n\nFirst experiment B.D. Ripley\'s Synthetized Dataset\nWe will use the prnn_synth dataset, a synthetized data from Pattern Recognition and Neural Networks\' by B.D. Ripley. Cambridge University Press (1996)  ISBN  0-521-46086-7. the file attached is prnn_synth.csv\nThis dataset contains 250 instances, 2 features and 2 classes. the classes are not completely separable.\nImporting the data\ndf = pd.read_csv(\'prnn_synth.csv\')\ndf.dropna(how=""all"", inplace=True)\n\nclass_in_strings = lambda x: \'Class \' + str(x)\ndf[\'yc\']= df[""yc""].apply(class_in_strings)\n\ndf.describe()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nxs\nys\n\n\n\n\ncount\n250.000000\n250.000000\n\n\nmean\n-0.072758\n0.504362\n\n\nstd\n0.489496\n0.254823\n\n\nmin\n-1.246525\n-0.191313\n\n\n25%\n-0.509234\n0.323365\n\n\n50%\n-0.041834\n0.489827\n\n\n75%\n0.369964\n0.704390\n\n\nmax\n0.861296\n1.093178\n\n\n\n\nsns.pairplot(data = df, hue = ""yc"")\n<seaborn.axisgrid.PairGrid at 0x7fe923363ad0>\n\n\nX = np.asarray(df[[""xs"", ""ys""]])\ny = np.asarray(df[""yc""])\n\nenc = LabelEncoder()\nlabel_encoder = enc.fit(y)\ny = label_encoder.transform(y)\nExperimenting With Parametes\nLearning Rate vs Accuracy\ndict = {}\nlearning_rates = [1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1]\nfor i in learning_rates:\n    classifier = LogisticRegression(X,y)\n    f = classifier.train(50000, i, results = False)\n    y_predict = classifier.predict(X)\n    Accuracy = 100*np.sum(y == y_predict)/len(y)\n    dict[i] = Accuracy\nf, ax = plt.subplots()\nax.set(xscale=\'log\',title=\'Learning Rate vs Accuracy\', xlabel=\'Learning Rate\', ylabel=\'Accuracy\')\nsns.lineplot(list(dict.keys()),list(dict.values()), label = ""learning rate vs accuracy, @ 50,000 iterations"", ax = ax)\nax.legend(loc=\'lower center\', bbox_to_anchor=(.5, -.3), ncol=1)\n<matplotlib.legend.Legend at 0x7fe9232cc710>\n\n\nNo. of Iterations vs Accuracy\ndict = {}\nfor i in range(10000,100001, 10000):\n    classifier = LogisticRegression(X,y)\n    f = classifier.train(i, 1e-2, results = False)\n    y_predict = classifier.predict(X)\n    Accuracy = 100*np.sum(y == y_predict)/len(y)\n    dict[i] = Accuracy\nf, ax = plt.subplots()\nax.set(title=\'No. of iterations vs Accuracy\', xlabel=\'No. of iterations\', ylabel=\'Accuracy\')\nsns.lineplot(list(dict.keys()),list(dict.values()), label = ""No. of iterations, @ 1e-2 learning rate"", ax = ax)\nax.legend(loc=\'lower center\', bbox_to_anchor=(.5, -.3), ncol=1)\n<matplotlib.legend.Legend at 0x7fe9336fff90>\n\n\nTesting the Model on the Best Parameters we Found\nWe will split our data into training and test data using 80/20 split.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nclassifier = LogisticRegression(X_train,y_train)\nf = classifier.train(20000, 0.01, draw_history = False)\n@ no of iterations = 20000 and learning rate = 0.01 \n final W = [[-2.95402125]\n [ 1.29535646]\n [ 6.06386095]] \n final cost = [0.36986368]\n\n\nClassifying our test data\ny_predict = classifier.predict(X_test)\nAccuracy = 100*np.sum(y_test == y_predict)/len(y_test)\nprint(f""Accuracy of predicted labels = {Accuracy}%"")\nAccuracy of predicted labels = 88.0%\n\nconfusion(y_test, y_predict)\n\nEffect of the Order of the Samples on the Final Accuracy\nWe will use the parameter random_state in scikit\'s function train_test_split to generate randome training and test samples from our data, we will test all of them with the best parameters from above to see if they are going to affect the accuacy\ndict = {}\nfor i in range(1,51, 5):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n    classifier = LogisticRegression(X_train,y_train)\n    f = classifier.train(20000, 0.01, results = False)\n    y_predict = classifier.predict(X_test)\n    Accuracy = 100*np.sum(y_test == y_predict)/len(y_test)\n    dict[i] = Accuracy\nf, ax = plt.subplots()\nax.set(title=\'Order of Samples vs. Accuracy\', xlabel=\'Random State\', ylabel=\'Accuracy\')\nsns.lineplot(list(dict.keys()),list(dict.values()), label = ""Accuracy @ 20,000 iterations, eta = 0.01"", ax = ax)\nax.legend(loc=\'lower center\', bbox_to_anchor=(.5, -.3), ncol=1)\n<matplotlib.legend.Legend at 0x7fe9337d4550>\n\n\n\n$\\pagebreak$\nSecond Experiment: The Full Iris Dataset\nWe will start again with the Iris dataset, this time using the full feature set (4 features) but restricting it to two classes. can be found in the iris.data file attached:\nInformation about the dataset, quoted from the source\n""This is perhaps the best known database to be found in the pattern\nrecognition literature.  Fisher\'s paper is a classic in the field\nand is referenced frequently to this day.  (See Duda & Hart, for\nexample.)  The data set contains 3 classes of 50 instances each,\nwhere each class refers to a type of iris plant.  One class is\nlinearly separable from the other 2; the latter are NOT linearly\nseparable from each other.""\nWe are going to use two classes and according to the description they will be linearly separable:\n\n0 = \'Setosa\'\n1 = \'Versicolor\'\n\nImporting the data\nfeature_dict = {i:label for i,label in zip(\n                range(4),\n                  (\'sepal length in cm\',\n                  \'sepal width in cm\',\n                  \'petal length in cm\',\n                  \'petal width in cm\', ))}\n\ndf = pd.read_csv(\'iris.data\')\ndf.columns = [l for i,l in sorted(feature_dict.items())] + [\'class label\']\ndf.dropna(how=""all"", inplace=True)\n\ndf.describe()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nsepal length in cm\nsepal width in cm\npetal length in cm\npetal width in cm\n\n\n\n\ncount\n149.000000\n149.000000\n149.000000\n149.000000\n\n\nmean\n5.848322\n3.051007\n3.774497\n1.205369\n\n\nstd\n0.828594\n0.433499\n1.759651\n0.761292\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.400000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\nsns.pairplot(data = df, hue = ""class label"")\n<seaborn.axisgrid.PairGrid at 0x7fe922ad8550>\n\n\nTaking only Iris-setosa and Iris-versicolor instances\nThese are the two classes with the most separation.\nTraining the model\nWe will split our data into training and test data using 80/20 split.\nX = df[[\'sepal length in cm\',\n        \'sepal width in cm\',\n        \'petal length in cm\',\n        \'petal width in cm\']]\ny = df[""class label""]\n\n\nenc = LabelEncoder()\nlabel_encoder = enc.fit(y)\ny = label_encoder.transform(y)\n\nX = np.asarray(X)\ny = np.asarray(y)\nfilter = (y==0)|(y==1)\nX = X[filter,:]\ny = y[filter]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nclassifier = LogisticRegression(X_train,y_train)\nf = classifier.train(1000, 0.01, two_d = False)\n@ no of iterations = 1000 and learning rate = 0.01 \n final W = [[-0.20402627]\n [-0.29053776]\n [-1.04934456]\n [ 1.59764918]\n [ 0.68252448]] \n final cost = [0.06774462]\n\n\nClassifying our test data\ny_predict = classifier.predict(X_test)\nAccuracy = 100*np.sum(y_test == y_predict)/len(y_test)\nprint(f""Accuracy of predicted labels = {Accuracy}%"")\nAccuracy of predicted labels = 100.0%\n\nconfusion(y_test, y_predict)\n\nTaking only Iris-versicolor and Iris-verginica instances\nThese are the two classes with the least separation.\nImporting the data\nX = df[[\'sepal length in cm\',\n        \'sepal width in cm\',\n        \'petal length in cm\',\n        \'petal width in cm\']]\ny = df[""class label""]\n\n\nX = np.asarray(X)\ny = np.asarray(y)\nfilter = (y==""Iris-versicolor"")|(y==""Iris-virginica"")\nX = X[filter,:]\ny = y[filter]\nenc = LabelEncoder()\nlabel_encoder = enc.fit(y)\ny = label_encoder.transform(y)\nExperimenting With Parametes\nLearning Rate vs. Accuracy\ndict = {}\nlearning_rates = [1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1]\nfor i in learning_rates:\n    classifier = LogisticRegression(X,y)\n    f = classifier.train(10000, i, results = False)\n    y_predict = classifier.predict(X)\n    Accuracy = 100*np.sum(y == y_predict)/len(y)\n    dict[i] = Accuracy\nf, ax = plt.subplots()\nax.set(xscale=\'log\',title=\'Learning Rate vs Accuracy\', xlabel=\'Learning Rate\', ylabel=\'Accuracy\')\nsns.lineplot(list(dict.keys()),list(dict.values()), label = ""learning rate vs accuracy, @ 10,000 iterations"", ax = ax)\nax.legend(loc=\'lower center\', bbox_to_anchor=(.5, -.3), ncol=1)\n<matplotlib.legend.Legend at 0x7fe932c941d0>\n\n\nNo. of Iterations vs Accuracy\ndict = {}\nfor i in range(1000,25001, 1000):\n    classifier = LogisticRegression(X,y)\n    f = classifier.train(i, 1e-3, results = False)\n    y_predict = classifier.predict(X)\n    Accuracy = 100*np.sum(y == y_predict)/len(y)\n    dict[i] = Accuracy\nf, ax = plt.subplots()\nax.set(title=\'No. of iterations vs Accuracy\', xlabel=\'No. of iterations\', ylabel=\'Accuracy\')\nsns.lineplot(list(dict.keys()),list(dict.values()), label = ""No. of iterations, @ 1e-3 learning rate"", ax = ax)\nax.legend(loc=\'lower center\', bbox_to_anchor=(.5, -.3), ncol=1)\n<matplotlib.legend.Legend at 0x7fe932a3edd0>\n\n\nTesting the Model on the Best Parameters we Found\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nclassifier = LogisticRegression(X_train,y_train)\nf = classifier.train(10000, 1e-2, two_d = False)\n@ no of iterations = 10000 and learning rate = 0.01 \n final W = [[-1.46646965]\n [-2.40292358]\n [-2.16742733]\n [ 3.43459288]\n [ 3.5655377 ]] \n final cost = [0.16311965]\n\n\nClassifying our test data\ny_predict = classifier.predict(X_test)\nAccuracy = 100*np.sum(y_test == y_predict)/len(y_test)\nprint(f""Accuracy of predicted labels = {Accuracy}%"")\nAccuracy of predicted labels = 95.0%\n\nconfusion(y_test, y_predict)\n\nEffect of the Order of the Samples on the Final Accuracy\nWe will use the parameter random_state in scikit\'s function train_test_split to generate randome training and test samples from our data, we will test all of them with the best parameters from above to see if they are going to affect the accuacy\ndict = {}\nfor i in range(1,51, 5):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n    classifier = LogisticRegression(X_train,y_train)\n    f = classifier.train(10000, 1e-2, results = False)\n    y_predict = classifier.predict(X_test)\n    Accuracy = 100*np.sum(y_test == y_predict)/len(y_test)\n    dict[i] = Accuracy\nf, ax = plt.subplots()\nax.set(title=\'Order of Samples vs. Accuracy\', xlabel=\'Random State\', ylabel=\'Accuracy\')\nsns.lineplot(list(dict.keys()),list(dict.values()), label = ""Accuracy @ 10,000 iterations, eta = 0.01"", ax = ax)\nax.legend(loc=\'lower center\', bbox_to_anchor=(.5, -.3), ncol=1)\n<matplotlib.legend.Legend at 0x7fe933706f50>\n\n\n\n$\\pagebreak$\nThird Experiment: Banknote Authentication Dataset\nWe will use the banknote authentication Data Set which has 4 features, two classes and 1372 instances. It can be found in the data_banknote_authentication.csv file attached.\nInformation about the dataset, quoted from the source\n""Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.""\nImporting the data\ndf = pd.read_csv(\'data_banknote_authentication.csv\')\nfeature_dict = {i:label for i,label in zip(\n                range(4),\n                  (\'variance of Wavelet Transformed image \',\n                    \'skewness of Wavelet Transformed image\',\n                    \'curtosis of Wavelet Transformed image\',\n                    \'entropy of image\', ))}\n\ndf.columns = [l for i,l in sorted(feature_dict.items())] + [\'class label\']\ndf.dropna(how=""all"", inplace=True)\n\nclass_in_strings = lambda x: \'Class \' + str(x)\ndf[\'class label\']= df[""class label""].apply(class_in_strings)\n\ndf.describe()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nvariance of Wavelet Transformed image\nskewness of Wavelet Transformed image\ncurtosis of Wavelet Transformed image\nentropy of image\n\n\n\n\ncount\n1371.000000\n1371.000000\n1371.000000\n1371.000000\n\n\nmean\n0.431410\n1.917434\n1.400694\n-1.192200\n\n\nstd\n2.842494\n5.868359\n4.310105\n2.101683\n\n\nmin\n-7.042100\n-13.773100\n-5.286100\n-8.548200\n\n\n25%\n-1.774700\n-1.711300\n-1.553350\n-2.417000\n\n\n50%\n0.495710\n2.313400\n0.616630\n-0.586650\n\n\n75%\n2.814650\n6.813100\n3.181600\n0.394810\n\n\nmax\n6.824800\n12.951600\n17.927400\n2.449500\n\n\n\n\nsns.pairplot(data = df, hue = ""class label"")\n<seaborn.axisgrid.PairGrid at 0x7fe93294a6d0>\n\n\nX = df[[\'variance of Wavelet Transformed image \',\n        \'skewness of Wavelet Transformed image\',\n        \'curtosis of Wavelet Transformed image\',\n        \'entropy of image\']]\ny = df[""class label""]\n\nX = np.asarray(X)\ny = np.asarray(y)\n\nenc = LabelEncoder()\nlabel_encoder = enc.fit(y)\ny = label_encoder.transform(y)\nExperimenting With Parametes\nLearning Rate vs. Accuracy\ndict = {}\nlearning_rates = [1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1]\nfor i in learning_rates:\n    classifier = LogisticRegression(X,y)\n    f = classifier.train(2000, i, results = False)\n    y_predict = classifier.predict(X)\n    Accuracy = 100*np.sum(y == y_predict)/len(y)\n    dict[i] = Accuracy\nf, ax = plt.subplots()\nax.set(xscale=\'log\',title=\'Learning Rate vs Accuracy\', xlabel=\'Learning Rate\', ylabel=\'Accuracy\')\nsns.lineplot(list(dict.keys()),list(dict.values()), label = ""learning rate vs accuracy, @ 2,000 iterations"", ax = ax)\nax.legend(loc=\'lower center\', bbox_to_anchor=(.5, -.3), ncol=1)\n<matplotlib.legend.Legend at 0x7fe922c96150>\n\n\nNo. of Iterations vs Accuracy\ndict = {}\nfor i in range(100,3001, 100):\n    classifier = LogisticRegression(X,y)\n    f = classifier.train(i, 1e-3, results = False)\n    y_predict = classifier.predict(X)\n    Accuracy = 100*np.sum(y == y_predict)/len(y)\n    dict[i] = Accuracy\nf, ax = plt.subplots()\nax.set(title=\'No. of iterations vs Accuracy\', xlabel=\'No. of iterations\', ylabel=\'Accuracy\')\nsns.lineplot(list(dict.keys()),list(dict.values()), label = ""No. of iterations, @ 1e-3 learning rate"", ax = ax)\nax.legend(loc=\'lower center\', bbox_to_anchor=(.5, -.3), ncol=1)\n<matplotlib.legend.Legend at 0x7fe932c7c990>\n\n\nTraining the model\nWe will split our data into training and test data using 80/20 split.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nclassifier = LogisticRegression(X_train,y_train)\nf = classifier.train(3000, 0.1, two_d = False)\n@ no of iterations = 3000 and learning rate = 0.1 \n final W = [[ 3.0352988 ]\n [-2.82396881]\n [-1.63429646]\n [-1.95560593]\n [-0.18816712]] \n final cost = [0.02675094]\n\n\nClassifying our test data\ny_predict = classifier.predict(X_test)\nAccuracy = 100*np.sum(y_test == y_predict)/len(y_test)\nprint(f""Accuracy of predicted labels = {Accuracy}%"")\nAccuracy of predicted labels = 99.27272727272727%\n\nconfusion(y_test, y_predict)\n\nEffect of the Order of the Samples on the Final Accuracy\nWe will use the parameter random_state in scikit\'s function train_test_split to generate randome training and test samples from our data, we will test all of them with the best parameters from above to see if they are going to affect the accuacy\ndict = {}\nfor i in range(1,51, 5):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n    classifier = LogisticRegression(X_train,y_train)\n    f = classifier.train(3000, 0.1, results = False)\n    y_predict = classifier.predict(X_test)\n    Accuracy = 100*np.sum(y_test == y_predict)/len(y_test)\n    dict[i] = Accuracy\nf, ax = plt.subplots()\nax.set(title=\'Order of Samples vs. Accuracy\', xlabel=\'Random State\', ylabel=\'Accuracy\')\nsns.lineplot(list(dict.keys()),list(dict.values()), label = ""Accuracy @ 3,000 iterations, eta = 0.1"", ax = ax)\nax.legend(loc=\'lower center\', bbox_to_anchor=(.5, -.3), ncol=1)\n<matplotlib.legend.Legend at 0x7fe922a1e410>\n\n\n'], 'url_profile': 'https://github.com/mostafa-k-m', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '130 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/guozhaosengzs', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 20, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '429 contributions\n        in the last year', 'description': [""Flight delays at Newark Liberty International Airport\n1. List of key files:\n\nMain Jupyter notebook : flight_delay_notebook.ipynb\nFormulas Jupyter notebook :  library.py\nPresentation in Google Slides\nPresentation in PDF: Presentation_slides.pdf\nSaved trained models in Pickle : /pickle folder\nMain airline delay data: newark_flights.csv\nWeather API data import Jupyter notebook: dark_sky.ipynb\nAircraft data import Jupyter notebook: plane_registration.ipynb\n\n2. United Airlines at Newark Airport\nNewark Liberty International Airport is ranked as the worst US airport in terms of delays in 2019 according to New York Post whereby only 64% of flights departed on time. Our stakeholder, United Airlines, is not only by far the airport's biggest operator, but also has the airport's worst on-time performance compared to its competitors. The issue with flighy delays certainly does not help with the airline's public image, which was made worse by the release of a video of a passenger being forcefully dragged off the plane that went viral and in the case when a puppy was forced to be put in an overhead bin that lead to its death.\n3. Project Objectives\nWe seek to determine factors affecting United Airlines' delays (both departure and arrival) and if these factors could be reliably used as predictors to forecast if a given flight is delayed. Flight delays have negative impacts for airlines, airports and customers. For the airline, the ability to warn its passengers in advance and to be prepared with contingency plans would help them to retain the loyalty of its customer base and maintain a good public image. On the costs side of the equation, the airline will also be able to be better prepared to manage compensations, penalties and additional operational costs such as crew and aircraft retention.\n4. Methodology\nWe have chosen flight delays data for full year 2015 from Dept of Transportation via Kaggle. In addition, we also brought together data from Dark Sky weather API and aircraft registration data from FAA to deep deeper into the effects of weather conditions and the airline's fleet.\nAfter data cleansing, EDA and feature selection processes, we explored several classification models including Logistic Regression, Decision Tree, Random Forest, AdaBoost, Gradient Boost and SVM using train and validation data sets to classify whether a flight would be delayed or not. For the first three models, we also employed grid search with k-fold cross-validation technique to find the optimum value for the hyperparameters. After comparing the results from the different models, we decided upon Random Forest as the best model. The final stage of the the process is to identify the appropriate threshold for our model based on the airline's business needs and perform a final evaluation of the model using test data set.\n5. Findings\nOur main delay predictors are the time of flight schedule during the day, day of the week, distance of flight route and ground temperature. Flight delays often occurs 3pm and 9pm and they are more likely on Tuesdays. Surprisingly, high temperature have a higher impact on delays compared to colder and potentially snowy days. If temperature goes above 35 degrees Celsius, delays will happen more often. This can be explained by the fact that during winter, airlines will be more prepared for snow and rain. However, in the case of extremely hot weather the crew are not meant to work in the heat for too long due labour laws and union agreements. This is further exacerbated due to the fact that there are more flights during warmer summer months than in the winter.\n6. Classification model results\nDue to imbalanced data between on-time and delayed data, we employed a SMOTE over-sampling method during training and validation while training our models and hyperparameters tuning. Using various cost metrics including customer compensation, staff cost, penalties and other operational costs, we calculated our model threshold for our business needs to be 0.46 for the selected Random Forest model.\n\n\n\nData set\nSampling\nPower (TPr)\nAlpha (FPr)\nBeta (FNr)\n\n\n\n\nTraining data\nSMOTE\n87%\n15%\n13%\n\n\nTest data\nno sampling\n54%\n22%\n46%\n\n\n\nAt this stage, our model is only able to predict just over half of delayed flights. The high false negative is a concern as one of our key objectives is to reduce this error since it would be costlier for United Airlines to be unprepared for a flight delay when there is one, than to be slightly over-prepared for a delay when there is none.\nThe lower performing test results suggest that there has been some overfitting during our training stages. However, we are confident that our test results can be improved further given more time considering computational resource limitations that we faced in order to run more hyperparameter tuning tasks.\n\n7. Recommendations\nFrom our studies, we propose the following recommendations to United Airlines operating from its base in Newark Airport:\n\nDue to the higher likelihood of delays between 3pm to 9pm, there should be more stand-by aircrafts based at the airport in order absorb some of these delays by providing alternative aircraft without having to rely mainly on incoming aircrafts that will most likely be delayed.\nAs flights on Tuesdays are more likely to be delayed, additional casual or subcontracted staff should be employed in order to support existing full time staff at the airport, particularly in handling customer service issues. Delayed flights cause inconvenience to passengers and thus additional customer service representatives at the counters and all around the airport could help to improve the airline's image by providing the latest information and offer alternative assistance where applicable.\nWhile weather-related delays could anticipated using advanced weather warning system, delays stemming from hot summer days can be addressed by performing more maintenance checks during the evening during cooler temperature. Additional summer traffic to leisure destinations can be scheduled during less busy hours in the airport in order to free up the slots for other regular traffic in order to reduce the bottlenecks at critical hours during the day.\n\n""], 'url_profile': 'https://github.com/khairulomar', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': [""Blue Book for Bulldozers\nUsing random forest and linear regression in bluebook-for-bulldozers kaggle's dataset\nCompetation Link : Link\n""], 'url_profile': 'https://github.com/omarAlezaby', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 20, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['ML-Kaggle-Heart-disease-UCI\nThe implemented challenge solved comparing three different methods: Support Vector Machines, Logistic regression, Decision trees.\nnumpy, pandas, math, sklearn and imblearn are prerequested packages\n'], 'url_profile': 'https://github.com/AidaIdrisova', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jul 15, 2020', 'Jupyter Notebook', 'Updated Feb 4, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Python', 'Updated Jan 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mohanram123', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Oct 3, 2020', 'Updated Jan 22, 2020', 'CSS', 'Updated Sep 16, 2020', '1', 'C++', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'Łeba, Poland', 'stats_list': [], 'contributions': '299 contributions\n        in the last year', 'description': ['Spam-Model\nLogistic regression model for classifying messages to be spam or not spam.\nDescription\nThis repo contains machine learning logistic regression model based for classifying SMS messages to be spam or valid message. Model has been packed evaluated with scikit-learn library, then packed with pickle library and deployed as one page web application with Flask.\nData\nData used for training the model can be found in provided CSV file. I do not own the data. It can be found here. Credits to Tiago A. Almeida and José María Gómez Hidalgo.\nTechnologies\n\nPython\nscikit-learn\nJupter notebook\npickle\nBootstrap 4\nFlask\n\nSetup\nTo run the model on your device you need Python with Jupyer notebook (prefered) and sckikit-learn library. To run web application you need only Python with Flask.\n'], 'url_profile': 'https://github.com/jaheyy', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Oct 3, 2020', 'Updated Jan 22, 2020', 'CSS', 'Updated Sep 16, 2020', '1', 'C++', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Spline_Intro\nA project to help program regression splines rather than just use pre-built packages.\nI work through the Analytics Vidhya tutorial found at, https://www.analyticsvidhya.com/blog/2018/03/introduction-regression-splines-python-codes/.\n'], 'url_profile': 'https://github.com/vultronn', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Oct 3, 2020', 'Updated Jan 22, 2020', 'CSS', 'Updated Sep 16, 2020', '1', 'C++', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['How to use logistic regression to calculate class conditional probabilities.\nThe aim of this to show the general principle, but the worked example is a quick and dirty and there should be much more thoroughness in a real analysis.\nFor example, in a real analysis, I would not assume uniform distributions over the predictor variables, nor would I necessarily use stepwise regression to find a minimal set of predictors, and I would look at joint probability distributions using multivariate visualization rather than marginal probability distributions and averages.\n'], 'url_profile': 'https://github.com/mark-andrews', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Oct 3, 2020', 'Updated Jan 22, 2020', 'CSS', 'Updated Sep 16, 2020', '1', 'C++', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'Salt Lake City', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vilandao', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Oct 3, 2020', 'Updated Jan 22, 2020', 'CSS', 'Updated Sep 16, 2020', '1', 'C++', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'Ireland', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Amazon-Fine-Food-Reviews\nAmazon Fine Food Reviews Analysis Data Source: https://www.kaggle.com/snap/amazon-fine-food-reviews\nThe Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.\nNumber of reviews: 568,454 Number of users: 256,059 Number of products: 74,258 Timespan: Oct 1999 - Oct 2012 Number of Attributes/Columns in data: 10\nAttribute Information:\nId\nProductId - unique identifier for the product\nUserId - unqiue identifier for the user\nProfileName\nHelpfulnessNumerator - number of users who found the review helpful\nHelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\nScore - rating between 1 and 5\nTime - timestamp for the review\nSummary - brief summary of the review\nText - text of the review\nObjective: Given a review, determine whether the review is positive (Rating of 4 or 5) or negative (rating of 1 or 2).\n'], 'url_profile': 'https://github.com/manish1772', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Oct 3, 2020', 'Updated Jan 22, 2020', 'CSS', 'Updated Sep 16, 2020', '1', 'C++', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'Hong Kong', 'stats_list': [], 'contributions': '940 contributions\n        in the last year', 'description': [""Comparison of Newton's Method in Optimisation and Gradient Descent\nI experiment with and benchmark NM vs. GD for multivariate linear regression, on the Iris flower dataset. Newton's Method converges within 2 steps and performs favourably to GD. However, it requires computation of the Hessian, as well as depends heavily on the weight initialisation.\n(Vanilla) Gradient Descent:\n\nNewton's Method:\n\n""], 'url_profile': 'https://github.com/chinglamchoi', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Oct 3, 2020', 'Updated Jan 22, 2020', 'CSS', 'Updated Sep 16, 2020', '1', 'C++', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ilyas-Malik', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Oct 3, 2020', 'Updated Jan 22, 2020', 'CSS', 'Updated Sep 16, 2020', '1', 'C++', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'new delhi', 'stats_list': [], 'contributions': '261 contributions\n        in the last year', 'description': ['Machine_learning_project\nDescription\nit is  a (mine first ml project) machine learning model based on linear regression algorithm. it predicts salary .\n.predict the salary based on given scores...\nLibraries or Tools used\n» pandas or Numpy\n» Scikit learn \n»  Html5\n» CSS\n»  Flask (Backend)\nDeployed on\nHEROKU CLOUD\nsummary\nit is a regression problem(we used regresion to predict continuous data), we predicted the slary of employees based on their\nexam scores.\nLicence\nUnder @MiT Licence\ncontact\n\nEmail - niketkumardheeryan@gmail.com\nLinkedIn - https://linkedin.Com/in/niketkumardheeryan\n\nRelated topic\nScope\nupcoming related technologies\nversions of Libraries used\n'], 'url_profile': 'https://github.com/Niketkumardheeryan', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Oct 3, 2020', 'Updated Jan 22, 2020', 'CSS', 'Updated Sep 16, 2020', '1', 'C++', 'MIT license', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['TextRSN\nIJCAI2020 Paper ID 336:Regression Segmentation Network for Arbitrary Shape Text Detection;\nDue to copyright issues, we only open source part of the code. Later, we will open source all the code, including training and testing code\n1.Prerequisites\npython 3.7;\nPyTorch 1.2.0;\nNumpy >=1.16;\nCUDA 10.1;\nGCC >=9.0;\nNVIDIA GPU(with 10G or larger GPU memory for inference);\nNote: we perform the experiment on GTX-1080Ti and GeForce RTX-2080;\n2.Description\n\nGenerally, this code has following features:\n1.Just include complete inference code\n2.Support Total-text, CTW1500,TD500, ICDAR15, MLT-17 datasets\n\n3.Partial Dataset Links\n\nCTW1500\nTD500\nTotal-Text\n\n4.Pretrained Models\nThe model is pre-train on MLT-17 and fine-tune on each benchmark.\n5.Running tests\nrun:\nsh eval.sh\n\nThe details in a are as follows:\n#!/bin/bash\nCUDA_LAUNCH_BLOCKING=1 python eval_test.py --exp_name Totaltext --checkepoch 400 --gpu 0 \n\n# threshold=0.3, threshold=0.65; test_size=(256,1024)\n#CUDA_LAUNCH_BLOCKING=1 python eval_test.py--exp_name Totaltext --checkepoch 400 --gpu 0\n\n# threshold=0.3, threshold=0.65;test_size=(512,1024)\n#CUDA_LAUNCH_BLOCKING=1 python eval_test.py --exp_name Ctw1500 --checkepoch 115 --gpu 0\n\n# threshold=0.315, threshold=0.8; test_size=(0,832)\n#CUDA_LAUNCH_BLOCKING=1 python eval_test.py --exp_name TD500 --checkepoch 125 --gpu 0\n\n# threshold=0.4, threshold=0.8; test_size=(960, 1920)\n#CUDA_LAUNCH_BLOCKING=1 python eval_test.py --exp_name Icdar2015 --checkepoch 885 --gpu 0\n\n# threshold=0.4, threshold=0.8;--test_size=(256,1920)\n#CUDA_LAUNCH_BLOCKING=1 python eval_test.py --exp_name MLT2017 --checkepoch 64 --gpu 0\n\nNote: you should modify the corresponding parameters in option.py file according to the annotations in eval.sh\n6.Our Method vs PSENet-1s\n\n7.Ablation Study\n\n8.Qualitative results ( Total-Text CTW-1500)\n\nLicense\nThis project is licensed under the MIT License - see the LICENSE.md file for details\n'], 'url_profile': 'https://github.com/fendaq', 'info_list': ['Python', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 25, 2021', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', '1', 'Jupyter Notebook', 'Updated Feb 23, 2020', '1', 'Jupyter Notebook', 'Updated Oct 3, 2020', 'Updated Jan 22, 2020', 'CSS', 'Updated Sep 16, 2020', '1', 'C++', 'MIT license', 'Updated Jan 17, 2020']}"
"{'location': 'Cincinnati, Ohio', 'stats_list': [], 'contributions': '199 contributions\n        in the last year', 'description': ['Risky-Flight-Landing-Analysis\nAcademic Project: Predicting what factors affect risky flight landing using Linear Regression in R.\nGoal: To study what factors and how they would impact the landing distance of a commercial flight.\nData: Landing data (landing distance and other parameters) from 950 commercial flights (not real data set but simulated from statistical models). See two Excel files ‘FAA-1.xls’ (800 flights) and ‘FAA-2.xls’ (150 flights).\nLanguage Used: R\nApproach: Multiple Linear Regression\nMajor Finding: Speed Air, Aircraft & Height are the most influencing factors.\nAcknowledgement/Citation: This analysis was completed as a part of requirement for Statistical Modeling course instructed by Dr. Dungang Liu at Carl H. Lindners College of Business at the University of Cincinnati\n'], 'url_profile': 'https://github.com/nifaullah', 'info_list': ['HTML', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', '1', 'JavaScript', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 12, 2020']}","{'location': 'CHENNAI', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Donor_Choose_Dataset\nIn this repository shows different classification and regression algorithms applied on donor choose dataset\nBy using different hyper parameters and performance metrix for different modeling metonds on donor choose data set and finallay got accuracy results and log loss and f1 score as best of my knowledge.\nThanks to appliedaicourse.com\n'], 'url_profile': 'https://github.com/akhilvydyula', 'info_list': ['HTML', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', '1', 'JavaScript', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 12, 2020']}","{'location': 'Valencia, Spain', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['""# Doctoral_thesis""\n'], 'url_profile': 'https://github.com/gabriuma', 'info_list': ['HTML', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', '1', 'JavaScript', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 12, 2020']}","{'location': 'San Diego ', 'stats_list': [], 'contributions': '187 contributions\n        in the last year', 'description': [""Project_Yelp_Rating_Predictor\nLinear regression project determining attribute correlations and modeling effect on a business' overall star rating.\n""], 'url_profile': 'https://github.com/jonathanmatsen', 'info_list': ['HTML', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', '1', 'JavaScript', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Visual regression testing with Jest, Puppeteer and jest-image-snapshot\nWhat is visual regression testing?\nVisual regression tests help catch unintentional changes to the visual rendering of a web page or component. As engineers and testers, we already do this manually, but it takes a lot of time, and as humans, are prone to overlooking breaking changes. Automating this process gives us back time to focus on more important things, and gives us added confidence when updating code.\nHow it works\nThese tests are run in headless Chrome using Puppeteer. They are written using the Jest testing framework. jest-puppeteer is added to facilitate writing and running the tests, and jest-image-snapshot does the actual image comparison and lets us know when something has changed.\nEach test will load a page in the browser, perform necessary UI interactions, and then take a screenshot of the entire page or a specific element on the page. The screenshot file is saved alongside the test file and committed to source control. The next time the tests run, this image is used as a reference and compared against a current screenshot. If any differences between the two images are detected, the test will fail, and an image containing the diff is created and saved alongside the original reference screenshot.\nThese tests are setup to run automatically in Bitbucket Pipelines CI when a pull request is created (see bitbucket-pipelines.yml), but can also be run on a manual basic on a developer's local machine.\nWe run the tests in a Docker container to ensure there are no differences between local and CI environments. These tests should not be run on a developer's native OS because minor rendering differences between operating systems in the browser will cause test failures.\nSystem requirements\n\nNodeJS\nDocker CE\n\nSetup\nInstall NodeJS dependencies by running npm install.\nRunning the tests\nTo run the tests manually, first start the development server by running npm start. Next, in a separate console, run npm run vistest. This will start a Docker container on your machine and then run the tests.\nFor any given test, if no reference screenshot exists, one will be taken and saved alongside the test file in the __image_snapshots__ directory. Afterwards, be sure to commit these new reference files to version control so that they may be used for future tests.\nIf there are any failures due to differences between current and existing screenshots, a .png file containing the image diff will be saved in __diff_output__ directory alongside the existing reference screenshot for the test. These diff files are ignored by version control.\nUpdating reference screenshots\nIf you have made intentional changes to the way a component is rendered, you can update the existing reference screenshots in one of two ways. First, you can simply delete the reference files that you wish to update. Alternatively, when running npm run vistest:watch, you will have to option to update screenshots for any failed tests, one at a time. Afterwards, commit the new and/or updated files to version control so that your changes are respected during future test runs.\nWriting tests\nTest files must follow the naming pattern *.vistest.js and reside in the src directory, preferably alongside their component's source file. Tests are written using Jest and Puppeteer. A number of globals (e.g. browser and page) are made available by jest-puppeteer to reduce the amount of setup and teardown code required for each test. In addition, common utility methods are also defined in test-setup.js that can be imported and used within tests.\nAs a basic example, here is how you would write a test that takes a screenshot of a specific page and expects that it matches an existing reference screenshot.\n// landing.test.js\ndescribe('landing page', () => {\n  const { loadPage, matchBodyScreenshot } = global.utils;\n\n  test('should render correctly', async () => {\n    await loadPage(url);\n    await matchPageScreenshot();\n  });\n});\nAs a general rule, a test should only take one screenshot. Screenshot files are named after the test description. If you need to take multiple screenshots to test different states of a component, create a separate test for testing each state.\nGotchas\n\nBecause the reference screenshots are named using the test description, the file names can become quite long. On Windows OS, this can lead to issues where there may be a maximum limit to the character limit in a file name.\n\n""], 'url_profile': 'https://github.com/thinkcompany', 'info_list': ['HTML', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', '1', 'JavaScript', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 12, 2020']}","{'location': 'Wellington, New Zealand', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AndyFrench11', 'info_list': ['HTML', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', '1', 'JavaScript', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 12, 2020']}","{'location': 'Melbourne,Australia', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Sales_Prediction\nSales Prediction Using Linear Regression , dataset is taken from Analytics Vidhya\n'], 'url_profile': 'https://github.com/trilochanyadav', 'info_list': ['HTML', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', '1', 'JavaScript', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 12, 2020']}","{'location': 'Kerala', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['reg1_net\nImage registeration of moved image to a single fixed image usig CNN!\nCNN estimates affine transform(change parameters to do perspective also) to match input moved image to the template/fixed image used for training.\n#Run:\ntrain.py\ntest.py\n#Notebook:\nOpen terminal\nEnter: jupyter notebook\nOpen: register_single_cnn.ipynb\n#To Do\n-Adding option of perspective\n-Adding proper data generator to use parallel processing\n-Adding moved image folder for training CNN with real moved images in addition to augmented ones.\n\n'], 'url_profile': 'https://github.com/jerinka', 'info_list': ['HTML', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', '1', 'JavaScript', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 12, 2020']}","{'location': 'Mandi, India', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Neural-Network-Regression-Model-using-Keras\nThis is the implementation of Artificial Neural Network for Regression using Keras\n'], 'url_profile': 'https://github.com/whitewolf1905', 'info_list': ['HTML', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', '1', 'JavaScript', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 12, 2020']}","{'location': 'The Hague', 'stats_list': [], 'contributions': '1,779 contributions\n        in the last year', 'description': ['classifiers\nLogistic Regression, SVM, Decision Trees, K-NN and Naïve Bayes with 5-cross validation\n'], 'url_profile': 'https://github.com/ioannapap', 'info_list': ['HTML', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jun 13, 2020', '1', 'JavaScript', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Python', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 12, 2020']}"
"{'location': 'Munich, Germany', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['regression visualizations\nA simple single-variable regression model trained on the houston housing dataset for visualizations\n\nreal-time plot:\n\n\n'], 'url_profile': 'https://github.com/hollowcodes', 'info_list': ['1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '250 contributions\n        in the last year', 'description': [""Ames Housing Dataset\nBuild an ML regression model to predict real estate prices using the Ames Housing Dataset. In this project, we use a dataset and perform a rapid analysis, followed by minimal data transformation. The goal of the project is to place as accurate model as possible as quickly as possible.\nDataset\nThe Ames Housing Dataset contains features on 2930 houses and was compiled by Dean De Cock for use in data science education.\nProject Architecture\nThe train dataset is transformed using tensorflow_transform. The result is a transformed train dataset as well as a transformation function. The transformation function is used together with the model at serve time.\nRepository Structure\nThe exploratory data analysis is performed in the eda.ipynb notebook. The actual training is performed in the trainer module.\nThe prototype.ipynb notebook contains an initial proof of concept of the transform functions.\nThe trainer module contains a training script. At training time, the trainer module is packaged as a docker container based on the instructions in the Dockerfile. It is then submitted for training on GCP's ML Engine.\n""], 'url_profile': 'https://github.com/rossrco', 'info_list': ['1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['Linear Regression or KNN regressor ?\nPredict the tensile strength of steel\n'], 'url_profile': 'https://github.com/waseemshariff126', 'info_list': ['1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Kenya', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': [""Cell-prediction-using-Logistic-Regression.\nShort Project using Logistic Regression to determine whether a patient's cells are Benign or Malignant.\nData obtained form Cognitive Class- IBM\n""], 'url_profile': 'https://github.com/Amonoff', 'info_list': ['1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Linear_Regression_Project_Ecommerce.ipynb\n'], 'url_profile': 'https://github.com/beizaa', 'info_list': ['1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Epfeif', 'info_list': ['1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Lead-Scoring-with-Logistic-Regression\nProject Brief\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses.\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%.\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as ‘Hot Leads’. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone.\nThere are a lot of leads generated in the initial stage but only a few of them come out as paying customers at the last stage. In the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.\nX Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance.\n'], 'url_profile': 'https://github.com/ankan2709', 'info_list': ['1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '306 contributions\n        in the last year', 'description': ['linear regression, implemented many ways\nthis demonstrates many ways of fitting a linear model.\nthe point here is to showcase not linear regression per se, but rather different ways of fitting models in general. i just work with linear regression because it\'s easy, and i can get fancy with various (sometimes complicated) model fitting algorithms.\nby ""model fitting algorithms"" i mean:\n\nanalytically calculating the least squares fit using linear algebra (i.e. \napproximating the least squares fit with gradient descent (with ordinary least squares as a loss function, or negative log likelihood as well)\napproximating the predictive distribution using monte carlo sampling (and maybe variational calculus, if i have time) (aka bayesian regression)\n\ntypes of modeling\n\nregression\nclassification\nothers? rank, ordinal / level\n\nrecord trials\n\nname\ndata used\ncoefficient estimates (MLE, MAP, median posterior)\nobjective function, and value\ntime to execute\nframework name\n\ndata\n\nfake data (given (N, p))\ntitanic dataset?\n\npoint estimation\noptimize an objective function\n\nanalytically\n\nOLS for linear regression\n\n\ngradient descent\n\nfirst order gradient\nsecond order gradient (with exact hessian)\nsecond order gradient (with approx hessian)\nproximal gradient descent for lasso?\n\n\n\napproximate the posterior\n\nmonte carlo\n\nbasic\nimportance sampling\nrejection sampling\n(adaptive something?)\n\n\nmarkov chain monte carlo\n\nmetropolis-hastings\ngibbs\nhamiltonian\n\nno u-turn\n\n\n\n\n\nvariational\n\n???\n\nobjective functions\n\n(regression) squared error\n(regression) negative log likelihood\n(regression) absolute error\n(classification) cross entropy\n(ranking / ordinal) ???\n\nmodels\n\nlinear regression\nlinear regression with L2 penalty (ridge) / gaussian prior\nlinear regression with L1 penalty (lasso) / laplacian prior\nlinear regression with L1 and L2 penalty (elastic net) / mixed prior\nlogistic regression\ngeneralized linear model\n\nframeworks\n\nsklearn\nstatsmodels\npure python\nnumpy + scipy\njax\nnumba?\ntensorflow\npytorch\npystan\npymc3\n(R) rstanarm\n\nplatforms\n\ncpu\ngpu\n\nmath\n\nhessian is not always positive definite... when is it not for negative log likelihood?\n\nvisualization\n\nfor optimization:\n\nthe objective function surface in coefficient space\npoint evaluations by iteration\n\n\nfor monte carlo:\n\nthe posterior as it samples\nmode, median, mean points of posterior\n\n\n\n'], 'url_profile': 'https://github.com/grisaitis', 'info_list': ['1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Madrid, Spain', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['poc-ui-visual-regression-testing\nBackstopJS Docs: https://github.com/garris/BackstopJS\nEjemplo de uso de cookies: https://medium.com/manati-web-agency/tests-de-regresi%C3%B3n-visual-para-drupal-usando-backstopjs-fe9b1b257d87\nDocumento sobre pruebas de regresion para interfaces:  https://miso-4208-labs.gitlab.io/book/chapter6/pruebas-de-regresion-para-interfaces.html\nstorybook & visual testing: https://www.learnstorybook.com/visual-testing-handbook/\n'], 'url_profile': 'https://github.com/next-miguelminguez', 'info_list': ['1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Hass-Consulting-Company-Regression-Analysis\n'], 'url_profile': 'https://github.com/lydiandiba', 'info_list': ['1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}"
"{'location': 'Madrid, Spain', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['poc-ui-visual-regression-testing\nBackstopJS Docs: https://github.com/garris/BackstopJS\nEjemplo de uso de cookies: https://medium.com/manati-web-agency/tests-de-regresi%C3%B3n-visual-para-drupal-usando-backstopjs-fe9b1b257d87\nDocumento sobre pruebas de regresion para interfaces:  https://miso-4208-labs.gitlab.io/book/chapter6/pruebas-de-regresion-para-interfaces.html\nstorybook & visual testing: https://www.learnstorybook.com/visual-testing-handbook/\n'], 'url_profile': 'https://github.com/next-miguelminguez', 'info_list': ['JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Hass-Consulting-Company-Regression-Analysis\n'], 'url_profile': 'https://github.com/lydiandiba', 'info_list': ['JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/devon-c', 'info_list': ['JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dyadav4', 'info_list': ['JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'kafrsaqr-zagazig-egypt', 'stats_list': [], 'contributions': '643 contributions\n        in the last year', 'description': ['-ahmedhasRegression_using_multiple-variable\nahmedhasRegression_using_multiple-variable based on library in python      using numpy and pandas and matplotlib\n'], 'url_profile': 'https://github.com/ahmed-hassan97', 'info_list': ['JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Stock Market Prediction using Linear Regression and Python3.\n'], 'url_profile': 'https://github.com/abaziz026', 'info_list': ['JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/satyan-dot', 'info_list': ['JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '132 contributions\n        in the last year', 'description': ['Predicting-Income-with-Logistic-Regression\nDescription:  Using the  1996 U.S. Census ""Adults"" dataset w/ Logistic Regression Machine Learning modeling to predict whether or not a person is in the middle class-- >$50K or <=$50K salary yearly.\nsee full analysis in capstone_2_thinkful.ipynb\n'], 'url_profile': 'https://github.com/andheartsjaz', 'info_list': ['JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'Delhi,India', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/redrivals', 'info_list': ['JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'MATLAB', 'Updated Jan 20, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Linear-regression-algorithm-from-scratch\nBuilt an Linear regression algorithm from scratch using octave.\nIn this folder you will find the code needed for calculating gradient descent, cost function and the data needed for it\n'], 'url_profile': 'https://github.com/vamsiabbireddy', 'info_list': ['JavaScript', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Updated May 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'MATLAB', 'Updated Jan 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '263 contributions\n        in the last year', 'description': [""Regression-over-IoT-Data\nPull IoT sensor data from mariadb and analyse it with sklearn\nOverview\nThis project uses python scripts to interface with a remote mariadb server. The original sensor data is being pulled from a TP-Link HS110 smart outlet interfacing with a local OpenHAB server running on a raspberry Pi. This local server logs incoming sensor data to an OpenHAB Cloud server running on an EC2 AWS.\nHere is where our python scripts will analyze the logged data from mariadb.\nUsage\nimport-db.py is used to import data from a specified mariadb server and output a json file with it's contents.\nanalyze-db.py is used to perform polynomial regression on the dataset using sklearn and display it through matplotlib\n""], 'url_profile': 'https://github.com/DiegoVinasco', 'info_list': ['Python', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Linear_regression_with_R_Duke_university_coursera\nLinear_regression_with_R_Duke_university_coursera\n'], 'url_profile': 'https://github.com/tanviredusecond', 'info_list': ['Python', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'Boston', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Prediction of NBA player 5-year career longevity with logistic regression\nImplementing a basic classification model using logistic regression.\nThe provided dataset ""nba_logreg"" contains statistics for new NBA players.\nThe goal is to train and test a logistic regression model to predict whether a player will have a career of at least five years in the NBA.\n'], 'url_profile': 'https://github.com/nidhi15', 'info_list': ['Python', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/beizaa', 'info_list': ['Python', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Python', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '289 contributions\n        in the last year', 'description': ['Kaggle_House-Prices_Advanced_Regression_Techniques\nkaggle submit\n'], 'url_profile': 'https://github.com/mokpolar', 'info_list': ['Python', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'Atlanta, GA', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Class Overview\nThis was a project completed during the Fall 2019 Semester of my undergraduate career at Georgia Tech, in a class entitled Machine Learning for Trading that taught both basic trading strategies and financial vehicles along with Machine Learning techniques such as Reinforcement Learning, Decision Trees, and Bag Learners.\nProject Overview\nThis project involved cerating a Decision Tree learner, Random Forest, and Bag Learner to learn the trends in the MSCI emerging market index to predict the a buy or sell outcome based on training data. The tree model was built using numpy as follows:\n# TREE \n# -------------------------------------------------------\n# [ FACTOR | SPLIT_VALUE | LEFT_RELATIVE | RIGHT_RELATIVE ]\n#          |             |               |               \n#          |             |               |               \n#          |             |               |               \n#          |             |               |               \n\nDT Implementation\naddEvidence()\ndef addEvidence(self, Xtrain, Ytrain):\n\nThis is a wrapper function for recursive function calls used to build the tree.\nbuildTree()\ndef buildTree(self, data):\n\nIn order to build our model, training evidence was appended to the the tree and split on the absolute correlation coefficient, calculated using a helper method. This recursive function was used to split the training data on the condition that values in the Xth feature will be to the left of the root caller and greater values will be to the right.  A varying leaf size was used and experimented with to prevent overfitting our training data to the test data. Base cases involve maximum stack depth, the remaining values are all the same with no duplicate, and aggregating the the remaining rows if below the hyper-parameter ""leaf size.""\nquery()\ndef query(self, Xpoints):\n\nUsing our test data, with Xtest and Ytest broken up appropriately, the tree is traversed according to the split value of a certain node, resulting in a \'Y prediction\' that is appended to a list for error checking as compared to \'Ytest.\'\n'], 'url_profile': 'https://github.com/svia3', 'info_list': ['Python', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'Denver', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/veenaa014', 'info_list': ['Python', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': [""WordProbability Estimation\nThis is a representation of HalfLife Regression algorithm used to estimate the probability that a user reminds a word after learning it.\nThis is based on duolingo's working algorithm mention in the paper below.\nhttps://s3.amazonaws.com/duolingo-papers/publications/settles.acl16.pdf\nConcepts\nOverfitting\nGradient descent\nGradient descent algorithms (Adagrad may not be the best solution, depending on the data)\nCross validation, tuning\nMaximum Likelihood Estimation\nIntroduction to theano (theano might be useful because gradients need not be calculated manually. Also, a lot of gradient descent algorithms can be implemented on the fly. Has a bit of a learning curve)\ntheano documentation\nkeras (uses theano as a backend. Useful for implementing neural networks easily)\n""], 'url_profile': 'https://github.com/snehapvs', 'info_list': ['Python', 'Updated Jan 25, 2020', 'Updated Jan 22, 2020', 'R', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Updated Jan 24, 2020', 'Python', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 22, 2020']}"
"{'location': 'Buffalo', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JadhavBhakti', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'R', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ramkishan1359', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'R', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'R', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['House_Price-Prediction-Linear-Regression-Regularization-\nThis dataset is been taken from Kaggle and contains 81 Features.\nIn this is case study, we will use Linear Regression to predict house prices, based on the various different independent variables.\nHere we start with cleaning the data. Firstly, it shows large null values in various different features, but as we read the description file, we come to know that these are actually non-null values.\nThus, we treat these values and replace them accordingly.\nThen we scale our data and start with model building. First, we start with Linear Regression Stats Model and then finally reach to Regularization.\nWe will use all three types of Regularization (Lasso, Ridge & Elastic Net), and then find out the best one.\n'], 'url_profile': 'https://github.com/Isha-Pandey27', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'R', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'R', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '76 contributions\n        in the last year', 'description': ['Iris dataset Interactive Regression - Shiny Application\nThis project builds an interactive tool that, based on the iris dataset,\nwould predicts the width of the sepals/petals given their corresponding length.\nThe project was carried out as a final (peer-reviewed) assignment  within the Coursera course Developing Data Products of the Johns Hopkins University Data Science Specialization in January 2020\n'], 'url_profile': 'https://github.com/gtsa', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'R', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'R', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Logistic-and-Polynomial-Regression-with-noisy-data\n'], 'url_profile': 'https://github.com/rgawande', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'R', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vinaykumarraju', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'R', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '251 contributions\n        in the last year', 'description': [""Module 5 Project - Loan Default Classification\nData Scientist: Andrew Cole\nGoals:\nLendingClub is a peer-to-peer lending company which offers a miscellaneous variety of loan options to anybody seeking one. LendingClub has the applicant enter a variety of background information necessary for loan procedures (credit score, income, term, purpose, FICO ranges, etc.) and then offers that consolidated information to peer investors who can purchase notes of the loans in a non-traditional marketplace. The result is a lower cost to borrow for the borrower, and an easier access to value growth for the investor. The mission is to create a machine learning classification model which can best classify whether a loan recipient, given their application data, will default on the loan or fully pay it off. A variety of classification algorithms will be prepared, tested, and tuned to arrive at the best performing model, which will then be used by LendingClub to measure risk of default.\nResponsibilities:\n\nCreate Github repository for files and efficient tracking of project process\nResearch LendingClub's data structure and schemas\nImport necessary libraries for gathering of data\nExploratory Data Analysis & preprocessing of gathered LendingClub data\nSplit cleaned data into model training & testing sets\nTrain multiple classification algorithms with training set\nTune respective algorithms via adjusting hyperparameters\nTest well performing algorithms on test sets\nCreate proper visualizations for explaining processes & outcomes of models\nCreate helper modules for efficient running of script\nCreate README file for project overview\nCreate presentation slide deck\nPresent concepts & actionable insights\n\nSummary of Included Files:\n\n\nMod_5_project_main.ipynb\n\nJupyter Notebook for technical audience\nPEP 8 Standards\nImports necessary packages for EDA & model testing\nLibrary importation, statistical testing, hyperparameter tunings\nModels tested: Random Forrest, Random Forest w/ GridSearch, KNN, XGboost, SVM, Logistic Regression\n\n\n\nmod_5_project_main-Copy1.ipynb\n\nJupyter notebook for scratch/experimental work\nNon-essential notebook\n\n\n\nREADME.md\n\nSee whole file\nOverview of project including goals, responsibilities, & summary of included files\n\n\n\nLendingClub Loan Classification Presentation Slide Deck\n\nGoogle slides deck for presentation\nIncludes non-technical summary of goals & procedures\nActionable Insights for LendingClub\n\n\n\nLCloans_07.csv\n\nCSV file containing LendingClub loan data\nYears: 2007-2011\n42000 entries\n\n\n\nLCDataDictionary.csv\n\nCSV file\nContains feature definitions for LCloans_07.csv file\n\n\n\n""], 'url_profile': 'https://github.com/andrewcole33', 'info_list': ['Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'Python', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'R', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}"
"{'location': 'Rwanda', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PeyGis', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'C++', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'R', 'Apache-2.0 license', 'Updated Sep 18, 2020']}","{'location': 'Melbourne', 'stats_list': [], 'contributions': '510 contributions\n        in the last year', 'description': ['Predict Final Grade\nThis project was completed to showcase an understanding of Data Science Project using Machine Learning (ML) Algorithm.\nLinear Regression ML was used to predict the final grade of students.\nConclusion: We compared the base model using all attributes and tuned model using only highly correlated independent attribute. We evaluated Root Mean Square Error (RMSE) of both the model. Hence, based on RMSE we can states that the base model performed better.\nFuture work:\nData sets can be further tuned by cleaning:\n\nPrevious failed students\nWe can explore the relationship between previously failed student vs final grade\n\nGetting Started\nPrerequisites\nYou only need Jupyter Notebook to run this program.\nBuilt With\nR Language\nJupyter\nAuthors\nAkash Chhetri\n'], 'url_profile': 'https://github.com/achhetr', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'C++', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'R', 'Apache-2.0 license', 'Updated Sep 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Customer Churn Prediction\n'], 'url_profile': 'https://github.com/niteshankit', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'C++', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'R', 'Apache-2.0 license', 'Updated Sep 18, 2020']}","{'location': 'Salt Lake City', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Mushroom classification with Support Vector Machine\nAlthough this dataset was originally contributed to the UCI Machine Learning repository nearly 30 years ago,\nmushroom hunting (otherwise known as ""shrooming"") is enjoying new peaks in popularity.\nOur goal for this project is to build a machine learning model that can predict whether a mushroom is poisonous or edible based various mushroom features!\nAbout the dataset\nThe dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom\ndrawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous,\nor of unknown edibility and not recommended. This latter class was combined with the poisonous one.\nSome of the features provided are: cap shape, cap surface, cap color, odor, gill size, gill color, stalk shape, stalk root, veil type, ring number, population, etc.\nKernel content\n\nLoad an clean the dataset\nExploratory data analysis\nBuild and compare different predictive machine learning algorithms\n\nConclusion\nAfter trying 5 different algorithms (Logistic regression, Decision tree classifier, KNN classifier, Random forest classifier, and Support Vector Machine),\nwe conclude that Decision tree, Random forest, and KNN gave us the highest R-squared score.\n'], 'url_profile': 'https://github.com/vilandao', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'C++', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'R', 'Apache-2.0 license', 'Updated Sep 18, 2020']}","{'location': 'Melbourne, Australia', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pqiao29', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'C++', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'R', 'Apache-2.0 license', 'Updated Sep 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': [""predicting-mpg-for-hidden-class-of-cars-\nClustering the cars based on its hidden class and applying linear regression to predict their mpg(miles per gallon)\nI have used non-hierarchial clustering method (k-means) for grouping.\nThe dataset is uploaded in the repository, 'cars.csv'.\n.ipynb file has the entire code for this project.\nABOUT DATA-\nThe dataset was used in the 1983 American Statistical Association Exposition. The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 2 multivalued discrete and 4 continuous variables.\nAttribute Information:\nCar Name – Name of the car\nCyl – No of cylinders in the car – Multivalued discrete\nDisp – Displacement – continuous\nHp – Horsepower – continuous\nWt – Weight – continuous\nAcc – Accleration – continuous\nYr – Model of the car – Multivalued discrete\n""], 'url_profile': 'https://github.com/Pratoshraj', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'C++', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'R', 'Apache-2.0 license', 'Updated Sep 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': [""University-Admission-Prediction\nA logistic regression model aiming to predict whether a candidate will be admitted by a university.\nGrades of two exams and admission result of past candidates are given as training data. We will train a classifier from scratch to predict the admission probability of a certain candidate based on two exam scores.\nThe following methods are established:\n\n\nsigmoid : maps to probability\n\n\nmodel : returns probability result\n\n\ncost : computes loss based on parameters\n\n\ngradient : computes each parameter's gradient direction\n\n\ndescent : updates parameters\n\n\naccuracy: computes accuracy on test set\n\n\nBesides, I will also explore the behavior of gradient descent based on three different stop strategies.\n""], 'url_profile': 'https://github.com/yxzhu16', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'C++', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'R', 'Apache-2.0 license', 'Updated Sep 18, 2020']}","{'location': 'Angers, France', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['Random Forest with Python\nTutorial to compute random forests, gradients boostings eand extrem gradient boostings in Python\nMachine learning and regression\nComputing of many methods of machine learning to obtain the best possible model\nMachine learning and classification\nData-challenge on an unknown dataset\nThe report explains the best model, i.e. a neural network after a features selection\n'], 'url_profile': 'https://github.com/Sithamfr', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'C++', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'R', 'Apache-2.0 license', 'Updated Sep 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rraasch', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'C++', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'R', 'Apache-2.0 license', 'Updated Sep 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '95 contributions\n        in the last year', 'description': ['\nFunction Space Optimization: A symbolic regression method for estimating parameter transfer functions for hydrological models\n\n\n\nAccompanying code for the publication ""Function Space Optimization: A symbolic regression method for estimating parameter transfer functions for hydrological models""\nThe code in this repository was used to produce all results and figures in our manuscript. The data for geo-physical properties is available under https://doi.org/10.5281/zenodo.3676053 and the discharge data used in this study is available at https://www.ehyd.gv.at. The meteorological data from the INCA dataset cannot be made public, because the rights belong to the Zentralanstalt für Meteorologie und Geodynamik (ZAMG). It can be acquired from https://www.zamg.ac.at.\nContent of the repository\n\nFunctions All functions used in the Paper code. This mainly includes functions for the context free grammar implementation, FSO VAE training and generator functions, d-GR4J optimization routines and additional helper functions.\nPaper code The scripts used to produce the results of the publication.\n\nLicense\nApache License 2.0\n'], 'url_profile': 'https://github.com/MoritzFeigl', 'info_list': ['Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Jul 22, 2020', 'C++', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 3, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'HTML', 'Updated Feb 11, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', '1', 'R', 'Apache-2.0 license', 'Updated Sep 18, 2020']}"
"{'location': 'Germany', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abrarum', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Nov 11, 2020', 'Apache-2.0 license', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DeltaOptimist', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Nov 11, 2020', 'Apache-2.0 license', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['House-Price-Prediction-System\nProject of House Price Prediction System using XGBoost and linear regression. Also used Seaborn for feature engineering.\n'], 'url_profile': 'https://github.com/VedantGabani', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Nov 11, 2020', 'Apache-2.0 license', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'West Lafayette, IN', 'stats_list': [], 'contributions': '293 contributions\n        in the last year', 'description': ['#Get data for tarfile in Github, extract it in datasets/housing:\nimport os\nimport tarfile\nfrom six.moves import urllib\n\nDOWNLOAD_ROOT = ""https://raw.githubusercontent.com/ageron/handson-ml/master/""\nHOUSING_PATH = os.path.join(""datasets"",""housing"")\nHOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + ""/housing.tgz""\n\n#function to read tarfile and unzip it. And save in datasets/housing\ndef getdata(housing_url = HOUSING_URL, housing_path = HOUSING_PATH):\n    if not os.path.isdir(housing_path):\n        os.makedirs(housing_path)\n    tgzpath = os.path.join(housing_path, ""housing.tgz"")\n    urllib.request.urlretrieve(housing_url, tgzpath)\n    housing_tgz = tarfile.open(tgzpath)\n    housing_tgz.extractall(path = housing_path)\n    housing_tgz.close()\n    \nhousing = getdata()\n#From datasets/housing read housing.csv as pd Dataframe. \nimport pandas as pd\n\nfile_path = os.path.join(""datasets/housing"", ""housing.csv"")\n\ndef readcsv(path = file_path):\n    return pd.read_csv(path)\n\nhousing = readcsv()\n\n#checking if csvread was sucessesful and as expected: \nhousing.head()\n#type(housing) == pd dataframe\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n#Know the data. See how many attributes are missing how many values.. would need to impute..\nhousing.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\nlongitude             20640 non-null float64\nlatitude              20640 non-null float64\nhousing_median_age    20640 non-null float64\ntotal_rooms           20640 non-null float64\ntotal_bedrooms        20433 non-null float64\npopulation            20640 non-null float64\nhouseholds            20640 non-null float64\nmedian_income         20640 non-null float64\nmedian_house_value    20640 non-null float64\nocean_proximity       20640 non-null object\ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n#Get a sense data: Categorical variable. \nhousing[""ocean_proximity""].value_counts()\n<1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: ocean_proximity, dtype: int64\n\n#Get more sense of the data: Quantitative: mean, std, min, max and percentiles. \nhousing.describe()\n\n#Inferences: Scales are widely different. Range is huge- population, household, median income. Total_rooms, \n#total_bedroom dont make too much sense. \n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n#Get somemore sense of the data, plot histograms for all quant variables. \n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nhousing.hist(bins = 50, figsize = (25,20))\nplt.show()\n\n#Inferences: many attributes are tail-heavy: housing age, housing_value, median_income have max caps. \n#Houses in CA in 1990s costed an average of 100-200k (Haha! Now the downpayments are more than that.).\n#Median income is not in USD. \n\n#Runup to creating a stratified test set based on median income: \n\nimport numpy as np\n\n#Create income categories, (0,1,2,3,4,5).\nhousing[""income_cat""] = np.ceil(housing[""median_income""]/1.5) #divide income categories. \nhousing[""income_cat""].where(housing[""income_cat""]<5, 5, inplace = True)\n#housing.head()\n#Create test set. \n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n\nfor train_index, test_index in split.split(housing, housing[""income_cat""]):\n    train_set = housing.loc[train_index]\n    test_set = housing.loc[test_index]\n    \n    \n#Check stratification: \n\n#housing[""income_cat""].value_counts()/len(housing)\n#test_set[""income_cat""].value_counts()/len(test_set)\n\n#dropping the ""income_cat"" column.. \nfor set_ in (train_set, test_set):\n    set_.drop(""income_cat"", axis = 1, inplace = True)\n## EXPLORING THE DATA MORE:: \n\nhousing = train_set.copy()\nhousing.plot(kind = ""scatter"", x = ""longitude"", y = ""latitude"", alpha = 0.4, s = housing[""population""]/100, label = ""Population""\n                , c = housing[""median_house_value""], cmap = plt.get_cmap(""jet""), colorbar = True, figsize = (18,12))\nplt.legend()\n<matplotlib.legend.Legend at 0x113efc3c8>\n\n\n#Checking correlation matrix:: \n\ncorr_matrix = housing.corr()\ncorr_matrix[""median_house_value""].sort_values(ascending = False)\n\n#Inferences: As expected, median_income is a important factor in determining the house value. \nmedian_house_value    1.000000\nmedian_income         0.687160\ntotal_rooms           0.135097\nhousing_median_age    0.114110\nhouseholds            0.064506\ntotal_bedrooms        0.047689\npopulation           -0.026920\nlongitude            -0.047432\nlatitude             -0.142724\nName: median_house_value, dtype: float64\n\nfrom pandas.plotting import scatter_matrix\n\nattributes = [""median_house_value"", ""median_income"", ""total_rooms"", ""housing_median_age""]\nscatter_matrix(housing[attributes], figsize = (15,20))\n#Inferences: \narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x113e62860>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x113daa048>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x1140bb518>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x113ec1ac8>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x11f5210b8>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x11f53d668>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x113d05c18>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x114211240>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x114211278>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x114072d68>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x113e1b358>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x113f23908>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x113efdeb8>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x113ff74a8>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x113f85a58>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x11419c048>]],\n      dtype=object)\n\n\n#Checking if rooms/house, bedrooms/room, population/household have significant correlation. \nhousing[""rooms_per_house""] = housing[""total_rooms""]/housing[""households""]\nhousing[""bedrooms_per_room""] = housing[""total_bedrooms""]/housing[""total_rooms""]\nhousing[""population_per_household""] = housing[""population""]/housing[""households""]\nhousing.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\nrooms_per_house\nbedrooms_per_room\npopulation_per_household\n\n\n\n\n17606\n-121.89\n37.29\n38.0\n1568.0\n351.0\n710.0\n339.0\n2.7042\n286600.0\n<1H OCEAN\n4.625369\n0.223852\n2.094395\n\n\n18632\n-121.93\n37.05\n14.0\n679.0\n108.0\n306.0\n113.0\n6.4214\n340600.0\n<1H OCEAN\n6.008850\n0.159057\n2.707965\n\n\n14650\n-117.20\n32.77\n31.0\n1952.0\n471.0\n936.0\n462.0\n2.8621\n196900.0\nNEAR OCEAN\n4.225108\n0.241291\n2.025974\n\n\n3230\n-119.61\n36.31\n25.0\n1847.0\n371.0\n1460.0\n353.0\n1.8839\n46300.0\nINLAND\n5.232295\n0.200866\n4.135977\n\n\n3555\n-118.59\n34.23\n17.0\n6592.0\n1525.0\n4459.0\n1463.0\n3.0347\n254500.0\n<1H OCEAN\n4.505810\n0.231341\n3.047847\n\n\n\n\n#Checking correlation with the newly created attributes. \ncorr_mat = housing.corr()\ncorr_mat[""median_house_value""].sort_values(ascending = False)\n\n#Inferences: Bedrooms/room is low, implies cost is high. Perhaps rooms like study, gym etc only come with fancies houses. \n#Higher the rooms/household, higher the value of the house. Makes sense. \n#Population_per_household; does not seem to have any strong correlation to the median_house_value. \nmedian_house_value          1.000000\nmedian_income               0.687160\nrooms_per_house             0.146285\ntotal_rooms                 0.135097\nhousing_median_age          0.114110\nhouseholds                  0.064506\ntotal_bedrooms              0.047689\npopulation_per_household   -0.021985\npopulation                 -0.026920\nlongitude                  -0.047432\nlatitude                   -0.142724\nbedrooms_per_room          -0.259984\nName: median_house_value, dtype: float64\n\n#Data cleaning.. \n\n#seperate data from the labels: \nhousing = train_set.drop(""median_house_value"", axis = 1)\nhousing_labels = train_set[""median_house_value""]\n#Note that ""total_bedrooms"" has a few values missing. Replace that with the median.. Imputing with median\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy = ""median"")\n\nhousing_temp = housing.drop(""ocean_proximity"", axis = 1)\nimputer.fit(housing_temp)\n\n#housing_temp.info()\nX = imputer.transform(housing_temp)\nhousing_num = pd.DataFrame(X, columns = housing_temp.columns )\nhousing_num.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 16512 entries, 0 to 16511\nData columns (total 8 columns):\nlongitude             16512 non-null float64\nlatitude              16512 non-null float64\nhousing_median_age    16512 non-null float64\ntotal_rooms           16512 non-null float64\ntotal_bedrooms        16512 non-null float64\npopulation            16512 non-null float64\nhouseholds            16512 non-null float64\nmedian_income         16512 non-null float64\ndtypes: float64(8)\nmemory usage: 1.0 MB\n\n#Handling categorical variable = ocean_proximity.. \nfrom sklearn.preprocessing import LabelEncoder\n\nencode = LabelEncoder()\nhousing_cat = housing[""ocean_proximity""]\n\nhousing_cat_encoded = encode.fit_transform(housing_cat)\n#Changing encoded categorical variables into one-hot boolean attributes. \nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder()\n\nhousing_cat_encoded_onehot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\nhousing_cat_encoded_onehot.toarray()\narray([[1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1.],\n       ...,\n       [0., 1., 0., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0.]])\n\nencoder.categories_\n[array([0, 1, 2, 3, 4])]\n\n#Writing a class method to add 2/3 additional attributes; rooms/household, population/household, bedrooms/room (hyper-parameter)\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n#Indexes of required attributes\nrooms_id, bedrooms_id, population_id, households_id = 3, 4, 5, 6\n\nclass AttributeAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedroom_per_room = True):\n        self.add_bedroom_per_room = add_bedroom_per_room\n    def fit(self, X, y = None):\n        return self\n    def transform(self, X, y = None):\n        rooms_per_household = X[:,rooms_id]/X[:,households_id]\n        population_per_household =  X[:,population_id]/X[:,households_id]\n        if self.add_bedroom_per_room:\n            bedroom_per_room =  X[:,bedrooms_id]/X[:,rooms_id]\n            return np.c_[X,rooms_per_household, population_per_household, bedroom_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\nadder = AttributeAdder(add_bedroom_per_room = True)\nhousing_added = adder.transform(housing.values)\n#Write a class method to convert pd DataFrame to numpy for data cleaning pipeline: \n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y = None):\n        return self\n    def transform(self, X, y = None):\n        return X[self.attribute_names].values\n\ndftoarr = DataFrameSelector(list(housing_num))\nnpdata = dftoarr.transform(housing_num)\n\nclass LabelBinarizerPipelineFriendly(LabelBinarizer):\n    def fit(self, X, y=None):\n        """"""this would allow us to fit the model based on the X input.""""""\n        super(LabelBinarizerPipelineFriendly, self).fit(X)\n    def transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).transform(X)\n\n    def fit_transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).fit(X).transform(X)\nnum_attributes = list(housing_num)\ncat_attributes = ""ocean_proximity""\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, LabelBinarizer\n\n\nnum_pipeline = Pipeline([\n                            (\'selector\', DataFrameSelector(num_attributes)),\n                            (\'imputer\', SimpleImputer(strategy = \'median\')),\n                            (\'attrAdder\', AttributeAdder()), \n                            (\'sclarer\', StandardScaler())\n])\ncat_pipeline = Pipeline([\n                            (\'selector\', DataFrameSelector(cat_attributes)),\n                            (\'labelBinarize\', LabelBinarizerPipelineFriendly())\n])\n\nfull_pipeline = FeatureUnion(transformer_list = [\n                                (""num_pipeline"", num_pipeline), \n                                (""cat_pipeline"", cat_pipeline)\n])\nhousing_prepared = full_pipeline.fit_transform(housing)\nlen(housing_prepared[1,:])\n16\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n\nsome_data= housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\n\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(""Predictions: "", lin_reg.predict(some_data_prepared))\nPredictions:  [210644.60459286 317768.80697211 210956.43331178  59218.98886849\n 189747.55849879]\n\nprint(""Labels: "", list(some_labels))\nLabels:  [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\n\nfrom sklearn.metrics import mean_squared_error as mse\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n\ncalc_mse = mse(lin_reg.predict(housing_prepared), housing_labels)\ncalc_rmse = np.sqrt(calc_mse)\nprint(calc_rmse)\n#Clearly underfitting.. \n68628.19819848923\n\n#Cross validation: \n\nfrom sklearn.model_selection import cross_val_score\n\ndef summarizescores(scores):\n    print(""Scores: "", scores)\n    print(""Mean Score: "", scores.mean())\n    print(""Standard Deviation of Scores: "", scores.std())\n\nlinscores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring = ""neg_mean_squared_error"", cv = 10)\nlinreg_rmse_scores= np.sqrt(-linscores)\n\nsummarizescores(linreg_rmse_scores)\nScores:  [66782.73843989 66960.118071   70347.95244419 74739.57052552\n 68031.13388938 71193.84183426 64969.63056405 68281.61137997\n 71552.91566558 67665.10082067]\nMean Score:  69052.46136345083\nStandard Deviation of Scores:  2731.674001798344\n\n#Trying a RF Regressor: \n\nfrom sklearn.ensemble import RandomForestRegressor as RFR\n\nforest_reg = RFR()\nforest_reg.fit(housing_prepared, housing_labels)\n\nrfmsescores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring = ""neg_mean_squared_error"", cv = 10)\nrfrmse_scores = np.sqrt(-rfmsescores)\nsummarizescores(rfrmse_scores)\n\n#better than lin_reg model.. viz. with 10 fold cross validation. \nScores:  [49753.03992868 47771.64986198 49901.36245274 51843.79499726\n 49951.23140349 53531.98253441 49150.54788608 48163.56269269\n 53008.15010946 50237.26796529]\nMean Score:  50331.25898320824\nStandard Deviation of Scores:  1817.0066284370694\n\n#Fine tuning: Hyper parameter grid search. \n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [{\'n_estimators\': [3,10,30], \'max_features\': [2,4,6,8]},\n             {\'bootstrap\': [False], \'n_estimators\': [3,10], \'max_features\': [2,3,4]}]\n\nforest_reg = RFR()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv = 5, scoring = \'neg_mean_squared_error\')\ngrid_search.fit(housing_prepared, housing_labels)\n\ngrid_search.best_params_\n{\'max_features\': 8, \'n_estimators\': 30}\n\n#best parameters obtained were at features = 8, n_estimators = 30. Increasing limits.. \n#best parameters updated to features = 6, estimators = 40\nparam_grid = [{\'n_estimators\': [10,30, 35, 40], \'max_features\': [4,6,8,10,12]},\n             {\'bootstrap\': [False], \'n_estimators\': [3,10], \'max_features\': [2,3,4]}]\n\nforest_reg = RFR()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv = 5, scoring = \'neg_mean_squared_error\')\ngrid_search.fit(housing_prepared, housing_labels)\n\ngrid_search.best_params_\n{\'max_features\': 6, \'n_estimators\': 40}\n\n#Printing out the mean scores.. best model has mean score = 49721, better than 50331 obtained before. \ncvres = grid_search.cv_results_\n\nfor mean_score, params in zip(cvres[""mean_test_score""], cvres[""params""]):\n    print(np.sqrt(-mean_score), params)\n52493.62744299238 {\'max_features\': 4, \'n_estimators\': 10}\n50435.29450663813 {\'max_features\': 4, \'n_estimators\': 30}\n50248.831475287254 {\'max_features\': 4, \'n_estimators\': 35}\n50435.440454048134 {\'max_features\': 4, \'n_estimators\': 40}\n52150.519655840624 {\'max_features\': 6, \'n_estimators\': 10}\n49854.64518139054 {\'max_features\': 6, \'n_estimators\': 30}\n49952.369952389185 {\'max_features\': 6, \'n_estimators\': 35}\n49721.01551420078 {\'max_features\': 6, \'n_estimators\': 40}\n52152.922965233985 {\'max_features\': 8, \'n_estimators\': 10}\n50270.20364669988 {\'max_features\': 8, \'n_estimators\': 30}\n49893.59482062571 {\'max_features\': 8, \'n_estimators\': 35}\n49726.04561292837 {\'max_features\': 8, \'n_estimators\': 40}\n52539.90844326223 {\'max_features\': 10, \'n_estimators\': 10}\n50250.215141372304 {\'max_features\': 10, \'n_estimators\': 30}\n49737.00402873527 {\'max_features\': 10, \'n_estimators\': 35}\n50026.8377811246 {\'max_features\': 10, \'n_estimators\': 40}\n52515.54233436503 {\'max_features\': 12, \'n_estimators\': 10}\n50462.99592949592 {\'max_features\': 12, \'n_estimators\': 30}\n50438.162873868976 {\'max_features\': 12, \'n_estimators\': 35}\n50378.732740776766 {\'max_features\': 12, \'n_estimators\': 40}\n62613.79182173217 {\'bootstrap\': False, \'max_features\': 2, \'n_estimators\': 3}\n54911.610262831455 {\'bootstrap\': False, \'max_features\': 2, \'n_estimators\': 10}\n59870.98277063176 {\'bootstrap\': False, \'max_features\': 3, \'n_estimators\': 3}\n52462.16728623582 {\'bootstrap\': False, \'max_features\': 3, \'n_estimators\': 10}\n58650.78982507723 {\'bootstrap\': False, \'max_features\': 4, \'n_estimators\': 3}\n51470.073738779494 {\'bootstrap\': False, \'max_features\': 4, \'n_estimators\': 10}\n\n#Get information on importance of each features...\n#Inferences: Important cat variables are only INLAND. Other have low importance. \n#Can consider removing lesser important attr. \n\nfeature_importance = grid_search.best_estimator_.feature_importances_\nfeature_importance\n\nextra_attr = [""rooms_per_household"", ""population_per_household"", ""bedrooms_per_room""]\ncat_one_hot = [ ""<1H OCEAN"",""INLAND"",""ISLAND"", ""NEAR BAY"" , ""NEAR OCEAN""]\n\nall_attr = num_attributes + extra_attr + cat_one_hot\nsorted(zip(feature_importance, all_attr), reverse = True)\n[(0.3666834375641405, \'median_income\'),\n (0.13268782374506766, \'INLAND\'),\n (0.11137178540545738, \'population_per_household\'),\n (0.07166944962330979, \'longitude\'),\n (0.06906677397903115, \'latitude\'),\n (0.06486978415466575, \'bedrooms_per_room\'),\n (0.05486987165971509, \'rooms_per_household\'),\n (0.043308281845873584, \'housing_median_age\'),\n (0.017884119538956057, \'total_rooms\'),\n (0.017199535397174375, \'population\'),\n (0.01698476534676911, \'total_bedrooms\'),\n (0.01654223058030064, \'households\'),\n (0.009664297518918254, \'<1H OCEAN\'),\n (0.004199158191213003, \'NEAR OCEAN\'),\n (0.002936556889268899, \'NEAR BAY\'),\n (6.212856013875081e-05, \'ISLAND\')]\n\n#Evaluating on Test set.. \n\nfinal_model = grid_search.best_estimator_\n\nX_test = test_set.drop(""median_house_value"", axis = 1)\nY_test = test_set[""median_house_value""].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\n\nfinal_predict = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(Y_test, final_predict)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse\n#Final rmse close to rmse from cross validation.. Model seems fine! \n47592.032925344836\n\n'], 'url_profile': 'https://github.com/PadmanabhanAshwin', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Nov 11, 2020', 'Apache-2.0 license', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'Gibraltar, Gibraltar', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['Wine quality prediction\nThis project is a part of my Small Projects series with the objective of taking a practical approach into machine learning.\nProject Status: On-Hold\nProject Intro\nThe purpose of this project is to predict wine quality based on different set of features. Using for this objective UCI ML White wine dataset\nMethods used\n\nData Visualization\nLinear Regression\nLasso Regression\nRandom Forest\nGrid Search\nCross validation\n\nTechnologies\n\nPandas\nJupyter Notebook\nScikit learn\nScipy\n\nNotebooks\n\nExploratory Data Analysis on UCI wine dataset (under development)\nRegression for Wine quality prediction\nStacking models and gradient boosting for wine quality prediction (backlog)\n\n'], 'url_profile': 'https://github.com/juanlurg', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Nov 11, 2020', 'Apache-2.0 license', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'Dublin, Ireland', 'stats_list': [], 'contributions': '152 contributions\n        in the last year', 'description': ['EVALUATION OF MACHINE LEARNING ALGORITHMS FOR REGRESSION AND CLASSIFICATION PROBLEMS\nBy using HOUSING PRICE DATA, BANK LOAN DATA and CRIME DATA, The following machine learning alrgorithms were applied and evaluated.\nNOTE: Data mining techiniques were applied to the datasets before carrying out the model building and evaluation.\n\nRandom Forest: Used for classification and regression problems.\nSupport Vector Machines: Used for classification, regression, density estimation etc.\nDecision Trees: Used for classification and regression problems.\nMultiple Linear Regression: Used for regression problems.\nNaïve Bayes Classifier: Used for classification and regression problems.\nLogistic Regression: Used for classification problems.\n\n'], 'url_profile': 'https://github.com/Tobi-Ek', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Nov 11, 2020', 'Apache-2.0 license', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['explore-supervised-learning\nA collection of documents and programs to learn about supervised learning\n'], 'url_profile': 'https://github.com/my-thought-experiments', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Nov 11, 2020', 'Apache-2.0 license', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'Sobral-CE', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['computational-intelligence-studies\nOs algoritmos foram implementados na disciplina de Inteligência Computacional (2018.1)\n'], 'url_profile': 'https://github.com/hugosousa111', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Nov 11, 2020', 'Apache-2.0 license', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Maddy4115', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Nov 11, 2020', 'Apache-2.0 license', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': [""Context\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\nContent\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\nInspiration\nIdentify fraudulent credit card transactions.\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\nAcknowledgements\nhttps://www.kaggle.com/mlg-ulb/creditcardfraud\n""], 'url_profile': 'https://github.com/pradeepmuni', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'R', 'Updated Nov 11, 2020', 'Apache-2.0 license', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020']}"
"{'location': 'Boston', 'stats_list': [], 'contributions': '144 contributions\n        in the last year', 'description': ['The steps in this notebook are:\n\nBuild a Random Forest model with all of your data (X and y)\nRead in the ""test"" data, which doesn\'t include values for the target.  Predict home values in the test data with your Random Forest model.\nSubmit those predictions to the competition and see your score.\nOptionally, come back to see if you can improve your model by adding features or changing your model. Then you can resubmit to see how that stacks up on the competition leaderboard.\n\n*test.csv and train.csv are the data type that are used in the program for training and testing the ML algorithms\n*Change the path of the file to this directory when reading the dataset!!\ncreated by-\nAbhishek Dabas\n'], 'url_profile': 'https://github.com/abhishekdabas31', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'MIT license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SumanthReddy9', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'MIT license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': [""Predicting-House-Price-in-IOWA-using-Advanced-Regression-and-Classification\nKaggle Competition\nCompetition Description:\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this  competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n""], 'url_profile': 'https://github.com/codinggeniusonline', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'MIT license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'MIT license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Santa Barbara, California', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['sgd\nImplementing a iterative stochastic gradient descent linear regressor with regularization\nUsed Kaggle data set “Graduate Admission 2” https://www.kaggle.com/mohansacharya/graduate-admissions\nThe dataset contains several parameters which are considered important during the application for Master’s programs in India.\nThe model predicts the admission chances for a given set of paramaters.\n'], 'url_profile': 'https://github.com/atahiraj', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'MIT license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'MIT license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'MIT license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'MIT license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'MIT license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Python', 'MIT license', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020']}","{'location': 'Montreal, Quebec, Canada', 'stats_list': [], 'contributions': '287 contributions\n        in the last year', 'description': ['MiniProject 1: COMP 551 (001/002), Applied Machine Learning, Winter 2020, McGill University: Logistic Regression and Naive Bayes\nContents\n\nPreamble\nBackground\nTask 1 Acquire preprocess and analyze the data\nTask 2 Implement the models\nTask 3 Run experiments\nDeliverables\nEvaluation\nFinal remarks\nHow to run the Python program\n\nAssignment description (PDF) (COMP551 P1 Winter 2020)\nFinal report (PDF) (COMP551 P1 Winter 2020)\nPreamble\n\nThis mini-project is due on February 11th at 11:59pm. Late work will be automatically subject to a 20% penalty, and can be submitted up to 5 days after the deadline. No submissions will accepted after this 5 day period.\nThis mini-project is to be completed in groups of three. There are three tasks outlined below which offer one possible division of labour, but how you split the work is up to you. All members of a group will receive the same grade. It is not expected that all team members will contribute equally to all components. However every team member should make integral contributions to the project.\nYou will submit your assignment on MyCourses as a group. You must register your group on MyCourses and any group member can submit. See MyCourses for details.\nYou are free to use libraries with general utilities, such as matplotlib, numpy and scipy for Python. However, you should implement everything (e.g., the models and evaluation functions) yourself, which means you should not use pre-existing implementations of the algorithms or functions as found in SciKit learn, etc.\n\nBackground\nIn this miniproject we implement two classification techniques (logistic regression and naive Bayes) and compare these two algorithms on four distinct datasets.\nGo back to Contents.\nTask 1 Acquire preprocess and analyze the data\nThe first task is to acquire the data, analyze it, and clean it (if necessary). We will use two fixed and two open datasets in this project, outlined below.\n\n\nDataset 1 (Ionosphere): this is a dataset where the goal is to predict whether a radar return from ionosphere is \'good\' or \'bad\'. This radar data was collected by a system in Goose Bay, Labrador. Get it from: https://archive.ics.uci.edu/ml/datasets/ionosphere\n\n\nDataset 2 (Adult Data Set): also known as ""Census Income"" dataset, this is a dataset where the goal is to predict the whether income exceeds $50K/yr based on census data. Get it from: https://archive.ics.uci.edu/ml/datasets/Adult\n\n\nDataset 3 and 4: We choose the following additional datasets for classification from the UCI datasets at: https://archive.ics.uci.edu/ml/datasets.php\n\nDataset 3 (Wine Quality):\n\nLink: https://archive.ics.uci.edu/ml/datasets/Wine+Quality\nThis is a dataset where the goal is to predict the quality of wine based on its chemical properties.\nNote: We will only be using the red wine subset of the data.\nNote: This data contains quality ratings from 0-10. We will convert this task to a binary classification by defining the ratings of {6,7,8,9,10} as positive (i.e., 1) and all other ratings as negative (i.e., 0).\n\n\nDataset 4 (Breast Cancer Diagnosis):\n\nLink: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\nThis is a dataset where the goal is to predict whether a tumour is malignant or benign based on various properties.\nNote: We will use the data folder titles breast-cancer-wisconsin.data for this project.\n\n\n\n\n\nThe essential subtasks for this part of the project are:\n\nDownload the datasets (noting the correct subsets to use, as discussed above).\nLoad the datasets into NumPy objects (i.e., arrays or matrices) in Python.\nClean the data. Are there any missing or malformed features? Are there are other data oddities that need to be dealt with? You should remove any examples with missing or malformed features and note this in your report. For categorical variables you can use one-hot encoding.\nCompute basic statistics on the data to understand it better. E.g., what are the distributions of the positive vs. negative classes, what are the distributions of some of the numerical features? what are the correlations between the features? how does the scatter plots of pair-wise features look-like for some subset of features?\n\nGo back to Contents.\nTask 2 Implement the models\nImplement these models as you see fit, but you should follow the equations that are presented in the lecture slides, and you must implement the models from scratch (i.e., you cannot use SciKit Learn or any other pre-existing implementations of these methods).\nIn particular, your two main tasks in the part are to:\n\nImplement logistic regression, and use (full batch) gradient descent for optimization.\nImplement naive Bayes, using the appropriate type of likelihood for features.\n\nYou are free to implement these models in any way you want, but you must use Python and you must implement the models from scratch (i.e., you cannot use SciKit Learn or similar libraries). Using the numpy package, however, is allowed and encouraged. Regarding the implementation, we recommend the following approach (but again, you are free to do what you want):\n\n\nImplement both models as Python classes. You should use the constructor for the class to initialize the model parameters as attributes, as well as to define other important properties of the model.\n\n\nEach of your models classes should have (at least) two functions:\n\nDefine a fit function, which takes the training data (i.e., X and y) - as well as other hyperparameters (e.g., the learning rate and/or number of gradient descent iterations) - as input. This function should train your model by modifying the model parameters.\nDefine a predict function, which takes a set of input points (i.e., X) as input and outputs predictions (i.e., ^y) for these points. Note that you need to convert probabilities to binary 0-1 predictions by thresholding the output at 0.5!\n\n\n\nIn addition to the model classes, you should also define a functions evaluate_acc to evaluate the model accuracy. This function should take the true labels (i.e., y), and target labels (i.e., ^y) as input, and it should output the accuracy score.\n\n\nLastly, you should implement a script to run k-fold cross validation.\n\n\nGo back to Contents.\nTask 3 Run experiments\nThe goal of this project is to have you explore linear classification and compare different features and models. Use 5-fold cross validation to estimate performance in all of the experiments. Evaluate the performance using accuracy. You are welcome to perform any experiments and analyses you see fit (e.g., to compare different features), but at a minimum you must complete the following experiments in the order stated below:\n\nCompare the accuracy of naive Bayes and logistic regression on the four datasets.\nTest different learning rates for gradient descent applied to logistic regression. Use a threshold for change in the value of the cost function as termination criteria, and plot the accuracy on train/validation set as a function of iterations of gradient descent.\nCompare the accuracy of the two models as a function of the size of dataset (by controlling the training size).\nAs an example, see Figure 1 in: Ng AY, Jordan MI. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In Advances in neural information processing systems 2002 (pp. 841-848).\n\nNote: The above experiments are the minimum requirements that you must complete; however, this project is open-ended. For example, you might investigate different stopping criteria for the gradient descent in logistic regression, develop an automated approach to select a good subset of features. You do not need to do all of these things, but you should demonstrate creativity, rigour, and an understanding of the course material in how you run your chosen experiments and how you report on them in your write-up.\nGo back to Contents.\nDeliverables\nYou must submit two separate files to MyCourses (using the exact filenames and file types outlined below):\n\ncode.zip: Your data processing, classification and evaluation code (as some combination of .py and .ipynb files).\nwriteup.pdf: Your (max 5-page) project write-up as a pdf (details below).\n\nProject write-up\nYour team must submit a project write-up that is a maximum of five pages (single-spaced, 11pt font or larger; minimum 0.5 inch margins, an extra page for references/bibliographical content can be used). We highly recommend that students use LaTeX to complete their write-ups. This first mini-project report has relatively strict requirements, but as the course progresses your project write-ups will become more and more open-ended. You have some flexibility in how you report your results, but you must adhere to the following structure and minimum requirements:\nAbstract (100-250 words) Summarize the project task and your most important findings. For example, include sentences like ""In this project we investigated the performance of linear classification models on two benchmark datasets"", ""We found that the logistic regression approach was achieved worse/better accuracy than naive Bayes and was significantly faster/slower to train.""\nIntroduction (5+ sentences) Summarize the project task, the two datasest, and your most important findings. This should be similar to the abstract but more detailed. You should include background information and citations to relevant work (e.g., other papers analyzing these datasets).\nDatasets (5+ sentences) Very briefly describe the and how you processed them. Describe the new features you come up with in detail. Present the exploratory analysis you have done to understand the data, e.g. class distribution.\nResults (7+ sentences, possibly with figures or tables) Describe the results of all the experiments mentioned in Task 3 (at a minimum) as well as any other interesting results you find. At a minimum you must report:\n\nA discussion of how the logistic regression performance (e.g., convergence speed) depends on the learning rate. (Note: a figure would be an ideal way to report these results).\nA comparison of the accuracy of naive Bayes and logistic regression on both datasets.\nResults demonstrating that the feature subset and/or new features you used improved performance.\n\nDiscussion and Conclusion (5+ sentences) Summarize the key takeaways from the project and possibly directions for future investigation.\nStatement of Contributions (1-3 sentences) State the breakdown of the workload across the team members.\nEvaluation\nThe mini-project is out of 100 points, and the evaluation breakdown is as follows:\n\nCompleteness (20 points)\n– Did you submit all the materials?\n– Did you run all the required experiments?\n– Did you follow the guidelines for the project write-up?\nCorrectness (40 points)\n– Are your models implemented correctly?\n– Are your reported accuracies close to the reference solutions?\n– Do your proposed features actually improve performance, or do you adequately demonstrate that it was not possible to improve performance?\n– Do you observe the correct trends in the experiments (e.g., comparing learning rates)?\nWriting quality (25 points)\n– Is your report clear and free of grammatical errors and typos?\n– Did you go beyond the bare minimum requirements for the write-up (e.g., by including a discussion of related work in the introduction)?\n– Do you effectively present numerical results (e.g., via tables or figures)?\nOriginality / creativity (15 points)\n– Did you go beyond the bare minimum requirements for the experiments? For example, you could investigate different stopping criteria for logistic regression, investigate which features are the most useful (e.g., using correlation metrics), or propose an automated approach to select a good subset of features.\n– Note: Simply adding in a random new experiment will not guarantee a high grade on this section! You should be thoughtful and organized in your report.\n\nFinal remarks\n\nYou are expected to display initiative, creativity, scientific rigour, critical thinking, and good communication skills. You don\'t need to restrict yourself to the requirements listed above - feel free to go beyond, and explore further. You can discuss methods and technical issues with members of other teams, but you cannot share any code or data with other teams.\n\nHow to run the Python program?\n\n\nInstall virtualenv\n\nTo activate the virtualenv on Linux or MacOS: source venv/bin/activate\nTo activate the virtualenv on Windows: \\venv\\Script\\activate.bat\n\n\n\nRun the program\n\n\ncd <folder_name>/\n\nvirtualenv venv -p python3\n\nsource venv/bin/activate\n\npip install -r requirements.txt\n\npython <name_of_python_program>.py\nNote: To desactivate the virtual environment\ndeactivate\nGo to Contents\n'], 'url_profile': 'https://github.com/ramonfigueiredopessoa', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 21, 2020', '1', 'Python', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'R', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'R', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'New York, New York', 'stats_list': [], 'contributions': '161 contributions\n        in the last year', 'description': [""AirBNB-NYC-Cluster-Analysis-and-Multivariate-Regression\nMultivariate Regression Private  Updated 23 hours ago Analyzed a dataset that details AirBNB's total listings within the five New York City boroughs in 2019.\nOngoing project that analyzed AirBNB's New York City rental information from the year 2019. Created a cluster analysis and multivariate regression analysis\nthat prospects the price of a listing in relation to the neighborhood and the number of reviews. Skills utilized in this project\nrequired feature scaling and engineering, implementation of machine learning models, data tidying, and subsequent visualizations.\nThe dataset was kindly borrowed from Kaggle @ https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data\n\n""], 'url_profile': 'https://github.com/artwang31', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'R', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'R', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'R', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '483 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/karanaryan07', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'R', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'R', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'R', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'R', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jan 31, 2020', 'R', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 14, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MohammedRaheemP', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Diamond-Price-Prediction-Linear-Regression-Stats-Model-Assumptions-Check\nIt is a Supervised Machine Learning Project(Diamond Price Prediction).\nIn this project,we will build a model to predict the price of Diamonds,\nwith the help of given features like- carat, cut, color, width, depth, etc.\nWe have used Linear Stats Model for Price prediction and Have also checked the Assumptions of Linear Regression.\nFirst we will start with Basic Exploratory Data Analysis, then cleaning the data, scaling it, and then proceeding with model building.\nThen, model prediction and checking the accuracy. Finally, proceeding with the Linear Assumptions Check.\n'], 'url_profile': 'https://github.com/Isha-Pandey27', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 8, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 24, 2020', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Diamond-Price-Prediction-Linear-Regression-Stats-Model-Assumptions-Check\nIt is a Supervised Machine Learning Project(Diamond Price Prediction).\nIn this project,we will build a model to predict the price of Diamonds,\nwith the help of given features like- carat, cut, color, width, depth, etc.\nWe have used Linear Stats Model for Price prediction and Have also checked the Assumptions of Linear Regression.\nFirst we will start with Basic Exploratory Data Analysis, then cleaning the data, scaling it, and then proceeding with model building.\nThen, model prediction and checking the accuracy. Finally, proceeding with the Linear Assumptions Check.\n'], 'url_profile': 'https://github.com/Isha-Pandey27', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 8, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 24, 2020', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 8, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 24, 2020', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 8, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 24, 2020', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 8, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 24, 2020', 'Updated Jan 26, 2020']}","{'location': 'Atlanta, GA', 'stats_list': [], 'contributions': '222 contributions\n        in the last year', 'description': [""GradBoost\nThis project was meant to be educational. XGBoost and scikit-learn's gradient boosting algorithms are strongly recommended over GradBoost. Ijust did this as an excercise.\nGradBoost takes in a regression model with methods .fit(X, y) and .predict(X), a set of testing X data to be predicted, training X and y data to fit on, and returns a tuple of the predicted y-values for the training data and testing data. The boosting rounds and learning rate are adjustable. The flexibility of model choice allows custom loss functions for the weak learners.\nGradBoost\nPackage to perform gradient boosting for regression using sklearn style regression models.\nMore details on the gradient boosting are here: https://en.wikipedia.org/wiki/Gradient_boosting\nExample\nThe example folder in this repository applies this function to simulated data using sklearn's DecisionTreeRegressor and RidgeCV models.\n""], 'url_profile': 'https://github.com/jkclem', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 8, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 24, 2020', 'Updated Jan 26, 2020']}","{'location': 'Dhanbad,Jharkhand', 'stats_list': [], 'contributions': '364 contributions\n        in the last year', 'description': ['Support Vector Machine Example - Machine Learning\nThis is code to practice concepts of Support Vector Machines\nOverview\nThis code is implementing a Support Vector Machine with Hinge loss and gradient descent to find the optimal decision boundary between two classes of data.\nThe svm.py file is extensively documented with information about Support Vector Machines and the math that goes into them.\nThis implementation is using a pre-defined, small sample of meaningless data. It was used to learn the concepts of SVMs.\nDependencies\n\nPython\nnumpy\nmatplotlib\n\nUsage\nJust run python3 svm.py to see the results:\nPlot of the Rate of classification errors during training\n\nPlot of the Hyperplane generated by the SVM\n\nCredits\nCredits for this project go to this video on Youtube by Siraj Raval that explains Support Vector Machines.\n'], 'url_profile': 'https://github.com/krishnapalS', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 8, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 24, 2020', 'Updated Jan 26, 2020']}","{'location': 'Denmark', 'stats_list': [], 'contributions': '302 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aplorenzen', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 8, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 24, 2020', 'Updated Jan 26, 2020']}","{'location': 'Lagos, Nigeria.', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Employee-Retention-Analysis\nUsing a simple logistic regression and secondary dataset gotten from kaggle, I created a model that predicts employee retention in an organization.\n'], 'url_profile': 'https://github.com/abu-yuwa', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 8, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 24, 2020', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '130 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/guozhaosengzs', 'info_list': ['Jupyter Notebook', 'Updated Jan 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jul 13, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Aug 8, 2020', 'Python', 'MIT license', 'Updated Jan 26, 2020', 'Updated Jan 23, 2020', '1', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 24, 2020', 'Updated Jan 26, 2020']}"
"{'location': 'Hyderabad', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['salary Prediction\nProblem : Predict whether income exceeds $50K/yr based on census data. Also known as ""Census Income"" dataset\nData Set Information:\nExtraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\nPrediction task is to determine whether a person makes over 50K a year.\nalgorithms used:\nused logistic regression, Random Forest, Naive Bayes and choosed best one out of them\nModel Evaluation:\nFor evaluating model used cross val score\nmodel saving\nfinally saved the model as a okl object.\n'], 'url_profile': 'https://github.com/swethakothapalli', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Oct 9, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['crop-prediction-and-geo-fencing\nhere you get the code for load google map on your browser and we will also add linear regression algo. for crop prediction.\n#which file contain what data\nThe main.py file contain the UI but the actual code of ""Prediction"" button is in test.py\nso if prediction button not working then run the test.py\n'], 'url_profile': 'https://github.com/mayurboss', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Oct 9, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Dubai, UAE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['AllState-Claims-Severity-\nAllstate is currently developing automated methods of predicting the cost, and hence severity, of claims by using different regression models and with better Algorithm\n'], 'url_profile': 'https://github.com/nairshobhit94', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Oct 9, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Healthcare-prediction\nClassifier used: Linear Classifiers : Logistic Regression  For\xa0running\xa0the\xa0given\xa0script: Python\xa0, Numpy, Pandas\xa0for\xa0data\xa0i/o ,Scikit-learn\xa0Provides\xa0all\xa0the\xa0classifiers\n'], 'url_profile': 'https://github.com/mehakgupta092', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Oct 9, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['DeNoiseFromDualReg\nThese are the tools for performing denoising of fMRI data using FSL MELODIC, FSL Dual Regression (shell script) and MATLAB scripts & functions.\nThese tools have been used in these publications\nCyran, C.A.M., Boegle, R., Stephan, T. et al. Brain Struct Funct (2016) 221: 1443. https://doi.org/10.1007/s00429-014-0983-6\nAge-related decline in functional connectivity of the vestibular cortical network\nBrain Structure and Function April 2016, Volume 221, Issue 3, pp 1443–1463\nhttps://link.springer.com/article/10.1007%2Fs00429-014-0983-6\nV. Kirsch, R. Boegle, D. Keeser, E. Kierig, B. Ertl-Wagner, T. Brandt, M. Dieterich,\nHandedness-dependent functional organizational patterns within the bilateral vestibular cortical network revealed by fMRI connectivity based parcellation,\nNeuroImage, Volume 178, 2018, Pages 224-237, ISSN 1053-8119,\nhttps://doi.org/10.1016/j.neuroimage.2018.05.018.\nhttps://www.sciencedirect.com/science/article/pii/S1053811918304166?via%3Dihub\n'], 'url_profile': 'https://github.com/RainerBoegle', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Oct 9, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['lin-reg\nThis is a linear regression tool that can be used to evaluate the theoretical ""IQ"" of an AI for specific application data sets.\n'], 'url_profile': 'https://github.com/goelbenj', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Oct 9, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Health-Care-Prediction\nClassifier used: Linear Classifiers : Logistic Regression  For\xa0running\xa0the\xa0given\xa0script: Python\xa03 Numpy Pandas\xa0for\xa0data\xa0i/o Scikit-learn\xa0Provides\xa0all\xa0the\xa0classifiers\n'], 'url_profile': 'https://github.com/mehakgupta092', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Oct 9, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Survival_Prediction_On_Titanic_Dataset\n1.) This work is a part of a class assignment where I performed the logistic regression on Titanic dataset to predict survival utilizing Bayesian Statistical modelling techniques.\n2.) R statistical software has been used to do analysis. Coding is done using R markdown and LaTeX to generate reports in PDF as wells HTML.\n3.) ggplot was extensively used to visualize the data and generate plots like histogram, density plots, bar graphs and boxplots.\n4.) FInally, Logistic regression has been performed to predict the chance of survival based on sex(Male or Female) for three different categories of tickets (1st, 2nd and 3rd class).\nReference book: Kruschke, J. 2015. Doing Bayesian Data Analysis. A free online version of the book can be obtained from: http://www.sciencedirect.com/science/book/9780124058880.\n'], 'url_profile': 'https://github.com/abhi812', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Oct 9, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Seattle, WA, USA', 'stats_list': [], 'contributions': '322 contributions\n        in the last year', 'description': ['FeatureFinder\nData exploration and pre-processing:\nDataExplorationAndPreProcessing.ipynb: Load the employee turnover rate, employee’s answers to the questions, tenure of key positions, and median household income for the location (from US Census). Transform the data to make it normally distributed, standardize the data, save the data.\nAddingEmployeeSalary.ipynb: Load the salary for each employee and calculate the median salary for each location. Standardize the data and save it.\nAddingResponseStandardDeviation.ipynb: Load the individual employee’s answers to the questions and calculate how varied the responses are at each location. Standardize and save it.\nRegression Analysis:\nRun_LassoRegression_and_Average.ipynb: Load all the features saved by the notebooks above, and run Lasso regression (1000 times). Produce a model that predicts employee turnover rate using few selected features. Save the results for linear regression step.\nLinearRegressionWithInteractions.ipynb: Load the results of the lasso regression. Choose the top 3 features and run linear regression using these features and their interactions. A model confirms the validity of the features and shows that interactions are not significantly big. Save the results for the use in Streamlit app.\nInteractive Web App:\nFeaturePredictionForTurnover_Streamlit.py: A Python script for ""Streamlit"" Web app. The app will interactively show how much each senior care center has to improve on key features in order to meet their ""target"" emplyee turn over rate.\nFor running the Web App, please install “Streamlit” from (https://www.streamlit.io/), and run the file by typing: streamlit run FeaturePredictionForTurnover_Streamlit.py\n'], 'url_profile': 'https://github.com/MamiyaA', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Oct 9, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}","{'location': 'Knoxville, TN', 'stats_list': [], 'contributions': '128 contributions\n        in the last year', 'description': [""Stock Prediction for Small Biotech Companies\nStock price prediction is a difficult endeavor even using machine learning techniques. For long term stock prediction, careful analysis of company fundamentals provide much more consistent results. Although machine learning techniques can provide an effective tool for short term trading, giving an experienced investor important information. The techniques can produce potential entry and exit points for a particular stock. In conjunction with a well designed dashboard these predictions could help a well informed investor improve their trading performance.\nProject Overview\n\nPredicting stock price for early stage biotech for short term purposes.\n    \nOutputting Trading Projected Intervals for a Future Date.\nInclude Uncertain in the Prediction Output\n\nTime Series Model\n    \nBuilding a recurrent neural network (RNN) to predict daily intervals\n\nDashboard\n    \nMaking an interactive dashboard to show stock prediction\n\n\n\nBackground Information on Early Biotech Investing\nIn the project, I will be modeling stock price for early biotech stocks. These companies are in the research and approval process for a particular drug or drugs. They have similar characteristics like low market capitalization, low trading volume, and high sensitivity to events all of which lead to high volatility. Those events are included below.\n\nThe results of experimental trails (negative or positive)\nRegulator approvals (negative or positive)\nAcquisitions by large companies (generally positive)\nAccounting issues (generally negative)\n\nCommon company financials like revenues and profit margin are not as influential as they are in other industries. These stocks do not have many revenues and usually are losing money. Their price is trading on the potential approval and sale of the drug or drugs in the approval process. The most important thing to know when dealing with stock in the industry are below.\n\nScience Behind the Treatment and the Condition\nAddressable Market\nEvent Schedule\n\nScience\nThe investor does not need to be a subject matter expert in the science of a particular drug or the condition it is treating, but they do need to understand basic scientific principles. In particular, it is important to understand the basic biological concepts like how cancer cells develop for oncology drugs, and the understand of basic genetic processes like the Central Dogma for DNA and RNA for treatment using gene therapy. This general medical knowledge will help an investor understand the drug or treatment and key company events that effect the stock price.\n\nAddressable Market\nThe addressable market is the patient population that  would be helped by the company drug under development and the value that drug would provide to those patients. The value measure is key, because that can determine the cost that can charge for a course of treatment.\nAlso important is the concept of an Orphan Drug. An orphan drug is a medicine that treats a rare disease effecting less than 200,000 people. Normally these numbers would not constitute a large enough market to justify investment in drug research, but in 1983 congress pass the Orphan Drug Act giving drug financial incentive to pursue treatment for these conditions. In addition to funding for research, the law gave significant marketing and patent protection. Combine these protections with laws preventing health insurance companies from discriminating against pre-existing conditions allows firms to charge exorbitant prices for these drugs after approval.\n\nEvents\n\nThe results of experimental trails (negative or positive)\nRegulator approvals (negative or positive)\nAcquisitions by large companies (generally positive)\nAccounting issues (generally negative)\n\nDates associated with key events like approval and trail results are the most important information for an investor. Outside exits like an acquisition the firm or unforeseen financial and accounting issue, these key events have the most influence on the price of the stock. In  general there is a regular schedule for drug approvals and trail conclusions given a phase of the approval process the treatment is in.\n\nEven with a stock with good drug prospects have a 1 out of 10 chance of being a success. Most stocks will fail and lose all their value, but the 10% that succeed could increase 10, 50\xa0, 100, or even 1000 times. Below is are examples of three biotech stocks. The first two, T2 Biosystems Inc (TTOO) and Affymax, Inc (AFFY), did not succeed and lost all their value. The last, Celgene (CELG),  was successful increasing over 20 times it starting price.\n\n\n\nEven with information about the investment potential of biotech stocks, an investor could use help determining the best time to buy or sell a stock. Especially with the volatility of these stocks, there is great opportunity for small short term trading.  A 1% to 2% price change can provide a substantial trading profit with good buy and sell recommendations, and as can be seen below there is extensive price movement both up and down for these stocks. The below distributions represent the price change of one day from the Virtus LifeSci Biotech ETF (with the increases and decrease capped).\n\n\n\nModel Input\nThe stocks being looked at is from the Virtus LifeSci Biotech ETF. The ETF is focused on companies currently in clinical trials. Currently there are 96 companies in the ETF. The stock price information was obtained from Yahoo Finance API using the pandas datareader library get_data_yahoo() method. There are 148,088 data points, but that number will decrease with data processing. Below is the data pulled from the API. The price related features (High, Low, Open, Close, and Adj Close) were scaled the maximum price value of all of the features for a given stock. Volume was scaled by dividing by the maximum volume value. In addition to the price and volume data seen below a daily percent change of the opening price was added to the model input.\n\nModel Target\nThe target of the model is future stock pricing price. Originally I planned to the next day, 3 day, 10 day, and 30 day price, but I decided the just to model the next price for simplicity sake and because I questioned the usefulness of the longer dated predictions. So the model targets are the calculated percentage difference of the next day highs and lows versus the current day open price.\nI also capped the percentage increases and decreases of the target values. The values were very skewed with increases stretching out to close to 7000% and the decrease going down to  -100%. The capping was done based on the statistical distribution of the calculated next day's high and low. See below original next day lows and the distribution after the increase and decrease were capped.\n\n\nThe maximum increase and decrease was capped base on the 5th and 95th quantile percentages. Below are the range of values.\n\nLimiting next day lows to 12% decrease and 5% increase.\nLimiting next day highs to 5% decrease and 14% increase\n\nTime Series Model\nThere is exactly two models one for the Next Day Low and one for the Next Day High. These two models have the same input but use the separate target for training. There multiple architectures that I considered, see below.\n\nGated Recurrent Unit (GRU) Recurrent Nueral Network (RNN)\nThe GRU is the model that was choice. Although not a performing model, a regression output did produce exceptable R squared values. \n\nOne Dimensional Convolutional Neural Network (CNN)\nThe 1D CNN and the combination RNN and CNN did not converge to the more accurate models, so they were discarded.\n\nPretrained Image Processing CNN (ResNet)\nThe pretrained image classification model show some promise, but the model input would have to be converted to a image tensor to be process by the network\xa0. The processing was too computationally intensive to be a workable solution. Below is example of the image that need to be created.\n\n\nCombination GRU and CNN\nThe combination RNN and CNN did not converge to the more accurate models, so they were discarded.\n\nModel training entailed running 100 epoch training run for each of the 96 stock in the ETF. The last 100 trading days were used for validation. After this was completed, the model weights were saved as the base model for future training. Then an individual stock was trained using 6000 epoch long fit runs. Although mean squared error was used for the lost function, I saved the model weights for the final model by highest R squared value.\nDuring the modeling formation process, I chose between a regression model and a classification model where the percentage changes where separated into bins that where one percent point wide. I leaned toward a classification model because I wanted the model output to be a probability distribution displaying  the chance given a percent increase or decrease would occur. Ultimately the classification model did not performed well for either high or low models because the output tended to the extremes of the percentage range. The regression was also not particularly good only 0.643- 0.646 R Squared for the high and low, but there was improvement in the model using the loss criteria mean squared error. See loss function and model output below.\n\n\nModel Output\nIn order to produce the probability distribution, I decided to use the residuals of the predicted values to create a normal distribution. Each model input took the last 1000 days of trading and fed they into the model. The residuals of the model were found using the actual next day values and predicted values. Then the capped target values describe earlier were removed from the analysis, because they would add a higher level variability in model then what was accurate.\nUsing median and standard deviation of the remaining residuals, I created a 10,000 sample simulations of normal distributions centered on the predicted value of either high or low model using the Numpy random.normal() method. These distributions where then graphed using the Plotly method, create_distplot(), removing the histogram just using the kernel density (KDE) plot line. The day's opening and closing(or current) price were plotted as vertical lines. Also recommended buy and sell points were graphed as points on the high and low KDE lines.\n\n\nUsing the Results\nKnowledge of the stock in question is important. In particular because the R squared valued for modeled stock generally does not go over 0.65. A base of investor knowledge will act as a check on the short term predictions in the model.\nAlso avoiding the extremes of the model's range is important. Large percentage point moves are not the target of this model, because they are generally cause by events that cannot be predicted by the stock price inputs of the model. The model is trying to predict smaller changes less than 10% either negative or positive. Also with capping of losses and gains, big price movements are masked by the model. The horizontal axis of the graph is scaled to be at or near the capped values, so when the distributions are close to the edges of the graph\xa0, the trader should be cautious with buy and sell recommendations from the model.\n\nDashboard\nUsing the Dash App from Plotly\xa0, I constructed an interactive dashboard to display the model outputs. A representation of the dashboard can be seen below.\n\nThere are two prediction graphs. One for today's stock price based on the yesterday's  inputs, and one for tomorrow's pricing based on current pricing. In addition a time chart was added displaying the last 100 days of price movement using a candlestick chart. Plotted on top of the candlestick chart are the high and low prediction for each day. Last are the points were the model missed. Red markers indicate were the low prediction was higher then actual low, and green markers indicate were high prediction was too low compared to the actual high.\nConclusion and Future Work\nIn conclusion, stock modeling is very difficult even using machine learning techniques. My model's R squared values topped out around 0.65. That is not a particularly good model. But I used the uncertainty  of the model as a feature of the model and not just a limitation, in this case the residuals, greatly improving the model output. The dashboard provides an investor a good view of where the price could be going and suggested buy and sell points that an investor could use.\nThe opportunities for future work center around improving the model with better information. This model use daily price quotes because the were readily available and free, but stock price information is available for shorter periods down to the second. Also additional stock price measurement could be used like bollinger bands and volatility measures. In addition to model improvement, one could take the buy and sell recommendation of the model and build an automated trading program.\n\n""], 'url_profile': 'https://github.com/menach123', 'info_list': ['Jupyter Notebook', 'Updated Feb 11, 2020', 'Python', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'MATLAB', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'R', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'Updated Oct 9, 2020', 'Jupyter Notebook', 'Updated Feb 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Updated Jan 21, 2020', '3', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Updated Jan 21, 2020', '3', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Updated Jan 21, 2020', '3', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Updated Jan 21, 2020', '3', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Updated Jan 21, 2020', '3', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Statistical-Inference-of-Regression-Model-for-Longitudinal-Counting-Data-with-Zero-Expansion\nThis paper extended this GEE-type approach to zero-inﬂation count longitudinal data. In this paper, a generalized estimating equation (GEE)-type mixture model is proposed to jointly model the response of interest and the zero-inﬂated predictors. The estimates of parameters have been acquired by constructing estimating equations. In simulation study, this paper used Monte Carlo method to analyze three diﬀerent classes of response data and got the results of estimators to verify the reliability of this method.\nFor the request of simulation code, please send an email to jmu063@uottawa.ca\n'], 'url_profile': 'https://github.com/Mujingrui', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Updated Jan 21, 2020', '3', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Florida, USA', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Fatema29', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Updated Jan 21, 2020', '3', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Optimising-Prediction\nVarying model parameters to optimize a prediction algorithm, using a Decision Tree Regressor model.\n'], 'url_profile': 'https://github.com/dzeo', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Updated Jan 21, 2020', '3', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['DeepCReg\nCode for the paper: DeepCReg: ""Improving Cellular-based Outdoor Localization using CNN-based Regressors"" by Karim Elawaad, Mohamed Ezzeldin and Marwan Torki\nContent\nThis repo contains a jupyter notebook containing everything required to reproduce the results in the paper. We recommend running this notebook in Google Colab with GPU enabled for faster training.\nThe notebook uses the data for Testbed 2 only due to copyright concerns.\n'], 'url_profile': 'https://github.com/kelawaad', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Updated Jan 21, 2020', '3', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}","{'location': 'Moscow', 'stats_list': [], 'contributions': '613 contributions\n        in the last year', 'description': ['Numerai_Auto_ML\n\nA complete notebook to train/test/predict on Numerai Data-sets using multiple Regressors\n-- To be updated\n'], 'url_profile': 'https://github.com/YoushaaMurhij', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Jan 21, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Updated Jan 21, 2020', '3', 'Jupyter Notebook', 'Updated Jan 26, 2020', 'Python', 'Updated Feb 15, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020']}"
"{'location': 'Toulouse', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BrunoPlzk', 'info_list': ['Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Python', 'Updated Sep 30, 2020', 'Java', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'CC-BY-4.0 license', 'Updated Jan 28, 2020', 'Updated Jan 22, 2020', '1', 'HTML', 'Updated Jan 26, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['Blog-Posts-Code\nThis repository contains notebooks that demonstrate the concepts discussed in my articles.\n'], 'url_profile': 'https://github.com/Sakchhi', 'info_list': ['Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Python', 'Updated Sep 30, 2020', 'Java', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'CC-BY-4.0 license', 'Updated Jan 28, 2020', 'Updated Jan 22, 2020', '1', 'HTML', 'Updated Jan 26, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '324 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shreyash1811', 'info_list': ['Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Python', 'Updated Sep 30, 2020', 'Java', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'CC-BY-4.0 license', 'Updated Jan 28, 2020', 'Updated Jan 22, 2020', '1', 'HTML', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['The provided Matlab code accompanies the following work:\n@InProceedings{lederer2019uniform,\nauthor    = {Lederer, Armin and Umlauft, Jonas and Hirche, Sandra},\ntitle     = {Uniform Error Bounds for {G}aussian Process Regression with Application to Safe Control},\nbooktitle = {Conference on Neural Information Processing Systems (NeurIPS)},\nyear      = {2019},\n}\nPlease acknowledge the authors in any academic publication that have made\nuse of this code or parts of it by referencing to the paper.\nThe most recent version is available at:\nhttps://gitlab.lrz.de/ga68car/GPerrorbounds4safecontrol\nPlease send your feedbacks or questions to:\narmin.lederer_at_tum.de\n'], 'url_profile': 'https://github.com/jumlauft', 'info_list': ['Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Python', 'Updated Sep 30, 2020', 'Java', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'CC-BY-4.0 license', 'Updated Jan 28, 2020', 'Updated Jan 22, 2020', '1', 'HTML', 'Updated Jan 26, 2020']}","{'location': 'Chandigrah, India', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Priyanshu0', 'info_list': ['Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Python', 'Updated Sep 30, 2020', 'Java', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'CC-BY-4.0 license', 'Updated Jan 28, 2020', 'Updated Jan 22, 2020', '1', 'HTML', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Big-Data-ML-Tools\nComparison of different technologies for processing large volumes of data: Apache Spark , TensorFlow, Apache Mahout and R. A binary classification algorithm (Logistic Regression) is implemented to investigate the differences.\n'], 'url_profile': 'https://github.com/EduardoMorenoOrtigosa', 'info_list': ['Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Python', 'Updated Sep 30, 2020', 'Java', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'CC-BY-4.0 license', 'Updated Jan 28, 2020', 'Updated Jan 22, 2020', '1', 'HTML', 'Updated Jan 26, 2020']}","{'location': 'Dallas/TX/USA', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Predicting-the-Price-of-the-house-for-King-county\nThis dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.  https://www.kaggle.com/mayanksrivastava/predict-housing-prices-simple-linear-regression/data\nUsed various regression methods and try to predict the house prices by using different features which contribute to the prediction of the house price.\nAs we know we have different methods to get the solution but we need to find the best solution among the various method because each method has pros and cons.\nOnce after visualizing some features and a data mining process. we tried to find the best regression model for this dataset.\n'], 'url_profile': 'https://github.com/sudhirrai15', 'info_list': ['Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Python', 'Updated Sep 30, 2020', 'Java', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'CC-BY-4.0 license', 'Updated Jan 28, 2020', 'Updated Jan 22, 2020', '1', 'HTML', 'Updated Jan 26, 2020']}","{'location': 'Glendale, CA', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Analyze Results of AB Testing\nThis is a data analytics project utilizing Python v.3 libraries Numpy, PANDAS, Matplotlib & Statsmodel through Jupyter Notebook in order to analyze the A/B test results and assist an e-commerce company in deciding whether to implement a new page or keep an old page.\nRequired Software:\n Jupyter Notebook\n Numpy\n PANDAS\n Matplotlib\n Statsmodel\nAnalysis Outline:\n Wrangle with data to ensure data quality by programmatically cleaning and assessing.\n Utilize descriptive statistics to obtain preliminary analysis of data.\n Visualize data for further analysis and document observations.\n Utilize concepts of hypothesis testing as well as regression modeling in order to accurately choose the correct.\n Document predictions formed from the use of inferential statistics.\n'], 'url_profile': 'https://github.com/jpadillo', 'info_list': ['Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Python', 'Updated Sep 30, 2020', 'Java', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'CC-BY-4.0 license', 'Updated Jan 28, 2020', 'Updated Jan 22, 2020', '1', 'HTML', 'Updated Jan 26, 2020']}","{'location': 'Oxford, UK', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['CBCRTschizophrenia\nCode and data used for the meta regression planned for the Cochrane review ""Computer-Based Cognitive Remediation Therapy plus standard care versus standard care for people with schizophrenia or related disorders"".\n'], 'url_profile': 'https://github.com/santiagocdo', 'info_list': ['Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Python', 'Updated Sep 30, 2020', 'Java', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'CC-BY-4.0 license', 'Updated Jan 28, 2020', 'Updated Jan 22, 2020', '1', 'HTML', 'Updated Jan 26, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': [""Predicting_Heart_Attacks\nGroup project using R and CDC data to predict heart attacks based on health risk factors.  Exploratory data analysis, hypothesis testing, decision trees (random forest and bagged), logistic regression, and model evaluation.\nMotivation\nI performed this project as part of a group for an intro to data science class at the George Washington University. The requirement for this project was to find a sizeable dataset and perform predictive modeling.  Our instructor allowed us to use any of the modeling techniques learned during the semester.  My group opted to use Classification Trees and Logistic Regression given that most of our variables were categorical.\nData Source\nThe data for this project was found on the CDC's website: https://www.cdc.gov/brfss/annual_data/annual_2013.html.  BRFSS stands for Behavioral Risk Factor Surveillance System and is essentially survey data gathered by calling households and cell phones across the country.\nUsing the Files\n\nThe .RData file shows the raw data used for the project.\nThe .Rmd file shows all of my group's code and write-up.  This file can be downloaded and knitted to get the final output.\nThe .html file is the knitted R-Markdown file.  Look at this if you are only interested in the output of the analysis.  To view the rendered html file, go to http://htmlpreview.github.io/?https://github.com/jonathangiguere/Predicting_Heart_Attacks/blob/master/Group-Project-2-Summary--Part-III-.html.\nThe .pdf file contains my groups presentation slides for this project.\n\n""], 'url_profile': 'https://github.com/jonathangiguere', 'info_list': ['Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 23, 2020', 'MATLAB', 'Updated Jan 23, 2020', 'Python', 'Updated Sep 30, 2020', 'Java', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'CC-BY-4.0 license', 'Updated Jan 28, 2020', 'Updated Jan 22, 2020', '1', 'HTML', 'Updated Jan 26, 2020']}"
"{'location': 'MA, USA', 'stats_list': [], 'contributions': '257 contributions\n        in the last year', 'description': ['ToWear\n\nToWear is an application that suggests outfits to users from their closets based on the weather. We use linear regression to personalize the results to each user. Check out the current state of the app here.\nCheck out the more detailed wiki here.\nTable of contents\n\nMain Ideas\na. For Users\nb. For Developers\nSlideshow\nIn the works\n\n1. The main ideas of the app -- for users and for developers: \nA. For Users \nWe want our users to provide as little inputs as possible because we believe that will give them a better experience using the app.\nYou only have to create your closet based on the clothes you have. Each garment has a score from 1 to 10 to\nspecify the warmth. Each garment warms one of four parts of the body: (1) head, (2) top, (3) bottom, (4) feet.\nYou can then ask for a suggestion and you will be given a suggested outfit based on the current weather\noutside.\nIf desired, you can give the app information every day about what you wore and how you felt. This will be compared to the weather outside for that given day; the next time you ask for a suggestion, you will get more personalized outfits.\nB. For Developers \nMake sure to download all dependencies using ""pip install -r requirements.txt"" without quotes in your cmd.\nIn main.py you will find the Flask code. MUST see first for anyone who wants to understand the code.\nIn try_towear.py you will find most of the backend code for predicting body coefficients and desired temperature, which are used to suggest outfits.\nIn points_to_english.py you will find code relating to the representation of garments and clothes, as well as how the array outfit generated by the machine learning algorithm is translated into clothes.\nNote that you can use user_data.py and points_to_english.py alone to develop features using the cmd and without the flask front-end. Simply download these two files and run ""python user_data.py"" to see a text-based version of the app.\n2. Slideshow explaining the app \nView PDF.\n3. Features in the works \n\nMain to do list\nFeature to do list\n\n'], 'url_profile': 'https://github.com/Mandawi', 'info_list': ['CSS', 'GPL-3.0 license', 'Updated Jun 1, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'R', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jun 26, 2020']}","{'location': 'Queens,NY', 'stats_list': [], 'contributions': '501 contributions\n        in the last year', 'description': ['Natural-Language-Processing\nhttps://www.youtube.com/watch?v=p-XCC0y8eeY\nSOFTMAX VID ABOVE\nFeature Selection:\nBackward elimination : you’re evaluating the model with all of the features and then you pick each feature and remove it once and evaluate how accurate the model is without that feature and keep looping through.\nForward selection: you start with one feature and then you add every other feature as your 2nd and see which feature leads to a better and accurate model.\nStepwise selection is a combination of above\nK-fold cross validation:\nYour folding your dataset into k folds  and you use k-1 folds to train and 1 fold to test and you do this k times. each time your fiting your model and then your taking the mean accuracy of all of the combinations. This will be your average accuracy and this helps avoiding overfitting.\nSo now when you test a data, you have k models that your dataset goes into and gets predicted by?\nGridSearchCV:\nhyper parameter tuning, you’re figuring out what parameters to use when training your model. Depending on the classifier/model, each model takes different parameters for example, for Decision Trees you have max depth and for Support vector machines you either have a linear kernel (Straight line)  or a rbf kernel (squiggly line). Grid Search CV also does cross validation with CV = 5 or 10. Grid search returns you ‘best_params” and best accuracy. so it can tell you what your maximum depth should be to fit your model in order to obtain the highest accuracy.\nLinear Regression:\n-predicts a continuos number such as the weather or the cost of a house.\n-single linear regression is when you have one independent variable and one dependent variable so one feature and one label. An example of this is calculating the maxtemp given the minimum temperature.\n-these points of data when training are plotted on the graph so (mintemp,maxtemp) and then linear regression computes the mean of the x values and y values as a starting \tpoint for the line. this line is computed by minimizing the distance of each point. the line is a straight line in the middle of the data with the minimum distance to all of the training \tpoints.\n-linear regression returns the coefficient which is the slope and y intercept which is b so you have an equation y = mx+b and now you can plug in ur minimum temp to get a \t\tpredicted maxtemp\n-to evaluate the performance metric you can use mean absolute error, mean squared error or root mean squared error which is just the sum of [(predicted-actual)^2]/n\nMultiple linear regression is when you have multiple independent variables so many features and one label (y).\n-the main difference between single and multiple is that now you have multiple features and each feature has a different slope/weight on how important it is in determining the label. the linear regression algorithm after training returns you the coefficients for each feature and which direction the feature is leaning towards so negative or positive. if its a negative coefficient that means more of that feature will lean toward a smaller y.\n-you can plot your line and scatter plot your training data to see how well your line fitted.\nLogistic Regression:\nSimilar to linear however it classifies the input(feature vector) into a discrete/finite value e.g [0,1] which would be a binary classifier or even more values such as [0,1,2,3,4..] but the values for y aren’t continuous. We can predict if a passenger in the titanic ship survived or not so y=[survived,died].\nSimilar to linear regression as it uses weights for each feature and a bias ( y intercept) and sums it up however this value will not be discrete so logistic regression uses a sigmoid function to convert the value into a probability.\n-we then have a decision boundary which is a threshold and if the y value we receive is 0.7 which is greater then the threshold of 0.5 then the input can be classified in the direction of > 0.5, etc.\n-similar to linear regression we have a cost function(how far your line is from data points that are included in the training set) which we minimize to get the best line when training such that the line is closest to every point in the training set, however, with logistic regression we have discrete values between 0 and 1 which makes it difficult to minimize the cost value. You use gradient descent in logistic regression.\nDecision Tree Classifier:\nHas a tree where each root node is a question and this question’s goal is to split the training set such that each child node has the most information gain.\n-Information gain is calculated using entropy.\n-entropy is how diverse the data is, so if all the data points the same type in that node then the entropy will be 0. entropy is between 0 and 1. The root node so the beginning of the tree the entropy is 1 because your data is the most diverse possible.\nAfter the textual values are encoded to numerical values, we will see some values which will be greater than the other values. Higher values imply they have higher importance. This can lead to our models treating features differently. As an instance, Fashion news type might get a value of 1 and Economical news type might get a value of 10. This makes the machine learning model assume that Economical news type has more importance than Fashion news type.\nSolution:\xa0We can solve this by using One-Hot Encoding\nOne Hot Encoding\nTo prevent some categorical values getting higher importance than the others, we could use the one hot encoding technique before we feed encoded data into our machine learning model.\nOne hot encoding technique essentially creates a replica (dummy) feature for each distinct value in our target categorical feature. Once the dummy values are created, a boolean (0 or 1) is populated to indicate whether the value is true or false for the feature. As a consequence, we end up get a wide sparse matrix which has 0/1 values populated.\nAs an instance, if your feature has values “A”, “B” and “C” then three new features (columns) will be created: Feature A, Feature B and Feature C. If first row’s feature value was A then for feature A, you will see 1 and for feature B and C, it will be 0 and so on.\nNeural Networks:\n1) pass data to the network using forward propagation. Starting from the input layer, each neuron in the input layer is connected to each neuron in the first hidden layer. Each connection has a weight. The weights are multiplied by the actual inputs and then are summed up and passed to an activation function either relu(max(x,0)), tanh or sigmoid.\n2) the output layer consists of probabilities for each prediction, so if the model is a multinomial classifier then the output layer contains a neuron. for each class.\ni.e if it is a hard classification task, then only one class if the correct one and in this case we use a one hot encoded vector.\n3)We then calculate loss on the output\n4) use Stochastic gradient descent to minimimze the loss by calculating the gradient of the loss function and update weights accordingly using back-propogation.\n5) backpropation uses the derivative of the loss with respect to each weight.\n'], 'url_profile': 'https://github.com/akhil1213', 'info_list': ['CSS', 'GPL-3.0 license', 'Updated Jun 1, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'R', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jun 26, 2020']}","{'location': 'Seoul', 'stats_list': [], 'contributions': '287 contributions\n        in the last year', 'description': ['Yelp Fine-Grained Sentiment Analysis with BERT\nIn this project, we fine-tune a customized BERT1 (Bidirectional\nEncoder Representations from Transformers)-based model to fine-grained sentiment analysis of the Yelp-5 dataset. Our\nmain objective is to build a BERT-based model that predicts a review text\'s score as a real-valued number in [0, 4].\nI looked at template files from  huggingface\'s PyTorch\ntransformers repository when writing run_yelp.py and utils_yelp.py. For conciseness, this project only uses the\noriginal BERT model and does not support multi-GPU training.\nBackground\nTwo dominant approaches exist for fine-grained sentiment analysis: using a classification model to predict the label of\na review text as [0, 1, 2, 3, 4] or using a regression model to predict a real-valued score in [0, 4].\nA regression model has an advantage over a classification model: it produces a real-valued score\nof a review text while the classification based approach can only output the review text\'s probability for each label.\nHowever, the regression based approach results in a lower accuracy than the classification based approach.\nClassification Model\n\nGenerate the embedding of a review text by extracting the BERT embedding of the [CLS] token. Typically, the output\nof this step is a two dimensional tensor of size [batch_size, hidden_size].\nUse a linear layer of size [hidden_size, num_labels = 5] to map the review\'s BERT embedding to five outputs.\nEach of these five outputs correspond to the probability of the review\'s text score being [0, 1, 2, 3, 4]. The output\nis a two dimensional tensor of size [batch_size, 5].\nTrain the model using the cross-entropy loss function to perform a multi-label classification.\nWhen testing, use the model to produce a review text\'s probability for each label and find the label with the\nhighest probability.\n\nRegression Model\n\nGenerate the embedding of a review text by extracting the BERT embedding of the [CLS] token.\nUse a linear layer of size [hidden_size, num_labels = 1] to map the review\'s BERT embedding to a single\noutput. This will correspond to the review\'s score.\nTrain the model using the mean-squared loss function to perform a regression.\nWhen testing, use the model to produce a review text\'s real-valued score.\n\nLoss function of a Regression Model for Fine-Grained Sentiment Analysis\nIn this section, we discuss necessary properties of a cost function for regression-based fine-grained sentiment\nanalysis. Furthermore, we choose and test 4 different cost functions, two of which are custom-built.\nHow Should the Model Compute the Loss in Edge Cases?\nThe model\'s prediction of a review text\'s score can be any real number while the label of the text is one of [0, 1, 2,\n3, 4]. This observation implies the loss function for a regression-based fine-grained sentiment analysis model should\n\napply a small loss when the absolute value of (the model\'s prediction - label) < 0.5\napply a small loss when the model predicts a score < 0 for a review text whose label is 0.0\napply a small loss when the model predicts a score > 4 for a review text whose label is 4.0\n\nThe first property comes from the fact that the model rounds its real-valued prediction to the nearest integer. If\nthe label of a review text is 2, its real-valued score can range from 1.5 to 2.5. Therefore, we should not penalize\nthe model by much when the model makes a prediction within the range. To ensure our model learns more when\n(the model\'s prediction - label) < 0.5, we want the first derivative of our loss function to be smaller when the error\nis less than 0.5. Common loss functions that satisfy this property include the mean squared loss and the smooth l1 loss.\nThe second and third properties are necessary because the real-valued score of an extreme review can be significantly\nsmall or large. For instance, if the label of a review text is 0, its real-valued score can range from -inf to 0.5.\nTherefore, if the model predicted a score lower than 0, it made a correct prediction and does not need to learn at all.\nEnforcing learning in this case can actually make the model to treat all extreme reviews to have the same degree of\npositivity or negativity. To mitigate this problem, we mask the loss to be 0 in these cases.\nConsidering the three properties mentioned above, we implement two custom loss functions called masked mean squared\nloss and masked smooth l1 loss.\nThe original regression model\'s loss function is\n\nwhereas a masked loss function is\n\ndef masked_smooth_l1_loss(input, target):\n    t = torch.abs(input - target)\n    smooth_l1 = torch.where(t < 1, 0.5 * t ** 2, t - 0.5)\n\n    zeros = torch.zeros_like(smooth_l1)\n\n    extreme_target = torch.abs(target - 2)\n    extreme_input = torch.abs(input - 2)\n    mask = (extreme_target == 2) * (extreme_input > 2)\n\n    return torch.where(mask, zeros, smooth_l1).sum()\ndef masked_mse_loss(input, target):\n    t = torch.abs(input - target)\n    mse = t ** 2\n\n    zeros = torch.zeros_like(mse)\n\n    extreme_target = torch.abs(target - 2)\n    extreme_input = torch.abs(input - 2)\n    mask = (extreme_target == 2) * (extreme_input > 2)\n\n    return torch.where(mask, zeros, mse).sum() / mse.size(-1)\nHere, we report the results of our experiments on 10% of Yelp-5 dataset.\n\n\n\nModel\nMAE\nMSE\n\n\n\n\nClassification-Based, CrossEntropyLoss\n0.4902\n0.7134\n\n\nRegression-Based, MSELoss\n0.5404\n0.5919\n\n\nRegression-Based, SmoothL1Loss\n0.5342\n0.5849\n\n\nRegression-Based, Masked MSELoss\n0.5406\n0.5894\n\n\nRegression-Based, Masked SmoothL1Loss\n0.5355\n0.5824\n\n\n\nDataset\nTo download the original Yelp-5 dataset, follow this link and download\n""yelp_review_full.csv.tar.gz"". The dataset contains 650k examples consisting of 130k examples for each label.\nRequirements\nTo install required packages for this project, run the following command on your virgtual environment.\npip install -r requirements.txt\nRun it on CPU/GPU\nTo run the project on our machine, copy and paste one of the following consoles. Note that the full dataset takes\nabout 2hrs/epoch to train on Nvidia RTX 2080 Ti. For experiments, we recommend using the spit_data function from\nutils_yelp.py to take a desired fraction of data.\nTo test the model with different loss functions for the regression based approach, uncomment the desired loss function\nin model.py.\nSample Command for Running Classification on CPU\npython3 run_yelp.py \\\n    --data_dir ./ \\\n    --model_name_or_path bert-base-multilingual-cased \\\n    --output_dir masked-loss \\\n    --max_seq_length  128 \\\n    --num_train_epochs 3 \\\n    --per_gpu_train_batch_size 32 \\\n    --save_steps 100000 \\\n    --seed 1 \\\n    --overwrite_output_dir \\\nSample Command for Running Classification on GPU\nCUDA_VISIBLE_DEVICES=0 python3 run_yelp.py \\\n    --data_dir ./ \\\n    --model_name_or_path bert-base-multilingual-cased \\\n    --output_dir masked-loss \\\n    --max_seq_length  128 \\\n    --num_train_epochs 3 \\\n    --per_gpu_train_batch_size 32 \\\n    --save_steps 100000 \\\n    --seed 1 \\\n    --overwrite_output_dir \\\nSample Command for Running Regression with MSE on GPU\nCUDA_VISIBLE_DEVICES=0 python3 run_yelp.py \\\n    --data_dir ./ \\\n    --model_name_or_path bert-base-multilingual-cased \\\n    --output_dir masked-loss \\\n    --max_seq_length  128 \\\n    --num_train_epochs 3 \\\n    --per_gpu_train_batch_size 32 \\\n    --save_steps 100000 \\\n    --seed 1 \\\n    --overwrite_output_dir \\\n    --regression\n    --loss mse\nSample Command for Running Regression with masked_smooth_l1 on GPU\nCUDA_VISIBLE_DEVICES=0 python3 run_yelp.py \\\n    --data_dir ./ \\\n    --model_name_or_path bert-base-multilingual-cased \\\n    --output_dir masked-loss \\\n    --max_seq_length  128 \\\n    --num_train_epochs 3 \\\n    --per_gpu_train_batch_size 32 \\\n    --save_steps 100000 \\\n    --seed 1 \\\n    --overwrite_output_dir \\\n    --regression\n    --loss masked_smoothl1\n'], 'url_profile': 'https://github.com/hpark3910', 'info_list': ['CSS', 'GPL-3.0 license', 'Updated Jun 1, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'R', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Kaggle-HR-Analytics-Case-Study\nThe aim of this project was to predict whether a given employee is willing to leave a company. Prediction was made using logistic\nregression, on HR data set provided by Kaggle: https://www.kaggle.com/vjchoudhary7/hr-analytics-case-study/kernels\nDetailed description of the project is provided in file ""Project description""\n'], 'url_profile': 'https://github.com/g3nt3lman', 'info_list': ['CSS', 'GPL-3.0 license', 'Updated Jun 1, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'R', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['DUPLEX-data-split-function\nApply DUPLEX data split to the given dataset and return training and test datasets.\nNote:\nThe code need to be modified for use for different datasets. The example given is 3D inputs and outputs designed for LSTM network training.\nREF: Snee, R. D. (1977). Validation of regression models: methods and examples. Technometrics, 19(4), 415-428.\n'], 'url_profile': 'https://github.com/yuerongz', 'info_list': ['CSS', 'GPL-3.0 license', 'Updated Jun 1, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'R', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Detection-of-Fraudulent Transactions\nDeveloped a Fraud Detection Framework in Financial Payment Services over an imbalanced synthetic financial dataset generated by Paysim having over 6.5 million financial transactions with using Logistic Regression, Decision Tree, Naive Bayes, Random and KNN. The Recall values of Naïve Bayes, Decision Tree, Logistics Regression and KNN are 0.40,0.66,0.72 and 0.76 respectively. Also, the AU-ROC values are 0.87, 0.91, 0.95 and 0.93 respectively.\n'], 'url_profile': 'https://github.com/avranil26', 'info_list': ['CSS', 'GPL-3.0 license', 'Updated Jun 1, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'R', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jun 26, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Admission-Prediction-based-on-Tofel-Score-Random-Forest-\nUsing Random Forest Regressor a simple predictive model for chance to admission based on TOFEL score and dependent paramters.\n'], 'url_profile': 'https://github.com/erShoeb', 'info_list': ['CSS', 'GPL-3.0 license', 'Updated Jun 1, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'R', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Falls among the elderly is an important health issue. Fall detection and movement tracking are therefore instrumental in addressing this issue. This paper responds to the challenge of classifying different movements as a part of a system designed to fulfill the need for a wearable device to collect data for fall and near-fall analysis. Four different fall trajectories (forward, backward, left and right), three normal activities (standing, walking and lying down) and near-fall situations are identified and detected.\nFalls are a serious public health problem and possibly life threatening for people in fall risk groups. We develop an automated fall detection system with wearable motion sensor units fitted to the subjects’ body at six different positions. Each unit comprises three tri-axial devices (accelerometer, gyroscope, and magnetometer/compass). Fourteen volunteers perform a standardized set of movements including 20 voluntary falls and 16 activities of daily living (ADLs), resulting in a large dataset with 2520 trials. To reduce the computational complexity of training and testing the classifiers, we focus on the raw data for each sensor in a 4 s time window around the point of peak total acceleration of the waist sensor, and then perform feature extraction and reduction.\nAcknowlegement:\nÖzdemir, Ahmet Turan, and Billur Barshan. “Detecting Falls with Wearable Sensors Using Machine Learning Techniques.” Sensors (Basel, Switzerland) 14.6 (2014): 10691–10708. PMC. Web. 23 Apr. 2017\nReference:https://www.kaggle.com/pitasr/falldata\nGeneral information about the dataset, what the columns contain, and what the codes in the ACTIVITY column mean:\nFall detection data set of Chinese hospitals of old age patients.\nColumns\nACTIVITY: activity classification\nTIME: monitoring time\nSL: sugar level\nEEG: EEG monitoring rate\nBP: Blood pressure\nHR: Heart beat rate\nCIRCLUATION: Blood circulation\nACTIVITY\n0- Standing\n1- Walking\n2- Sitting\n3- Falling\n4- Cramps\n5- Running\n'], 'url_profile': 'https://github.com/pradeepmuni', 'info_list': ['CSS', 'GPL-3.0 license', 'Updated Jun 1, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'R', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jun 26, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['ML-Algorithm-Analysis\nThis project is developed for Machine Learning problems. I mean, initially started it for my own use but after some effort it started looking good. So thought why not share it, maybe it will be useful here.  So below is brief description on its features or steps: • Select Report File : User can select/ upload report that they want to analyze. • Describe Date : Gives user option to view basic data features( mean/ standard dev/ count etc.). Can add data correlation details, plus few graphs here.. • Select Problem : Choose Regression or classification as problem, This needs to be done as algorithms available for both are different. • Regression: for quantitative analysis, ex: prices, inventory etc. • Classification: for class analysis, ex: pass/ fail, groups etc. • Neural Networks • Select Features: This will be your independent variable set, more like inputs based on which you want to analyze your data. • Select Label: This will be Target variable, value that you want to predict. • Select Model: So for both Regression and Classification, different model/ algorithm option will be available. Below options are available for now • Regression: Linear Regression/ Support Vector Machine/ Decision Tree/ Random Forest • Classification: Logistic Regression/ K-Nearest Neighbor/ Naive Bayes/ XGBoost/ Random Forest/ LightGBM For each algorithm different set of Parameters are available that one can modify, and check model accuracy results. Based on accuracy results, you can chose best Model to solve your problem • Upload Input: Upload input data that you want to predict. So here we are using historical data to finalize best solution for future predication. • Download Results: Download results. • Download Notes: User can download analysis notes.\n'], 'url_profile': 'https://github.com/mohitsingh28', 'info_list': ['CSS', 'GPL-3.0 license', 'Updated Jun 1, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'R', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jun 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '302 contributions\n        in the last year', 'description': ['\nReal Estate Valuation with GUI\nIn this project I have created a machine learning model to predit house prices of a certain area using multiple-regression\nand gradient descent algorithm. Along with it a GUI was also built using which you can input data to get predictions.\nDataset\nData for training and testing was taken from UCI ML Repository.\n-https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set\nExample\n\nThanks\n'], 'url_profile': 'https://github.com/gaggudeep', 'info_list': ['CSS', 'GPL-3.0 license', 'Updated Jun 1, 2020', '1', 'Jupyter Notebook', 'Updated May 20, 2020', '1', 'Python', 'Updated Jan 29, 2020', 'R', 'Updated Jan 25, 2020', 'Python', 'Updated Feb 10, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jun 26, 2020']}"
"{'location': 'Phoenix, AZ', 'stats_list': [], 'contributions': '180 contributions\n        in the last year', 'description': ['WeatherPy\nHere, I created a Python script to visualize the weather of 500+ cities across the world of varying distance from the equator. To accomplish this, I utilized a simple Python library and the OpenWeatherMap API to create a representative model of weather across world cities.\nI first build a series of scatter plots to showcase the following relationships:\n\nTemperature (F) vs. Latitude\nHumidity (%) vs. Latitude\nCloudiness (%) vs. Latitude\nWind Speed (mph) vs. Latitude\n\nI then ran a linear regression on each relationship, and separated them into Northern Hemisphere and Southern Hemisphere:\n\nNorthern Hemisphere - Temperature (F) vs. Latitude\n\n\n\nSouthern Hemisphere - Temperature (F) vs. Latitude\n\nNorthern Hemisphere - Humidity (%) vs. Latitude\n\nSouthern Hemisphere - Humidity (%) vs. Latitude\n\nNorthern Hemisphere - Cloudiness (%) vs. Latitude\n\nSouthern Hemisphere - Cloudiness (%) vs. Latitude\n\nNorthern Hemisphere - Wind Speed (mph) vs. Latitude\n\nSouthern Hemisphere - Wind Speed (mph) vs. Latitude\n\n\n'], 'url_profile': 'https://github.com/cawthornebr', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Bank-Loan-Status\nThis dataset is about whther the customers of a bank paid the loan fully or not based on several other features. Here we will be trying to predict whether the customer will pay the loan amouont fuly or not based on the historical data of several features.\nThis dataset contains 100514  rows and 19 columns\nReference:https://www.kaggle.com/zaurbegiev/my-dataset\nColumns:\nLoan ID\nCustomer ID\nCurrent Loan Amount\nTerm\nCredit Score\nAnnual Income\nYears in current job\nHome Ownership\nPurpose\nMonthly Debt\nYears of Credit History\nMonths since last delinquent\nNumber of Open Accounts\nNumber of Credit Problems\nCurrent Credit Balance\nMaximum Open Credit\nBankruptcies\nTax Liens\n'], 'url_profile': 'https://github.com/pradeepmuni', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020']}","{'location': 'Arlington,Texas', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ArchithaHarinath', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Unit-Predictor\nThis is a machine learning model that is used to predict the production quantity and the area of production based on the country, state of India. We have used Random Forest Regression model for the same.\n'], 'url_profile': 'https://github.com/aravind1910', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020']}","{'location': 'Austin, TX', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': [""PRSNL-Names_Analysis\nIn this project, I've created a function to go through a list and deconstruct each name to then be fitted to a Logistic Regression model. This will help determine what percentage of each word's length, on average would produce a consolidated list of words.\nMore information on Edit Distance and Approximate Name Matching:\nhttps://stackoverflow.com/questions/5843518/remove-all-special-characters-punctuation-and-spaces-from-string\nhttps://en.wikipedia.org/wiki/Levenshtein_distance\nhttps://en.wikipedia.org/wiki/Approximate_string_matching\nhttps://en.wikipedia.org/wiki/Edit_distance\nhttps://towardsdatascience.com/fuzzy-string-matching-in-python-68f240d910fe\nMore information on calculating decision boundary of logistic regression:\nhttps://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\nhttps://towardsdatascience.com/decision-boundary-visualization-a-z-6a63ae9cca7d\nhttps://stackoverflow.com/questions/28256058/plotting-decision-boundary-of-logistic-regression\nhttps://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf\n""], 'url_profile': 'https://github.com/ferozumair786', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020']}","{'location': 'Dhanbad,Jharkhand', 'stats_list': [], 'contributions': '364 contributions\n        in the last year', 'description': [""Random-Forest-Classifier\nA very simple Random Forest Classifier implemented in python. The sklearn.ensemble library was used to import the RandomForestClassifier class. The object of the class was created. The following arguments was passed initally to the object:\n\nn_estimators = 10\ncriterion = 'entropy'\n\nThe inital model was only given 10 decision tree, which resulted in a total of 10 incorrect prediction. Once the model was fitted with more the decision trees the number of incorrect prediction grew less.\nIt was found that a the optimal number of decision trees for this models to predict the answers was 200 decision trees. Hence the n_estimator argument was given a final value of 200.\nAnything more that 200 will result in over-fitting and will lead further incorrect prediction.\n""], 'url_profile': 'https://github.com/krishnapalS', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Black-Friday-Dataset\nThis dataset comprises of sales transactions captured at a retail store. It’s a classic dataset to explore and expand your feature engineering skills  and day to day understanding from multiple shopping experiences. This is a regression problem. The dataset has 550,069 rows and 12 columns.\nUsed Linear Regression,Lasso Regression intially but was not of much use, then after extensive searching found a boosting algorithm for a dataset this big, Used XGBoost which is a gradient boosting algorithm based on the decision tree model.\nApplying XGBoost is the easy part, the difficult part is tweaking the parameters to gain your results.\nAfter a lot of hit and trial, found these parameters worked the best for me.\nA great learning experience overall.\n'], 'url_profile': 'https://github.com/Foxtrot2000', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020']}","{'location': 'Kolkata', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['ML_Project-MBC\nIn this ML project, we were provided with a dataset of Breast Cancer and here we have to predict the MALIGNANCY OF BREAST CANCER by using different types of classification algorithms like decision tree, logistic regression, k-NN and SVM. We have applied this algorithms and made a comparative study of accuracies obtained from different classification algorithms applied on that dataset.\n'], 'url_profile': 'https://github.com/blackfly007', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020']}","{'location': 'Punjab', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Flight-Prediction-System\nA flight prediction system using the real-world data-set. I used pandas, sci-learn, numpy and matplotlib libraries of python in this project. I applied Random Forest algorithm along-with Support Vector Machine and Linear regression. According to my observations, random forest algorithm produced the most accurate result.\n'], 'url_profile': 'https://github.com/Adarsh-07', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Loan_payment_prediction_using-ML\nThis is a loan payment prediction to tell whether a customer will default on their payment or not prior to approving their loan.  Different classification models will be applied to determine the most accurate one by using the following algorithms:\n\nk-Nearest Neighbour\nDecision Tree\nSupport Vector Machine\nLogistic Regression\n\n'], 'url_profile': 'https://github.com/ibourzgui', 'info_list': ['Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Feb 5, 2020', 'Python', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 11, 2020']}"
"{'location': 'Dublin, Ireland', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Cricket-Dataset-Analysis\nIn cricket, the prediction has become one of the most interesting things nowadays. The researchers use different methods such as linear regression, Naive Bayes, K nearest neighbor, and other machine learning algorithms to predict final run-rate, which team will win in bilateral, triangular and World Cup matches, and what will be the final score of a team batting first. Prediction always adds value for team authors, coaches, and management to instruct the players and also making strategies. A player’s performance has a dependency on various constraints such as venue, whether it is an in-house or abroad game, how is the current form, what is record against the opposition, how big the boundaries are, etc. This project will predict how many runs a player will score in the current match. To predict the number of runs, Multiple Regression has been used.\n'], 'url_profile': 'https://github.com/dangepalash2', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Predicting-Song-Popularity-using-Million-Song-Dataset-\nAbstract \u2028 \u2028 The project aims to classify whether a song will be a hit or not, and will it reach Top-Charts based on several music and artist attributes as predictor variables. It will be crucial for us to predict the popularity of a song and we will be using various classification models in our analysis to predict the probability of a particular song reaching the Top Charts.\u2028 \u2028 Music evokes emotions: joy, nostalgia, peacefulness, power, sadness, tenderness, tension and transcendence. It is also used in order to manipulate or influence their own emotional states. Different kind of people experience different sensations and have a different personal perception about certain kinds of music. A hit song is one which appeals to a majority of its listeners. We wanted to build a mathematical relationship between the likelihood of liking a song and explanatory features related to the artist and the song audio. Can the artist and audio features be used as the only predictors needed to classify a certain song as a “Hit” or “Not-Hit”. Our approach included Data Engineering the Million Song Dataset according to our problem needs, addition, removal of certain attributes and missing value imputations. Consequently, we ran various classification models on this final data like KNN, Decision Trees, Random Forest ,Naive Bayes, Logistic Regression and SVM. After completing our analysis and on comparison of our results from each of these models with the help of ROC plots and cross validation results, we find that Random Forest and Logistic Regression (L1 Regularisation) have the maximum accuracy. But for our particular problem we had to place more focus on the “Hit” predictions accuracy. F1 score comparisons of all the models showed L2 Regularised Logistic Regression as our best model. This was primarily due to the fact that Lasso Regression regularises the predictors, removes unimportant variables thereby giving us accurate predictions for “Hit” and “Not-Hit” categories.\n'], 'url_profile': 'https://github.com/Ajay6140', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': [""The file daily_weather.csv is a comma-separated file that contains weather data.  This data comes from a weather station located in San Diego, California.  The weather station is equipped with sensors that capture weather-related measurements such as air temperature, air pressure, and relative humidity.  Data was collected for a period of three years, from September 2011 to September 2014, to ensure that sufficient data for different seasons and weather conditions is captured.\nLet's now check all the columns in the data.\nEach row in daily_weather.csv captures weather data for a separate day.  \nSensor measurements from the weather station were captured at one-minute intervals.  These measurements were then processed to generate values to describe daily weather. Since this dataset was created to classify low-humidity days vs. non-low-humidity days (that is, days with normal or high humidity), the variables included are weather measurements in the morning, with one measurement, namely relatively humidity, in the afternoon.  The idea is to use the morning weather values to predict whether the day will be low-humidity or not based on the afternoon measurement of relative humidity.\nEach row, or sample, consists of the following variables:\n\nnumber: unique number for each row\nair_pressure_9am: air pressure averaged over a period from 8:55am to 9:04am (Unit: hectopascals)\nair_temp_9am: air temperature averaged over a period from 8:55am to 9:04am (Unit: degrees Fahrenheit)\nair_wind_direction_9am: wind direction averaged over a period from 8:55am to 9:04am (Unit: degrees, with 0 means coming from the North, and increasing clockwise)\nair_wind_speed_9am: wind speed averaged over a period from 8:55am to 9:04am (Unit: miles per hour)\n** max_wind_direction_9am:** wind gust direction averaged over a period from 8:55am to 9:10am (Unit: degrees, with 0 being North and increasing clockwise)\nmax_wind_speed_9am: wind gust speed averaged over a period from 8:55am to 9:04am (Unit: miles per hour)\nrain_accumulation_9am: amount of rain accumulated in the 24 hours prior to 9am (Unit: millimeters)\nrain_duration_9am: amount of time rain was recorded in the 24 hours prior to 9am (Unit: seconds)\nrelative_humidity_9am: relative humidity averaged over a period from 8:55am to 9:04am (Unit: percent)\nrelative_humidity_3pm: relative humidity averaged over a period from 2:55pm to 3:04pm (*Unit: percent *)\n\n""], 'url_profile': 'https://github.com/pradeepmuni', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Updated Jan 26, 2020']}","{'location': 'Raleigh,North Carolina', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anmolrpant', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['ATMS-597-Python\nGraduate level class taught by Dr. Stephen Nesbitt at the University of Illinois Spring 2020\nIn this course, graduate students will gain experience using python language-based tools to analyze weather and climate datasets, and learn tools that will allow for the development of statistical representations of data and their uncertainties. Topics covered will include (1) object-oriented python programming, (2) version control, open source, and open data best practices, (3) using pandas and xarray for wrangling data, (4) parallelization in python and big data, (5) supervised learning, including linear, logistic, Bayesian, random forest regression techniques, and (6) unsupervised learning, including clustering, empirical orthogonal functions, and neural and deep learning.\n'], 'url_profile': 'https://github.com/rmiller34', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Updated Jan 26, 2020']}","{'location': 'Mahendragarh , Haryana', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Loan_prediction\n'], 'url_profile': 'https://github.com/Rupesh-11', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Updated Jan 26, 2020']}","{'location': 'Pittsburgh', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Varshini-524', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Updated Jan 26, 2020']}","{'location': 'Champaign, IL', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Skin-Disease-Image-Classification---Machine-Learning-Approach\n\n\nWe all know how strong neural network techniques(like CNN) are in such image recognition & classification, even a person without many knowledge in data science may follow the codes & packages people share online to perform such an analysis within hours.\n\n\nI would like to try to take a different approach to perform image classification using only machine learning methods. In this project, I tested three classification machine learning models: Random Forest, Logistic Regression and Support Vector Machine, and I am doing my best to collect features from image as variables to identify healthy and ill skin.\n\n\nThe result turns out to be quite accurate if using pixels as the image features, however, what if pixel is hard to understand by people in health industries? Can we use something else rather than pixel? This part is explored in Question 2.\n\n\nOf course, this project, given a lot of limitation, takes effort, but I feel I had a deeper understanding of image classification.\n\n\n'], 'url_profile': 'https://github.com/yuanshi4', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Updated Jan 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['KDD-Data---PVA-Fundraising & Target Marketing\n(from http://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98doc.txt)\nBACKGROUND -------------------------\nThe data set for this year\'s Cup has been generously provided by the Paralyzed Veterans of America (PVA). PVA is a not-for-profit organization that provides programs and services for US veterans with spinal cord injuries or disease. With an in-house database of over 13 million donors, PVA is also one of the largest direct mail fund raisers in the country. Participants in the \'98 CUP will demonstrate the performance of their tool by analyzing the results of one of PVA\'s recent fund raising appeals. This mailing was sent to a total of 3.5 million PVA donors who were on the PVA database as of June 1997. Everyone included in this mailing had made at least one prior donation to PVA. The mailing included a gift (or ""premium"") of personalized name & address labels plus an assortment of 10 note cards and envelopes. All of the donors who received this mailing were acquired by PVA through similar premium-oriented appeals such as this. One group that is of particular interest to PVA is ""Lapsed"" donors. These are individuals who made their last donation to PVA 13 to 24 months ago. They represent an important group to PVA, since the longer someone goes without donating, the less likely they will be to give again. Therefore, recapture of these former donors is a critical aspect of PVA\'s fund raising efforts. However, PVA has found that there is often an inverse correlation between likelihood to respond and the dollar amount of the gift, so a straight response model (a classification or discrimination task) will most likely net only very low dollar donors. High dollar donors will fall into the lower deciles, which would most likely be suppressed from future mailings. The lost revenue of these suppressed donors would then offset any gains due to the increased response rate of the low dollar donors. Therefore, to improve the cost-effectiveness of future direct marketing efforts, PVA wishes to develop a model that will help them maximize the net revenue (a regression or estimation task) generated from future renewal mailings to Lapsed donors.\nPOPULATION ----------\nThe population for this analysis will be Lapsed PVA donors who received the June \'97 renewal mailing (appeal code ""97NK""). Therefore, the analysis data set contains a subset of the total universe who received the mailing. The analysis file includes all 191,779 Lapsed donors who received the mailing, with responders to the mailing marked with a flag in the TARGET_B field. The total dollar amount of each responder\'s gift is in the TARGET_D field. The overall response rate for this direct mail promotion is 5.1%. The package cost (including the mail cost) is $0.68 per piece mailed.\nANALYSIS TIME FRAME AND REFERENCE DATE --------------------------------------\nThe 97NK mailing was sent out on June 1997. All information included in the file (excluding the giving history date fields) is reflective of behavior prior to 6/97. This date may be used as the reference date in generating the ""number of months since"" or ""time since"" or ""elapsed time"" variables. The participants could also find the reference date information in the filed ADATE_2. This filed contains the dates the 97NK promotion was mailed.\n+--------------------------------------------------------------------+ |\nDATA SOURCES and ORDER &\nTYPE OF THE VARIABLES IN THE DATA SETS\n| +--------------------------------------------------------------------+\nThe dataset includes: o 24 months of detailed PVA promotion and giving history (covering the   period 12 to 36 months prior to the ""97NK"" mailing) o A summary of the promotions sent to the donors over the most recent   12 months prior to the ""97NK"" mailing (by definition, none of these   donors responded to any of these promotions) o Summary variables reflecting each donor\'s lifetime giving history (e.g., total # of donations prior to ""97NK"" mailing, total $ amount of the donations, etc.) o Overlay demographics, including a mix of household and area level   data o All other available data from the PVA database (e.g., date of first gift, state, origin source, etc.) The fields are described in greater detail in the data dictionary file <filename: cup98DIC.txt>\nTARGET MARKETING\nBackground\nA national veteran’s organization wishes to develop a data mining model to improve the cost-\neffectiveness of their direct marketing campaign. The organization, with its in-house database of over 13 million donors, is one of the largest direct mail fundraisers in the United States. According to their recent mailing records, the overall response rate is 5.1%. Out of those who responded (donated), the average donation is $13.00. Each mailing, which includes a gift of personalized address labels and assortments of cards and envelopes, costs $0.68 to produce and send. Using these facts, we take a sample of this dataset to develop a classification model that can effectively capture donors so that the expected net profit is maximized. Weighted sampling is used, under-representing the non-responders so that the sample has a more balanced numbers of donors and non-donors.\nData\nThe modeling dataset is in the file pvaDataForModeling_Fall2018.csv. The data has been sampled (from the original PVA training dataset) to carry a higher proportion of donors (TARGET−B = 1) than in the original data. The amount of donation (TARGET−D) is also included but is not used in this assignment. The file contains all 480 attributes.\n'], 'url_profile': 'https://github.com/Ajay6140', 'info_list': ['Jupyter Notebook', 'Updated Apr 21, 2020', 'R', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', '1', 'Python', 'Updated Jan 23, 2020', 'Jupyter Notebook', 'Updated Jan 27, 2020', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Jupyter Notebook', 'Updated Jan 22, 2020', '1', 'Jupyter Notebook', 'Updated Jan 25, 2020', 'Updated Jan 26, 2020']}",
