"{'location': 'Beijing', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['ECCV2020: Adaptive Mixture Regression Network with Local Counting Map for Crowd Counting\nIntroduction\nIn this work, we introduce a new learning target named local counting map, and\nshow its feasibility and advantages in local counting regression. Meanwhile, we\npropose an adaptive mixture regression framework in a coarse-to-fine manner.\nIt reports marked improvements in counting accuracy and the stability of the\ntraining phase, and achieves the start-of-the-art performances on several author-\nitative datasets. For more details, please refer to our arXiv paper.\nFramework\n\nDemo\n\nGetting Started\n\n\nPrerequisites\n\nPython >= 3.5\nPytorch >= 1.0.1\nother libs in requirements.txt, run pip install -r requirements.txt.\n\n\n\nData Preparation\n\nDownload ShanghaiTech, UCF-QNRF, UCF_CC_50 datasets from the official websites\nand unzip them in ./ProcessedData.\nRun cd ./datasets/XXX/ and python prepare_XXX_mod64.py to resize images and generate training labels.\nUCF-QNRF: [home link],\nShanghaiTech: [home link],\nUCF-CC-50: [home link]\n\n\n\nPretrained Model (Only for Training)\n\nSome Counting Networks (such as VGG, CSRNet and so on) adopt the pre-trained models on ImageNet.\nDownload vgg16-397923af.pth from torchvision.models.\nPlace the pre-trained model to ./models/Pretrain_model/.\nvgg16-397923af.pth : download link\n\n\n\nFolder Tree\n+-- source_code\n|   +-- datasets\n    |   +-- SHHA\n    |   +-- ......\n|   +-- misc     \n|   +-- models\n    |   +-- Prerain_Model\n    |   +-- SCC_Model\n    |   +-- ......\n|   +-- ProcessedData\n    |   +-- shanghaitech_part_A\n    |   +-- ......\n\n\n\nModels\nQNRF-model (MAE/MSE: 86.6/152.1):\nGoogle Drive: download link,\nBaidu Yun: download link (key: pe2r)\nDemo Testing\n\nput test images in ./demo_image.\nrun python demo.py.\nresults are saved at ./demo_image/result.\n\nDataset Testing\n\nset the parameters (such as MODEL_PATH) in test_config.py.\nrun python test.py.\n\nReferences\nhttps://github.com/gjy3035/C-3-Framework\n'], 'url_profile': 'https://github.com/xiyang1012', 'info_list': ['50', 'Python', 'MIT license', 'Updated Oct 8, 2020', '24', 'C#', 'Updated Mar 18, 2020', '9', 'C++', 'GPL-3.0 license', 'Updated Jan 21, 2021', '4', 'Julia', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', '4', 'Python', 'MIT license', 'Updated Dec 22, 2020', '3', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '265 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jake0303', 'info_list': ['50', 'Python', 'MIT license', 'Updated Oct 8, 2020', '24', 'C#', 'Updated Mar 18, 2020', '9', 'C++', 'GPL-3.0 license', 'Updated Jan 21, 2021', '4', 'Julia', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', '4', 'Python', 'MIT license', 'Updated Dec 22, 2020', '3', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Regressino \xa0\xa0\xa0\xa0\nLibrary to calculate potential, exponential, logarithmic, lineal and logistic regression in Arduino.\nLineal: Y = a*X + b\nExponential: Y = b * e^(a*X)\nLogarithmic: Y = a*ln(X) + b\nPotential: Y = b * x^a\nLogistic: Y = 1 / (1 + e^-(a*x+b))\nPolynomial: Y = a1x^2 + a2x + b\nLinear2d: Y = a1x1 + a2x2 + b\nLogistic2d:Y = 1 / (1 + e^-(1x1 + a2x2 + b))\nGraphs with a = 2 and b = 3:\n\nLogistic regression:\n\nPolynomial regression:\n\nLinear and Logistic 2 variables\n\nUse\nInclude\n1 variable\nLinear\n#include <LinearRegression.h>\n\nLinearRegression lr = LinearRegression();\nExponential\n#include <ExponentialRegression.h>\n\nExponentialRegression er = ExponentialRegression;\nLogarithmic\n#include <LogarithmicRegression.h>\n\nLogarithmicRegression lr = LogarithmicRegression();\nPotential\n#include <PotentialRegression.h>\n\nPotentialRegression pr = PotentialRegression();\nLogistic\n#include <LogisticRegression.h>\n\nLogisticRegression pr = LogisticRegression();\nPolynomial\n#include < PolynomialRegression.h>\n\n PolynomialRegression pr =  PolynomialRegression();\n2 variables\nLinear2d\n#include <LinearRegression2d.h>\n\nLinearRegression lr = LinearRegression2d();\nLogistic2d\n#include <LogisticRegression2d.h>\n\nLogisticRegression pr = LogisticRegression2d();\nMethods 1 variable\nvoid learn(double x, double y);  \nLearns one example.\n\nx: value of X\ny: value of Y\n\n\ndouble calculate(double x1);  \nEstimates value of Y for X\n\nx: value of x\n\n\ndouble correlation();  \nReturn correlation value\n\nvoid reset();  \nReset values. Start learning since zero.\n\nvoid parameters(double values[]);\nReturn parameters of the regression y = mx + b\n\nvalues[0] = m;\nvalues[1] = b;\n\nIn polynomial values are y = b1x^2 + b2x + a :\n\nvalues[0] = b1;\nvalues[1] = b2;\nvalues[2] = a;\n\n\ndouble error(double x, double y);  \nReturn estimation error. If you need more options to calculate error you can use error module from SimpleStatisticsArduino\n\nvoid samples();\nReturn number of examples learned\nMethods 2 variables\nvoid learn(double x1, double x2, double y);  \nLearns one example.\n\nx1: value of X1\nx2: value of X2\ny: value of Y\n\n\ndouble calculate(double x1, double x2);  \nEstimates value of Y\n\nx1: value of x1\nx2: value of x2\n\n\ndouble correlation();  \nReturn correlation value\n\ndouble correlationX1Y();  \nReturn correlation value between X1 and Y\n\ndouble correlationX1Y();  \nReturn correlation value between X1 and Y\n\ndouble correlationX2Y();  \nReturn correlation value between X2 and Y\n\ndouble correlationX1X2();  \nReturn correlation value between X1 and X2\n\nvoid reset();  \nReset values. Start learning since zero.\n\nvoid parameters(double values[]);\nReturn parameters of the regression y = b1x1 + b2x2 + a\n\nvalues[0] = b1;\nvalues[1] = b2;\nvalues[2] = a;\n\n\ndouble error(double x1, double x2, double y);  \nReturn estimation error. If you need more options to calculate error you can use error module from SimpleStatisticsArduino\n\nvoid samples();\nReturn number of examples learned\n'], 'url_profile': 'https://github.com/cubiwan', 'info_list': ['50', 'Python', 'MIT license', 'Updated Oct 8, 2020', '24', 'C#', 'Updated Mar 18, 2020', '9', 'C++', 'GPL-3.0 license', 'Updated Jan 21, 2021', '4', 'Julia', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', '4', 'Python', 'MIT license', 'Updated Dec 22, 2020', '3', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['COVID Dashboard  \\\nJulia dashboard following exponential growth for Coronavirus (COVID-19) new cases.\nOur goal is to examine countries with steepest curves.\nR script data/data-wrang.R downloads and formats data taken from John\'s Jopkins compilation here and outputs to data/covid-long16Mar.csv which is later used by the Matte.jl dashboard defined in app.jl.\nRun\n~$git clone https://github.com/fargolo/covid-dash.git\nCloning into \'covid-dash\'...\nremote: Enumerating objects: 16, done.\n(...)\n~$julia\njulia>cd(""covid-dash"")\nEnter the Pkg REPL by pressing ""]"":\n(v1.3) pkg> activate .\nActivating environment at `~/covid-dash/Project.toml`\n(covid-dash)pkg> instantiate \njulia> using Matte\njulia> using Revise\njulia> includet(""app.jl"")\njulia> run_app(CovidDash)\nWeb Sockets server running at 127.0.0.1:8001 \nWeb Server starting at http://127.0.0.1:8000 \nWeb Server running at http://127.0.0.1:8000 \n\n'], 'url_profile': 'https://github.com/fargolo', 'info_list': ['50', 'Python', 'MIT license', 'Updated Oct 8, 2020', '24', 'C#', 'Updated Mar 18, 2020', '9', 'C++', 'GPL-3.0 license', 'Updated Jan 21, 2021', '4', 'Julia', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', '4', 'Python', 'MIT license', 'Updated Dec 22, 2020', '3', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['50', 'Python', 'MIT license', 'Updated Oct 8, 2020', '24', 'C#', 'Updated Mar 18, 2020', '9', 'C++', 'GPL-3.0 license', 'Updated Jan 21, 2021', '4', 'Julia', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', '4', 'Python', 'MIT license', 'Updated Dec 22, 2020', '3', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['50', 'Python', 'MIT license', 'Updated Oct 8, 2020', '24', 'C#', 'Updated Mar 18, 2020', '9', 'C++', 'GPL-3.0 license', 'Updated Jan 21, 2021', '4', 'Julia', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', '4', 'Python', 'MIT license', 'Updated Dec 22, 2020', '3', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['50', 'Python', 'MIT license', 'Updated Oct 8, 2020', '24', 'C#', 'Updated Mar 18, 2020', '9', 'C++', 'GPL-3.0 license', 'Updated Jan 21, 2021', '4', 'Julia', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', '4', 'Python', 'MIT license', 'Updated Dec 22, 2020', '3', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'seoul', 'stats_list': [], 'contributions': '347 contributions\n        in the last year', 'description': ['linear_regression_project\n한국 개봉영화 관람객 예측 프로젝트\n\n팀구성\n김민혜, 배빛나, 송이준\n\n\nGoal\n개봉한 영화의 관람객 수 예측하기\n\nTechnical Skills\npython, scikit-learn, statsmodels, matplotlib, pandas\n\nProcedure\n\n주제선정\n기존의 연구동향 파악(선행연구 논문 읽기)\n데이터수집&EDA\n회귀분석 모델링\n최종 모델을 통한 에측\n\n\n\nProject Details\n1. 영화흥행 예측모델 생성을 목표로 정하고, 종속변수는 관객수로 하기로한다. (독립변수는 2015~2019년간의 반기별 top100 영화)\n\n영화 흥행의 척도는 크게 매출액 or 관객수\n하지만 매출액을 기준으로 하면 프라임 시간대를 배정받는 영화와 그렇지 않은 영화를 구분해야 할 뿐 아니라, 물가 상승률 또한 고려해야 한다(정확한 예측이 힘들다)\n따라서 관객수를 종속변수로 하기로 한다\n\n\n\n2. 선행 연구논문들을 통해 영화흥행에 영향을 미치는 요인들에 어떤한 것들이 있는지 파악하고 관련 데이터를 수집한다.(최대한 많은 컬럼을 수집하는 것을 1차 목표로 한다)\n\n배우 파워, 감독 파워, 장르, 국적, 마케팅(신문 기사 홍보 등..), 배급사 파워, 개봉시점, 원작여부, 실화바탕 여부, 관람객 평점, 할당받은 상영관 갯수 등...\n\n\n\n\n3. EDA를 통해 수집한 데이터들을 한번 더 재 가공한다.\n\n범주형 변수의 경우, 특징(컬럼)의 갯수를 줄여서 과접합을 막고 모델이 복잡해지는 것을 방지한다\n도메인에 대한 서칭을 통해 이 과정이 적합한지, 해서는 안될 특별한 이유가 없는지 확인함\n또한 실제로 모델간 비교를 통해 이러한 범주형 변수의 범위 축소과정이 더 나은 선택이라는 것을 확인하였음\n\n\n\n\n\n4. try&error 과정을 통해 모델링을 진행한다.\n\nstatsmodels의 r-squared, kfold 교차검증 r-squared, test r-squared를 향상시키는 방향으로 모델을 발전시켜나간다\n\n\n\n5. 완전히 새로운 데이터로 영화 관객수 예측해보기(최종 모델 사용)\n\n생각보다 굉장히 잘나와서 놀람...\n\n\n\n\nPresentation ppt\nhttps://bit.ly/3dYX7cV\n\n'], 'url_profile': 'https://github.com/yeejun007', 'info_list': ['50', 'Python', 'MIT license', 'Updated Oct 8, 2020', '24', 'C#', 'Updated Mar 18, 2020', '9', 'C++', 'GPL-3.0 license', 'Updated Jan 21, 2021', '4', 'Julia', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', '4', 'Python', 'MIT license', 'Updated Dec 22, 2020', '3', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""\n\n\namorf - A Multi-Output Regression Framework\nThis Project is still a work in Progress\namorf is a Python library for multi-output regression. It combines several different approaches to help you get started with multi-output regression analysis.\nThis project was created as part of a masters thesis by David Hildner\nMotivation\nMulti-output (or multi-target) regression models are models with multiple continuous target variables.\nThis framework was largely inspired by\n\nBorchani, Hanen & Varando, Gherardo & Bielza, Concha & Larranaga, Pedro. (2015). A survey on multi-output regression. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. 5. 10.1002/widm.1157.\n\nThis framework/library aims to collect and combine several different approaches for multi-output regression in one place. This allows you to get started real quick and then extend and tweak the provided models to suit your needs.\nGetting Started\nInstallation\nUse the package manager pip to install amorf.\npip install amorf\nUsage\nimport amorf.neuralNetRegression as nnr \nfrom amorf.metrics import average_relative_root_mean_squared_error as arrmse\n\n# for data generation\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_regression(n_samples=10000, n_features=12, n_targets=3, noise=0.1) \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nregressor = nnr.NeuralNetRegressor(patience=5, training_limit=None) #initialize neural net regressor\nregressor.fit(X_train, y_train) #fit regressor to training data\nprediction = regressor.predict(X_test) #predict test data \nprint(arrmse(prediction, y_test)) #print error\nDocumentation\nThe documentation is hosted via ReadTheDocs\nRunning The Tests\nClone repository\ngit clone https://github.com/DSAAR/amorf/\nChange directory\ncd amorf/\nDiscover and run tests\npython -m unittest discover -s tests -p 'test_*.py'\nAuthors\n\nDavid Hildner - Student at Eberhard-Karls Universität Tübingen\n\nLicense\nMIT License\n""], 'url_profile': 'https://github.com/DSARG', 'info_list': ['50', 'Python', 'MIT license', 'Updated Oct 8, 2020', '24', 'C#', 'Updated Mar 18, 2020', '9', 'C++', 'GPL-3.0 license', 'Updated Jan 21, 2021', '4', 'Julia', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', '4', 'Python', 'MIT license', 'Updated Dec 22, 2020', '3', 'Jupyter Notebook', 'Updated May 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': [""Heart-Disease-Prediction\n#Logistic Regression  #Decision Tree  #Random Forest  #Boosting\nThe features of the data are:\nage: The person's age in years \nsex: The person's sex (1 = male, 0 = female)\ncp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\ntrestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\nhol: The person's cholesterol measurement in mg/dl\nfbs: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\nrestecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite\n      left ventricular hypertrophy by Estes' criteria)\nthalach: The person's maximum heart rate achieved\nexang: Exercise induced angina (1 = yes; 0 = no)\noldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\nslope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\nca: The number of major vessels (0-3)\nthal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\ntarget: Heart disease (0 = no, 1 = yes)\n\nTo use these features properly, we apply one-hot encode categorical features.\n""], 'url_profile': 'https://github.com/Nada-Zaki', 'info_list': ['50', 'Python', 'MIT license', 'Updated Oct 8, 2020', '24', 'C#', 'Updated Mar 18, 2020', '9', 'C++', 'GPL-3.0 license', 'Updated Jan 21, 2021', '4', 'Julia', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Jul 7, 2020', '4', 'Python', 'MIT license', 'Updated Dec 22, 2020', '3', 'Jupyter Notebook', 'Updated May 21, 2020']}"
"{'location': 'Banglore', 'stats_list': [], 'contributions': '341 contributions\n        in the last year', 'description': ['Visibility_Climate\nTo build a regression model to predict the visibility distance based on the given different climatic indicators in the training data.\nData Description\nData Description: This dataset predicts the visibility distance based on the different indicators as below:\n\nVISIBILITY - Distance from which an object can be seen.\nDRYBULBTEMPF-Dry bulb temperature (degrees Fahrenheit). Most commonly reported standard temperature.\nWETBULBTEMPF-Wet bulb temperature (degrees Fahrenheit).\nDewPointTempF-Dew point temperature (degrees Fahrenheit).\nRelativeHumidity-Relative humidity (percent).\nWindSpeed-Wind speed (miles per hour).\nWindDirection-Wind direction from true north using compass directions.\nStationPressure-Atmospheric pressure (inches of Mercury; or ‘in Hg’).\nSeaLevelPressure- Sea level pressure (in Hg).\nPrecip\tTotal-precipitation in the past hour (in inches).\n\nApart from training files, we also require a ""schema"" file from the client, which contains all the relevant information about\nthe training files such as:\nName of the files, Length of Date value in FileName, Length of Time value in FileName, Number of Columns, Name of the Columns,\nand their datatype.\nAuthor\nKrishna Heroor\n'], 'url_profile': 'https://github.com/heroorkrishna', 'info_list': ['2', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['2', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['2', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['linear-regression-london-ds\n\n\nLinear Regression\n\n\nExit Ticket | Introduction to Linear Regression\n\n\nLearning Goals\nIntroduction to Linear Regression\nStudent should be able to:\n\n Identify problems where linear regression can be applied\n Explain what a statistical model is in simple language\n Describe what kind of DV is needed for linear regression\n Describe what kind of IVs can be used for linear regression\n Define and describe a line of best fit\n Define what a residual is\n Describe what the slope of the model represents\n Run a linear regression model using statsmodels with a single continuous IV.\n\nLesson Outlines\n\n90 Minutes; 45 minute break\n\n\nComing Soon\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['2', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['COVID-19_pandemic_visualization_and_regression\n'], 'url_profile': 'https://github.com/sdittmer', 'info_list': ['2', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['multiple-linear-regression-london-ds\n\n\nMultiple Linear Regression\n\n\nExit Ticket | Multiple Regression\n\n\nMultiple Linear Regression + Assumptions\nBy the end of the lesson, the student should be able to:\n\n Run a multiple linear regression model using statsmodels\n Explain how to interpret a coefficient in context of multiple regression\n Explain how the idea of a hyperplane relates to bi-variate multiple regression\n Run a multiple linear regression model using statsmodels with multiple continuous variables\n Run a multiple linear regression model using statsmodels with multiple continuous and categorical variables.\n Explain what a dummy coded variable is in the context of linear regression\n Interpret the coefficients, R2, p values, confidence intervals of each predictor variable\n Explain what it means for observations to be independent\n\nOutline\n\n90 Minutes; 45 min break\n\n\nReview and New Concepts\nDefine Terms\n\nR2\nCoeffeient Table\nCategorical Predictors\n\n\nAssumptions\nBreak\nMultiple Regression Notebook\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['2', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['COVID-19 Regression Analysis\nThis script uses the latest data from\n2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository by Johns Hopkins CSSE to form  a completely naïve regression analysis.\nIANAEpidemiologist. IANYEpidemiologist. This is not medical advice.\n\n'], 'url_profile': 'https://github.com/jsharkey', 'info_list': ['2', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bigdatacr', 'info_list': ['2', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '709 contributions\n        in the last year', 'description': ['Regression\nContains all the regression techniques performed\n'], 'url_profile': 'https://github.com/Dharana23', 'info_list': ['2', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '749 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/vibhutivadje', 'info_list': ['2', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Oct 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Python', 'Apache-2.0 license', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['regression\n'], 'url_profile': 'https://github.com/piacadar', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', '3', 'R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hellohangzhang', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', '3', 'R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'New Delhi, India', 'stats_list': [], 'contributions': '401 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nscikit library used for linear regression model\npandas library used for reading data frames\nmatplotlib library used for any plotting\n'], 'url_profile': 'https://github.com/siddharthchd', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', '3', 'R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SURYANSH05', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', '3', 'R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Regression\n'], 'url_profile': 'https://github.com/ae144de', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', '3', 'R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mausoleoo', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', '3', 'R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '389 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Khamparia1988', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', '3', 'R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['LinearRegressionTOPolynomialRegression\nLinear Regression to polynomial regression\n'], 'url_profile': 'https://github.com/XinWenRuYi', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', '3', 'R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['minIONLogReg\nCorrection of Qscore of Nanopore reads using logistic regression (manuscript Espada et al.)\nHow to use it\nThis is an R package and an R code.\nFirst download the a compress version (zip or.tar.gx) and install it: in R type install.packages(""path_to_minIONLogReg.tar.gx"", repos=NULL).\nSecond, edit main_minIONLogReg.r to add your inputs and run the command lines.\n'], 'url_profile': 'https://github.com/rocioespci', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', '3', 'R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Learning-to-Rank\nLinear Regression\n'], 'url_profile': 'https://github.com/AishwaryaMano', 'info_list': ['Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', '3', 'R', 'MIT license', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashwindeena', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '2', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'SAHARANPUR', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Titanic-Dataset\nThis file contains how to check a seperate test file in any ML model\nLogistic Regression\n'], 'url_profile': 'https://github.com/sakshij25', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '2', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['LogisticRegression\nHere Logistic regression is used for classifying the type of Iris plant.\nI have used Iris data set from https://archive.ics.uci.edu/ml/datasets/iris.\nAnd was able to achieve accuracy of 90%.\n'], 'url_profile': 'https://github.com/ninadtongay', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '2', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '2', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon’s Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '2', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['Concrete-Compressive-Strength-\nCompressive strength or compression strength is the capacity of a material or structure to withstand loads tending to reduce size, as opposed to tensile strength, which withstands loads tending to elongate.\nCompressive strength is one of the most important engineering properties of concrete. It is a standard industrial practice that the concrete is classified based on grades. This grade is nothing but the Compressive Strength of the concrete cube or cylinder. Cube or Cylinder samples are usually tested under a compression testing machine to obtain the compressive strength of concrete. The test requisites differ country to country based on the design code.\nHere, we used some regression models to apply them on this data and find the best one of them to fit the data\n'], 'url_profile': 'https://github.com/Nada-Zaki', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '2', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['\nSplit training data in 3 folds\n\ntraining_data_1, prices_1 = training_data[:num_samples_fold], \\\n                            prices[:num_samples_fold]\n\ntraining_data_2, prices_2 = training_data[num_samples_fold: 2 * num_samples_fold], \\\n                            prices[num_samples_fold: 2 * num_samples_fold]\n\ntraining_data_3, prices_3 = training_data[2 * num_samples_fold:], \\\n                            prices[2 * num_samples_fold:]\n\n\nDefine normalizing method using the standard scaler from sklearn\n\ndef normalize(train_data, test_data):\n\nscaler = preprocessing.StandardScaler()\n\n\nDefine function that trains the model depending on the regression, calculates predictions based on the normalized data and returns the mean absolute error and mean square error\n\nreg = model.fit(norm_train, train_labels)\n\npredict = reg.predict(norm_test)\n\nmae = mean_absolute_error(test_labels, predict)\n\nmse = mean_squared_error(test_labels, predict)\n\n\n\nCalculate values for the ridge regression with alpha taking 4 values: 1, 10, 100, 1000\n\n\nFor the best performing alpha calculate values for the ridge regression on the whole training data\n\n\nmodel = Ridge(10)\n\nscaler = preprocessing.StandardScaler()\n\nscaler.fit(training_data)\n\nnorm_train = scaler.transform(training_data)\n\nmodel.fit(norm_train, prices)\n\n'], 'url_profile': 'https://github.com/iuliaapopescu', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '2', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tiankongyushang', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '2', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['Simple and Multiple Linear Regression models in Python and R, following tutorials\ncreated by Kirill Eremenko and Hadelin de Ponteves\n'], 'url_profile': 'https://github.com/SolaGbenro', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '2', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'Seattle', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AakankshaBaid', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '2', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated May 21, 2020', 'Python', 'Updated Mar 31, 2020', 'R', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'R', 'Updated Mar 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Avinash308', 'info_list': ['Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'Guatemala', 'stats_list': [], 'contributions': '502 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rmadrazo97', 'info_list': ['Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '214 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/riyagoel192', 'info_list': ['Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wxiwang', 'info_list': ['Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Walmart-sales-prediction\nMultiple linear regression\n'], 'url_profile': 'https://github.com/lorenzdes', 'info_list': ['Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Ames Housing Project\nData Source and data dictionary --> https://www.kaggle.com/c/dsi-us-10-project-2-regression-challenge/data\nGOAL - Way to predict a house price ~ actuals\n-- Choose attributes to formulate algorithms using past data\n-- To test the theory , use the algorithm found, to predict the price of known houses and check how it fared by measuring errors.\n-- Tweak the algorithm to reduce the errors as much as possible\n-- Keep testing to the extent possible to find the best fit model (repeat)\n-- Actual predictions using the model(s) within a threshold of +/- $20,000\n-- Publish and present the findings\nHow to begin?\n-- Data cleaning\n-- Checking for missing values\n-- checking for incorrect values\n-- start exploring the data using data dictionary\n-- plot the distribution of your feature\nPredictions\n-- Base line predictions using mean\n-- Predictions using linear Regression. (Note: Tried RidgeCV, Lasso, but the scores did not improve so removed those from the list)\n-- Feature Selections using co-relation, statsmodels (OLS)\n-- model creation\n-- using metrics to score the model\n-- using the model to test kaggle\nBusiness Recommendations\n\nFeatures appear to add the most value to a home : Overall quality , living area , age of the house, lot area, have a wood deck , open porch , ratio of bedroom to bathroom , garage area\nWhich features hurt the value of a home the most : Pool related data did not impact the housing prices.\nWhat are things that homeowners could improve in their homes to increase the value? - mostly over all quality of the house, remodelling helped house prices.\nWhat neighborhoods seem like they might be a good investment? Stone Brook, North Ames, North Ridge\nDo you feel that this model will generalize to other cities? How could you revise your model to make it more universal OR what date would you need from another city to make a comparable model? - Cannot generalise to other cities. It can be only used as a baseline to think.\n\nPresentation :\nLink\n'], 'url_profile': 'https://github.com/gourikrishnamoorthy', 'info_list': ['Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': '桃園中壢', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['Linear Regression Model\nA linear regression model\n'], 'url_profile': 'https://github.com/gpwork4u', 'info_list': ['Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogistic Regression with python\n'], 'url_profile': 'https://github.com/giramonikranthi', 'info_list': ['Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'Queensland', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Advance-Regression-for-HousePrice\nAdvance Regression for HousePrice\n'], 'url_profile': 'https://github.com/markhsieh42', 'info_list': ['Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'Koblenz, Germany', 'stats_list': [], 'contributions': '186 contributions\n        in the last year', 'description': ['Project Title\nTickets pricing monitoring system.\nDescription\nThis is a tickets pricing monitoring system. It scrapes tickets pricing data periodically and stores it in a database. Ticket pricing changes based on demand and time, and there can be significant difference in price. We are creating this product mainly with ourselves in mind. Users can set up alarms using an email, choosing an origin and destination (cities), time (date and hour range picker) choosing a price reduction over mean price, etc.\nThis project is part of my Data Science and Machine Learning course.\n'], 'url_profile': 'https://github.com/ifaizankhan', 'info_list': ['Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mausoleoo', 'info_list': ['Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gokul-srinath', 'info_list': ['Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SURYANSH05', 'info_list': ['Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'Nagpur', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['Simple_Logistic_Regression\nSimple Logistic Regression\n'], 'url_profile': 'https://github.com/shivamtawari', 'info_list': ['Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['linear_regression_model\nAnalysis of Linear Regression\n'], 'url_profile': 'https://github.com/joezhou12', 'info_list': ['Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'Ontario, Canada', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kaleembukhari90', 'info_list': ['Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'Nagpur', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['Weather_Logistic_Regression\nWeather Prediction Logistic Regression\n'], 'url_profile': 'https://github.com/shivamtawari', 'info_list': ['Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Analytical Method\nMulti Variate linear regression using python\nLibrary used\n\nNumpy\nPandas\n\n'], 'url_profile': 'https://github.com/SUJITHhubpost', 'info_list': ['Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'North Macedonia', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['keras_project\nRegression Model in Keras\n'], 'url_profile': 'https://github.com/zokigorgiev83', 'info_list': ['Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['air-quality-regression\nThis code explores the dependence of Beijing air pollution quantity on month using a variety of regression models\nin scikit-learn (linear regression, polynomial regression, Support Vector Regression and Ensemble Regression).\nTo run the code, simply run the python script:\npython air_quality_regression.py\nThe default regression model is polynomial regression. To change between different regression models, change the\n""method"" option.\nThe air quality data is from:\nhttp://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data\nThe relevant paper is:\nLiang, X., Zou, T., Guo, B., Li, S., Zhang, H., Zhang, S., Huang, H. and Chen, S. X. (2015). Assessing Beijing\'s PM2.5 pollution: severity, weather impact, APEC and winter heating. Proceedings of the Royal Society A, 471, 20150257.\n'], 'url_profile': 'https://github.com/andrew-griffin', 'info_list': ['Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Oct 21, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'Updated Mar 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abilkas', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Elixir', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'BSD-2-Clause license', 'Updated Feb 2, 2021', 'R', 'Updated Mar 21, 2020', 'Updated Mar 18, 2020', 'CSS', 'Updated May 1, 2020', 'R', 'Updated May 27, 2020', 'C++', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 15, 2020', 'MIT license', 'Updated Mar 17, 2020']}","{'location': 'Alameda, CA', 'stats_list': [], 'contributions': '3,764 contributions\n        in the last year', 'description': ['TestMatches\nTODO: Add description\nInstallation\nIf available in Hex, the package can be installed\nby adding test_matches to your list of dependencies in mix.exs:\ndef deps do\n  [\n    {:test_matches, ""~> 0.1.0""}\n  ]\nend\nDocumentation can be generated with ExDoc\nand published on HexDocs. Once published, the docs can\nbe found at https://hexdocs.pm/test_matches.\n'], 'url_profile': 'https://github.com/ityonemo', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Elixir', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'BSD-2-Clause license', 'Updated Feb 2, 2021', 'R', 'Updated Mar 21, 2020', 'Updated Mar 18, 2020', 'CSS', 'Updated May 1, 2020', 'R', 'Updated May 27, 2020', 'C++', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 15, 2020', 'MIT license', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': [""Efficient multi-target ridge regression\nNOTE: This code was written by Storm Slivkoff in 2016. It is a subset of the Gallant lab analysis codebase. More recently, the lab has moved on to GPU multi-kernel methods. This repo is being shared for expository purposes.\nThis package contains many algorithms for fitting cross-validated ridge regression models, particularly for the case where there are many regression targets that need to be solved in parallel.\nFiles of Interest\n\nnotebooks/\n\nridge_benchmarks/ speed and precision benchmarks of regression code\nridge_examples/ examples of how to use the code\nnpfast/ speed and precision benchmarks of npfast functions\n\n\nregression_code/storm/\n\ncv.py code for various types of cross-validation\nridge.py implements main solve_ridge function\nsolvers.py contains many optimized algorithms for solving the ridge problem\nnpfast reimplementation of numpy functions for speed (see below)\n\n\n\nRidge Regression\nBackground\nRidge regression is a type of regularized linear regression. Whereas ordinary least squares finds a set of weights  that minimizes the quantity , ridge regression instead finds a   that minimizes the quantity . Here  is a regularization hyperparameter.\nSolving Ridge regression\nRidge regression problems can be solved efficiently because they have a known analytical solution, . With the right numerical linear algebra methods, it is possible to efficiently obtain solutions for large numbers of  values and  regressands. The solution given above works well when there are more samples than features in , but for cases where there are more features than samples, the equivalent kernel formulation of the solution () can be used for efficiency.\nThere are many algorithms that can be used to obtain .\nThe optimal method depends on the sizes of the input matrices, the number of  values that need to be evaluated, and the required levels of precision and stability. For example, Cholesky decompositions are the fastest way to solve the problem for a single  value, but they can lead to significant instability. Using spectral decompositions and singular value decompositions are fastest when solving a large number of  values because they can express the  inversion as a simple matrix product. QR decompositions are a relatively fast and stable way to solve the problem for single  values. The regression_code.storm.solvers module documents these algorithms in extensive detail and provides heuristics for when each of them should be used.\nCross Validation\nThe value of  can significantly affect the quality of the model, especially in regimes of high noise and/or liited numbers of samples. In many cases, there will be an optimal value of  that produces the lowest error in the quantity . To find this optimal value, cross validation (or other related procedures) should be used. In particular there should be a separation between fit and test sets to avoid overfitting. The code in regression_code.storm.cv provides utilities for performing different types of cross validation, including nested cross-validation and temporally-aware data chunking to account for possible autocorrelation in the input data.\nnpfast\nNOTE: These functions were developed as improvements to numpy 1.11.1. There have since been many improvements to numpy that render some of these npfast functions obsolete (including a pull request where I merged a small optimization from npfast upstream to numpy). Again, this repo is shared for expository purposes. You should perform your own benchmarks to determine whether any of these techniques will be useful for your particular use case.\nThis package implements faster versions of common numpy functions. These improvements were possible because the native numpy versions were either single-threaded or created unnecessary matrix copies.\nTechniques for speedup\n\ncreate multi-threaded versions of functions Functions like sum() and mean() are single-threaded in numpy. However, they can be expressed in terms of vector operations, allowing them to take advantage of numpy's multi-threaded machinery. This can come with a precision penalty for certain matrix sizes (see notebook below).\navoid creating unncessary matrix copies Multi-step operations such as standard deviation, zscores, or Pearson's correlation are computed in multiple steps. Using in-place operations for the intermediate steps and/or the final step can save time when dealing with large matrices.\nuse numexpr numexpr is a 3rd party library that uses its own op-codes to perform sequences of array operations. These op-codes are multi-threaded and can reduce unnecessary matrix copies.\n\nFunctions\n\nArray Functions: astype(), copy(), nan_to_num(), isnan()\nFunctions implemented: sum(), mean(), nanmean(), std(), zscore(), correlate(), multi_dot()\n\nOther numpy optimizations you should do\n\navoid loops, or if you must loop, use numba\nuse broadcasting\nuse np.linalg.multi_dot when multiplying 3 or more matrices\nuse in-place operators and out arguments when appropriate\nmeasure whether openBLAS / MKL / BLIS work best for your cpu\n\n""], 'url_profile': 'https://github.com/sslivkoff', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Elixir', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'BSD-2-Clause license', 'Updated Feb 2, 2021', 'R', 'Updated Mar 21, 2020', 'Updated Mar 18, 2020', 'CSS', 'Updated May 1, 2020', 'R', 'Updated May 27, 2020', 'C++', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 15, 2020', 'MIT license', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['STA141CFinal\nImplementation of Multiple Linear Regression using BLB\nFinal Project for STA 141C at UC Davis, Winter 2020\nWilliam Shih, Ricardo Simpao, Nilay Varshney, Luke Yee\nThis R package provides functions for fitting linear regression models on datasets with continuous response variables through the use of Bag of Little Bootstraps. Users can determine regression coefficients, estimate the variance of errors, and predict new data. Users also have the option to use parallel computing.\nThe dataset used in the user guide can be found at the link below. Note that the three rightmost columns are to be disregarded as the dataset has four response variables.\narchive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance\nTestRandC.pdf file shows the results of benchmarking basic R and C functions to see what to use for the final package.\nUser_Guide.pdf file shows the user guide on how to use the package.\npackage_demo.pdf benchmarks the 8 R functions in the package versus the 4 C++ functions in the package.\nThere are 8 R files corresponding to 8 R functions:\nPI.R\ncoef_CI.R\nlinear_reg_bs.R\ns2_CI.R\nPI_par.R\ncoef_CI_par.R\nlinear_reg_bs_par.R\ns2_CI_Par.R\nThere are 4 C++ files corresponding to 4 C++ functions:\nPI_C.cpp\ncoef_CI_C.cpp\nlinear_reg_bs_C.cpp\ns2_CI_C.cpp\n'], 'url_profile': 'https://github.com/STA141c-LNRW', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Elixir', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'BSD-2-Clause license', 'Updated Feb 2, 2021', 'R', 'Updated Mar 21, 2020', 'Updated Mar 18, 2020', 'CSS', 'Updated May 1, 2020', 'R', 'Updated May 27, 2020', 'C++', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 15, 2020', 'MIT license', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '182 contributions\n        in the last year', 'description': ['bake-off\n'], 'url_profile': 'https://github.com/bellisor', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Elixir', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'BSD-2-Clause license', 'Updated Feb 2, 2021', 'R', 'Updated Mar 21, 2020', 'Updated Mar 18, 2020', 'CSS', 'Updated May 1, 2020', 'R', 'Updated May 27, 2020', 'C++', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 15, 2020', 'MIT license', 'Updated Mar 17, 2020']}","{'location': 'Zürich', 'stats_list': [], 'contributions': '165 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mgreter', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Elixir', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'BSD-2-Clause license', 'Updated Feb 2, 2021', 'R', 'Updated Mar 21, 2020', 'Updated Mar 18, 2020', 'CSS', 'Updated May 1, 2020', 'R', 'Updated May 27, 2020', 'C++', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 15, 2020', 'MIT license', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HM-Builds', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Elixir', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'BSD-2-Clause license', 'Updated Feb 2, 2021', 'R', 'Updated Mar 21, 2020', 'Updated Mar 18, 2020', 'CSS', 'Updated May 1, 2020', 'R', 'Updated May 27, 2020', 'C++', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 15, 2020', 'MIT license', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': [""Simple header-only library of 1-D Full Gaussian Process Regression model.\n\nFeatures:\n\npredictions with noise-free observations\npredictions with  noisy observations\nestimation the kernels parameters using likelihood maximization\n\nImplemented kernels:\nRadial Basis Function\n\nMatern 3/2\n\nMatern 3/2 periodic, based on realisation https://github.com/mblum/libgp\n\nMatern 5/2\n\nRational Quadratic\n\nExtended kernels for noisy observations\nfor noisy observations , where \nfor with case kernels represented as  where \nDependencies:\n\nEigen 3.x\nNLopt 2.6.x\n\nThird party\n\nFindNLopt.cmake from here https://github.com/dartsim/dart\ncxxopts command line parser\n\nTest application\n\ninput CSV file: sample, observations, with space delimiters\noutput CSV file: samples, predictions, variances\n\nTest application keys\n\ni,input   -input   CSV file with  the {sample 'space, \\t' observation} structure\no,output  -output CSV file with  the  {sample 'space' prediction 'space' variance}  structure\nk,kernel  -{rbf, matern32, matern32p, matern52, rq} covariance kernel function, default = rbf\ns,sigma   -kernel bandwidth parameter, default = 1.0\nl,length  -kernel characteristic length scale, default = 1.0\np,period  -period for Matern 3/2 kernel, default = 1.0\na,alpha   -rational quadratic kernel degree, default =1.0\nn,noise   -{true, false} GPR model for noisy observations and  noise-free observations, default = true\ne,epsilon -prior error level, default  =.01\nj,jitter  -jitter,default =1e-08\nf,factor  -upsampling factor,default = 10.0\nm,max     -optimization solver maximum  iteration numbers, default=100.\nt,optimization -{gradient, direct} optimization  solver type for the kernel parameters, default = direct\n\nExperiments\n\nwith keys --input=data/parameter.dat --kernel=matern52\n""], 'url_profile': 'https://github.com/ToyOwl', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Elixir', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'BSD-2-Clause license', 'Updated Feb 2, 2021', 'R', 'Updated Mar 21, 2020', 'Updated Mar 18, 2020', 'CSS', 'Updated May 1, 2020', 'R', 'Updated May 27, 2020', 'C++', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 15, 2020', 'MIT license', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shanky-21', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Elixir', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'BSD-2-Clause license', 'Updated Feb 2, 2021', 'R', 'Updated Mar 21, 2020', 'Updated Mar 18, 2020', 'CSS', 'Updated May 1, 2020', 'R', 'Updated May 27, 2020', 'C++', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 15, 2020', 'MIT license', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['RegressionModels\nRepo containing course project for the Regression Models class of the JHU data science specialization\n'], 'url_profile': 'https://github.com/izaakjephsonbw', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Elixir', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'BSD-2-Clause license', 'Updated Feb 2, 2021', 'R', 'Updated Mar 21, 2020', 'Updated Mar 18, 2020', 'CSS', 'Updated May 1, 2020', 'R', 'Updated May 27, 2020', 'C++', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Sep 15, 2020', 'MIT license', 'Updated Mar 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['RegressionData\n'], 'url_profile': 'https://github.com/ViktorijaPr', 'info_list': ['Updated Mar 18, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Aug 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/negar67', 'info_list': ['Updated Mar 18, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Aug 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['MachineLearning\nLogistic Regression\n-Confusion matrix\n-precission score\n-recall score\n-f1 score\nSupport Vector Regression\nDecision Tree\n#Tring to calculate category of pictures\n#training data is too large for github so this link is for train.csv\nhttps://drive.google.com/open?id=1mDy6QuWXo0Esbhr66atBrs7zMNUB8CLC\n'], 'url_profile': 'https://github.com/semihyilmazz', 'info_list': ['Updated Mar 18, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Aug 25, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Auto-LR_Report\nAutomate the LoadRunner Report for Regression Testing\n'], 'url_profile': 'https://github.com/logickiran', 'info_list': ['Updated Mar 18, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Aug 25, 2020']}","{'location': 'Seattle', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AakankshaBaid', 'info_list': ['Updated Mar 18, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Aug 25, 2020']}","{'location': 'Bracknell, England', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['House-Prices-Advanced\nKaggle Competition - House Prices: Advanced Regression Techniques\n'], 'url_profile': 'https://github.com/kamran-sheikh', 'info_list': ['Updated Mar 18, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Aug 25, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '149 contributions\n        in the last year', 'description': ['Credit Card Approval Prediction: Project Overview\n\n\nThe project aims to predict how likely a credit card request will get approved based on age, gender, credit score, income, debt, etc.\nExplored the data to understand the relationship of various features with the target (Approval Status)\nImputed the missing values with mean imputation for the numeric features and  mode imputation to categorical features\nImplemented MinMax Scaling to the features.\nOptimized Logistic Regression using GridsearchCV to obtain the best model.\n\nDataSet Source:\nThe dataset is Credit Card Approval dataset from the UCI Machine Learning Repository. The data consists of 16 columns and 160 records.\nResources Used\nPython Version: 3.7 \nPackages Used: Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn\nModel Building\nBuilt a dataframe for our model with relevant columns.\nTransformed the categorical variables into dummy variables. In addition, the data was split into train and test set.\nPerformed classification analysis using Logistic Regression and used GridSearchCV to increase accuracy.\nModel Performance\n\nLogistic Regression: Accuracy = 85.99%\nUsing GridSearchCV : Accuracy = 85.99%\n\nConclusion\nOn applying the logistic model, we have achieved the accuracy of 86% which is significantly high for predicting how likely a credit card request will get approved.\n'], 'url_profile': 'https://github.com/nanthasnk', 'info_list': ['Updated Mar 18, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Aug 25, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '352 contributions\n        in the last year', 'description': ['DS4400 Final Project\n""Next came Dionysus, the son of the virgin, bringing the counterpart to bread:\nwine and the blessings of life\'s flowing juices. His blood, the blood of the grape,\nlightens the burden of our mortal misery.""  — Euripides\nWho?\nThis project was completed by Jay Sherman and Michael Wheeler for Northeastern\'s\nDS4400: Introduction to Machine Learning 1, taught by Prof. Ehsan Elhamifar in the Spring of 2020.\nWhat?\nWe use basic machine learning methods to determine if the color and quality of a wine can be\npredicted from its physical properties. We examine a dataset including red/white classification and\na rating of quality on a 10-point scale. We use linear regression models and basis function expansions\nto predict the rating of each wine. We also use logistic regression and support vector\nmachines to classify a wine by color.\nHow?\nTo run the code:\n\nClone the repository from GitHub: git clone https://github.com/mikewheel/ds4400-final-project\nCreate and activate a Python 3.8 virtual environment: python3.8 -m venv virtualenv && source virtualenv/bin   /activate\nInstall the project requirements: pip install -r requirements.txt\nExecute training: python3 -m main\n\n'], 'url_profile': 'https://github.com/mikewheel', 'info_list': ['Updated Mar 18, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Aug 25, 2020']}","{'location': 'Hyderabad ', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Built a regression accuracy prediction algorithm\n'], 'url_profile': 'https://github.com/siddharthakanchar', 'info_list': ['Updated Mar 18, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Aug 25, 2020']}","{'location': 'Jakarta, Indonesia', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['RM-medical-cost-analysis\n'], 'url_profile': 'https://github.com/tomytjandra', 'info_list': ['Updated Mar 18, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Jun 5, 2020', 'Python', 'Updated Apr 26, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'HTML', 'Updated Aug 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Coding Logistic Regression\nBerikut adalah implementasi Logistic Regression untuk multiclass di Python dari nol.\nAda dua pendekatan, yaitu:\n\nOne-vs-all\nsoftmax\n\n'], 'url_profile': 'https://github.com/patuanPT', 'info_list': ['Jupyter Notebook', 'Updated Sep 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jul 6, 2020', 'JavaScript', 'Updated Jun 8, 2020', 'Go', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['ml_regression_model_evaluation\nMachine Learning project - regression model evaluation:\nVisualized the dependency of the target on each continuous feature for Sydney housing dataset;\nPreprocessed both categorical and continuous variables by using a pipeline;\nEvaluated OLS, Ridge, Lasso and Elastic Net using cross-validation and tuned parameters using grid search;\nAnalyzed the 20 most important coefficients of the resulting models.\nData Source: Sydney housing data from Kaggle.\nData Link: https://www.kaggle.com/shree1992/housedata\n'], 'url_profile': 'https://github.com/xiayunj', 'info_list': ['Jupyter Notebook', 'Updated Sep 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jul 6, 2020', 'JavaScript', 'Updated Jun 8, 2020', 'Go', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YCipriani', 'info_list': ['Jupyter Notebook', 'Updated Sep 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jul 6, 2020', 'JavaScript', 'Updated Jun 8, 2020', 'Go', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020']}","{'location': 'Italy', 'stats_list': [], 'contributions': '106 contributions\n        in the last year', 'description': [""logistic_regression\nSome notes\nThe implemetation of code in this logistic regression is from free video from Andrew Ng's Note.\nCoursera\nYoutube\nSummary test cases\nClassification test\nLogistic regression is based on the propability in order to classify a single data point is belong to which class.\nLogistic regression can be considered as a single neutron using in the neutral networks with many single neutrons, this has ability to classify or regression the dataset. In this example, the classification of 2 classes is presented.\n\nIn the test I generated two classes randomly using sklearn.datasets.make_blobs with each class is assigned with label 1 and 0.\nThen, gradient descent is used with 10000 iterations with learning rate value 0.1.\nThe cost functions versus each iteration is plotted to see how cost function is updated after 200 iterations (for the sake of visualization).\n\nFinally, the plot of 2D line classification is shown to see how good logistic regression is.\n\nAs we can see from the classification, there blue line is the optimum line to separate the class 0 and 1 in the dataset. Of course, there are few data points are misclassified, because the data is linearly non-separable.\n\nNotes\nThere are many advantages of logistic regression are:\n\nThe features are not needed to scale.\nSimple computation and very efficiently.\n\nOne disadvantage of logistic regression is that, they can not seprarate the linearly non-separable dataset. However, to overcome this disadvantage:\n\nSVM with radial basis kernel can be helped.\nStack multiple neutrons of logistic functions to solve for linearly non-separable dataset.\n\n""], 'url_profile': 'https://github.com/MossyFighting', 'info_list': ['Jupyter Notebook', 'Updated Sep 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jul 6, 2020', 'JavaScript', 'Updated Jun 8, 2020', 'Go', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020']}","{'location': 'Indore', 'stats_list': [], 'contributions': '275 contributions\n        in the last year', 'description': ['In this project I am trying to create a basic Air quality prediction model\nMethod used\nI am using linear regression model considering I have a small data\ndataset\ndata is taken from this site\nInstall\n\n\nPandas 0.18.0\nNumpy 1.10.4\nMatplotlib 1.5.1\nScikit-learn 0.17.1\nSeaborn 0.8.0\nflask\n\n\nsaving the model\nto save the model i have used pickle module from sklearn\ndeploying the model\nI have used flask to deploy my machine larning model using basic HTML and CSS.\nthese files can be viewed in template folder and css in static folder\nother than that you can refer app.py to understand how i have deployed my model\nhere are some screen shots of the application\n\n\n'], 'url_profile': 'https://github.com/MananJethwani', 'info_list': ['Jupyter Notebook', 'Updated Sep 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jul 6, 2020', 'JavaScript', 'Updated Jun 8, 2020', 'Go', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020']}","{'location': 'Cambridge, MA', 'stats_list': [], 'contributions': '1,032 contributions\n        in the last year', 'description': ['Tovly GP Logo\nThis is a little art project I made to approximate and animate my name via Gaussian Process Regression. You can view a live demo at https://tovlydeutsch.github.io/gp-logo/ (animates every 4 seconds). Created using d3 and scikit-learn.\n\n'], 'url_profile': 'https://github.com/TovlyDeutsch', 'info_list': ['Jupyter Notebook', 'Updated Sep 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jul 6, 2020', 'JavaScript', 'Updated Jun 8, 2020', 'Go', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ChiragSehra', 'info_list': ['Jupyter Notebook', 'Updated Sep 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jul 6, 2020', 'JavaScript', 'Updated Jun 8, 2020', 'Go', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['LinearRegression\nUsing python for linear regression(1st repo)\nChecking Commit User\n'], 'url_profile': 'https://github.com/harsha-1993', 'info_list': ['Jupyter Notebook', 'Updated Sep 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jul 6, 2020', 'JavaScript', 'Updated Jun 8, 2020', 'Go', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020']}","{'location': 'San Jose, CA', 'stats_list': [], 'contributions': '267 contributions\n        in the last year', 'description': ['Regression\nCoursera Machine Learning Specialization [2/4]\nMachine Learning: Regression\\\nUniversity of Washington\nInstructors: Carlos Guestrin, Emily Fox\n(https://www.coursera.org/learn/ml-regression)\n'], 'url_profile': 'https://github.com/limeunhee', 'info_list': ['Jupyter Notebook', 'Updated Sep 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jul 6, 2020', 'JavaScript', 'Updated Jun 8, 2020', 'Go', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020']}","{'location': 'coimbatore', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['crime-data-analysis\npredicts different crimes rates using multiple regression\nstoring the data in linked list\nusing various sort and search techiques\n'], 'url_profile': 'https://github.com/guhan93', 'info_list': ['Jupyter Notebook', 'Updated Sep 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 30, 2020', '1', 'Jupyter Notebook', 'Updated Jul 6, 2020', 'JavaScript', 'Updated Jun 8, 2020', 'Go', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Nov 24, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ahmedmoawad124', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 20, 2020', 'MIT license', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'SAS', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'shanghai', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': [""PointRegression_hg\nan implementation of hourglassNet based point regression\nmain.py : the training script. including the data generation class, the model generation class and self-defined callbacks for evaluate the point regression precision.\npredict.py: the inference script.\npbpred.py: a CardObjRegress class defined to use the .pb model for card's endpoints regression. The .pb model should be generated by converting the .h5 model\ntrained by main.py\n""], 'url_profile': 'https://github.com/rfdeng', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 20, 2020', 'MIT license', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'SAS', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Its-Hot-Outside\nRegression Analysis of IoT Temperature Readings\n'], 'url_profile': 'https://github.com/psohn', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 20, 2020', 'MIT license', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'SAS', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '138 contributions\n        in the last year', 'description': ['benchmark_map4\nBenchmarking the MAP4 fingerprint in regression models\n'], 'url_profile': 'https://github.com/PatWalters', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 20, 2020', 'MIT license', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'SAS', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'Ontario, Canada', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['Logistic_Regression\nStudies with Logistic Regression in Python\nThe repository consists of the following projects:\n1. Telecom Churn Prediction using Logistic Regression \nBased on all this past information of telecom users, you want to build a model which will predict whether a particular customer will churn or not, i.e. whether they will switch to a different service provider or not. So the variable of interest, i.e. the target variable here is ‘Churn’ which will tell us whether or not a particular customer has churned. It is a binary variable - 1 means that the customer has churned and 0 means the customer has not churned.\nSolution Overview:\nUnderstanding and exploring the data\nPreparing the Data: \nThis involves splitting data into train and test, creating dummy variables and scaling the data\nModel building\nA logistic model is built using python statsmodel GLM (Generalized Linear Model). RFE is used here for feature elimination.\n2. Scoring Leads of an online Education company\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses.\nWhen these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not.\nThe company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance.\nSolution Overview:\nA logistic Regression model has been built to classify whether a student whether a lead is going to be converted.The below solution methodology has been adopted to arrive at the best model that can assign a Lead Score between 0 to 100 based on various features.\n Data Analysis:\nA thorough EDA has been conducted to identify key drivers that can help achieve a conversion rate of atleast 80%.\nHandling Categorical variables:\n1.Variables with high skewness have been removed.\n2.Handling Missing values and deriving new categories\n4.All the categorical columns have been visually analysed using boxplots,bar charts as needed.\nHandling Numerical variables:\n1.Of the 5 numerical variables,the two variables - Asymmetrique Activity Score, Asymmetrique Profile Score have > 45% missing values and hence dropped from analysis.\n2.For the remaining 3 columns,the missing values in the 2 columns-Total Visits,PageViews Per Visit,missing values have been imputed using the median(mean was also close to median) of the columns.\n3.The outliers in these columns have been visualized using boxplots and have been treated by soft  capping the 99% value.\nPreprocessing and Data Preparation\nDummy variable creation for categorical variables:\nBinary variables with values Yes/No have been converted to 1/0.\nDummy variables for different categories have been created and the dummy columns of category “Others” have been dropped.\nTrain test split\nTrain size of 70% data and test size of 30% data was used.\nScaling data\nStandard scaling was used to scale all the numerical variables.\nModel Building\nFor features identification, a combination of manual and automated(RFE) approach have been adopted.\nA logistic regression model was built and was finalized with 13 variables with VIF < 5 and p-value < 0.1.\nOptimal cutoff of 0.35 was decided based on accuracy,sensitivity and specificity trade off.\nThe model predicted the probabilities which were further used to calculate the Lead Score.\nFinal Analysis\nThe project contains the supporting files whhich discusses about the analysis and recommendation for the company\n'], 'url_profile': 'https://github.com/kanika1101', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 20, 2020', 'MIT license', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'SAS', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'Rourkela, Odisha', 'stats_list': [], 'contributions': '1,301 contributions\n        in the last year', 'description': ['Multi_Output_Regression\nMulti output (y1,y2,y3) regression problem\nAs in this case, there are 3 outputs and 9 inputs(x1,x2,x3...x9),I have used the Neural Network method of finding the output.\nHere I have used MSE Loss to get the most nearby value to the actual Y.\nOther method of finding the ouutput is using SVM.\nIf the number of output is more ie. Suppose there are y1,y2,y3,,,y9 but the no of input column is less then we have to use the GAN method to predict the output of the regression.\n'], 'url_profile': 'https://github.com/bislara', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 20, 2020', 'MIT license', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'SAS', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': '5349 N Fort Yuma Trail Tucson AZ 85750', 'stats_list': [], 'contributions': '1,457 contributions\n        in the last year', 'description': ['utl-locating-breakpoints-for-dogleg-mutiple-regression-lines\nLocating breakpoints for dogleg mutiple regression lines\nLocating breakpoints for dogleg mutiple regression lines\nDaily rice production in tons for the first hundred days of summer\n\ngithub\nhttps://tinyurl.com/wz4pdoa\nhttps://github.com/rogerjdeangelis/utl-locating-breakpoints-for-dogleg-mutiple-regression-lines\n\nSAS Forum\nhttps://tinyurl.com/v45q2k6\nhttps://communities.sas.com/t5/SAS-Programming/interrupted-time-series-segmented-regression/m-p/633400\n\nHi res R plot\nhttps://tinyurl.com/yxyhm5d3\nhttps://github.com/rogerjdeangelis/utl-locating-breakpoints-for-dogleg-mutiple-regression-lines/blob/master/dogleg.png\n\n*_                   _\n(_)_ __  _ __  _   _| |_\n| | \'_ \\| \'_ \\| | | | __|\n| | | | | |_) | |_| | |_\n|_|_| |_| .__/ \\__,_|\\__|\n        |_|\n;\noptions validvarname=upcase;\nlibname  sd1 ""d:/sd1"";\ndata sd1.have;\n  call streaminit(421);\n  do day=0 to 100;\n     if  day<=39 then rice=day + int(25*rand(\'uniform\'));\n     if  day>=38 then rice=40   + (.2*day +int(25*rand(\'uniform\')));\n       output;\n  end;\nrun;quit;\n\noptions ls=72 ps=40 nocenter;\nproc plot data=sd1.have;\nplot rice*day=\'+\'/ href=31 box;\nrun;quit;\n\n/*\nUp to 40 obs SD1.HAVE total obs=100\n\n DAY    RICE\n\n   1    17.0\n   2    16.0\n   3    25.0\n   4    19.0\n   5    24.0\n   6    14.0\n   7    21.0\n   8     9.0\n   9    29.0\n  10    25.0\n*/\n\n        0        20        40        60        80        100\n     ---+---------+---------+---------+---------+---------+---\n 100 +                                                       +100\n     |                                                       |\n     |                                                       |\n     |                                                       |\n     |                                                       |\n     |                                                       |\n  80 +                                            + ++       + 80\n     |                                    + +      +    + +  |\n     |                              + +         +   ++       |\nRICE |                      ++ ++    + +    +  +  +      +   |  RICE\n     |                                ++++ + +               |\n     |                             + +    ++ ++              |\n  60 +                        +  +              ++ +  +++    + 60\n     |                   ++  + + +++             +           |\n     |                    ++            ++         +++       |\n     |                + +       +                            |\n     |                   +                                   |\n     |             +                                         |\n  40 +                                                       + 40\n     |           + +  +++                                    |\n     |          +   +  +                                     |\n     |       + ++   +                                        |\n     |    +  +       +                                       |\n     |     +   +  +                                          |\n  20 +    + + +   +                                          + 20\n     |   +    +  +                                           |\n     |     +                                                 |\n     |      +                                                |\n     |                                                       |\n     |                                                       |\n   0 +                                                       +  0\n     ---+---------+---------+---------+---------+---------+---\n        0        20        40        60        80        100\n\n                               DAYS\n\n*            _               _\n  ___  _   _| |_ _ __  _   _| |_\n / _ \\| | | | __| \'_ \\| | | | __|\n| (_) | |_| | |_| |_) | |_| | |_\n \\___/ \\__,_|\\__| .__/ \\__,_|\\__|\n                |_|\n;\n\n\nFROM R (SAS macro variable)\n\n%put &=breakpoints;\n\n  BREAKPOINTS=38\n\n\nUp to 40 obs from PARMS total obs=4\n\nObs           MODEL           PARAMETER    ESTIMATE\n\n 1     Day_less_than_38       Intercept     12.1869\n 2     Day_less_than_38       SLOPE          0.9991\n 3     Day_greater_than_38    Intercept     51.4847\n 4     Day_greater_than_38    SLOPE          0.2241\n\n\n\n        0        20        40        60        80        100\n     ---+---------+---------+---------+---------+---------+---\n 100 +                                                       +100\n     |                                                       |\n     |                                                       |\n     |                   Day_greater_than_38 Intercept 51.5  |\n     |                   Day_greater_than_38 SLOPE      0.2  |                                  |\n     |                                                       |\n  80 +                                            + ++       + 80\n     |                   38               + +      +    + +  |\n     |                    ^         + +         +   ++       |\nRICE |                    | ++ ++    + +    +  +  +      +   |  RICE\n     |                  + |--------------------------------  |\n     |                   /|          + +    ++ ++            |\n  60 +                + / |     +  +              ++ +  +++  + 60\n     |            +  + /  |+  + + +++         *   +          |\n     |            ++  /   |++            ++                  |\n     |         +    +/ + +|      +                           |\n     |          +  +/     |                                  |\n     |       +     /+  +  |                                  |\n  40 +          + /       |                                  + 40\n     |       +   /+ +  +  |                                |\n     |          /+   +  + |                                  |\n     |       + /++   +    |                                  |\n     |    +  +/       +   |                                  |\n     |     + /  +  +      |                                  |\n  20 +    + /+ +   +      |                                  + 20\n     |   + /   +  +       |                                  |\n     |    / +             |                                  |\n     |      +             |                                  |\n     | Day_less_than_38  Intercept  12.18                    |\n     | Day_less_than_38  SLOPE       1.00                    |\n   0 +                    |                                  +  0\n     ---+---------+-------+-+---------+---------+---------+---\n        0        20       ^ 40       60        80        100\n                          |\n                         38    DAYS\n\n*\n _ __  _ __ ___   ___ ___  ___ ___\n| \'_ \\| \'__/ _ \\ / __/ _ \\/ __/ __|\n| |_) | | | (_) | (_|  __/\\__ \\__ \\\n| .__/|_|  \\___/ \\___\\___||___/___/\n|_|\n;\n\nThe R code below creates a the macro variable breakpoints;\n\n%symdel breakpoints / nowarn;\n\n%utl_submit_r64(\'\nlibrary(tseries);\nlibrary(strucchange);\nlibrary(forecast);\nlibrary(haven);\nprod_df<-as.data.frame(read_sas(""d:/sd1/have.sas7bdat""));\nbp=breakpoints(prod_df$RICE ~ prod_df$DAY, h = 25);\nbp$breakpoints;\nwriteClipboard(as.character(paste(bp$breakpoints, collapse = "" "")));\npng(""d:/png/dogleg.png"");\nplot(prod_df$RICE ~ prod_df$DAY, pch = 19);\nlines(fitted(bp, breaks = 1) ~ prod_df$DAY, col = 4, lwd = 1.5)\n\',returnvar=breakpoints);\n\n%put &=breakpoints;\n\n* SAS REGRESSION;\n\nproc datasets lib=work;\n  delete parms parmle38 parmgt38;\nrun;quit;\n\ndata parms(keep=model parameter estimate);\n  if _n_=0 then do; %dosubl(\'\n\n     * first leg;\n     ods output  parameterEstimates=parmle38;\n     proc reg data=sd1.have(where=(day<=38));\n     Day_less_than_38: model rice=day;\n     run;quit;\n\n     * second leg;\n     ods output  parameterEstimates=parmgt38;\n     proc reg data=sd1.have(where=(day>38));\n     Day_greater_than_38: model rice=day;\n     run;quit;\n     \')\n  end;\n  set\n     parmle38\n     parmgt38\n  ;\n  rename variable=PARAMETER;\n  if variable=""DAY"" then variable=""SLOPE"";\nrun;quit;\n\n%put &=brealpoints;\n\n'], 'url_profile': 'https://github.com/rogerjdeangelis', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 20, 2020', 'MIT license', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'SAS', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'Izmir', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Data Analysing\nThe project developed for CE 475 course.\nProject Detail\nHow to find the best regression model for six variables which relation unknown? I analyzed with four model which are Linear Regression, Random Forest Regression, Decision Tree Regression and Lasso Regression with two methods that are score of regression given library and Mean Square Error with using cross-validation. I worked with my default set.\n'], 'url_profile': 'https://github.com/msarigul20', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 20, 2020', 'MIT license', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'SAS', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['House_prices\nKaggle competition House prices: Advanced Regression Techniques\nOur task is to analyze the dataset named House Prices: Advanced Regression Techniques. It contains the sale price of about 1500 residential homes located in Ames, Iowa, along with 79 explanatory variables describing (almost) every aspect of the houses. The dataset has already been preprocessed to deal with missing values, so we will work on a reduced dataset containing 68 variables.\nOur aim within this project is to focus on dimensionality reduction by doing variable selection. Variable selection can be defined as selecting a subset of the most relevant features.\nThe objectives of feature selection include: building simpler and more comprehensible models, improving performance, and preparing clean, and understandable data. Indeed, with a large number of features, learning models tend to overfit which may cause performance degradation on unseen data.\nWe can, therefore, address the following question: What are the most relevant features to explain the sale price of houses of the dataset?\nTo answer this question we will first analyze the variables and assess their relevance by looking at their correlation with the regression target: SalePrice. We will also build and compare several linear regression models with different numbers of variables and finally conclude on the relevance of the features. Our work will be focused on finding the best linear prediction model using a minimum number of variables. We can, therefore, state our research hypothesis as follows: Can we construct a performant linear regression model by selecting only the most appropriate variables? How does it compare to larger or other models?\n'], 'url_profile': 'https://github.com/AdrienToulouse', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 20, 2020', 'MIT license', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'SAS', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'São Paulo', 'stats_list': [], 'contributions': '115 contributions\n        in the last year', 'description': ['MLSimpleLinearRegression\nA simple linear regression ML demonstration.\n'], 'url_profile': 'https://github.com/gustavozantut', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 20, 2020', 'MIT license', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'SAS', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HirotakeIto', 'info_list': ['R', 'Updated Mar 23, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Shell', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'Juelich', 'stats_list': [], 'contributions': '99 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alperyeg', 'info_list': ['R', 'Updated Mar 23, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Shell', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'Gr. NOIDA', 'stats_list': [], 'contributions': '553 contributions\n        in the last year', 'description': ['Random-Forest\nEnsemble learning method for classification and regression\n'], 'url_profile': 'https://github.com/prakharR534', 'info_list': ['R', 'Updated Mar 23, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Shell', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Marian regression tests\nMarian is an efficient Neural Machine Translation framework written in\npure C++ with minimal dependencies.\nThis repository contains the regression test framework for the main development\nrepository: https://github.com/marian-nmt/marian-dev.\nStructure\nDirectories:\n\ntests - regression tests\ntools - scripts and repositories\nmodels - models used in regression tests\ndata - data used in training or decoding tests\n\nEach test consists of:\n\ntest_*.sh file\nsetup.sh (optional)\nteardown.sh (optional)\n\nUsage\nDownloading required data and tools:\nmake install\n\nRunning regression tests:\nMARIAN=/path/to/marian-dev/build ./run_mrt.sh\n\nEnabling multi-GPU tests:\nCUDA_VISIBLE_DEVICES=0,1 ./run_mrt.sh\n\nMore invocation examples:\n./run_mrt.sh tests/training/basics\n./run_mrt.sh tests/training/basics/test_valid_script.sh\n./run_mrt.sh previous.log\n./run_mrt.sh '#tag'\n\nwhere previous.log contains a list of test files, one test per line.  This\nfile is automatically generated each time ./run_mrt.sh finishes running.\nThe last example starts all regression tests labeled with '#tag'.  The list of\ntests annotated with each available tag can be displayed by running\n./show_tags.sh.\nCleaning test artifacts:\nmake clean\n\nDebugging failed tests\nFailed tests are displayed at the end of testing or in previous.log, e.g.:\nFailed:\n- tests/training/restoring/multi-gpu/test_async.sh\n- tests/training/embeddings/test_custom_embeddings.sh\n---------------------\nRan 145 tests in 00:48:48.210s, 143 passed, 0 skipped, 2 failed\n\nLogging messages are in files ending with .sh.log suffix:\nless tests/training/restoring/multi-gpu/test_async.sh.log\n\nThe last command in most tests is an execution of a custom diff tool, which\nprints the exact invocation commands with absolute paths. It can be used to\ndisplay the differences that cause the test fails.\nAdding new tests\nUse templates provided in tests/_template.\nPlease follow these recommendations:\n\nTest one thing at a time\nFor comparing outputs with numbers, please use float-friendly\ntools/diff-nums.py instead of GNU diff\nMake your tests deterministic using --no-shuffle --seed 1111 or similar\nMake training execution time as short as possible, for instance, by reducing\nthe size of the network and the number of iterations\nDo not run decoding or scoring on files longer than ca. 10-100 lines\nIf your tests require downloading and running a custom model, please keep it\nas small as possible, and contact me (Roman) to upload it into our storage\n\nJenkins\nThe regression tests are run automatically on Jenkins after each push to the\nmaster branch and a successful compilation with g++ 5.4.0 20160609 and CUDA\n10.1.243: http://vali.inf.ed.ac.uk/jenkins/view/marian/\nOn Jenkins, Marian is compiled using the following commands:\ncmake -DUSE_SENTENCEPIECE=ON -DCOMPILE_TESTS=ON -DCOMPILE_EXAMPLES=ON \\\n    -DCUDA_TOOLKIT_ROOT_DIR=/var/lib/jenkins/cuda-10.1 ..\nmake -j\nmake test\n\nIf this succeeds, created executables are used to run regression tests.\nAcknowledgements\nThe development of Marian received funding from the European Union's\nHorizon 2020 Research and Innovation Programme under grant agreements\n688139 (SUMMA; 2016-2019),\n645487 (Modern MT; 2015-2017),\n644333 (TraMOOC; 2015-2017),\n644402 (HiML; 2015-2017),\n825303 (Bergamot; 2019-2021),\nthe Amazon Academic Research Awards program,\nthe World Intellectual Property Organization,\nand is based upon work supported in part by the Office of the Director of\nNational Intelligence (ODNI), Intelligence Advanced Research Projects Activity\n(IARPA), via contract #FA8650-17-C-9117.\nThis software contains source code provided by NVIDIA Corporation.\n""], 'url_profile': 'https://github.com/mozilla', 'info_list': ['R', 'Updated Mar 23, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Shell', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'Pleasant Hill, California', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Analyze-A-B-Test-Results\nUse Regression to perform A/B testing.\n'], 'url_profile': 'https://github.com/YuehHanChen', 'info_list': ['R', 'Updated Mar 23, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Shell', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['MASR\nIt is for multiple adaptive regression spline and discussion paper.\n'], 'url_profile': 'https://github.com/Hulalazz', 'info_list': ['R', 'Updated Mar 23, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Shell', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Boston-Housing\nCross validation performed using different regression algorithms\n'], 'url_profile': 'https://github.com/pranavkaushik26', 'info_list': ['R', 'Updated Mar 23, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Shell', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'Linkou, Taiwan ', 'stats_list': [], 'contributions': '435 contributions\n        in the last year', 'description': ['Ploynomial-regression-COVID-19\nPloynomial regression: COVID-19 USC:california\n'], 'url_profile': 'https://github.com/Sumit-ai', 'info_list': ['R', 'Updated Mar 23, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Shell', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'Turkey', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['car-price-prediction\nCar Price Prediction with Multiple Regression\n'], 'url_profile': 'https://github.com/erhanbicerr', 'info_list': ['R', 'Updated Mar 23, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Shell', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['LinearRegression\nFinal project for linear regression MOOC course\n'], 'url_profile': 'https://github.com/sumAllie', 'info_list': ['R', 'Updated Mar 23, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Shell', 'Updated Apr 21, 2020', 'Jupyter Notebook', 'Updated Jun 8, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['Mood Journal\nBasic mood tracker; GUI uses tkinter, data is stored in Excel.\nHappiness 2019\nExploratory data analysis and an attempt at manual linear regression on the World Happiness Index Data 2019\nGrad Admission\nUnivariate and multivariate regression. An exercise in interpreting regression models and understanding pitfalls\nGraph Traversal\nDFS and BFS traversal basics\nMinesweeper\nCLI version of Minesweeper developed for a Cognitive Science experiment. Followed this tutorial for the basics.\n'], 'url_profile': 'https://github.com/aasthas97', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', '1', 'R', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '2', 'Jupyter Notebook', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['Logistic_Regresssion_by_R\n'], 'url_profile': 'https://github.com/amitdivekar30', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', '1', 'R', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '2', 'Jupyter Notebook', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Hyderabad, IN', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cpuranda', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', '1', 'R', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '2', 'Jupyter Notebook', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/iakhil', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', '1', 'R', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '2', 'Jupyter Notebook', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Barcelona', 'stats_list': [], 'contributions': '121 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mgranica', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', '1', 'R', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '2', 'Jupyter Notebook', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vaibhav2095', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', '1', 'R', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '2', 'Jupyter Notebook', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', '1', 'R', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '2', 'Jupyter Notebook', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', '1', 'R', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '2', 'Jupyter Notebook', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'NJ, USA', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': [""Project Topic : Health-care-Insurance-prediction-using-regression\nProblem Statement :\nMedical expenses are difficult to estimate because the most costly conditions are rare and seemingly random. Still, some conditions are more prevalent for certain segments of the population. For instance, lung cancer is more likely among smokers than non-smokers, and heart disease may be more likely among the obese.  The goal of this project is to use patient data to estimate the average medical care expenses using regression for provided population segments.\nData set Dictionary :\n\n\n\nVariable Name\nDescription\nDatatype\n\n\n\n\nage\nAge of the Beneficiary\nint64\n\n\nsex\nWhether the beneficiary is a male or a female\nobject\n\n\nbmi\nBody mass index, providing an understanding of body, weights that are relatively high or low relative to height\nfloat64\n\n\nchildren\nNumber of children covered by health insurance/Number of dependents\nint64\n\n\nsmoker\nWhether the customer smokes or not (Yes, No)\nobject\n\n\nregion\nThe beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\nobject\n\n\ncharges\nIndividual medical costs billed by health insurance\nfloat64\n\n\n\n""], 'url_profile': 'https://github.com/niveadabre', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', '1', 'R', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '2', 'Jupyter Notebook', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}","{'location': 'Krakow/Warsaw', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Logistic Regression\nAn introduction to Logistic Regression. The notebook can be used as a template to create Logistic Regression models.\nYou can recreate the environment by using the requirements.txt\nconda create --name <env> --file requirements.txt\nMake sure that requirements.txt has a correct path.\n'], 'url_profile': 'https://github.com/datandard', 'info_list': ['Jupyter Notebook', 'Updated Feb 23, 2021', '1', 'R', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', '2', 'Jupyter Notebook', 'Updated Oct 1, 2020', 'Jupyter Notebook', 'Updated Mar 24, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Vasudevavenkatasai', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 23, 2020', 'R', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '170 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/raahimkhan', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 23, 2020', 'R', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yiying-wang', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 23, 2020', 'R', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['COVID-19-Detection-Regression-Model\nwe use regression model with random generated data in python.\n'], 'url_profile': 'https://github.com/programmerMayur', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 23, 2020', 'R', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rahulbhadja', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 23, 2020', 'R', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '126 contributions\n        in the last year', 'description': ['Linear-Regression-in-R\nLinear Regression in R for fitting a simple sales data\n'], 'url_profile': 'https://github.com/rizardrenanda', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 23, 2020', 'R', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'Amstedam', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Survival-analysis\nKaplan-Meier estimator, Cox regression on stock MGUS data\n'], 'url_profile': 'https://github.com/GStravinsky', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 23, 2020', 'R', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': [""Logistic-Regression-on-IRIS-Dataset\nLogistic Regression to classify the IRIS Dataset\nThis dataset contains data of different species of IRIS and our job is classify them into different species based on the sepal length, sepal width, petal length and petal width of the IRIS flowers.\nInputs\nThe inputs to our model will be sepal length, sepal width, petal length and petal width.\nOutput\nThe output of our model will be the name of the species of the plant (Iris-setosa,Iris-versicolor,Iros-virginicia).\nDataset\nThe Dataset that I've used is from kaggle. It contains data of 150 different samples.\nModel\nThe model used in this code is LogisticRegression from sklearn.linear_model\n""], 'url_profile': 'https://github.com/ujwal98', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 23, 2020', 'R', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['PM2.5_prediction_regression\nPREDICTING PM2.5 WITH LINEAR REGRESSION, TECHINIQUES INCLUDING FEATURE ENGINEERING, CROSS-VALIDATION, NORMALIZATION, AND ADAGRAD GRADIENT OPTIMIZATION.\n'], 'url_profile': 'https://github.com/shyDaniel', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 23, 2020', 'R', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YCipriani', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 23, 2020', 'R', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['PM2.5_prediction_regression\nPREDICTING PM2.5 WITH LINEAR REGRESSION, TECHINIQUES INCLUDING FEATURE ENGINEERING, CROSS-VALIDATION, NORMALIZATION, AND ADAGRAD GRADIENT OPTIMIZATION.\n'], 'url_profile': 'https://github.com/shyDaniel', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YCipriani', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['models\nMy machine learning models\n'], 'url_profile': 'https://github.com/Dougy140', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['House Price Regression Analysis\nThis was a project from an entry level kaggle compettion that orignally took place on 8/30/2016\nIt was used as my first project to demonstrate my DS skills and was completed on 02/12/2018 with a RMSE score of 0.15453 using XGBoost\nIt was later revisted on 12/7/2019 and updated\nAdded to a new Repo on 03/21/2020\nTensorFlow was added as an additional ML technique on 4/22/2020 but even after extensive tuning, it could not perform better than XGBoost.\n'], 'url_profile': 'https://github.com/adammarcus930', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['PG3\nPeer Graded Assignment: Build a Regression Model in Keras\n'], 'url_profile': 'https://github.com/jharahul1', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['Simple-Lin-Reg-Model-from-scratch\nThis repository contains:\nmy-simple_lin-reg.py: it is a package that contains a class with functions to build my model from scratch, these functions are:\n                     1- TrainTestSplitter: to split data to train and test \n                     \n                     2- StandardScalertransform: to standardize X\n                     3- minMaxScaler: to normalize X\n                     \n                     4- coefficents: to get the weights\n                     \n                     5- SimpleLinearRegression: to get my own predictions\n                     6- Evaluate: to evaluate my model\n\nmy_package.ipynb: my own package in ipynb extension\nCompare_my_model.ipynb: it is a programm where import my own package in it and compare my model with sklearn model\nInsurance.xls: The data which i used to build my model, it contains one feature (X) and label (y)\n'], 'url_profile': 'https://github.com/Nada-Zaki', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'Chennai', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': [""House-Prices-Advanced-Regression-Problem\nMy solution for the House Prices Advanced Regression Problem\nThe first step to be taken in the project is to view and explore the data using various plotting methods. One such method that I applied was to group the categorical value type columns according to their count and plot them against the sale price to gain a better understanding of the data.\nDepending on the relation between the data in the columns and the sale price, we group one or more categories into a band so that the model has better prediction. The 'replace()' function is meant to take in the dataset (either train or test) and categorize them into usable values by the regression model.\nThe replace() function makes use of the band_create() function. The band_create function is a simple function that replaces the string values of a categorical type data column and converts them into integer values for use by the regression model. However, using the band_create function for all data columns would be crude and counterproductive, hence the replace function specifically converts and replaces bands in a few columns such as 'LotShape', 'OverallCond' and 'ExterCond' and many more based on the way the data behaves, rather than simply classifying each different kind of string. For example the prices for the houses having sale condition as 'Abnormal' and 'Family' were almost always equal, and hence they are grouped into one band.\nSince the data variables have a huge factor of interconnection between the variables, through hit and trial it was found that the Ridge Regressor works the best for our particular model.\nThus after replacing 'Obj' type missing data with a blank string that will later be categorized in band_create, and replacing missing values in integer type values as 0, we fit the data into the model and finally obtain our answer after choosing almost all variables, excluding a few such as Utilities(since almost all houses have the value 'AllPub') and PoolQC('99% lf values are missing), we fit and predict our answer.\n""], 'url_profile': 'https://github.com/vatsaryan07', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '337 contributions\n        in the last year', 'description': ['kaggle-HousePrice\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/daehanchoi-dev', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['German-Credit-Risk-Analysis\nCredit Risk Analysis using Logistic Regression and Decision Trees\n'], 'url_profile': 'https://github.com/jainmt', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '381 contributions\n        in the last year', 'description': [""Sentiment Analysis on IMDb Movie Reviews\nBased on the fundamentals of sentiment analysis, and build a logistic regression model to classify movie reviews as either positive or negative. I had used the  IMDB dataset for the purpose of this project. My goal was to use a simple logistic regression estimator from scikit-learn for document classification.\nOverview\n1. Transforming Documents into Feature Vectors\n\nRepresent text data using the bag-of-words model from natural language processing and information retrieval.\nConstruct the vocabulary of the bag-of-words model and transform the provided sample sentences into sparse feature vectors.\n\n2. Term Frequency-Inverse Document Frequency\n\nIn information retrieval and text mining, we often observe words that crop up across our corpus of documents. These words can lead to bad performance during training and test time because they usually don’t contain useful information.\nUnderstand and implement a useful statistical technique, Term frequency-inverse document frequency (tf-idf), to downweight these class of words in the feature vector representation. The tf-idf is the product of the term frequency and the inverse document frequency.\n\n3. Calculate TF-IDF of the Term 'Is'\n\nApply scikit-learn’s TfidfTransformer to convert sample text into a vector of tf-idf values and apply the L2-normalization to it.\n\n4. Data Preparation\n\nCleaning and pre-processing text data is a vital process in data analysis and especially in natural language processing tasks.\nStrip the data set of reviews of irrelevant characters including HTML tags, punctuation, and emojis using regular expressions.\n\n5. Tokenization of Documents\n\nEnsures that k-means image compression is performed only on the slider widget's mouse release events.\nRepurpose the data preprocessing and k-means clustering logic from previous tasks to operate on images of your choice.\nVisualize how the image changes as the number of clusters fed to the k-means algorithm is varied.\n\n6. Document Classification Using Logistic Regression\n\nFirst, split the data into training and test sets of equal size.\nThen create a pipeline to build a logistic regression modelModel Accuracy\nIn this final task, we take a look at the best parameter settings, cross-validation score, and how well our model classifies the sentiments of reviews it has never seen before from the test set.\nTo estimate the best parameters and model, we employ cross-validated grid-search over a parameter grid.\n\n7. Load Saved Model from Disk\n\nAlthough the time it takes to train logistic regression models is very little, estimating the best parameters for the model using GridSearchCV can take hours given the size of our training set.\nLoad a pre-trained model that will later be used to find the best parameter settings, cross validation score, and the test accuracy.\n\n8. Model Accuracy\n\nFinally, take a look at the best parameter settings, cross-validation score, and how well our model classifies the sentiments of reviews it has never seen before from the test set.\n\n""], 'url_profile': 'https://github.com/TheClub4', 'info_list': ['Jupyter Notebook', 'Updated Mar 25, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '193 contributions\n        in the last year', 'description': ['Linear Regresssion from scratch\nDataset\nThe simple cengage systolic blood pressure dataset:\nhttps://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/frame.html\nPurpose\nGain familiarity with the algorithm by developing it from scratch.\nHence, best ML practices such as train/test/cross-validation splits\nare NOT prioritized.\nRun verified with python 3.7 and numpy 1.18.1\npython Runner.py\nVisualize\n\nMin-Max Normalized\n(data - min) / (max - min)\nPuts feature data and labels within [0, 1] range\n\nTraining Learning Curve\n\nTrained Model\n\nNormal Equation Trained Model\n\nActual vs Projected\n\nNormal Equation Actual vs Projected\n\n'], 'url_profile': 'https://github.com/boyko11', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'C++', 'Updated Mar 17, 2020', 'JavaScript', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '709 contributions\n        in the last year', 'description': ['Loan-Approval-\nPrediction of loan approval of people by Logistic Regression\n'], 'url_profile': 'https://github.com/Dharana23', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'C++', 'Updated Mar 17, 2020', 'JavaScript', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Palo Alto, CA', 'stats_list': [], 'contributions': '1,288 contributions\n        in the last year', 'description': [""COVID-19 case growth\nThe following code fits a Kaggle dataset to a log-normalized linear regression.\nThe results are plotted comparing the relative growth of the virus in British Columbia, California and New York.\nYou'll need to have kaggle cli tools installed and sign up for a Kaggle API (free) key to fetch the data.\nDataset is from https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/data\n""], 'url_profile': 'https://github.com/codeocelot', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'C++', 'Updated Mar 17, 2020', 'JavaScript', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Munich', 'stats_list': [], 'contributions': '372 contributions\n        in the last year', 'description': ['MNIST\nRepository for training a classifier on the mnist datatset using PyTorch,\nfollows the tutorial What is torch.nn really?.\nUsage\nCreate a virtual enviroment and install the required packages\npip install -r requirements.txt\n\nAdjust the config files and run\npython train.py\n\nThis will train a classifier based on Logistic Regression and upload the\nmodel artifact to an AWS S3 bucket.   The model artifact will be used by\nthe web application here.\n'], 'url_profile': 'https://github.com/celis', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'C++', 'Updated Mar 17, 2020', 'JavaScript', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'San Jose, CA, USA', 'stats_list': [], 'contributions': '919 contributions\n        in the last year', 'description': ['pytorch-mnist-logisticRegression\nThe most basic PyTorch logistic regression model on MNIST - 70k images, only for the purpose of getting a saved PyTorch model object at the end and using it as part of an end-to-end machine learning pipeline to get its inference.\n\nTrained only for 50 epochs\nUsing SGD optimizer\nOptimized against cross entropy\nNo hyperparameter tuning done, as increasing the accuracy is not the main purpose.\n\n'], 'url_profile': 'https://github.com/aslisabanci', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'C++', 'Updated Mar 17, 2020', 'JavaScript', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['coverage_control_uavs\nTo play this project, do these following commands:\n\nPut this ""coverage_control_uavs"" folder to your ros ""catkin_ws"" folder then do ""catkin_make""\nPull the PX4 Firmware project from ""https://github.com/yaqubotics/Firmware/tree/cc_uavs"" outside your ros ""catkin_ws"" folder\nOpen ""coverage_control_uavs/bash_scripts/start_mission.sh"" and edit the following variables:\n\nros_wd for ROS working directory\npx4_wd for PX4 Firmware working directory\n\n\nRun ""start_mission.sh"" in bash_scripts folder by\n\nsource start_mission.sh\n\nThe program should look like this:\n\nReferences:\n[1] Todescato, Marco, Andrea Carron, Ruggero Carli, Gianluigi Pillonetto, and Luca Schenato. ""Multi-robots Gaussian estimation and coverage control: From client–server to peer-to-peer architectures."" Automatica 80 (2017): 284-294.\n[2] Snape, Jamie, Jur Van Den Berg, Stephen J. Guy, and Dinesh Manocha. ""The hybrid reciprocal velocity obstacle."" IEEE Transactions on Robotics 27, no. 4 (2011): 696-706.\nPlease cite:\nPrabowo, Y.A. and Trilaksono, B.R., 2019. Collision-Free Coverage Control of Swarm Robotics Based on Gaussian Process Regression to Estimate Sensory Function in non-Convex Environment. International Journal on Electrical Engineering & Informatics, 11(1).\n'], 'url_profile': 'https://github.com/yaqubotics', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'C++', 'Updated Mar 17, 2020', 'JavaScript', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Santa Fe, Argentina', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Linear Regression with TensorFlow\n'], 'url_profile': 'https://github.com/FacuRot', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'C++', 'Updated Mar 17, 2020', 'JavaScript', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Zanjan', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['Spline Linear Regression\nThis code provides a theoritical simple implementation of Spline Linear Regression from scratch.\nThe purpose of Spline learning is to fit linear models to each section of data. These sections have been chosen by diving the space to a specific number of equal sections.\nUnfortunately this code does not use the Roughness Penalty Approach and as you see in the image below that is why it lacks roughness.\nResult\nA visualization for the result has been demonstrated below.\n\n'], 'url_profile': 'https://github.com/arnejad', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'C++', 'Updated Mar 17, 2020', 'JavaScript', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Ann Arbor', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NBLR\nImplementation of Naive Bayes Classifier and Logistic Regression.\n'], 'url_profile': 'https://github.com/yashajoshi', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'C++', 'Updated Mar 17, 2020', 'JavaScript', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': [""Jobandtalent's Task\nDescription and datasets of the task in src folder.\nSolutions in output folder.\nGetting Started\nPython 3.7 procedures written in jupyter notebook Main.ipynb\nPrerequisites\nRequirements in requirements.txt\nInstalling\nUsed conda environment and conda install package.\nRunning the tests\nNo test needed\nDeployment\nBuilt With\n\nConda - Package manager\nJupyterNotebook - Web-based interactive development environment\n\nAuthors\nDaniel Coll\nLicense\nNo license\n""], 'url_profile': 'https://github.com/danielcollsol', 'info_list': ['Python', 'Updated Apr 24, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 21, 2020', 'Python', 'Updated Mar 25, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'C++', 'Updated Mar 17, 2020', 'JavaScript', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}"
"{'location': 'Nashville', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': [""A Simple Regression\nWhat is the relationship between salary and years of experience? This Data and code are pulled from\xa0https://www.superdatascience.com/pages/machine-learning. This is apart of the\xa0Machine Learning A-Z course on Udemy. I did make some charts in Google Data Studio. It consists of key stats from the data such as: Average, Standard Deviation, Maximum, Minimum.\xa0\nGetting Started\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.\nPrerequisites\nI'm using Python Version 3.\nInstalling\nI used this code in Google colaboratory. Go here: https://colab.research.google.com/notebooks/intro.ipynb#recent=true to learn how to use it. You will have to upload the data either from your machine or by uploading the data into your google drive account.\nBuilt With\n\nsklearn\npandas\nnumpy\nmatplotlib\npython\n\nAcknowledgments\n\nhttps://www.superdatascience.com/pages/machine-learning\n\n""], 'url_profile': 'https://github.com/matthgray', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['House Prices Prediction\nMachine Learning Predict house prices Simple Linear Regression\nReferences\n[1] https://www.coursera.org/lecture/ml-foundations/predicting-house-prices-a-case-study-in-regression-aI5W6\n[2] https://www.slideshare.net/DanielLIAO9/c21-intro\n[3] https://www.youtube.com/watch?v=GhrxgbQnEEU\n[4] https://www.w3schools.com/python/python_ml_getting_started.asp\n'], 'url_profile': 'https://github.com/mr-m4hdi', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Nagpur', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shivamtawari', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': [""Polynomial and Radial Basis Function Regression\nThe purpose of this project is to show how to implement ordinary least squares regression using polynomial and radial basis functions.\nEach type of basis function has its own domain where it produces reliable results compared to others. Polynomial basis functions allow\nus to look at global trends while radial basis functions are more useful in identifying local patterns. You should be able to import the code and use it as well as understand it based on the extra comments provided.\nIDE: PyCharm community 2019\nPython version: 3.7\nRunning the tests\nRandomly generated data was used to produce test data for polynomial regression. This data should be visualized using matplotlib. For the radial basis function, it was tested on a corrupted file of a jellyfish picture which is not provided.\nLicense\nThis project is licensed under the License - see the LICENSE.md file for details\nAcknowledgments\nProfessor David Fleet and his team of TA's for CSCC11 for providing starter code\n""], 'url_profile': 'https://github.com/arhum96', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Jakarta, IDN', 'stats_list': [], 'contributions': '449 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ssentinull', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '237 contributions\n        in the last year', 'description': ['mod2_mult_linear_reg\nMod 2 Multiple Linear Regression lab on KC housing data\n'], 'url_profile': 'https://github.com/bryandps', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['SLR-with-SKLearn\nA simple linear regression model using SKLearn (Predictions)\nThe real estate dataset contains only size and price\nUsing SkLearn, we should be able to predict the price based on the size given\n'], 'url_profile': 'https://github.com/Benniah', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['Predicting-Appstore-Searchterm-Popularity\nPredicting appstore searchterm popularity using gradient boosting regression.\n'], 'url_profile': 'https://github.com/Jack-0-0', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': [""Ames Housing Project README\nAuthor: Joshua Leibow (LA)\nThis repo houses my workflow and organization for the Ames Housing project.\nAmes Housing Price Predictor\nProblem Statement\nTo create a template for an app that predicts sale prices for homes all over the country, I will build a model that predicts housing prices in Ames Iowa.\nData Dicitonary\nHere is a link to a complete data dictionary of the Ames housing dataset, provided by the Journal of Statistics Education.\nIn addition, I have created my own data dictionary based on the X variables I considered using for my model.  The variables are listed in descending order according to their correlation to saleprice values.\n\nContents\n\n\n01_Cleaning - Here I import the data, drop non-numeric columns, deal with missing values and create dummy columns for non-numeric categories.\n\n\n02_EDA - Here I explore the data through the usage of plots and visualizations.  From the visualizations, I made inferences that helped me to determine which variables would be good for predictive modeling.  There were some variables that were not neccessarily good for my model, yet, helped me, nonetheless, to understand the diversity of qualities that bring value to a home.\n\n\n03A_MLR_Model - In this notebook I create my features matrix (X) and my target vector (y).  Afterwards, I create the train test split function to use to instantiate and fit my model with Linear Regression.  I evaluate my model using train score and K-fold cross validation to obtain the train /test R2 and RMSE scores.  I then plot the predicted values of y_hat vs. the actual values of y to get a visual representation of how well fit my model is.  There are 3 different versions of Multi Linear Regression (MLR) in this notebook in descending order.  The first version contains all of the variables I chose for my model (full features), the second version has a few features taken away (minimal features), and the third is a version in which y is transformed into log_y and has has all the same features as the first version.\n\n\n03B_Ridge_Model - In this notebook I scale my MLR data and use it to instantiate and fit to the Ridge Model.  The Ridge Model is a type of regularization that determines and subtracts the variables that contribute to the overfitting of my model.  With the Ridge model, I obtain a signicantly higher R2 score from the log model, which was my best model that I fit manually.\n\n\n03C_Lasso_Model - Similar to the Ridge Model, but unique in its own way, I again subject my MLR model to a process of scaling and regularization, this time to the Lasso Model.  I obtain a similar score similar to the Ridge Model by computationally cutting down on the variables that are not ideal for my model.\n\n\nKaggle Submission - Here I take my best linear regression model, in this case the log model, and submit it to Kaggle to predict prices for new test data that has been provided as part of a Kaggle competition.\n\n\nFindings\n\nFrom the table we can see that the log, ridge and lasso models gave the best scores. The lasso and ridge models have the highest R2 scores and lowest RMSE scores, with deference to the Ridge model.\nThe Ridge model's test R2 is .9286, which we interpret to mean that 92.86% of the variability in our data is explained by our model, relative to a model with no predictors.\nTher Ridge model's test RMSE score is 21873.74, which means that the model predicts an approximate level of accuracy that is  + or - $21,873 of the true sale price value.\n\nI believe that my model can be used to infer coefficient variables such as square footage, overall quality, number of bedrooms, number of bathrooms and garage car size.\nI believe my model will generalize well to new data, because the train and test R2 scores are within a close range of 0.002.  The implication is that my model is well fit.\nSlides\nPresentation slides are available here.\n""], 'url_profile': 'https://github.com/jleibow27', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/apurva-chindarkar', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Aug 29, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Apr 1, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['BLBLogistic\nBag of Little Bootstrap for Binary Logistic Regression Model. Taking in a clean binary logit dataset, this package utilizes Bag of Little Bootstraps upon a binomial logistic model to generate various estimated intervals and errors, including:\n\nLogistic Regression coefficient estimates and their corresponding confidence intervals\nSigma and its corresponding confidence interval\nPrediction values and their corresponding prediction intervals\n\nThis package was created as a final project for STA 141C, Winter Quarter 2020, UC Davis.\nTo load and build Package onto R\nOpen a new R file, and run:\nlibrary(devtools)\ndevtools::install_github(""ipalvis/BLBLogistic"")\nlibrary(BLBLogistic)\nAuthors\nAlvis Ip\nKoral Buch\nNikhila Thota\nSongwen Su\nThe vignette offers a detailed example of how to use the package.\n'], 'url_profile': 'https://github.com/ipalvis', 'info_list': ['1', 'R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Aug 28, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '223 contributions\n        in the last year', 'description': ['housing-price-regression\nA regression problem of predicting the price of a house\n\nThe goal is to find the best model for a given dataset\nRidge Regression with an alpha hyperparameter value of 1000 is found to be the best model for this problem.  RMSE was used to compare the models.\n'], 'url_profile': 'https://github.com/ali-bulatov', 'info_list': ['1', 'R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Aug 28, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Yuhang19971213', 'info_list': ['1', 'R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Aug 28, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Istanbul, Turkey', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fsahbaz', 'info_list': ['1', 'R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Aug 28, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': [""Heart-Disease\nIntroduction:\nThe below mentioned project has been completed for a case study competition by IIT-Roorkee\nProblem Statement:\nThe goal is to predict the rate of heart disease (per 100,000 individuals) across the United States at the county-level from other socioeconomic indicators. The data is compiled from a wide range of sources .\nThere are 33 variables in this dataset. Each row in the dataset represents a United States county, and the dataset we are working with covers two particular years, denoted a, and b We don't provide a unique identifier for an individual county, just a row_id for each row.\nThe variables in the dataset have names that of the form category__variable, where category is the high level category of the variable (e.g. econ or health). variable is what the specific column contains. Both training and test dataset are from same population.\nWe're trying to predict the variable heart_disease_mortality_per_100k (a positive integer), for each row of the test data set.\nThe resultant accuracy of the model obtained is 98.6%.\nThe Steps used for the project include:\n\nData Collection\nData Cleaning\nData Exploration\nModel Building\nExplaining Model\n\n""], 'url_profile': 'https://github.com/sneha1606', 'info_list': ['1', 'R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Aug 28, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Sydney, Australia', 'stats_list': [], 'contributions': '217 contributions\n        in the last year', 'description': ['ames-housing-price-regression\nThe Ames housing data set on Kaggle is a popular beginner data set for testing regression algorithms.\nKaggle: House Prices Advanced Regression Techniques\nThe goal is to fit a regression model on the training set and predict housing prices on the test set. The project specifies a root mean squared logarithmic error (RMSLE) as the objective function, which is accomplished by taking the log of the sales price as the target variable.\nAfter comparing several regression techniques, I used a Lasso regression model and achieved a top 20% score on the leaderboard. My notebook contains data cleaning, feature transformation, model comparison and building, hyperparameter tuning and cross-validation evaluation.\nVisualisation\nI made a visualiaton of how the log transform affects the distribution of sales price.\nDashboard\n'], 'url_profile': 'https://github.com/xorana', 'info_list': ['1', 'R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Aug 28, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Titanic-prediction\nRandom Forest Regressor\n'], 'url_profile': 'https://github.com/lorenzdes', 'info_list': ['1', 'R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Aug 28, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ealiv', 'info_list': ['1', 'R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Aug 28, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Regression_Zuudoo\n'], 'url_profile': 'https://github.com/priyankasahu200187', 'info_list': ['1', 'R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Aug 28, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['Pyspark_Regression\nUsing pyspark apache to predict the yearly amount spent by VectorsAssembler\n'], 'url_profile': 'https://github.com/suyogyaman', 'info_list': ['1', 'R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Jupyter Notebook', 'Updated May 13, 2020', 'Python', 'Updated Mar 17, 2020', 'Updated Aug 28, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}"
"{'location': 'Bengaluru', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Logistic-Regression\nAll notebooks for Logistic Regressopm\n'], 'url_profile': 'https://github.com/chandan54', 'info_list': ['Jupyter Notebook', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['Linear_regression\n'], 'url_profile': 'https://github.com/Kulna11', 'info_list': ['Jupyter Notebook', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '86 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sriramaraju423', 'info_list': ['Jupyter Notebook', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jasminkarki', 'info_list': ['Jupyter Notebook', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/naman-singh0', 'info_list': ['Jupyter Notebook', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'New York City', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Samarche92', 'info_list': ['Jupyter Notebook', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chedana', 'info_list': ['Jupyter Notebook', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '457 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/bsingh17', 'info_list': ['Jupyter Notebook', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/smartdragon2462', 'info_list': ['Jupyter Notebook', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nikitavig', 'info_list': ['Jupyter Notebook', 'Updated Jun 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 24, 2020', 'Python', 'Updated Mar 22, 2020', 'Python', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['linear Regression (Hồi quy tuyến tính)\nĐịnh nghĩa\nTrong thống kê, hồi quy tuyến tính là một phương pháp dùng để mô hình hóa mối quan hệ giữa một đại lượng vô\nhướng với một hoặc nhiều biến độc lập\nLoss Function (Hàm mất mát)\nĐịnh nghĩa\nHàm mất mát trả về một số không âm thể hiện mức độ chênh lệch giữa các giá trị mà odel của chúng ta dự đoán và giá giá trị thực tế\n'], 'url_profile': 'https://github.com/anhky', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['multi_regression\n'], 'url_profile': 'https://github.com/Darshit7796', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/coderile', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['Logistic-Regression-Project\nHere in this repository you will find the Logistic Regression problem which i worked on.\nLogistic Regression can be applied when the dependent variable is discrete/category/binary. Here the function used is probablistic function.\nProjects which i worked on are\n\nCar Insurance dataset have to check whether customer claimed insurance or not.\nTitanic Dataset here we have to check whether the people survived or not based on their Gender,Class: standard, Economy, Luxury.\n\n'], 'url_profile': 'https://github.com/bantu07', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Linear-regression  Logistic regression MATH\n'], 'url_profile': 'https://github.com/khaja7000', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['This project has an example of R - code developed for Airfoil related data set.\nThe data set information is given below:\nData Set Information:\nThe NASA data set comprises different size NACA 0012 airfoils at various wind tunnel speeds and angles of attack. The span of the airfoil and the observer position were the same in all of the experiments.\nAttribute Information:\nThis problem has the following inputs:\n\nFrequency, in Hertzs.\nAngle of attack, in degrees.\nChord length, in meters.\nFree-stream velocity, in meters per second.\nSuction side displacement thickness, in meters.\n\nThe only output is:\n6. Scaled sound pressure level, in decibels.\nThe primary objective of the project is to establish a relation between Scaled sound pressure level as a dependent variable and other variables as independent variables. Several, relationships including, linear, polynomial and quadratic (non-linear) have been tried along with model assumptions validations for illustration.\nThe .Rmd file contains the r-code and pdf document contains the code as well as the outputs from R execution.\n'], 'url_profile': 'https://github.com/sv-gopal', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shashikant18596', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020']}","{'location': 'Bhubhaneswar, India', 'stats_list': [], 'contributions': '210 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amanjeetsahu', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jpamintuan1', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['logistic-regression-\nCredit Fraud detection\n'], 'url_profile': 'https://github.com/wang718204', 'info_list': ['Jupyter Notebook', 'Updated Mar 31, 2020', 'Python', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['logistic-regression\nCredit Fraud detection\n'], 'url_profile': 'https://github.com/wang718204', 'info_list': ['Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Mar 18, 2020', 'Updated Mar 24, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020']}","{'location': 'Gurgaon', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Linear Regression\nBased on the Age of individual, find yearly Avg. spent on medical bills\n'], 'url_profile': 'https://github.com/sivakumargurram', 'info_list': ['Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Mar 18, 2020', 'Updated Mar 24, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020']}","{'location': 'Los Angeles, California', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mansikapoor333', 'info_list': ['Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Mar 18, 2020', 'Updated Mar 24, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020']}","{'location': 'Kubang Kerian, Kelantan', 'stats_list': [], 'contributions': '171 contributions\n        in the last year', 'description': ['poisson_regression\n'], 'url_profile': 'https://github.com/puterabemi', 'info_list': ['Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Mar 18, 2020', 'Updated Mar 24, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020']}","{'location': 'Toronto, Ontario', 'stats_list': [], 'contributions': '404 contributions\n        in the last year', 'description': ['Logistic-Regression\nBuilt a logistic regression model to\npredict whether a student gets admitted into a university based off the marks of their midterm and final (2 features).\n\nRegularized logistic regression to increase accuracy and prevent overfitting data\nutilized Feature mapping\nWrote cost function and gradient\nplotted data visual with decision boundary\n\n'], 'url_profile': 'https://github.com/JJwilkin', 'info_list': ['Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Mar 18, 2020', 'Updated Mar 24, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression code with hyperparameter tuning for the weights . Inputs and outputs are randomly generated .\nImport the dataset and give input as x and output as y . Set the weight and bias to dimensions of the input\n'], 'url_profile': 'https://github.com/AmritRajeev', 'info_list': ['Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Mar 18, 2020', 'Updated Mar 24, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Heroku-Demo\n'], 'url_profile': 'https://github.com/YogeshGovi', 'info_list': ['Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Mar 18, 2020', 'Updated Mar 24, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020']}","{'location': 'Nagpur', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['Logistic_Regression\nBank Marketing Data\n'], 'url_profile': 'https://github.com/shivamtawari', 'info_list': ['Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Mar 18, 2020', 'Updated Mar 24, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020']}","{'location': 'Dresden, Germany', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Pavel2125', 'info_list': ['Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Mar 18, 2020', 'Updated Mar 24, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020']}","{'location': 'delhi', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashimasinghal', 'info_list': ['Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'HTML', 'Updated Mar 18, 2020', 'Updated Mar 24, 2020', 'MATLAB', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020']}"
"{'location': 'Dresden, Germany', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Pavel2125', 'info_list': ['HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020', 'Updated Mar 20, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'SAS', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Updated Mar 18, 2020']}","{'location': 'delhi', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ashimasinghal', 'info_list': ['HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020', 'Updated Mar 20, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'SAS', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Updated Mar 18, 2020']}","{'location': 'Laxmi nagar,Delhi-110092', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sanjeev1996', 'info_list': ['HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020', 'Updated Mar 20, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'SAS', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rajithap88', 'info_list': ['HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020', 'Updated Mar 20, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'SAS', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression code with hyperparameter tuning for the weights . Inputs and outputs are randomly generated .\nImport the dataset and give input as x and output as y . Set the weight and bias to dimensions of the input\n'], 'url_profile': 'https://github.com/AmritRajeev', 'info_list': ['HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020', 'Updated Mar 20, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'SAS', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Heroku-Demo\n'], 'url_profile': 'https://github.com/YogeshGovi', 'info_list': ['HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020', 'Updated Mar 20, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'SAS', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Updated Mar 18, 2020']}","{'location': 'Nagpur', 'stats_list': [], 'contributions': '464 contributions\n        in the last year', 'description': ['Logistic_Regression\nBank Marketing Data\n'], 'url_profile': 'https://github.com/shivamtawari', 'info_list': ['HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020', 'Updated Mar 20, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'SAS', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Updated Mar 18, 2020']}","{'location': 'NY, SF', 'stats_list': [], 'contributions': '1,103 contributions\n        in the last year', 'description': ['Multivariate Regression In SAS\nDIABETES\n\n\n\n\nDependent Variable Y1\nDependent Variable Y2\n\n\n\n\n\n\n\n\n\n\n\n\nWINE\n\n'], 'url_profile': 'https://github.com/BenitaDiop', 'info_list': ['HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020', 'Updated Mar 20, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'SAS', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kristiee', 'info_list': ['HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020', 'Updated Mar 20, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'SAS', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Updated Mar 18, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Regression-Projects\nThese are some short projects I made while learning machine learning methods and regression.\nAll projects are completed in R. I am uploading both the .Rmd file and a pdf of the end notebook.\nUnfortunately the data is not public and so the code cannot be run as-is.\n'], 'url_profile': 'https://github.com/suzanaiacob', 'info_list': ['HTML', 'Updated Mar 16, 2020', 'Updated Mar 19, 2020', 'Updated Mar 20, 2020', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 19, 2020', 'SAS', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Updated Mar 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Logistic_Regression\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['Python', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', '1', 'MATLAB', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Python', 'MIT license', 'Updated Jan 30, 2021', 'Jupyter Notebook', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Regression-Analysis\n'], 'url_profile': 'https://github.com/teddy2037', 'info_list': ['Python', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', '1', 'MATLAB', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Python', 'MIT license', 'Updated Jan 30, 2021', 'Jupyter Notebook', 'Updated Mar 19, 2020']}","{'location': 'Toronto, Ontario', 'stats_list': [], 'contributions': '404 contributions\n        in the last year', 'description': ['Single/Multi-Variable-Linear-Regression\nImplementation of a single/multi variable linear regression model on a sample data set provided by Coursera written using Octave/Matlab\nCreated cost function, gradient descent for both single and multi variable\n'], 'url_profile': 'https://github.com/JJwilkin', 'info_list': ['Python', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', '1', 'MATLAB', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Python', 'MIT license', 'Updated Jan 30, 2021', 'Jupyter Notebook', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Python-Regression\n'], 'url_profile': 'https://github.com/qwertyuiop2k20', 'info_list': ['Python', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', '1', 'MATLAB', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Python', 'MIT license', 'Updated Jan 30, 2021', 'Jupyter Notebook', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/Smeths', 'info_list': ['Python', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', '1', 'MATLAB', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Python', 'MIT license', 'Updated Jan 30, 2021', 'Jupyter Notebook', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Linear-Regression\nA simple Linear Regression ML model to predict the price of the plot using a dummy dataset.\nThis simple project includes a dummy self made dataset to work with Linear Regression.\n'], 'url_profile': 'https://github.com/hasankhan88999', 'info_list': ['Python', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', '1', 'MATLAB', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Python', 'MIT license', 'Updated Jan 30, 2021', 'Jupyter Notebook', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['Multiplelinear_regression\n'], 'url_profile': 'https://github.com/Kulna11', 'info_list': ['Python', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', '1', 'MATLAB', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Python', 'MIT license', 'Updated Jan 30, 2021', 'Jupyter Notebook', 'Updated Mar 19, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '165 contributions\n        in the last year', 'description': ['Logistic_regression\n'], 'url_profile': 'https://github.com/Tabrez911', 'info_list': ['Python', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', '1', 'MATLAB', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Python', 'MIT license', 'Updated Jan 30, 2021', 'Jupyter Notebook', 'Updated Mar 19, 2020']}","{'location': 'Athens', 'stats_list': [], 'contributions': '111 contributions\n        in the last year', 'description': ['Regression\ndatasets are used from http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/frame.html\nHow to run\nrun any python3 health.py , bicarbonate.py or hollywood.py for the already implemented datasets\nIn health.py dataset contents are X1 = death rate per 1000 residents\nX2 = doctor availability per 100,000 residents\nX3 = hospital availability per 100,000 residents\nX4 = annual per capita income in thousands of dollars\nX5 = population density people per square mile\nIn bicarbonate.py dataset X = pH of well water\nY = Bicarbonate (parts per million) of well water\nIn hollywood.py dataset X1 = first year box office receipts/millions\nX2 = total production costs/millions\nX3 = total promotional costs/millions\nHow to run any model\nThe regression.py file implements the Regression class where it can calculate a problem.\nThe data is entered in the class initialization model = Regression (y, x)\nTo enter the data y must be an NumPy array of n lines and 1 column and this will be the target of our prediction.\nx should be a table of NumPy n lines equal to y and m columns where the different attributes will be for a Multiple Variable Regression\nAfter the data entry, the model needs training\nFor training we use the train (a, epochs, print_loss) function where a is the learning rate epochs are the iterations of the learning process\nprint_loss is a True or False value if we want to print it in real time the loss of the regression\nAnd finaly to predict the results we use the predict function predict(x1, x2 ...)\nwhich input are the X different attributes from which we want to arrive at a prediction Y\nVisualization\nHollywood movies revenue prediction based on production and promotion costs\n\nand loss in epochs\n\nPrediction in ppm of bicarbonates based on ph of water\n\n'], 'url_profile': 'https://github.com/AGiannoutsos', 'info_list': ['Python', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', '1', 'MATLAB', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Python', 'MIT license', 'Updated Jan 30, 2021', 'Jupyter Notebook', 'Updated Mar 19, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yashdalvi007', 'info_list': ['Python', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', '1', 'MATLAB', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', '1', 'Python', 'MIT license', 'Updated Jan 30, 2021', 'Jupyter Notebook', 'Updated Mar 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/rubythalib33', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Sep 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Chriskang028', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Sep 18, 2020']}","{'location': 'Kharagpur', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/swapnil-ss', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Sep 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kraumar', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Sep 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anuj-kh', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Sep 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Logistic-Regression\nThis is a simple logistic regression classifier model. The model is trained to predict if a candidates with aptitude test scores get admitted or not. The model calculates the probabilty of a candiates admission success given the candidates test scores.\n'], 'url_profile': 'https://github.com/waltwissle', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Sep 18, 2020']}","{'location': 'Dhaka,Bangladesh', 'stats_list': [], 'contributions': '1,021 contributions\n        in the last year', 'description': ['PolynomialRegression\n'], 'url_profile': 'https://github.com/MdShahadatHossainbd', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Sep 18, 2020']}","{'location': 'New Delhi', 'stats_list': [], 'contributions': '150 contributions\n        in the last year', 'description': ['LinearRegression\n'], 'url_profile': 'https://github.com/rehanfazalkhan', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Sep 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['Logistic-regression\n'], 'url_profile': 'https://github.com/Kulna11', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Sep 18, 2020']}","{'location': 'Trinidad and Tobago', 'stats_list': [], 'contributions': '427 contributions\n        in the last year', 'description': ['Peer-graded Assignment: Regression Models Course Project\nOverview\nThis repo was prepared by Yohance Nicholas in partial fulfilment of the Regression Models Course which comprises one of the five courses necessary for the Data Science: Statistics and Machine Specialization offered by Johns Hopikins University through Coursera.\nIn this report, candidates wear the hat of a Motor Trend Researcher who seeks to explore the relationship between vehicle characteristics and fuel efficiency with the assistance of the mtcars data set.     This data, which was extracted from the 1974 Motor Trend US magazine, comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\nExploratory data analyses and regression models are utilised to mainly explore the extent to which  automatic (am = 0) and manual (am = 1) transmissions features impact fuel efficiency, as measured by the the Miles/(US) gallon variable - MPG .   The research begins with data preparation and exploratory data analysis. Subsequently, several candidate linear regression models are estimated in order to identify the model with the best fit - i.e. the highest Adjusted R-squared value.\nKey Questions\nThe Board fo Directors at Motor Trend is particularly interested in the following:\n\nIs an automatic or manual transmission better for MPG?\nQuantify the MPG difference between automatic and manual transmissions""\n\nContents\nThis repository contains the following files:\n\nREADME.md- which provides an overview of the dataset and how it was created\nregression_mpg_transmission.Rmd - which contains the R Markdown for the report generated\nregression_mpg_transmission.pdf- the PDF submitted to Coursera\nRegression-Models.zip- which contains high resolution PNGs of the plots generated in the investigation\n\n'], 'url_profile': 'https://github.com/yohance-nicholas', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 26, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Updated Sep 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Predicting_Diabetes_via_Logistic_Regression\nPredicting Diabetes via Logistic_Regression\n'], 'url_profile': 'https://github.com/ziodos', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jun 11, 2020', 'R', 'Updated Jun 9, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '165 contributions\n        in the last year', 'description': ['Ridge_Lasso_regression\n'], 'url_profile': 'https://github.com/Tabrez911', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jun 11, 2020', 'R', 'Updated Jun 9, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/quant-jaime', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jun 11, 2020', 'R', 'Updated Jun 9, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Beijing, China', 'stats_list': [], 'contributions': '368 contributions\n        in the last year', 'description': ['Linear-Regression-Prediction\n使用多元线性回归预测北京房价。Use multiple linear regression to forecast Beijing house price.\n输出线性回归模型分别在训练集和测试集上的效果图，以及在训练集和测试集上的误差。\n最后可以交互式得进行房价预测。\n\nTrain Data:\n\nTest Data:\n\n交互预测:\n\n\n'], 'url_profile': 'https://github.com/stxupengyu', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jun 11, 2020', 'R', 'Updated Jun 9, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Bhaktapur', 'stats_list': [], 'contributions': '122 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rachanakafle', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jun 11, 2020', 'R', 'Updated Jun 9, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ARUSHIMITTAL93', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jun 11, 2020', 'R', 'Updated Jun 9, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Regression Discontinuity Design\nAbstract\nThe regression discontinuity (RD) design is a branch of the observational study, which locally resembles the randomized experiment and is used to estimate the local causal effect of a treatment that is assigned fully, or partly by the value of a certain variable and a threshold. As the RD design is the subject of the causal inference, important concepts of the causal inference are covered to properly proceed discussions. Based on those concepts, the fundamental idea and structure of the RD design are explained including two sub types of the design: the sharp and the fuzzy RD designs. Furthermore, the assumptions of the RD design is formulated, which have been slightly different in different fields. In order to accurately estimate the local causal effect without confounding, we introduce the bandwidth and use the data that are within the bandwidth away from a threshold only. Since there is still no settled way of finding a ""good"" bandwidth, we propose a novel approach for bandwidth selection along with two existing methods. Performances of these bandwidth selection methods are compared with simulated data, and it can be inferred that the newly proposed method may yield better results. At the end, we intentionally violate the unconfoundedness assumption and analyze three potential confounding models with simulated data.\nKeywords: Regression discontinuity design; Causal inference; Observational study; Randomized Experiment; Bandwidth selection; Sensitivity analysis\nThis repository contains the R source codes that were used to generate the figures and results of my thesis. Names of the R codes with corresponding figures or algorithms in the paper are listed below.\n\nRandomExpSolvesConfounding.R : Figure 2.2, 2.3\nAssignmentProbabilityConditionalExpectations.R : Figure 3.1, 3.2, 3.3, 3.4\nLocallyParamApp.R : Figure 4.1\nEconCVFigure.R : Figure 4.2\nBandwidthSelectionResults.R : Figure 4.3, Tabular Results in Appendix A\nEstimationFRD.R : Figure 4.4, 4.5\nXYconfounding.R : Figure 5.3\nXTconfounding.R : Figure 5.5\nEconCV.R : Cross Validation Approach (Ludwig and Miller, 2005; Imbens and Lemieux, 2008)\nmywindow.R : Newly Proposed Method\n\n'], 'url_profile': 'https://github.com/hkk828', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jun 11, 2020', 'R', 'Updated Jun 9, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Zanjan', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['Regulized Weighted Logistic Regression\nThis codes provides a theoritical implementation of the Regulized Weighted Logistic Regression from scratch.\noutput\nAn example illustration of the output is provided below.\n\n'], 'url_profile': 'https://github.com/arnejad', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jun 11, 2020', 'R', 'Updated Jun 9, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MayurG25', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jun 11, 2020', 'R', 'Updated Jun 9, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}","{'location': 'Pilani, Rajasthan, India', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['My solutions for University of Washington\'s MOOC ""Machine Learning: Regression"" taught by Prof. Carlos Guestrin and Prof. Emily Fox.\n'], 'url_profile': 'https://github.com/f2015537', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 17, 2020', 'Jupyter Notebook', 'Updated Jun 11, 2020', 'R', 'Updated Jun 9, 2020', 'MATLAB', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020']}"
"{'location': 'Mumbai', 'stats_list': [], 'contributions': '165 contributions\n        in the last year', 'description': ['Simple_linear_regression\n'], 'url_profile': 'https://github.com/Tabrez911', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/liammoore44', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '206 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SyedSaifAli1234', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Jaipur, Rajasthan, India', 'stats_list': [], 'contributions': '794 contributions\n        in the last year', 'description': ['Simple_Linear_Regression\n'], 'url_profile': 'https://github.com/Purvanshsingh', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wuyuchong', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'London, UK', 'stats_list': [], 'contributions': '699 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mattkerlogue', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Abu Dhabi, UAE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Simple-linear-Regression\n'], 'url_profile': 'https://github.com/Dhairyapbt', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Linear-and-ridge-regression\nExercitiul 1\nfunctia normalize(train_data, test_data):\n\ndefinim si antrenam modelul pe datele de antrenare date ca parametru (train_data);\nscalam datele de antrenare;\nverficam daca datele de testare date ca parametru (test_data) sunt de tipul None;\ndaca da, returnam doar datele de antrenare;\naltfel, scalam si datele de testare.\n\nExercitiul 2\n\nincarcam datele de antrenare (training_data) si etichetele (prices) si le impartim pe ambele in trei vectori (numpy arrays) de lungimi egale.\n\nfunctia linear_regression(train_data, train_labels, test_data, test_labels):\n\nnormalizam datele de antrenare si datele de testare;\ndefinim si antrenam modelul de regresie liniara pe datele de antrenare;\nprezicem etichetele datelor de testare;\ncalculam mean absolute error (mae), care reprezinta valoarea medie a diferantei absolute dintre valorile estimate si cele reale;\nsi mean squared error (mse), care reprezinta valoarea medie a difentei la patrat dintre valorile prezise si valorile reale.\n\nfunctia linear_mae_mse():\n\nsalvam cele trei mae si mse rezultate in urma apelarii functiei linear_regression pentru datele de antrenare si etichetele impartite in trei;\ncalculam valoare medie a celor 3 mae si mse.\n\nExercitiul 3\nfunctia linear_regression(train_data, train_labels, test_data, test_labels, alpha):\n\nnormalizam datele de antrenare si datele de testare;\ndefinim si antrenam modelul de regresie ridge cu paramentrul alpha pe datele de antrenare;\nprezicem etichetele datelor de testare;\ncalculam mean absolute error (mae), care reprezinta valoarea medie a diferantei absolute dintre valorile estimate si cele reale;\nsi mean squared error (mse), care reprezinta valoarea medie a difentei la patrat dintre valorile prezise si valorile reale.\n\nfunctia ridge_mae_mse():\n\npentru fiecare alpha din intervalul [1, 10, 100, 1000]:\nsalvam cele trei mae si mse rezultate in urma apelarii functiei ridge_regression pentru datele de antrenare si etichetele impartite in trei;\ncalculam valoare medie a celor 3 mae si a celor 3 mse;\nin cazul primului element din interval, initializam variabila best;\nverficam daca pentru parametrul alpha performanta este mai buna decat pentru parametrul best;\ndaca da, actualizam variabila best;\nvariabila best reprezinta parametrul cu cea mai buna performanta.\n\nExercitiul 4\nfunctia ridge_(train_data, train_labels):\n\nfolosim functia definita anterior pentru a afla paramentrul cu cea mai buna performanta;\nnormalizam datele de antrenare;\ndefinim si antrenam modelul de regresie ridge pe datele de antrenare;\ncalculam si afisam coeficienti si bias-ul;\nafisam cel mai semnificativ si cel mai putin semnificativ atribut cu ajutorul functiilor np.argmax si np.argmin, care returneaza indicele valorii maxime/minime dintr-un array;\nafisam al doilea cel mai semnificativ atribut cu ajutorul functiei np.argpartition care returneaza un array de indici a elementelor partitionate din care luam doar penultimul element.\n\n'], 'url_profile': 'https://github.com/popa-andreea', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/georgina1994', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alonsoggpablo', 'info_list': ['Jupyter Notebook', 'Updated Mar 30, 2020', 'Python', 'Updated Mar 27, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}"
"{'location': 'Hyderabad', 'stats_list': [], 'contributions': '182 contributions\n        in the last year', 'description': ['Ridge-and-Lasso-Regression\n'], 'url_profile': 'https://github.com/sharathchandrasuroj', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Python', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'bangalore', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Heroku\nUsing linear regression model Predicting sales achieved  by sales person\n'], 'url_profile': 'https://github.com/Sandhyakumari15', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Python', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/renatogritti', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Python', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Co-op-Regression-Prep\n\nRegression Overview\nStatsmodels Linear Regression\nsklearn Linear Regression\nsklearn Logistic Regression\ni. Data Preprocessing\nii. Machine Learning\niii. Deploying the Model\nPostgreSQL + Python\n\n'], 'url_profile': 'https://github.com/nfed5', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Python', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '334 contributions\n        in the last year', 'description': ['MULTIVARIATE-LINEAR-REGRESSION\nAssignment\nWe will predict employee salaries from different employee characteristics (or features).\nImport the data salary.csv to a Jupyter Notebook.\nA description of the variables is given in Salary metadata.csv.\nYou will need the packages matplotlib / seaborn, pandas and statsmodels.\n'], 'url_profile': 'https://github.com/Mahnatse-rgb', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Python', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['multivariate_linear_regression\n'], 'url_profile': 'https://github.com/AllenMkandla', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Python', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sakshipthk', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Python', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dedol1', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Python', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Mini_Project_Linear_Regression\n'], 'url_profile': 'https://github.com/vnguye12', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Python', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'Birmingham, Alabama, USA', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/yanlab19870714', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Apr 1, 2020', '1', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Jul 9, 2020', 'Python', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/paulina-rega', 'info_list': ['Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated May 20, 2020', 'MATLAB', 'MIT license', 'Updated May 14, 2020', 'R', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'Istanbul, Turkey', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fsahbaz', 'info_list': ['Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated May 20, 2020', 'MATLAB', 'MIT license', 'Updated May 14, 2020', 'R', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['The multivariate Regression Model\n'], 'url_profile': 'https://github.com/tshepojrampai', 'info_list': ['Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated May 20, 2020', 'MATLAB', 'MIT license', 'Updated May 14, 2020', 'R', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'London, England', 'stats_list': [], 'contributions': '4,605 contributions\n        in the last year', 'description': ['linear-regression-london-ds\n\n\nLinear Regression\n\n\nLinear Regression Extras\n\n\nMultiple Linear Regression\n\n\nExit Ticket | Introduction to Linear Regression\n\n\nLearning Goals\nIntroduction to Linear Regression\nStudent should be able to:\n\n Identify problems where linear regression can be applied\n Explain what a statistical model is in simple language\n Describe what kind of DV is needed for linear regression\n Describe what kind of IVs can be used for linear regression\n Define and describe a line of best fit\n Define what a residual is\n Describe what the slope of the model represents\n Run a linear regression model using statsmodels with a singl continuous IV.\n\nMultiple Linear Regression\n\nComing Soon!\n\nLinear Regression Assumptions\n\nComing Soon!\n\n'], 'url_profile': 'https://github.com/davidjohnbaker1', 'info_list': ['Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated May 20, 2020', 'MATLAB', 'MIT license', 'Updated May 14, 2020', 'R', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Linear-Regression.ipynb\n'], 'url_profile': 'https://github.com/ankitsuman97', 'info_list': ['Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated May 20, 2020', 'MATLAB', 'MIT license', 'Updated May 14, 2020', 'R', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Logistic_Regression_Titanic.ipynb\n'], 'url_profile': 'https://github.com/ankitsuman97', 'info_list': ['Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated May 20, 2020', 'MATLAB', 'MIT license', 'Updated May 14, 2020', 'R', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['Linear-Regression-Application\nAn application that reads excel files and performs linear regression on data to give predictions for parameters provided.\nIt does not use TensorFlow or Keras, all the functions are handwritten.\nThen data is normalized before processing and predictions can be made based upon the data fed.\nThe data given in the excel file contains:\nRow1: The size of the plot of a house.\nRow2: The number of bedrooms in the house.\nRow3: The price for the house.\nParameters should be fed into the GUI seperated by a space.\n'], 'url_profile': 'https://github.com/namanbansalcodes', 'info_list': ['Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated May 20, 2020', 'MATLAB', 'MIT license', 'Updated May 14, 2020', 'R', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'Cádiz, Andalucía, Spain.', 'stats_list': [], 'contributions': '3,056 contributions\n        in the last year', 'description': ['COVID 19 Prediction using Regression.\nPredict the progression of the coronavirus infection in Spain assuming that the trend is expected to be similar to the Chinese trend.\n'], 'url_profile': 'https://github.com/Jeffresh', 'info_list': ['Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated May 20, 2020', 'MATLAB', 'MIT license', 'Updated May 14, 2020', 'R', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['ANOVA_and_Regression\nFitting ANOVA and multiple linear regression model to mtcars dataset using R programming\n'], 'url_profile': 'https://github.com/ganeshbhatms', 'info_list': ['Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated May 20, 2020', 'MATLAB', 'MIT license', 'Updated May 14, 2020', 'R', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}","{'location': 'Bennett University', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['Sales-Linear-Regression-\n'], 'url_profile': 'https://github.com/pgandhi03', 'info_list': ['Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated May 20, 2020', 'MATLAB', 'MIT license', 'Updated May 14, 2020', 'R', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020']}"
"{'location': 'Dortmund', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Weighted Cox Regression\nOur method is focused on the situation of predefined, possibly heterogenous subgroups of patients with available survival endpoint and high-dimensional molecular measurements such as gene expression\ndata, with the aim of obtaining a separate risk prediction model for each subgroup.\nFor this purpose, we propose a l1-penalized Cox regression model with a weighted version of the partial likelihood that includes patients of all subgroups but assigns them individual weights.\nThe weights are estimated from the data such that patients who are likely to belong to the subgroup of interest receive a higher weight.\nWe compare our approach to fixed weights and unweighted Cox models in simulations and application to real lung cancer cohorts.\nThe main file to run the simulation study is Run_Simulation.R and the main file for the real data application is Run_RealDataApplication.R.\nFigures and tables can be generated based on the results collected in Collect_Results.R.\nOverview of R files:\nRun_Simulation.R\nSetup and running of the simulation, including computation and evaluation of our proposed weighted Cox model, as well as the two standard unweighted Cox models and the weighted Cox model with fixed weights.\nAll simulations are run using the R package batchtools for parallelization.\nRun_RealDataApplication.R\nSetup and running of the real data application, including computation and evaluation of our proposed weighted Cox model, as well as the two standard unweighted Cox models and the weighted Cox model with fixed weights.\nAll models can be run with different gene filters and covariate sets (here only genetic covariates are included).\nAll settings are run using the R package batchtools for parallelization.\nDataSimulation.R\nHelper functions for generation of simulated (training and test) data. Only needed in simulation study, not real data application.\nStratifiedSubsampling.R\nFunction for stratified random subsampling and generation of training and test data (""problem"" function in batchtools, see file Run_RealDataApplication.R). Only needed in real data application, not in simulation study.\nSurvivalWrapper.R\nWrapper function for running of a specific type of Cox model with, in the case of the real data application, a specific covariate set and gene filter.\nThe cox model is fitted based on the training data and evaluated based on the test data.\nThis function includes the following three functions.\nDataPreparation.R\nFunction for preparation of the training and test data for model fitting and evaluation: standardization of numerical covariates, application of the gene filter to the genetic covariates (if defined), and definition of a penalty value for each covariate (only interesting in the case of a combination of clinical and genetic covariates, where only genes are penalized and clinical variables are included as mandatory (unpenalized) into the model).\nEstimateWeights.R\nFunction to determine individual weights for each patient in the training set and in each subgroup model (either estimated weights as proposed by us with different classification methods or fixed weights as proposed by Weyer and Binder, 2015).\nCoxModelBuilder.R\nFunction to fit (weighted) l1-penalized Cox model.\nCollect_Results.R\nCollect and save all results from the simulations and the real data application (these results were used for the figures and tables in the corresponding publication).\nData:\nWeibull_param.RData\nRData object with parameters of the Weibull distribution used for data simulation (see file DataSimulation.R)\nRealdata_LC.RData\nRData object with preprocessed data of lung cancer cohorts, including information on survival endpoint, clinical covariates, genetic covariates, cohort membership of each patient, and gene filters (see file Run_RealDataApplication.R)\n'], 'url_profile': 'https://github.com/KatrinMadjar', 'info_list': ['R', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated May 30, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Python', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Mini_Project_Logistic_Regression\n'], 'url_profile': 'https://github.com/vnguye12', 'info_list': ['R', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated May 30, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Python', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Project 2 README\nTable of Contents\n1.0 Directory Structure\n2.0 Problem Statement\n3.0 Description of Data\n-3.1 Size\n-3.2 Source\n-3.3 Data Dictionary\n4.0 Data Visualization\n5.0 Conclusion\n6.0 Outside Sources\n7.0 Extras\n1.0 Directory Structure\n.\n├── project_2\n    ├── code\n        ├── p2main.ipynb\n    ├── dsi-us-10-project-2-regression-challenge\n        ├── datasets\n        ├── sample_sub_reg.csv\n        ├── test.csv\n        ├── train.csv\n    ├── plots\n        ├── allnumeric.png\n        ├── box.png\n        ├── dist.png   \n        ├── log.png\n    ├── submissions\n    ├── extras *\n    ├── README.md\n    └── project2_slides.pdf\n\n2.0 Problem Statement\nHere we have a robust dataset of real estate in Ames, Iowa. Using our our newly honed skills in areas including but not limited to feature engineering, polynomial feature creation, and linear regression modeling, we shall attempt to use our training data to accurately predict sale prices on a test set. In doing so, we will compete against ourselves, and our fellow General Assembly students on Kaggle.\n\n3.0 Description of Data\n3.1 Size\nTrain: 2051 rows x 81 columns \nTest: 878 rows x 80 columns \n3.2 Source\nhttps://www.kaggle.com/c/dsi-us-10-project-2-regression-challenge/data\n3.3 Data Dictionary\nhttp://jse.amstat.org/v19n3/decock/DataDocumentation.txt\n\n4.0 Data Visualization\n\n\n\n\n\n\n\n5.0 Conclusion\nThere are many approaches one can take when creating a model for this data. In my experience, plotting all numeric data against the target provided a result that was decent, but consistently underestimated the more expensive houses. There was a slight curve in the model. Therefore the greatest benefit came from using the log of y as the target. This, combined with just a few of the best variables created from our polynomial features, is the approach that gave me the best submission.\n\n6.0 Outside Sources\n\nhttps://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia\nhttps://community.alteryx.com/t5/Data-Science-Blog/Bias-Versus-Variance/ba-p/351862\nhttps://www.superheuristics.com/linear-regression-is-inaccurate-and-misleading/\n\n7.0 Extras\nExtras is in part for visualizations found online, but also there are two images containing the word ""suptitle"". In matplotlib, you can add a suptitle to be the overarching title for multiple subplots, but if you want the suptitle to appear higher than the subplot titles, you have to set y > 1, which causes it to get cut off when using plt.savefig(). Therefore those two plots had to be included in the slides via screenshots.\n'], 'url_profile': 'https://github.com/escBoston', 'info_list': ['R', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated May 30, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Python', 'Updated Mar 22, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nA simple Linear regression for Real estate dataset(Price and Size)\nKindly find the Real estate dataset used in this project attached\n'], 'url_profile': 'https://github.com/Benniah', 'info_list': ['R', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated May 30, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Python', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/agentdoublea', 'info_list': ['R', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated May 30, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Python', 'Updated Mar 22, 2020']}","{'location': 'Brisbane, Queensland', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['ML- Regression Examples\n'], 'url_profile': 'https://github.com/moe-ai', 'info_list': ['R', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated May 30, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Python', 'Updated Mar 22, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['Linear Regression on Boston Housing Dataset\nWe will take the Housing dataset which contains information about different houses in Boston. This data was originally a part of UCI Machine Learning Repository and has been removed now. We can also access this data from the scikit-learn library. There are 506 samples and 13 feature variables in this dataset. The objective is to predict the value of prices of the house using the given features.In this File Complete analysis of Boston data has been done on given variables.\nFollowing Actions has been taken in attached two files:\n\nUnderstanding Statistics of Data\nResidual Plots\nTransformations\nFeature Engineering\nFeature Selection\n\n'], 'url_profile': 'https://github.com/neerajseth000', 'info_list': ['R', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated May 30, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Python', 'Updated Mar 22, 2020']}","{'location': 'Johannesburg ', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\n'], 'url_profile': 'https://github.com/Faithie30', 'info_list': ['R', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated May 30, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Python', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Machine Learning Regression: House Prices\nThis is a Kaggle project I made to practice regression problems. My final position was 3343 out of 4570, which means I have a lot of room for improvement but I believe it isnt that bad for my first Kaggle competition.\nIn the future I will do some feature engineering and selection on the categorical features, which I didnt pay enough attention to, and use the gradient boosting regressor method instead of random forests, as I believe is more appropiate for this type of regression problems.\nUpdate v.1:\nGradient boosting improved my score from 0.17 to 0.145 which is a significant improvement, going from the percentile 70th to the percentile 50th (mean).\nDataset\nHere is the link of the data, including a description of every column.\n'], 'url_profile': 'https://github.com/Jvmd95', 'info_list': ['R', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated May 30, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Python', 'Updated Mar 22, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['Multiple_Linear_Regression\nPrediction of Profits (Dependent Var ) of 50 Startups companies with multiple features- R&D,States,Marketing ( independent var )\n'], 'url_profile': 'https://github.com/suyogyaman', 'info_list': ['R', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'Jupyter Notebook', 'Updated May 30, 2020', 'Jupyter Notebook', 'Updated Jul 17, 2020', '1', 'Jupyter Notebook', 'Updated Apr 7, 2020', 'Python', 'Updated Mar 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['suv_logistic_regression.ipynb\n'], 'url_profile': 'https://github.com/ankitsuman97', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['A trial repository to automate the submission of kaggle kernels from the local machine command line through the GitHub Workflow service. Borrowed from @harupy.push-kaggle-kernel.\n'], 'url_profile': 'https://github.com/cshamruk', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '220 contributions\n        in the last year', 'description': ['Linear_Regression_Demo\nAuthor: Nguyen Huynh Dang Khoa - Victor Nguyen\nJanuary-February 2020\nMajor: Embedded System - Computer Science\nUniversity: Danang University of Sciences and Technology\nAbout\nHere are some demo for those learning AI and ML or DL. This will demonstrate how Linear regression work by Gradient Descent. You can check the log of the browser while running it.\n\nThis is the link I hosted on 000.webhostapp.com: Linear regression by VictorES.\n\nHow to use ?\n\nPut the weight and bias\nPress Plot: Plot the y = w*x + b;\nPress Random: random 100 point around the line\nPress LRGD: Here the program will calculate the weight and bias by Gradient Descent.\nPress Reset: Reset the plot.\nIf there are any problem, please feel free to contact me at my email: khoanguyen1507dn@gmail.com\n\nHope you love this.\n'], 'url_profile': 'https://github.com/Winnguyen1511', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 31, 2020']}","{'location': 'bangalore', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': ['Linear-Regression\nA linear regression is one of the easiest statistical models in machine learning. Understanding its algorithm is a crucial part of the Data Science Certification’s course curriculum. It is used to show the linear relationship between a dependent variable and one or more independent variables.\n'], 'url_profile': 'https://github.com/shaik4182', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '200 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/wahabrind', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['Linear-Regression-Forecasting\n'], 'url_profile': 'https://github.com/abhishekjaglan', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 31, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '165 contributions\n        in the last year', 'description': ['Multiple_linear_regression\n'], 'url_profile': 'https://github.com/Tabrez911', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Multi-linear-Regression\n'], 'url_profile': 'https://github.com/Sibusiso1995', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kristiee', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jayagn', 'info_list': ['Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 19, 2020', 'JavaScript', 'Updated Apr 27, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Apr 11, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 30, 2020', 'Jupyter Notebook', 'Updated Apr 14, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'R', 'Updated Mar 31, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cusescholar', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 22, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'Morocco', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['LogisticRegression_BankruptcyDataset\nA logistic regression Classification model to study the existing relationship between an organisation internal risks and bankruptcy.\nInfomation Source:\nCreator:  Martin.A, Uthayakumar.j and Nadarajan.m (uthayakumar17691@gmail.com)\nGuided By: Dr.V.Prasanna Venkatesan\nDate:      February 2014\n'], 'url_profile': 'https://github.com/Youtir', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 22, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'Bucharest', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': ['reggie-s-linear-regression\nIn this project, I’ll combine my knowledge of lists, loops, and syntax to help a mad scientist perform some calculations on his data.\n'], 'url_profile': 'https://github.com/lorenz-terita', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 22, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/neerajseth000', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 22, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'Dallas', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Khushboobhatiautd', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 22, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shiv96', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 22, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['LinearRegression_diabetes\nLinear Regression using python on diabetes dataset\nData Set Description:\nIn this project, we will build a simple but robust linear regression model from scratch in Python and use it to predict the Blood Sugar of Diabetes Patients from their BMI  and BP data.\nRead more about Body Mass Index and how it impacts people’s health in various ways, and ultimately how it affects Blood Sugar among diabetes patients Here: https://en.wikipedia.org/wiki/Body_mass_index.\nData was taken from  http://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt is showing Blood Sugar (in mg/l) of 442 diabetes patient with 10 baseline variables, age, sex, body mass index, average blood pressure,  six blood serum measurements and finally target(Blood Sugar of patient).\nAgenda of the Project:\nFitting data around straight line equation y = mx+b\nComputing errors via Mean Square Error.\nMinimizing Errors using partial derivatives aka Gradient Descent\nComputing new m & b by using Gradient Descent with Learning Rate over many iterations\nUse this new m & b to predict Blood Sugar of sample/test data of BMI and BP.\nData Preprocessing:\nFor the DataSet which I have luckily has all integer values which makes clean of data easy😊.\nSelection of features:\n\nAs we have seen from the above, data has 9 input features and one output variable i.e(Blood Sugar of a patient).\nAfter a clear study and observation of the input features of the Diabetes Data. The BMI and the BP are the most co-related features for the prediction of the Blood sugar of the patient.\n\nBMI<18.5:\tBELOW NORMAL WEIGHT\nBMI>=18.5 AND <25:\tNORMAL WEIGHT\nBMI>=25 AND <30\tOVERWEIGHT\nBMI>=30 AND <35\tCLASS 1 OBESITY\nBMI>=35 AND <40\tCLASS 2 OBESITY\nBMI>=40\tCLASS 3 OBESITY\nThe people with BMI >25 and also having Hypertension will have a  high chance of getting diabetes.\n\nMoreover using the Pearson correlation coefficient the BMI and BP are having the highest values compared to other input features with the target output value.\n\nIn this project, we are going to use BMI and BP as two input features and Blood sugar becomes our output feature for training our model.\nHow to run the code and use the application:\nTwo methods to run the code\n1.Using Google Colab\n2.Jupyter Notebook.\nHere, I will tell you how to run the code in google colab\nStep1: Open a google colab in your browser\nStep2: Open a new notebook and click on the connect button which is located at the top right corner of the notebook.\nStep3: Wait until it connects, make sure that you get  RAM and disk space over there.\nStep4: Copy the code and run it using CLTL+ENTER keys.\n'], 'url_profile': 'https://github.com/yenduripavankalyan', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 22, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'Athens, Greece', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/giorgostsilivis', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 22, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nUsing Sweden Insurance Data Set\n'], 'url_profile': 'https://github.com/Pythonmite-Meet', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 22, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rahulbhadja', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 22, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['LinearRegression_diabetes\nLinear Regression using python on diabetes dataset\nData Set Description:\nIn this project, we will build a simple but robust linear regression model from scratch in Python and use it to predict the Blood Sugar of Diabetes Patients from their BMI  and BP data.\nRead more about Body Mass Index and how it impacts people’s health in various ways, and ultimately how it affects Blood Sugar among diabetes patients Here: https://en.wikipedia.org/wiki/Body_mass_index.\nData was taken from  http://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt is showing Blood Sugar (in mg/l) of 442 diabetes patient with 10 baseline variables, age, sex, body mass index, average blood pressure,  six blood serum measurements and finally target(Blood Sugar of patient).\nAgenda of the Project:\nFitting data around straight line equation y = mx+b\nComputing errors via Mean Square Error.\nMinimizing Errors using partial derivatives aka Gradient Descent\nComputing new m & b by using Gradient Descent with Learning Rate over many iterations\nUse this new m & b to predict Blood Sugar of sample/test data of BMI and BP.\nData Preprocessing:\nFor the DataSet which I have luckily has all integer values which makes clean of data easy😊.\nSelection of features:\n\nAs we have seen from the above, data has 9 input features and one output variable i.e(Blood Sugar of a patient).\nAfter a clear study and observation of the input features of the Diabetes Data. The BMI and the BP are the most co-related features for the prediction of the Blood sugar of the patient.\n\nBMI<18.5:\tBELOW NORMAL WEIGHT\nBMI>=18.5 AND <25:\tNORMAL WEIGHT\nBMI>=25 AND <30\tOVERWEIGHT\nBMI>=30 AND <35\tCLASS 1 OBESITY\nBMI>=35 AND <40\tCLASS 2 OBESITY\nBMI>=40\tCLASS 3 OBESITY\nThe people with BMI >25 and also having Hypertension will have a  high chance of getting diabetes.\n\nMoreover using the Pearson correlation coefficient the BMI and BP are having the highest values compared to other input features with the target output value.\n\nIn this project, we are going to use BMI and BP as two input features and Blood sugar becomes our output feature for training our model.\nHow to run the code and use the application:\nTwo methods to run the code\n1.Using Google Colab\n2.Jupyter Notebook.\nHere, I will tell you how to run the code in google colab\nStep1: Open a google colab in your browser\nStep2: Open a new notebook and click on the connect button which is located at the top right corner of the notebook.\nStep3: Wait until it connects, make sure that you get  RAM and disk space over there.\nStep4: Copy the code and run it using CLTL+ENTER keys.\n'], 'url_profile': 'https://github.com/yenduripavankalyan', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'R', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'MATLAB', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Oct 1, 2020']}","{'location': 'Athens, Greece', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/giorgostsilivis', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'R', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'MATLAB', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Oct 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Simple-Linear-Regression\nUsing Sweden Insurance Data Set\n'], 'url_profile': 'https://github.com/Pythonmite-Meet', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'R', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'MATLAB', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Oct 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rahulbhadja', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'R', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'MATLAB', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Oct 1, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\nAdjusted R-squared(deciding if an extra independent variable adds to the explanatory power of a model)\nThis is part of a Udemy Course i took online\nKindly find the dataset used in the model attached\nThe goal was to detremine how an unrelated random factor affects a multiple linear regression model\n'], 'url_profile': 'https://github.com/Benniah', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'R', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'MATLAB', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Oct 1, 2020']}","{'location': 'NIT Durgapur', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['ML_LinearRegression\nLinear Regression code in Python Sklearn using sklearn inbuilt Diabetes dataset\n'], 'url_profile': 'https://github.com/abhi6083', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'R', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'MATLAB', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Oct 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Logistic_Regression_German_Credit\nLogistic Regression for the generic German Credit dataset.\n'], 'url_profile': 'https://github.com/Bastien2c', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'R', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'MATLAB', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Oct 1, 2020']}","{'location': 'karatina university', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/muchirajunior', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'R', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'MATLAB', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Oct 1, 2020']}","{'location': 'Zanjan', 'stats_list': [], 'contributions': '98 contributions\n        in the last year', 'description': ['Weighted-Linear-Regression\nThis codes provides a theoritical implementation of the Weighted Linear Regression from scratch.\nThe purpose of this code is to find the best fitting linear model to the data based on the test sample. The hyperparameter sepcifies the effectiveness of each entity in the training set based on their distance to the test sample. Hence this is a non-parametric method.\noutput\nA illustration of the output based on the hyperparameter is provided below.\n\n'], 'url_profile': 'https://github.com/arnejad', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'R', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'MATLAB', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Oct 1, 2020']}","{'location': 'Katra', 'stats_list': [], 'contributions': '245 contributions\n        in the last year', 'description': ['Basic_Regression_Models\n'], 'url_profile': 'https://github.com/itsabhishekhere', 'info_list': ['Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'R', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'MATLAB', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Oct 1, 2020']}"
"{'location': 'Auckland', 'stats_list': [], 'contributions': '845 contributions\n        in the last year', 'description': ['Simple-LInear-Regression\n'], 'url_profile': 'https://github.com/manvimadandotai', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Mar 1, 2021', '2', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['Multiple-Regression-on-Hospital-Expenditure\nIn this repository for the given continuous data build the Multiple Linear regression model\n'], 'url_profile': 'https://github.com/Koorimikiran369', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Mar 1, 2021', '2', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['automatic-feature-design-for-regression\nThe program Galileo\'sRamp.py illustrates how the best possible features for a regression scenario can automatically be designed, here in the case of a polynomial basis.\nThe code includes functions to determine the features or weights and to calculate testing errors. Those are used to apply a ""leave one out 6-fold cross validation"" algorithm.\nTo run the program, the experimental data would have to be imported: https://github.com/jermwatt/machine_learning_refined/blob/gh-pages/mlrefined_datasets/superlearn_datasets/galileo_ramp_data.csv#L2\norigin:\nThe code originally was part of a students\' seminar talk in the Univerity of Cologne. It was made to get a better understanding of the interrelations of regression.\n'], 'url_profile': 'https://github.com/SScholz1994', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Mar 1, 2021', '2', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Boston-housing-forecast-sklearn-\nSeven regression models were used to predict home prices in Boston\n使用 scikit-learn（sklearn）机器学习包进行房价预测，sklearn 中已经内\n置了波士顿房价数据集，在打乱后去 90%数据作为训练集，10%的数据作为测试集。分别利用\nsklearn 提供的线性回归（Linear Regression）、岭回归（Ridge Regression）、鲁棒回归（使\n用 Huber Regression）、支持向量回归（SVR）、最近邻回归（Nearest Neighbors Regression）、\n决策树回归（Decision Trees）、神经网络回归（Neural Network Regression）共七种回归\n算法实现对波士顿房价的预测。并对训练出的七种回归器进行性能评估，利用测试集计算七\n种回归器的四项性能指标：解释方差（Explained Variance）、平均绝对误差（MAE，Mean\nAbsolute Error）、平均平方误差（MSE，Mean Squared Error）和中位绝对误差（MedAE，\nMedian Absolute Error）。\n'], 'url_profile': 'https://github.com/ANobility', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Mar 1, 2021', '2', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['CAT_REG\n'], 'url_profile': 'https://github.com/yunlee', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Mar 1, 2021', '2', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Ames_Housing_Missing_Data\nUsing different methods to impute missing data\n'], 'url_profile': 'https://github.com/juinerurkar', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Mar 1, 2021', '2', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/divyamsinha', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Mar 1, 2021', '2', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Istanbul', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['A-Customer-Churn-Prediction-with-Logistic-Regression\n'], 'url_profile': 'https://github.com/firatgunduz', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Mar 1, 2021', '2', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'Greater Boston', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['InsuranceFraudDetection\nThis project identifies fraudulent car insurance claims in R.\nThe dataset consists of 15,430 claims; each claim comprises 33 attributes.\nThis project was challenging since the dataset was highly imbalanced and contains missing data.\nI have created random forest and logistic regression models in R to identify the fraudulent claims and used feature selection, 10-fold cross validation and upsampling techniques to overcome the data imbalance to improve model accuracy.\n\ncapstone.Rproj is the R project file\nclaims.csv is the dataset\ninsurance_fraud1.Rmd is the R notebook\nLeenaHDamle_ch2879_Capstone_Report.pdf is the project report which summarizes the project, steps of analysis, results at each step and conclusion.\n\n'], 'url_profile': 'https://github.com/LeenaDamle', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Mar 1, 2021', '2', 'Jupyter Notebook', 'Updated Apr 20, 2020']}","{'location': 'São Carlos, Brazil', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['Predicting the next day value for confirmed cases of covid-19 in some countries.\nEvaluating and constructing simple linear regression models to predict the next day number for confirmed cases of covid-19 in Italy, Spain, USA, and Brazil.\n\n\n\n\nItaly\nSpain\nUSA\nBrazil\n\n\n\n\n3/22 prediction\n61,280\n30,220\n36,702\n1,502\n\n\n3/22 real cases\n59,138\n28,768\n33,272\n1,593\n\n\n3/23 prediction\n67,520\n34,738\n40,826\n1,735\n\n\n3/23 real cases\n63,927\n35,136\n43,847\n1,924\n\n\n3/24 prediction\n72,271\n41,391\n58,754\n2,724\n\n\n3/24 real cases\n69,176\n39,885\n53,740\n2,247\n\n\n3/25 prediction\n74,193\n47,498\n71,377\n2,935\n\n\n3/25 real cases\n74,386\n49,515\n65,778\n2,554\n\n\n3/26 prediction\n78,962\n57,916\n84,089\n3,139\n\n\n3/26 real cases\n80,589\n57,786\n83,836\n2,985\n\n\n3/27 prediction\n84,706\n73,091\n105,078\n3,531\n\n\n3/27 real cases\n86,498\n65,719\n101,657\n3,417\n\n\n3/28 prediction\n90,941\n77,689\n128,144\n4,006\n\n\n3/28 real cases\n92,472\n73,235\n121,478\n3,904\n\n\n3/29 prediction\n96,935\n85,432\n150,621\n4,521\n\n\n3/29 real cases\n97,689\n80,110\n140,886\n4,256\n\n\n3/30 prediction\n102,245\n92,105\n160,584\n4,893\n\n\n3/30 real cases\n101,739\n87,956\n161,807\n4,579\n\n\n3/31 prediction\n106,044\n94,074\n178,578\n5,167\n\n\n3/31 real cases\n105,792\n95,923\n188,172\n5,717\n\n\n4/1 prediction\n109,392\n101,756\n203,673\n6,332\n\n\n4/1 real cases\n110,574\n104,118\n213,372\n6,836\n\n\n4/2 prediction\n113,595\n109,769\n234,588\n7,797\n\n\n4/2 real cases\n115,242\n112,065\n243,453\n8,044\n\n\n4/3 prediction\n118,109\n117,753\n264,329\n9,296\n\n\n4/3 real cases\n119,827\n119,199\n275,586\n9,056\n\n\n4/4 prediction\n122,555\n124,819\n300,793\n10,517\n\n\n4/4 real cases\n124,632\n126,168\n308,850\n10,360\n\n\n4/5 prediction\n127,206\n131,260\n337,413\n11,896\n\n\n4/5 real cases\n128,948\n131,646\n337,072\n11,130\n\n\n4/6 prediction\n131,541\n136,493\n367,522\n12,756\n\n\n4/6 real cases\n132,547\n136,675\n366,614\n12,161\n\n\n4/7 prediction\n135,099\n140,747\n392,414\n13,664\n\n\n4/7 real cases\n135,586\n141,942\n396,223\n14,034\n\n\n4/8 prediction\n137,966\n145,266\n420,067\n15,629\n\n\n4/8 real cases\n139,422\n148,220\n429,052\n16,170\n\n\n4/9 prediction\n141,420\n150,903\n451,185\n18,145\n\n\n4/9 real cases\n143,626\n153,222\n461,437\n18,092\n\n\n4/10 prediction\n145,457\n155,970\n484,330\n20,427\n\n\n4/10 real cases\n147,577\n158,273\n496,535\n19,638\n\n\n4/11 prediction\n151,271\n160,685\n519,275\n22,106\n\n\n4/11 real cases\n152,271\n163,027\n526,396\n20,727\n\n\n4/12 prediction\n156,265\n165,240\n550,870\n23,104\n\n\n4/12 real cases\n156,363\n166,831\n555,313\n22,192\n\n\n4/13 prediction\n160,886\n168,959\n578,426\n23,561\n\n\n4/13 real cases\n159,516\n170,099\n580,619\n23,430\n\n\n4/14 prediction\n163,166\n172,002\n602,465\n24,713\n\n\n4/14 real cases\n162,488\n172,541\n605,193\n25,262\n\n\n4/15 prediction\n165,834\n174,265\n624,606\n26,296\n\n\n4/15 real cases\n165,155\n177,644\n636,350\n28,320\n\n\n4/16 prediction\n168,237\n178,613\n653,935\n30,644\n\n\n4/16 real cases\n168,941\n184,948\n667,801\n30,425\n\n\n4/17 prediction\n171,698\n185,435\n684,509\n33,118\n\n\n4/17 real cases\n172,434\n190,839\n699,706\n33,682\n\n\n4/18 prediction\n175,349\n191,701\n716,817\n36,541\n\n\n4/18 real cases\n175,925\n191,726\n732,197\n36,658\n\n\n4/19 prediction\n178,887\n193,430\n768,246\n39,899\n\n\n4/19 real cases\n178,972\n198,674\n759,086\n38,654\n\n\n4/20 prediction\n181,995\n199,192\n778,083\n42,055\n\n\n4/20 real cases\n?\n?\n?\n?\n\n\n\nAverage error = 3.02 %\nError formula\nerror = abs(y_predicted - y_real) / y_real\naverage_error = average(error_i, for each prediction i)\nDataset source\nJohns Hopkins CSSE\n'], 'url_profile': 'https://github.com/lucastsutsui', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'MIT license', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 22, 2020', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated May 8, 2020', 'Jupyter Notebook', 'Updated May 4, 2020', 'HTML', 'Updated Mar 1, 2021', '2', 'Jupyter Notebook', 'Updated Apr 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['HousePricePrediction\nPredicting price of house using Linear Regression algorithm of sklearn library\n'], 'url_profile': 'https://github.com/DhaneshPund', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'MIT license', 'Updated Mar 26, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '547 contributions\n        in the last year', 'description': ['spam-classifier\nBuilt a Spam Classifier using Random Forest, Logistic Regression, SVM and KNN in scikit learn\nSource of Dataset: UCI Machine Learning Repository\n'], 'url_profile': 'https://github.com/acg2176', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'MIT license', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '121 contributions\n        in the last year', 'description': ['ISTA regressor with Lasso regularization\nAn implementation from scratch of Linear and Logistic Regression with elastic-net regularization, Cross Validation, Preprocessing.\nMy article related to this implementation:\nhttps://towardsdatascience.com/unboxing-lasso-regularization-with-proximal-gradient-method-ista-iterative-soft-thresholding-b0797f05f8ea\n'], 'url_profile': 'https://github.com/wincenzo', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'MIT license', 'Updated Mar 26, 2020']}","{'location': 'Gdańsk', 'stats_list': [], 'contributions': '366 contributions\n        in the last year', 'description': ['Introduction-to-DS-I\n'], 'url_profile': 'https://github.com/Lipskii', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'MIT license', 'Updated Mar 26, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['Predictions_of_HousePrice\nAn accurate Linear Regression and gradient boosting classifier that predicts the House Prices.\nThe prediction of house price depends upon factors of x such as place, sqft, how many years old, floor and many.\n'], 'url_profile': 'https://github.com/afrozsamee', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'MIT license', 'Updated Mar 26, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '145 contributions\n        in the last year', 'description': ['Admission_Chances\nbuilt a logistic regression model to predict whether a student gets admitted into a university\nThe data files contains score of 2 exams on which the student is selected in University\nprediction contains the model\n'], 'url_profile': 'https://github.com/gauravmadan583', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'MIT license', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['BreastCancerWinsconsin\nA Machine learning project using Logistic Regression,Seaborn,Pandas,NumPy,Matplotlib and Scikit Learn\nSTEPSS:\n1)install All the libraries\n2)then load the dataset\n3)after seeing the infromation about the columns we get to see that ID and Unnamed 32 have no role in the analysis\n4)change the data.diagnosis from alphbets to binary\n5)here we separate the dependant and independant variables,y will contain the dependant variables and x will have all the indep variables\n6)Normalization\n7)Splitting the data set into training and testing\n8)Weights and Bias\n9)Sigmoid function,Calculating the z values\n10)forward and backward propogation\n11)Updating Parameters\n12)prediction\n13)Logistic Regression\n'], 'url_profile': 'https://github.com/Ayushshah2023', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'MIT license', 'Updated Mar 26, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['MLR-with-Dummy-Variables\nCreating Dummy Variables to enhance the Multiple Linear Regression (Real Estate Data)\nThis is part of a Udemy Course i took for Data Science\nFind the Real Estate dataset used in this code attached.\n'], 'url_profile': 'https://github.com/Benniah', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'MIT license', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '170 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JaloDO', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'MIT license', 'Updated Mar 26, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '184 contributions\n        in the last year', 'description': ['CoronaApproximationCurve\nApproximates the corona virus infection curve using exponential regression and plots it.\nFeatures:\n-select country\n-select predicted day count\n\nLive Page\n'], 'url_profile': 'https://github.com/sugeedarou', 'info_list': ['Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Nov 13, 2020', 'Jupyter Notebook', 'Updated Oct 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Python', 'MIT license', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Python', 'Updated Mar 26, 2020', 'JavaScript', 'MIT license', 'Updated Mar 26, 2020']}"
"{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '409 contributions\n        in the last year', 'description': [""QuantReg.jl\n\n\n\nThis package provides types and functions for specifying, fitting, and computing inference\nfor quantile regression models. This package was modeled after and employs FORTRAN libraries\nfrom the R package quantreg, authored by Roger Koenker.\nCurrently, the package implements many of the abstractions for statistical models from\nStatsBase.jl and requires tabular data in the form of a DataFrame.\nContributions to this package that incorporate more features from the R quantreg package,\nadd new features, or refine existing features are welcomed and encouraged\non the project's GitHub. Please feel free to\nopen an issue or pull request!\n""], 'url_profile': 'https://github.com/fogarty-ben', 'info_list': ['1', 'Julia', 'GPL-3.0 license', 'Updated Apr 17, 2020', 'Python', 'MIT license', 'Updated Aug 24, 2020', 'Java', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 19, 2020', 'C++', 'GPL-3.0 license', 'Updated Mar 20, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'MATLAB', 'Updated Feb 1, 2021', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '221 contributions\n        in the last year', 'description': [""Regression Using Decision Tree, Random Tree, Bootstrap Aggregating, and Boosting\nReference\nBased on project 3 in the Georgia Tech Spring 2020 course Machine Learning for Trading by Prof. Tucker Balch.\nCharacteristics\n\nThe code has been written and tested in Python 3.6.10.\nDecision and random tree implementation for regression.\nDecision tree: cross-correlation and median used to determine the best feature to split and the split value.\nRandom tree: best feature and split value determined randomly.\nTree-size reduction can be done by by leaf (defining the lowest number of leaves to keep on a branch) and by value (defining the tolerance to group close-values leaves).\nThe corner case where after a split all data end up in one branch is also implemented.\nBootstrap aggregating (bagging) can be applied to both decision tree and random tree.\nAdaBoosting is implemented as boosting algorithm.\nUsage: python test.py csv-filename.\n\nParameters\nsys.argv[1] File name with the dataset passed as argument. Data must be in a csv file, with each column a feature and the label in the last column.\nsplit_factor Split value between training and test data.\nlearner_type Type of learner (decision tree or random tree).\nleaf Lowest number of leaves to keep; any branch with equal or less leaves is substituted by a single leaf with a value equal to the average of the removed leaves.\ntol Tolerance to group leaves based on their labels; any branch where the leaves have a value that differ from their average less or equal to this tolerance is substituted by a single leaf with a value equal to the average.\nbags Number of bags to be used for bootstrap aggregating; no bagging is enforced setting this value to zero.\nbag_factor Number of data in each bag as a fraction of the number of training data.\nboost Specify if boosting should be used or not.\nExamples\nAll examples are for the file istanbul.csv. Correlation results are obtained averaging 20 runs.\n\nReference case: decision tree learner, no tree reduction, no bagging, no boosting. Correlation predicted/actual values: 0.9992 (training), 0.7109 (test).\n\nsplit_factor = 0.7\nlearner_type = 'dt'\nleaf = 1\ntol = 1.0e-6\nbags = 0\nbag_factor = 1.0\nboost = False\n\nAs reference case but using a random tree learner. Correlation predicted/actual values: 0.9708 (training), 0.6349 (test). Comment: worst results but faster computation.\n\nsplit_factor = 0.7\nlearner_type = 'rt'\nleaf = 1\ntol = 1.0e-6\nbags = 0\nbag_factor = 1.0\nboost = False\n\nAs reference case but changing the number of leaves. Correlation predicted/actual values: 0.8917 (training), 0.7843 (test). Comment: reduce overfitting and generalize better.\n\nsplit_factor = 0.7\nlearner_type = 'dt'\nleaf = 10\ntol = 1.0e-6\nbags = 0\nbag_factor = 1.0\nboost = False\n\nAs reference case but changing the tolerance. Correlation predicted/actual values: 0.9087 (training), 0.7642 (test). Comment: reduce overfitting and generalize better.\n\nsplit_factor = 0.7\nlearner_type = 'dt'\nleaf = 1\ntol = 1.0e-2\nbags = 0\nbag_factor = 1.0\nboost = False\n\nAs reference case but using bagging. Correlation predicted/actual values: 0.9748 (training), 0.8333 (test). Comment: generalize better, using boosting will slightly improve results.\n\nsplit_factor = 0.7\nlearner_type = 'dt'\nleaf = 1\ntol = 1.0e-6\nbags = 10\nbag_factor = 1.0\nboost = False\n""], 'url_profile': 'https://github.com/gabrielegilardi', 'info_list': ['1', 'Julia', 'GPL-3.0 license', 'Updated Apr 17, 2020', 'Python', 'MIT license', 'Updated Aug 24, 2020', 'Java', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 19, 2020', 'C++', 'GPL-3.0 license', 'Updated Mar 20, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'MATLAB', 'Updated Feb 1, 2021', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '696 contributions\n        in the last year', 'description': ['MobiDoc\nAndroid application which works on multiple neural networks and regression models that takes input from the user as text and images, then predicts the disease using the regression model and neural net. Users can also input medical reports which\nwill be analysed by the app using optical character recognition.\n\nDevelopers\nJustin John Mathew (LinkedIn) \nAlan Henry (LinkedIn) \nDonald Abraham (LinkedIn) \nNaveen Sreevalsan (LinkedIn) \nJoel Mathew Koshy (LinkedIn) \n'], 'url_profile': 'https://github.com/Rec0iL99', 'info_list': ['1', 'Julia', 'GPL-3.0 license', 'Updated Apr 17, 2020', 'Python', 'MIT license', 'Updated Aug 24, 2020', 'Java', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 19, 2020', 'C++', 'GPL-3.0 license', 'Updated Mar 20, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'MATLAB', 'Updated Feb 1, 2021', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '168 contributions\n        in the last year', 'description': ['Honeybees-Extinction-Prediciton\nlinear regression practice using dataset of honeybees honey production to predict the future of the production\n'], 'url_profile': 'https://github.com/abigaylerose03', 'info_list': ['1', 'Julia', 'GPL-3.0 license', 'Updated Apr 17, 2020', 'Python', 'MIT license', 'Updated Aug 24, 2020', 'Java', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 19, 2020', 'C++', 'GPL-3.0 license', 'Updated Mar 20, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'MATLAB', 'Updated Feb 1, 2021', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['LinearReg-Perceptron-Alg\nThis is my CS first semester project implementing linear regression, simple perceptron and KNN algorithms.\n'], 'url_profile': 'https://github.com/hsarraf', 'info_list': ['1', 'Julia', 'GPL-3.0 license', 'Updated Apr 17, 2020', 'Python', 'MIT license', 'Updated Aug 24, 2020', 'Java', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 19, 2020', 'C++', 'GPL-3.0 license', 'Updated Mar 20, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'MATLAB', 'Updated Feb 1, 2021', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'San Diego', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Fraud-Detection-based-on-Synthetic-Financial-Datasets\nWith the large amount of transactions and the changing characteristics carried out by fraudulent agents, the current methods in detecting fraud lack effectiveness. Most of the detection mechanisms are only based on simple assigned thresholds. For example, in this case only transactions over $200,000 would be flagged as fraudulent. Therefore, it is necessary to figure out the key features that are closely related to fraud so that we are able to prevent fraudulent transactions.\nWe trained several different classification models based on the transaction data we have in order to find the best one that can accurately detect fraudulent transactions. To this end, we attempted logistic regression, neural network and XGBoost in both PySpark and SageMaker.\nWe used Synthetic Financial Datasets for Fraud Detection from Kaggle. The data was generated by the PaySim mobile money simulator. Details about this simulator and the assumptions it makes can be found either on the Kaggle Site (https://www.kaggle.com/ntnu-testimon/paysim 1) or its research publication (https://www.researchgate.net/publication/31313 8956_PAYSIM_A_FINANCIAL_MOBILE_M ONEY_SIMULATOR_FOR_FRAUD_DETEC TION)\n'], 'url_profile': 'https://github.com/lingjiangj', 'info_list': ['1', 'Julia', 'GPL-3.0 license', 'Updated Apr 17, 2020', 'Python', 'MIT license', 'Updated Aug 24, 2020', 'Java', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 19, 2020', 'C++', 'GPL-3.0 license', 'Updated Mar 20, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'MATLAB', 'Updated Feb 1, 2021', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'Lagos', 'stats_list': [], 'contributions': '128 contributions\n        in the last year', 'description': ['Logistic-regression-algorithm\nThis is a simple implementation of logistic regression from the scratch.\nIn Logistic Regression, we don’t directly fit a straight line to our data like in linear regression. Instead, we fit a S shaped curve, called Sigmoid, to our observations.\nSteps on how to implement logistic regression from scratch\nDefine a sigmoid function. Which returns the probability of an event, which is always between 0 and 1.\nDefine an error function. Which returns how far the prediction is from the actual value\nDefine a log loss function. Which is just takes into account the uncertainty of your prediction based on how much it varies from the actual label. This gives us a more nuanced view into the performance of our model\nUpdate the weight and bias of the model, by using gradient descent approach. By constantly taking steps to decrease the error.\nHow do we know what weight should be bigger or smaller. We do this by getting the derivative of the loss function with respect to the weight and bias. Which is the negative of the gradient.\nHow to use\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_breast_cancer\n\n\nX, y = load_breast_cancer(return_X_y=True)  # load the dataset\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)  # split the dataset\n\n# Create an instance of the LogisticRegression class\n\nlr = LogisticRegression(learning_rate=0.1, num_iter=200000)  # you can play with hyperparameters to understand how learning rate works.\n\n# Train the model\n\nlr.fit(X_train, y_train)\n\n# predict or predict probabilities\n\nlr.predict_proba(X_train) ----> probabilities\nlr.predict(X_train) .  --------> binary scores 0 or 1\n\n# look at the score\n\nscore = lr.accuracy(y_train) \n'], 'url_profile': 'https://github.com/Sensei-akin', 'info_list': ['1', 'Julia', 'GPL-3.0 license', 'Updated Apr 17, 2020', 'Python', 'MIT license', 'Updated Aug 24, 2020', 'Java', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 19, 2020', 'C++', 'GPL-3.0 license', 'Updated Mar 20, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'MATLAB', 'Updated Feb 1, 2021', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '135 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shubhankartiwari99', 'info_list': ['1', 'Julia', 'GPL-3.0 license', 'Updated Apr 17, 2020', 'Python', 'MIT license', 'Updated Aug 24, 2020', 'Java', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 19, 2020', 'C++', 'GPL-3.0 license', 'Updated Mar 20, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'MATLAB', 'Updated Feb 1, 2021', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['FastSBL\nA fast sparse Bayesian learning algorithm based on gaussian scale mixture model for regression problem\nThis code is for paper titled ""An efficient sparse Bayesian learning algorithm Based on Gaussian-scale mixtures"".\nThe images in dataset are acquired from http://sparselab.stanford.edu/ and http://decsai.ugr.es/cvg/dbimagenes/.\nThe function FastLaplace.m in tools corresponds to the fast SBL algorithm based on Laplace priors, which is acquired from the origianl authors at http://www.dbabacan.info/publications.html. The paper is titled ""Bayesian Compressive Sensing using Laplace Priors"".\nGGAMP-SBL.m corresponds to the algorithm 1 in paper titled ""A GAMP based low complexity sparse bayesian learning algorithm"".\nFor comparison, sparseLab 2.1 and RVM V1.1 toolboxs are needed, which can be obtained from http://sparselab.stanford.edu/ and http://www.miketipping.com/downloads.htm, respectively.\nThis code is implemented in Matlab 2019b. If have any questions, please contact zhouwei@hust.edu.cn\nIf you use any part of our codes, please cite our paper.\nW. Zhou, H. -T. Zhang and J. Wang, ""An Efficient Sparse Bayesian Learning Algorithm Based on Gaussian-Scale Mixtures,"" IEEE Transactions on Neural Networks and Learning Systems, doi: 10.1109/TNNLS.2020.3049056.\nbibtex:\n@ARTICLE{zhou2021efficient,\nauthor={W. {Zhou} and H.-T. {Zhang} and J. {Wang}},\njournal={IEEE Transactions on Neural Networks and Learning Systems},\ntitle={An Efficient Sparse Bayesian Learning Algorithm Based on Gaussian-Scale Mixtures},\nyear={2021},\nvolume={},\nnumber={},\npages={1-14},\ndoi={10.1109/TNNLS.2020.3049056}}\n'], 'url_profile': 'https://github.com/zhouzhouwei', 'info_list': ['1', 'Julia', 'GPL-3.0 license', 'Updated Apr 17, 2020', 'Python', 'MIT license', 'Updated Aug 24, 2020', 'Java', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 19, 2020', 'C++', 'GPL-3.0 license', 'Updated Mar 20, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'MATLAB', 'Updated Feb 1, 2021', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '371 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kaustubh285', 'info_list': ['1', 'Julia', 'GPL-3.0 license', 'Updated Apr 17, 2020', 'Python', 'MIT license', 'Updated Aug 24, 2020', 'Java', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 19, 2020', 'C++', 'GPL-3.0 license', 'Updated Mar 20, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Mar 23, 2020', '1', 'MATLAB', 'Updated Feb 1, 2021', 'Jupyter Notebook', 'Updated Mar 22, 2020']}"
"{'location': 'Beijing', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BrotherR', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 21, 2020', 'Java', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020']}","{'location': 'Istanbul', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Customer-Life-Time-Value-Modelling-with-Multiple-Regression\n'], 'url_profile': 'https://github.com/firatgunduz', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 21, 2020', 'Java', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020']}","{'location': 'Ahmedabad,Gujarat,India', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['Regression On SQuaD Data\nImplemented Logistic Regression On Squad Dataset of Question Answering on Reading Comprehension\n'], 'url_profile': 'https://github.com/devshree07', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 21, 2020', 'Java', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""This is an archive repository to be able to conduct regression tests\nThe content is stripped from: https://github.com/dlt-rilmta/hunlp-GATE\nErrors will not be fixed and no further development will be made.\nVisit hfst-wrapper/README.md on build and usage instructions.\nThe Original README.md content\nThis directory should contain the binaries and the Hungarian HFST transducer file to run hfst.\nTo get the binaries read the README.md in your operating system's directory.\nThe hu.hfstol transducer file should be obtained separately by running complete.sh (on Linux).\nThe source for hsft-ol is available from here:\nhttps://sourceforge.net/projects/hfst/\nThe source for hsft-wrapper is inside the hfst-wrapper directory.\n""], 'url_profile': 'https://github.com/dlt-rilmta', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 21, 2020', 'Java', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Board_game_review\nused linear regression and random forest algorithms over a data set having 80k games\n'], 'url_profile': 'https://github.com/VineetLoyer', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 21, 2020', 'Java', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020']}","{'location': 'Canberra, Australia', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['ml-basics\nNotebooks with theory and code for some standard ML algorithms (sampling, logistic regression etc.)\n'], 'url_profile': 'https://github.com/jarrydmartinx', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 21, 2020', 'Java', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '397 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shreyanshsatvik', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 21, 2020', 'Java', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020']}","{'location': 'Fort Collins, CO', 'stats_list': [], 'contributions': '363 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MichaelBurak', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 21, 2020', 'Java', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['LIEE_EggPlant\nLIEE_EggPlant is an Automation Project for LIEE Sweeping tests and Regression test cases\n'], 'url_profile': 'https://github.com/AutomationWab', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 21, 2020', 'Java', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['ExploratoryDataAnalysis\nWe did EDA for Titanic Dataset and predict 77% using Logistic Regression model\n'], 'url_profile': 'https://github.com/suyogyaman', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 21, 2020', 'Java', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 23, 2020', 'Python', 'MIT license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020']}"
"{'location': 'Planet Earth', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['coronavirus-covid-19-exponential-regression-and-monte-carlo-simulation\nPredicting Covid-19 cases using exponential regression and Monte Carlo Simulation\n'], 'url_profile': 'https://github.com/slrbl', 'info_list': ['Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '144 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sudeeep885', 'info_list': ['Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Linear-Regression-with-USA-Data-Housing\nLinear Regression with USA Data Housing use Python Software. This Project describe about correlation between Average Area Income and Price\n'], 'url_profile': 'https://github.com/hanimustikaadi', 'info_list': ['Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/neerajseth000', 'info_list': ['Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Zenetor', 'info_list': ['Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/IThinkThereforeISuffer', 'info_list': ['Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '326 contributions\n        in the last year', 'description': ['MULTIVARIATE LINEAR REGRESSION\nWe will predict employee salaries from different employee characteristics (or features). Import the data salary.csv to a Jupyter Notebook. A description of the variables is given in Salary metadata.csv.\n'], 'url_profile': 'https://github.com/Shalom91', 'info_list': ['Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '146 contributions\n        in the last year', 'description': ['Kaggle-housing-prices\nJupyter Notebook for :\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques\nCurrent LB Score: 0.11937 (Rank 649)\nApproach:\n\nUses a two layer stacked Regression model\nIntroduces new features and polynomial features\n\n'], 'url_profile': 'https://github.com/mohammedFurqan', 'info_list': ['Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['kaggle_HousePrices\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview\n'], 'url_profile': 'https://github.com/varunkumarvaleti', 'info_list': ['Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Breast-cancer-prediction\nPredict if patient have breast cancer or not using logistic regression algorithm.\nFirst we take Data from file Data.csv using pandas library. Then we done data cleaning and processing by removing null data and convert categorical data into integer data.\nThen we split out data training set and testing set and do scaling of data by standardscalar.Now here logistic regression is best algorithm for our data. So we fit our model in Logistic regression algorithm and predict test data.\nAnd we have got prediction accuracy of 96.5%.AS per confusion metrix we have predicted 5/165 data wrong\n'], 'url_profile': 'https://github.com/Rajjani1277', 'info_list': ['Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}"
"{'location': 'Canada,London', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Flightdelayed\nDetermine which plane might be delayed.\nLogistics regression / naive bayes / neural network\n'], 'url_profile': 'https://github.com/JianmingDong', 'info_list': ['R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Julia', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'R', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 21, 2020', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PriyankaChauhan1802', 'info_list': ['R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Julia', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'R', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 21, 2020', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Hyperelastic-Generative-Model\nA Gaussian Process regression approach to learn strain energy density functions of human brain tissue\nDetailed instructions will be added as soon as possible.\nThanks for citing our work:\nIn press:\n@article{Granados2019,\ntitle = {{A Generative Model of Hyperelastic Strain Energy Density Functions for Real-time Simulation of Brain Deformation.}},\nauthor = {A. Granados and M. Schweiger and V. Vakharia and S. B. Vos and A. Miserocchi and A. W. McEvoy and J. S. Duncan and R. Sparks and S. Ourselin},\njournal = {Int Conf on Med Image Comp and Comp-Assisted Interv (MICCAI)},\nyear = {2019},\nvolume = {},\nnumber = {},\npages = {218-226}\n}\nUnder review:\n@article{Granados2020,\ntitle = {{A Generative Model of Hyperelastic Strain Energy Density Functions for Multi-tissue Brain Deformation.}},\nauthor = {A. Granados and F. Perez-Garcia and M. Schweiger and V. Vakharia and S. B. Vos and A. Miserocchi and A. W. McEvoy and J. S. Duncan and R. Sparks and S. Ourselin},\njournal = {International Journal of Computer Assisted Radiology and Surgery (IJCARS)},\nyear = {2020},\nvolume = {},\nnumber = {},\npages = {}\n}\n'], 'url_profile': 'https://github.com/agranadoseu', 'info_list': ['R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Julia', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'R', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 21, 2020', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': [""King County House Sale Prices\nContributors: Vivian Dang & Rashid Baset\n\nItems in Repository:\n\nkc_house_data_csv: King County house dataset\nKc_housing Notebook.ipynb: code for data processing and visualization\nKc_House.pdf: presentation slides\nimages: folder for visualizations\n\nPrompt:\nA real estate company, Remax, has hired our team to help them better understand factors affecting King County’s house sale prices for additional insights in the market and home price guidance for their clients.\nKey Questions:\n\nHow much are houses selling for in King County?\nHow do features of a house affect its price?\nHow does the location of a house affect its price?\n\nData Collection\nKing County House Price Dataset: https://github.com/viviandng/flatiron-project-2/blob/master/kc_house_data.csv\nData description:\nKing County Dataset: 21597 rows x 21 columns\n\n\nCategorical variables:\n\nCondition: index 0-5, rating of the house’s condition\nGrade: index 1-13, rating of the house’s construction/design\n\n\n\nContinuous variables:\n\nSqft_livingsquare: square footage of house's living space\nsqft_lotsquare: square footage of lot\nSqft_above: square footage of house excluding basement\nSqft_living15: square footage of living space for the nearest 15 neighbors\nSqft_lot15: square footage of lot for the nearest 15 neighbors\n\n\n\nDiscrete variables:\n\nBedrooms:  numbers of bedrooms\nBathrooms: number of bathrooms\nFloors: number of floors (levels) in house\n\n\n\nVariables dropped & reasonings:\n\nId: unique identifier for a house\n\nNot a feature of the house\n\n\nDate: house was sold\n\nNot a feature of the house\n\n\nLat: latitude coordinate\nLong: longitude coordinate\n\nNo meaningful information from one unit increase of lat or long to price\n\n\nWaterfront: house which has a waterfront\n\n17% non numerical values\n\n\nView:  index 0-4, rating of the views from the house\n\nWe were not able to find the grading scale for view to analyze it\n\n\nyr_renovated: year when house was renovated\n\n17,000 rows with 0.0 as values, we do not want to drop those rows\nWe cannot compare 0.0 (true vs false) to years (ex:1965-2015)\nWe could set yr_renovated to year built for houses that have not been renovated. However, we do not want to assume that a renovated house is the same as a newly built house\n\n\n\n\n\nExploratory Data Analysis:\nFigure 1. Price distribution of houses\n\nPrices are skewed to the right, meaning that there are outliers in the higher price range. These outliers range from $2-8M, causing our mean ($540k) to be $100,000 more than our median ($450k). 50% of house sale prices are between $322K and $645K. \n\n\n\nFigure 2: Square footage vs. Price of Houses\n\nSquare footage (sqft) shows a positive correlation with price; for each unit increase sqft, there is also an increase in the value of the house. Majority of the houses are under 4,000sqft and under $2M \n\n\n\nFigure 3: Grade vs. Price of House\n\nGrade shows a positive correlation with price; houses with a higher grade tends to cost more. Houses with grade equal to or less than 6 do not have many outliers in the higher price range because 6 is the lowest grade that meets building code. These houses have simple construction and design so it is difficult for them to be priced higher.\n\n\n\nFigure 4: Grade vs. Average Price of House\n\nOn average, houses under grade 10 will be valued at less than $1M. Whereas, houses with grade 13 are significantly higher, with an average value over $3.5M.\n\n\nMultivariate Linear Regression Model:\nA multivariate linear regression model was perfromed to predict house price with 80 variables:\n\n\nCreate a dummy dataframe:\n Set variables for modeling \n y = 'price' \n x_cols = ['bedrooms', 'bathrooms', 'sqft_living','floors','condition','yr_built', 'zipcode','sqft_living15', 'sqft_lot15', 'sqft basement']\n categorical_variables = ['floors', 'condition', 'zipcode']\n\n Create a DataFrame \n df_ohe = pd.get_dummies(kc_housing[x_cols], columns= categorical_variables, drop_first =True)\n\n\n\nUse statsmodel to obtain a model summary with x = df_ohe.values and y = np.log(‘price’) which is log_{e}. From the model, drop variables with a p-value >0.05 because these variables are not statistically significant in our model.\n\n\nUse numpy to convert our dummies dataframe (df_ohe) to a numpy array for training\n\n\nUse train_test_split from sklearn to split our data into X_train, X_test, y_train, y_test. \nX_train, X_test, y_train, y_test = train_test_split(x_array, y_array_log, test_size=0.2, random_state=50)\n\n\nUse LinearRegression() to find the best fit line and create a list of coefficients \n lr = LinearRegression() \n model = lr.fit(X_train, y_train) \n\n coefficients (m slope)\n m = lr.coef_ \n\n y-intercept \n b = lr.intercept_ \n\n\n\nPerform a cross validation \n cv_5_results  = (cross_val_score(lr, X_train, y_train, cv=5,  scoring='r2'))\n\n\n\nCreate a residual map with yellowbrick \n model = LinearRegression() \n \n visualizer = ResidualsPlot(model)\n visualizer.fit(X_train, y_train)  \n \n visualizer.score(X_test, y_test)  \n visualizer.show() \n\n\n\nVariables in our final model: 67 zip codes, sqft_living, sqft_living15, sqft_lot15, sqft basement, bedrooms, bathrooms, floors_1.5, floors_2, floor_3,  and condition 2-5. \nResults from model:\n\nThe mean of our R-squared values for our cross validation was 0.84; our model explains 84% of the variance.\nFactors that caused an increase in house price:\n\nAn increase in number of bathrooms, sqft_living, sqft_living15, sqft_lot15, conditions (2-5)\nHaving a 1.5 floor\nFactors that caused a decrease in house price:\n\nAn increase in bedrooms or sqft_basement\nHouses with 2 or 3 floors\n\n\n\n\nLocation has a high impact on house price:\n\nTop 3 zip codes with a high price factor: 98039, 98004, 98109 \n\n\n\n\n\nFigure 5: Price factors in descending order for each variable in the model.\n\nVariables with a price factor less than 1 means that the variable decreases the price of a house when in consideration with all variables \n\n\nLimitations & Further Research\nWe did not use factors like having a waterfront, quality of view, and year renovated. The top 3 zipcodes are located in areas with a waterfront so waterfront and view rating may have an impact on determining the value of the house \nFurther Research:\n\nprovide additional home quality insights by including:\n\nWaterfront (bin it to a yes or no column)\nView (bin it to rating 0-4)\nYear renovation (bin year by decade and associate it with a value score)\nAttribute a affluency value (tax rate) to zip codes\n\n\n\n""], 'url_profile': 'https://github.com/viviandng', 'info_list': ['R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Julia', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'R', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 21, 2020', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '170 contributions\n        in the last year', 'description': ['GNR-Assignment-1\nIn this assignment, we were required to implement four variants of multi-variate Regression.\nThe aim here is to predict the price of each house using these three attributes namely bedrooms, bathrooms, and sqft_living.\nWe approached the problem via 4 different regressions.\na. Linear and non-linear Regression\n1. Linear Regression\n2. Polynomial Regression\nb. Regularized Regression\n1. Ridge regression\n2. LASSO Regression\n'], 'url_profile': 'https://github.com/Meeta14', 'info_list': ['R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Julia', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'R', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 21, 2020', 'Updated Mar 16, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': [""Spatial Statistics - Linear Regression in R\nExecutive Summary\nThis was the code I prepared and used for my qualifying exams for the Geospatial Information Sciences program. Our task was to build a spatially adjusted linear regerssion from the 2016 voting data for Texas counties. If it sounds like an impossible or irrational task, it was. The idea was just to check our understanding and methodology based on two semesters of basic and advanced statistics for geospatial analysis. The list of review topics (mind you, not the topics actually explained) was 6 pages long.\nFiles Included\n\nQualifierTemplate.R - the code I prepared for the exam. There's not enough time to write it all from scratch even if you could.\nBurnsQual.R - the code I turned in.\n20180510 Burns Qualifier Data Analysis.pdf - the tasks assigned that generated the code along with my code and plot outputs.\nHelperFunctions.R - the code we were supplied and expected to use.\nA few of the plots are included.\n\nMethodology\nWe were to select and transform at least 5 features, build a linear model, and adjust it for spatial autocorrelation. To sum up the outcome, you would expect to see the urban areas mostly voting for Clinton and the rural areas voting for Trump. We selected features accordingly. We were instructed to use the voting results for each, the population, and consider the registered voters and turnout rate.\nThe features I selected were crime rate, Obama voting rate, Hispanic population, college education, poverty rates, and urban/rural. My rational for using crime was the research showing that cities scales at a rate of 15% efficiency gain with size. That applies to crime as well. My professor didn't buy it even though I cited sources.\nDiscussion\nThe main mistake I made on this exam was that I transformed the voting rate incorrectly which was the target variable. I made that mistake at the beginning and it made everything else wrong. We were taught to log transform skewed data to make it more normally distributed but we weren't told not to do that to percentages. I've since learned you're supposed to do a more complicated transformation that we didn't cover if you actually care about the shape of the data. There were other mistakes too but they weren't that interesting.\nI'll show the scatterplot matrix here because I like those:\n\nAnother issue with this test is that to adjust for spatial autocorrelation, we draw this network:\n\nThat allows R to follow the links to understand what is connected to what. The problem is that it assumes that things are related just because they are next to each other - which is a silly assumption in this instance. The urban/rural factor already accounts for that. And the fact that cities tend to vote Democratic and rural towns Republican was captured in the proxy for city size - crime. It also ignores the fact that geographic places are connected by highways. If we believe geographic proximity is important then we need to account for drive time, not riding a horse in a straight line to the center of each county.\nAnd then here are some diagnostic plots:\n\nThese are fun because it's easy to visually pick out outliers. Obviously Texas counties have several. The county population is estimated to range between 4.6 million for Harris County (Houston) down to Loving County with 134 people. As of the 2017 census survey, there were 8 counties with less than 1000 people which makes the Texas Senate a bit of a joke that 134 country people would have the same representation as 4.6 million city dwellers. It also makes this particular analysis a bit meaningless even if I had done the transformation correctly.\nResults\nI failed this qualifier. Funny thing was that out of the 3 of us taking it, 2 failed and the other technically should have failed because he didn't use enough features and took extra time other than what was allotted. So basically no one usually passes this test.\nThis and the Theories qualifier helped me understand that I didn't really want to do doctoral research with this program. My interests are in the implications of network science on geography and GIS as a field can't accept that network affects exist beyond transportation networks and proximity. It's similar to the state of economics before they realized psychology matters and evolved into behavioral economics.\nHopefully, it goes without saying that if you are a doctoral student about to take your GIS qualifier, don't use this code. You shall not pass. 😊\n""], 'url_profile': 'https://github.com/LouisRBurns', 'info_list': ['R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Julia', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'R', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 21, 2020', 'Updated Mar 16, 2020']}","{'location': 'Rome, Italy', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': [""covid19\nSimple tool in Python to analyze data and making simple predictions using a Polynomial Regression\nTo run\n\nClone repository\nInstall requirements\nRun Commands (see below)\n\n01-curva_crescita-lazio-lombardia.py\n\nShows total positive cases between two dates in Lazio and Lombardia.\nInitial date Lombardia (24.02.2020)\nInitial date Lazio (11.03.2020)\n\n02-graph.py\nTo run: `python3 02-graph.py [-nomeregione] [-tipodati]\n\nEx: python3 02-graph.py 'Lombardia' 'totale_casi'\n\n03-Polynominal_Regression.py\nShows predictions for region and data\nTo run: `python3 03-Polynominal_Regression.py [-nomeregione] [-tipodati] [-degree]\n\nEx: python3 02-graph.py 'Lombardia' 'totale_casi' 5\nIf degree is not set, the default value is 5\n\n04-Polynominal_Regression_ITA.py\nShows predictions for ITA\nTo run: `python3 03-Polynominal_Regression.py [-tipodati] [-degree]\n\nEx: python3 02-graph.py 'totale_casi' 5\nIf degree is not set, the default value is 5\n\nThe script always collects updated data from official sources.\n""], 'url_profile': 'https://github.com/avvRobertoAlma', 'info_list': ['R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Julia', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'R', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 21, 2020', 'Updated Mar 16, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '154 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amitdivekar30', 'info_list': ['R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Julia', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'R', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 21, 2020', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['housePricePrediction\nPython webapp for multivarient linear regression\nThis is an Implementation of multi variant linear regression using gradient descent from scratch using python\nHow to run\nRun the house.py file, this will start a local web server.\nGo to localhost:5000 and see the web page.\nWorking\nYoutube : https://youtu.be/X4uAEcIlPao\n'], 'url_profile': 'https://github.com/SUJITHhubpost', 'info_list': ['R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Julia', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'R', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 21, 2020', 'Updated Mar 16, 2020']}","{'location': 'Washington D.C.', 'stats_list': [], 'contributions': '266 contributions\n        in the last year', 'description': ['tennis_ace\nMachine Learning module to use linear regression analysis to explore ATP tennis data\n'], 'url_profile': 'https://github.com/yasheymateen', 'info_list': ['R', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Julia', 'Updated Mar 20, 2020', 'R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'R', 'Updated Mar 20, 2020', 'HTML', 'Updated Mar 21, 2020', 'Updated Mar 16, 2020']}"
"{'location': 'Israel', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Anime-Recommender-Hackaton\nCreate a recommender system for Anime. 3 Hours task.\ndataset - https://www.kaggle.com/CooperUnion/anime-recommendations-database#rating.csv\nCollaborative filtering, word Embedding dense vector representation - NeuralNetwork regression model and K-Fold Evaluation.\n'], 'url_profile': 'https://github.com/LiadzZ', 'info_list': ['Python', 'Updated Feb 11, 2021', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020']}","{'location': 'Urbana', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['SpamFilter\nCreated a spam filter using Neural Networks (keras) and Logistic Regression\nIn this file, I create a spam filter based on the sms data. The data is labelled (spam vs ham) for spam data vs not spam text messages.\n'], 'url_profile': 'https://github.com/jac0bmath3w', 'info_list': ['Python', 'Updated Feb 11, 2021', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kinishilpa', 'info_list': ['Python', 'Updated Feb 11, 2021', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020']}","{'location': 'Rochester, MN', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['KingMLR\nThis is a Python data project using scikit-learn\'s Multiple Linear Regression models and feature power tranformations to predict home values in King County, WA. This project uses the following libraries: numpy matplotlib pandas seaborn scipy scikit-learn yellowbrick\nFUTURE PLANS\nDidn\'t do anything with the multicollinearity of the dataset which I know know is a huge problem for MLR performance. Will do a entire rewrite and redo of the project in the next few days.\nFiles\n\nKing County Homes.csv - (16187 X 20) dataset with house prices and predictors\nKing County Homes - Multiple Linear Regression.ipynb - where all of the analysis and model implementation is done\nGraphics folder - contains residual plots and pairplots of the dataset and models\n\nLooking at the data\nFirst let\'s read in some of the data using .head()\n\tprice\tbedrooms\tbathrooms\tsqft_living\tsqft_lot\tfloors\twaterfront\tview\tcondition\tgrade\tsqft_above\tsqft_basement\tyr_built\tyr_renovated\trenovated\tzipcode\tlat\tlong\tsqft_living15\tsqft_lot15\n0\t221900\t3\t1.00\t1180\t5650\t1.0\t0\t0\t3\t7\t1180\t0\t1955\t0\t0\t98178\t47.5112\t-122.257\t1340\t5650\n1\t538000\t3\t2.25\t2570\t7242\t2.0\t0\t0\t3\t7\t2170\t400\t1951\t1991\t1\t98125\t47.7210\t-122.319\t1690\t7639\n2\t180000\t2\t1.00\t770\t10000\t1.0\t0\t0\t3\t6\t770\t0\t1933\t0\t0\t98028\t47.7379\t-122.233\t2720\t8062\n3\t510000\t3\t2.00\t1680\t8080\t1.0\t0\t0\t3\t8\t1680\t0\t1987\t0\t0\t98074\t47.6168\t-122.045\t1800\t7503\n4\t1230000\t4\t4.50\t5420\t101930\t1.0\t0\t0\t3\t11\t3890\t1530\t2001\t0\t0\t98053\t47.6561\t-122.005\t4760\t101930\n\n\nDescriptive Statistics\nThen lets see some of the descriptive statistics of the data.\n              price      bedrooms  ...  sqft_living15     sqft_lot15\ncount  1.618700e+04  16187.000000  ...   16187.000000   16187.000000\nmean   5.428020e+05      3.374374  ...    1989.630815   12677.886637\nstd    3.696331e+05      0.944437  ...     688.003602   27553.569520\nmin    7.500000e+04      0.000000  ...     399.000000     651.000000\n25%    3.246235e+05      3.000000  ...    1490.000000    5100.000000\n50%    4.510000e+05      3.000000  ...    1840.000000    7601.000000\n75%    6.488760e+05      4.000000  ...    2370.000000   10080.000000\nmax    7.700000e+06     33.000000  ...    6210.000000  871200.000000\n\nPairplot\n\nIt looks like some variable have right-skewed distributions. Something we will fix with transformations down the road.\nRemoving Features\nBecause I\'m not using any tree-based models--longitude, latitude, and zip would just confuse the model.\ndata = data.drop(labels=[\'long\', \'lat\', \'zipcode\'], axis=1)\nChecking for Null values\ndata.isnull().any()\nprice            False\nbedrooms         False\nbathrooms        False\nsqft_living      False\nsqft_lot         False\nfloors           False\nwaterfront       False\nview             False\ncondition        False\ngrade            False\nsqft_above       False\nsqft_basement    False\nyr_built         False\nyr_renovated     False\nrenovated        False\nsqft_living15    False\nsqft_lot15       False\ndtype: bool\n\nMean\nAnd finally finding the mean value of \'price\' to have a baseline for looking at how our errors stack up using data[\'price\'].mean() which return the mean price in USD.\nHistogram:\n\n542802.0316920986\n\nCreating the Dummy model\nAfter reading in the data using the read_csv function in the pandas library, the first key step in using the data is spliting the dataset into the variable we want to predict vs. the predictors themselves. This is done using the .iloc function to seperate the code into X and y DataFrames. We can then instantiate the train_test_split function from scikit-learn to split the data into a training set and a test set to test how well our model preforms.\nX = data.iloc[:, 1:]\ny = data.iloc[:, 0]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nNow to actually create a model. First, we will use the DummyRegressor from scikit-learn. With this first model, we will be given the worst-case scenario in terms of Mean Absolute Error in dollars. Cross Validation of 5 scores will be used to show which ""type"" of Multiple Linear Regression we should use\ndummyreg = DummyRegressor(strategy=\'mean\')\nscores = cross_val_score(dummyreg, X, y,\n                         scoring=metrics.make_scorer(metrics.mean_absolute_error),\n                         cv=5)\nscores\narray([237472.60747177, 238184.50756892, 229002.01176648, 238158.57318789,\n       237013.63527984])\n\nprint(""Average Mean Absolute Error: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std()*2))\nAverage Mean Absolute Error: 235966.27 (+/- 7019.61)\nPretty horrible, which is to be expected.\nModel 2: Scikit-Learn\'s LinearRegression()\nmlr1 = LinearRegression()\nscores = cross_val_score(mlr1, X, y,\n                         scoring=metrics.make_scorer(metrics.mean_absolute_error),\n                         cv=5)\nprint(""Average Mean Absolute Error: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std()*2))\nAverage Mean Absolute Error: 141855.23 (+/- 8407.33)\nResiduals:\n\nModel 3: Scikit-Learn\'s Normalized LinearRegression()\nlr2 = LinearRegression(normalize=True)\nscores = cross_val_score(mlr2, X, y,\n                         scoring=metrics.make_scorer(metrics.mean_absolute_error),\n                         cv=5)\nprint(""Average Mean Absolute Error: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std()*2))\nAverage Mean Absolute Error: 141860.65 (+/- 8394.38)\nresiduals:\n\nLooks about the same to me. Nothing really has changed in the residuals either.\nModel 4: MLR with Recursive Feature Selection\nm = feature_selection.RFECV(LinearRegression(normalize=True), cv=5)\n\nm.fit(X, y)\n\nm.support_\n\nmetrics.mean_absolute_error(y_test, m.predict(X_test))\narray([ True,  True,  True, False,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True])\n\nIt looks like here it only removed one feature: sqft_lot and return a mean absolute error of 235427.88012998947. Pretty trash. Something might be wrong in the way I\'ve set up the scoring, but it\'s not handling this data well.\nModel 5: MLR with log10(price) and Box-Cox transforms on skewed predictors\ndata3 = data2.copy()\nf = sstats.boxcox(data3[\'sqft_lot15\'])\ndata3[\'sqft_lot15\'] = f[0]\nf2 = sstats.boxcox(data3[\'sqft_above\'])\ndata3[\'sqft_above\'] = f2[0]\nf3 = sstats.boxcox(data3[\'sqft_living\'])\ndata3[\'sqft_living\'] = f3[0]\n\nX3 = data3.iloc[:, 1:]\ny3 = data3.iloc[:, 0]\n\nX_train, X_test, y_train, y_test = train_test_split(X3, y3, test_size = 0.2,\n                                                    random_state = 0)\nmlr4 = LinearRegression()\nmlr4.fit(X_train, y_train)\n\nmetrics.mean_absolute_error(10**y_test, 10**mlr4.predict(X_test))\nOnly transforming the price really messes up our residuals so let\'s not do that.\nLog only residuals:\n\nResiduals:\n\nGives us a Mean Absolute error of 130470.19222297132 and a similar residuals plot to the first two Linear Regression models.\nConclusion\nOverall it seems that Multiple Linear Regression is not a good fit for this type of problem--especially when ensemble bagged and boosted methods exist. However, this was great practice working with statistical transformations and a general deep-dive into scikit-learn\'s LinearRegression() model.\n'], 'url_profile': 'https://github.com/SamSchutz', 'info_list': ['Python', 'Updated Feb 11, 2021', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Linear-Regression-using-Gradient-Descent\nPredicting House prices from average number of rooms per dwelling and percentage lower status of population\nBatch Gradient Descent is implemented to predict the house prices and mean square error is used to get the error between actual and predicted values.\nThe Sci-kit learns Linear Regression is also used to predict the values and get the mean squared error.\nDataset\nSklearns Boston Housing Dataset\n'], 'url_profile': 'https://github.com/sidythakur', 'info_list': ['Python', 'Updated Feb 11, 2021', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['Multiple-Regression-on-Taxi-Fare-\n'], 'url_profile': 'https://github.com/Koorimikiran369', 'info_list': ['Python', 'Updated Feb 11, 2021', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020']}","{'location': 'Karachi,Sindh,Pakistan', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['Regression-Problem-and-Cost-Function\n'], 'url_profile': 'https://github.com/M-Arsalan-Siddiqui', 'info_list': ['Python', 'Updated Feb 11, 2021', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020']}","{'location': 'pala', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/abinet369', 'info_list': ['Python', 'Updated Feb 11, 2021', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Predictive-analysis-using-Linear-Regression\nLinear regression is a basic and commonly used type of predictive analysis.  The overall idea of regression is to examine two things: (1) does a set of predictor variables do a good job in predicting an outcome (dependent) variable?  (2) Which variables in particular are significant predictors of the outcome variable,\n'], 'url_profile': 'https://github.com/bharath1604', 'info_list': ['Python', 'Updated Feb 11, 2021', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/colbywight', 'info_list': ['Python', 'Updated Feb 11, 2021', 'Python', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Jupyter Notebook', 'Updated Mar 25, 2020', 'Updated Mar 20, 2020', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 19, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 18, 2020', 'Updated Mar 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['This File includes works related to the linear regression tasks\nprovided as a part of the Microsoft ML Crash Course\n'], 'url_profile': 'https://github.com/Eyvaz27', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Java', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['Introduction\nSince the industrial revolution the struggle between human and machine has been ongoing.\nJohn Henry vs The Steam Drill\n\nGarry Kasparov vs Deep Blue\n\nTerminators vs Everyone\n\nData Scientists vs Pure Processing Power\n\nProblem Statement\nWhile much of data science is being automated there will still be a need for insights that come from intuitive thinking that computers have yet to match.\nUsing publicly available housing data from Ames Iowa I will run three different scenarios showing that brute processing power of a machine cannot match the sly intuitiveness of a human.\nData\n\nData Summary and Dictionary\nData on housing sales for Ames Iowa was used to compare regressions between several scenarios.\nOutside Research\n\n\nImport Features that Drive Home Sales\n\n\nAdditional Features that Sell\n\n\nImportant Rooms and Types\n\n\nFeatures Desired by New Home Buyers\n\n\nExploratory data analysis paired with outside research led to the selection of several features for user selected models as well as feature selection of categorical data into dummy columns.\nModels\nAll models are run on a linear regression then scored on the R^2 Adjusted value. This metric was chosen as it will punish random models that have a large number of features.\nHuman Selection\nAll features selected through exploratory data analysis and outside research.\nRandom Selection\nA random number of randomly selected features will be run through the regression. The number of features is the square root of all features +- 10 percent.\nThe model is run 1_000_000 times and the highest R^2 Adjusted that is less then one is selected.\nCombination Model\nThe user features are combined with a randomly selected number of features similar in selection criteria in the Random Model. Function is modified though to prevent double feature assignment.\nThis model is also run 1_000_000 times\nRandom Model\nTo test if pure processing power can create a better model a random number of randomly selected features will be run through a linear regression. This will be iterated though 1,000,000 times.\nSCORES\nHuman Model\nLooking at that data and interpreting with the use of outside data a selection of features will selected to run a linear regression with.\nResults\n\n\n\nModel\nRSME\nR^2 Adjusted\nNumber of Features\n\n\n\n\nUser\n30818.38\n0.8449\n6\n\n\nRandom\n35770.18\n0.7882\n24\n\n\nCombination\n31190.64\n0.8567\n21\n\n\n\nConclusions\nTo compare each model scenario we will use the R^2 adjusted metric as that will penalize data sets for having a large number of features.\nAs we can see both scenarios that had user selected input have an increase in R^2 of over five percent.\nThis significant increase in efficiency shows the value of human input.\nThe highest score of using both human insight and machine processing power points us to further integrating the two.\nFurther Research\n\nCompare levels of R^2 of different numbers of n\nImprove data cleaning with normalizations and standardizations\nDevelop more feature engineering\nDevelop random model to include all related dummies if one is selected\nRedevelop models with ElasticNet to account or large numerical data and numerous dummy columns\n\n'], 'url_profile': 'https://github.com/jellena', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Java', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Springboard--Data-Science\nThis repository contains all the data, ipython notebooks, google doc reports and slides related to springboard data science career track workshop exercises. It also has a directory for the capstone project.\n'], 'url_profile': 'https://github.com/Asinha12', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Java', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Athens,Greece', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Οικονομικό Πανεπιστήμιο Αθηνών – Τμήμα Πληροφορικής\nΤεχνητή Νοημοσύνη\nΑναφορά 2ης εργασίας(Υλοποίηση Αλγορίθμων Μηχανικής Μάθησης)\nΜαριάνθη Μηνδρινού – 3150110\nΜιχαήλ Ρούσσος – 3150148\nΧρήστος Τασιόπουλος – 3150170\nΑκολουθεί συνοπτική περιγραφή της λειτουργίας των κλάσεων μας:\n(Σημ: Χρησιμοποιούμε το dataset lingspam_public\\lemm)\nΚλάση Reader: Με την κλάση αυτή διαβάζουμε τα δεδομένα εισόδου από ένα αρχείο (txt) και ελέγχουμε για πιθανά σφάλματα και αστοχίες κατά το διάβασμα.\nΚλάση TreeNode: Τα αντικείμενα τύπου TreeNode αντιπροσωπεύουν τους κόμβους του δέντρου απόφασης. Κάθε αντικείμενο TreeNode έχει δύο αναφορές TreeNode (leftNode,rightNode) που ειναι τα “παιδιά’’ του καθώς και μια αναφορά τύπου Examples που είναι το “περιεχόμενο” του .Ακόμη έχουμε ορίσει setters και getters για τα πεδία του αντικειμένου TreeNode.\nΚλάση Example: Σε αυτήν την κλάση περνάμε τα παραδείγματα (τύπου String) σε μια λίστα. Επίσης έχουμε μια μέθοδο για να παίρνουμε το μέγεθος της λίστας, ολόκληρη την λίστα, ένα παράδειγμα από την λίστα και να τοποθετούμε ένα παράδειγμα στην λίστα.\nΚλάση Examples: Σε αυτήν την κλάση περνάμε τα παραδείγματα (τύπου Example) σε μια λίστα. Επίσης έχουμε ένα κενό κατασκευαστή, μια μέθοδο για να προσθέτουμε στοιχεία στη λίστα, να παίρνουμε το μέγεθος της λίστας και να παίρνουμε ένα συγκεκριμένο στοιχείο από την λίστα.\nΚλάση ID3:Στην κλάση αυτή έχουμε αρχικά την μέθοδο ID3method(TreeNode type), στην οποία καλούμε την αναδρομική μορφή της μεθόδου ID3recursive με ορίσματα την λίστα των παραδειγμάτων, των ιδιοτήτων και μια συμβολοσειρά για την κατηγορία. Στην μέθοδο ID3recursive στην ουσία ακολουθούμε τον ψευδοκώδικα της 16ης διάλεξης για τον συγκεκριμένο αλγόριθμο, όπου ελέγχουμε αν τα παραδείγματα ανήκουν στην ίδια κατηγορία, στην συνέχεια ελέγχουμε αν η λίστα των ιδιοτήτων είναι κενή, ύστερα κοιτάμε για την καλύτερη ιδιότητα κι ορίζουμε το υποδέντρο.\nΚλάση main: Η κλάση main περιέχει την συνάρτηση main η οποία μας δίνει την δυνατότητα να τρέξουμε τους αλγορίθμους ID3 ,αφελείς ταξινομητές Bayes ή τον αλγόριθμο της Λογιστικής Παλινδρόμησης.(Με την προϋπόθεση ότι έχει καθοριστεί σωστά το path προς το dataset). Στην συνέχεια υλοποιούμε την μέθοδο loadFile για το διάβασμα των αρχείων και προσθέτουμε στις αντίστοιχες λίστες ham ,spam τα στοιχεία για να δημιουργήσουμε το λεξικό μας.Έπειτα κοιτάμε τα μέιλ και τοποθετούμε σε ένα vector για κάθε μέιλ που μας δείχνει αν περιέχεται ή όχι κάθε λέξη του λεξικού. Τέλος η μέθοδος readExample χρησιμοποιείται για τον ίδιο λόγο με την προηγούμενη (μόνο στο κομμάτι δημιουργίας vector) και χρησιμοποιείται από την Bayes.\nΚλάση Bayes:Σε αυτή την κλάση υλοποιείται η μέθοδος h_method που\nυπολογίζει τις απαραίτητες πιθανότητες όπως ορίστηκαν και στις διαλέξεις\nτου μαθήματος και επιστρέφει έναν ακέραιο που είναι η κατηγοριοποίηση\nτου αντικειμένου(π.χ. αν είναι spam ή ham).Σημειώνεται πως για τους\nυπολογισμούς των πιθανοτήτων χρησιμοποιήθηκαν λογάριθμοι για\nγρηγορότερους και ευκολότερους υπολογισμούς.\nΚλάση Properties: Η κλάση Properties είναι η κλάση τα αντικείμενα της\nοποίας περιέχουν τις ιδιότητες των παραδειγμάτων. Η Properties περιέχει\n2 κατσκευαστές και άλλες τρεις μεθόδους τις numofprop(), getprop(int i)\n,add(String ex). Κάθε μία\nαπό αυτές αντίστοιχα επιστρέφουν το πλήθος των ιδιοτήτων , επιστρέφουν\nμια συγκεκριμένη ιδιότητα και προσθέτουν μια ιδιότητα στη δομή\nαποθήκευσής τους.\nKλάση Functions: Η κλάση Functions περιέχει 6 μεθόδους. Η probability1\nυπολογίζει την πιθανότητα που έχει ένα όρισμα ενός παραδείγματος να\nέχει μια συγκεκριμένη τιμή c. Η probability2 υπολογίζει την πιθανότητα\nπου έχει ένα όρισμα ενός παραδείγματος να έχει μια συγκεκριμένη τιμή c\nδεδομένου ότι ένα άλλο όρισμα έχει την τιμή x. Η entropy1 υπολογίζει την\nεντροπία μιας ιδιότητας. Η entropy2 υπολογίζει την εντροπία μιας\nιδιότητας δεδομένου ότι ένα άλλο όρισμα έχει μια συγκεκριμένη τιμή . Η IG\nυπολογίζει το κέρδος πληροφορίας που μας παρέχει μια ιδιότητα. Η\nchoosebest επιλέγει την καλύτερη ιδιότητα με βάση το κέρδος\nπληροφορίας.\nΚλάση RegressionInstance: Η κλάση αυτή αντιπροσωπεύει ένα αντικείμενο\nμέιλ, έτσι ώστε να μπορεί να χρησιμοποιηθεί από τον αλγόριθμο μας.\nΚλάση LogisticRegression: Η κλάση αυτή περιέχει τις μεθόδους που\nχρησιμοποιούμε για την υλοποιήση του αλγορίθμου Λογιστικής\nΠαλινδρόμησης.Περιέχει μια μέθοδο για την ρύθμιση των βαρών(train),\nμια μέθοδο για την υλοποίηση της σιγμοειδούς\nσυνάρτησης(sigmoidFunction),μια μέθοδο για τον έλεγχο ενός\nσυγκεκριμένου παραδείγματος αν είναι ή όχι σπαμ(test) και τέλος μια\nμέθοδο που διαβάζει τα αρχεία μας και τα αποθηκεύει(read).\n'], 'url_profile': 'https://github.com/xristostass', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Java', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Russia', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['House-Prices-Advanced-Regression-Techniques\nPredict sales prices and practice feature engineering, RFs, and gradient boosting\n'], 'url_profile': 'https://github.com/MorenovAnton', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Java', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dbarrios213', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Java', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '101 contributions\n        in the last year', 'description': ['Titanic-problem-using-Logistic-Regression\n'], 'url_profile': 'https://github.com/rubythalib33', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Java', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/deekshajat17', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Java', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'La Paz, Bolivia', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': [""House-Prices-Advanced-Regression-Techniques\nThe main objective of this project is to determine what possible features could have a real impact on the overall value of a house\nFeature Engineering\n\nImputing missing values\nTransforming some numerical variables that seem really categorical\nLabel Encoding\nBox Cox Transformation\nGetting dummy variables for categorical features\n\nModels\n\nSklearn based models + sklearn API of DMLC's XGBoost and Microsoft's LightGBM\nCross-validation\nStacking/ensembling models\n\n""], 'url_profile': 'https://github.com/mariapushkareva', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Java', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Multiple_Linear_Regression_Basic_Struct\n'], 'url_profile': 'https://github.com/halilibrahimsimsek', 'info_list': ['Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Jul 30, 2020', 'Java', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Jul 23, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}"
"{'location': 'Mumbai', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['Simple-Linear-Regression-in-R\nThis dataset contains 2 variable only\nWe would be predicting Score based on number of hours studied by a student\n'], 'url_profile': 'https://github.com/anujjohri', 'info_list': ['R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Nov 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2021', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 20, 2020', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '129 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/parinith', 'info_list': ['R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Nov 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2021', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 20, 2020', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020']}","{'location': 'Minnesota, US', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['HR-Analytics-using-Logistic-Regression\n'], 'url_profile': 'https://github.com/fmurshed07', 'info_list': ['R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Nov 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2021', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 20, 2020', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '450 contributions\n        in the last year', 'description': ['linear-regression-repository\nThis repository contains a program of linear regression for advertising dataset.\n'], 'url_profile': 'https://github.com/nabilatajrin', 'info_list': ['R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Nov 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2021', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 20, 2020', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['Linear-Regression-using-Gradient-Descent\nPredicting House prices from average number of rooms per dwelling and percentage lower status of population\nBatch Gradient Descent is implemented to predict the house prices and mean square error is used to get the error between actual and predicted values.\nThe Sci-kit learns Linear Regression is also used to predict the values and get the mean squared error.\nDataset\nSklearns Boston Housing Dataset\nImplementation\nPrice of houses are predicted using Gradient Descent and Linear Regression in Scikit Learn library.\nOutput\nShows the mean squared error between predicted and actual values of houses using both the methods\nMean squared error using Gradient Descent\nround(((prediction - y) ** 2).mean(),4) =======> 0.3878\nMean squared error using Linear Regression Scikit-Learn\nmean_squared_error(y,pred)  =========>0.3614\n'], 'url_profile': 'https://github.com/siddhithakur', 'info_list': ['R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Nov 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2021', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 20, 2020', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['Reference:\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques\n'], 'url_profile': 'https://github.com/greenmac', 'info_list': ['R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Nov 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2021', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 20, 2020', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Ruchi-Gohil', 'info_list': ['R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Nov 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2021', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 20, 2020', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vinaykumarraju', 'info_list': ['R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Nov 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2021', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 20, 2020', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['Polynomial-Regression-B-spline-natrual-cubic-spline-\nA report about dataset ""BOSTON"".\n'], 'url_profile': 'https://github.com/Yimei120', 'info_list': ['R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Nov 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2021', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 20, 2020', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': [""House-price-prediction-using-Advanced-regression-techniques\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nThis competetion has been coded in R.\nThe preprocessing include checking the feature space and identifying the nulls in each feature using summary.\nThe numerical values for features containing nulls have been replaced by their corresponding mean, we can also try doing the same by replacing those nulls with median.\nThe categorical features have been paid special attention:\n\nApart from the features which had NA just as a missing value, those with NA as a different level have been added to the feature space to now adjust the number of levels according to the problem.\nThis increases the robustness of the model to any dataset (test dataset in this case) which was also dealt in the same way and then was trained on a bunch of different models.\nDifferent models tried on this data are:\nRandom Forest\nGradient Boosting\nDeep Neural Network(DNN using Caret)\nGBM\n\n""], 'url_profile': 'https://github.com/karunparashar', 'info_list': ['R', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Nov 10, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'Jupyter Notebook', 'Updated Jan 4, 2021', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 20, 2020', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'BENGALURU', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gaurav1210', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'Jaipur', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Aditya-171', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Multi Linear Regression in Company  development data.\n'], 'url_profile': 'https://github.com/hanimustikaadi', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'San Jose, California', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['Kaggle-Competitions\n'], 'url_profile': 'https://github.com/Jayrajsinh03', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Labour-Earninings-Evaluation-Data---Multiple-Linear-regression\n'], 'url_profile': 'https://github.com/Manimaran-R', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'MATLAB', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'Manaus - AM - Brazil', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['linear-regression-for-boston-dataset-practice\nCreated during the study of a practical tutorial\n'], 'url_profile': 'https://github.com/adielwesley', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'MATLAB', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shixi99', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'MATLAB', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '252 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/animesharma3', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'MATLAB', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'MATLAB', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '147 contributions\n        in the last year', 'description': ['Linear Regression Using A Neural Net\nA short example in MATLAB demonstrating the connection between a simple linear regression problem and the training of a neural net with a single hidden layer consisting of just one node. We show that the two problems are equivalent when using a single hidden layer of just one node with a linear activation function. That is, the weight and bias terms correspond to the gradient and intercept of the linear line of best fit.\nWe also include the option of using different activation functions (sigmoid, tanh, ReLU, leaky ReLU and ELU) and different batch sizes in the stochastic gradient descent algorithm used to train the model.\nThe path of the parameters (the weight W and bias b) in parameter space during training is plotted to demonstrate the difference in behaviour between stochastic gradient descent, batch method and full gradient descent.\nThanks to Steven Elsworth for his helpful comments and advice on this example.\n'], 'url_profile': 'https://github.com/jamesrynn', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'MATLAB', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a—\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'MATLAB', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'Lausanne', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['A-regression-analysis-of-airline-costs.\nA group project of the Applied Biostatistics course.\n'], 'url_profile': 'https://github.com/maximkryukov', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'MATLAB', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'MATLAB', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['STAA-551---Regression-Models-and-Applications\nColorado State STAA 551 Coursework and Code Examples\n'], 'url_profile': 'https://github.com/dollardjm', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 20, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Jul 5, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'MATLAB', 'Updated Apr 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'R', 'Updated Mar 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['Insurance Charges Forecasting with Linear Regression\nLinear Regression Theory\n\nIt is a statistical model that attempts to illustrate the relationship between independent and dependent variable, with a linear equation, if it is continuous. Line of linear regression is represented by the equation,\ny = mx + c\nwhere, y is the dependent variable, x is independent variable, m is slope of the line and c is y-intercept of the line.\nThe slope can be calculated by,\nm= Σ(x- x̄)(y- ȳ) / Σ(x- x̄)²\nwhere, x̄ and ȳ are the mean of x and y, respectively.\n\nDataset\nThe dataset has been taken from Kaggle. The main objective of the analysis is to accurately forecast insurance charges. This dataset consists of 1338 rows and 7 columns.\nData Source: Insurance Dataset\nage: age of primary beneficiary\ngender: insurance contractor gender, female, male\nbmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\nchildren: Number of children covered by health insurance / Number of dependents\nsmoker: Smoking\nregion: the beneficiary’s residential area in the US, northeast, southeast, southwest, northwest.\ncharges: Individual medical costs billed by health insurance\n\n'], 'url_profile': 'https://github.com/Harshita9511', 'info_list': ['Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['Cancer Tumor Prediction with Logistic Regression\n\n\nLogistic Regression Theory\nLogistic Regression solves the classification problem, where the target variable is categorical in nature. It models the data using sigmoid function that takes in any value and outputs it to be between 0 and 1.\nSigmoid function, f(z)= 1/(1+e^(-z))\n\nModel Evaluation\nConfusion matrix is used to evaluate classification models performance on a set of test data for which the true values are already known.\n\nTrue Positive (TP): Actual observation is positive and is predicted to be positive.\nTrue Negative (TN): Actual observation is negative and is predicted to be negative.\nFalse Positive (FP): Actual observation is negative but is predicted positive.\nFalse Negative (FN): Actual observation is positive but is predicted negative.\nEvaluation Parameters\nAccuracy: Ratio of correctly predicted classes to all the classes.\nAccuracy = (TP+TN) / (TP+TN+FP+FN)\nMisclassification Rate (Error rate): Ratio of wrong predictions to the total number of classes.\nError rate = (FP+FN) / (TP+TN+FP+FN)\nRecall: Ratio of correctly predicted positive classes to all the actual positive classes.\nRecall = TP / (TP+FN)\nPrecision: Ratio of correctly predicted positive classes to all positive predicted classes.\nPrecision = TP / (TP+FP)\nF-measure: It is the harmonic mean of Recall and Precision.\nF-measure = (2 * Recall * Precision) / (Recall + Precision)\nDataset\nThe dataset has been taken from UCI machine learning repository. The main objective of the analysis is to perform classification of tumors i.e., benign(B) or malignant(M). A benign tumor is a tumor that does not invade its surrounding tissue or spread around the body. A malignant tumor is a tumor that may invade its surrounding tissue or spread around the body. This dataset consists of 569 rows and 33 columns.\nData Source: Breast Cancer Dataset.\nid: ID number.\ndiagnosis: The diagnosis of breast tissues (M = malignant, B = benign).\nradius_mean: mean of distances from center to points on the perimeter.\ntexture_mean: standard deviation of gray-scale values.\nperimeter_mean: mean size of the core tumor.\narea_mean\nsmoothness_mean: mean of local variation in radius lengths.\ncompactness_mean: mean of perimeter² / area — 1.0.\nconcavity_mean: mean of severity of concave portions of the contour.\nconcave points_mean: mean for number of concave portions of the contour.\nsymmetry_mean\nfractal_dimension_mean: mean for “coastline approximation” — 1.\nradius_se: standard error for the mean of distances from center to points on the perimeter.\ntexture_se: standard error for standard deviation of gray-scale values.\nperimeter_se\narea_se\nsmoothness_se: standard error for local variation in radius lengths.\ncompactness_se: standard error for perimeter² / area — 1.0.\nconcavity_se: standard error for severity of concave portions of the contour.\nconcave points_se: standard error for number of concave portions of the contour.\nsymmetry_se\nfractal_dimension_se: standard error for “coastline approximation” — 1.\nradius_worst: “worst” or largest mean value for mean of distances from center to points on the perimeter.\ntexture_worst: “worst” or largest mean value for standard deviation of gray-scale values.\nperimeter_worst\narea_worst\nsmoothness_worst: “worst” or largest mean value for local variation in radius lengths.\ncompactness_worst: “worst” or largest mean value for perimeter² / area — 1.0.\nconcavity_worst: “worst” or largest mean value for severity of concave portions of the contour.\nconcave points_worst: “worst” or largest mean value for number of concave portions of the contour.\nsymmetry_worst\nfractal_dimension_worst: “worst” or largest mean value for “coastline approximation” — 1.\n\n'], 'url_profile': 'https://github.com/Harshita9511', 'info_list': ['Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Simple-Linear-Regression-from-Scratch---Python\nAn implementation of Linear regression in python using numpy.\n'], 'url_profile': 'https://github.com/pauljosh1995', 'info_list': ['Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Polynomial-Regression-for-Covid-19-in-Indonesia\n'], 'url_profile': 'https://github.com/dedewanta', 'info_list': ['Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['House-Price-Prediction-Multivariate-Linear-Regression-\nUsing multivariate Linear Regression to predict the price of a house based on various parameters such as: 1. #  of bedrooms 2. square feet 3. number of floors etc...\n'], 'url_profile': 'https://github.com/rahul01101991', 'info_list': ['Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Python', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Unlicense license', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Regression-Analysis-for-Apprentice-Chef-Inc.\nThe purpose of this analysis is to better understand how much revenue to expect from each customer within their first year of\norders. This project will develop top insights, and build a machine learning model to predict revenue over the first year of each\ncustomer’s life cycle.\nPlease see the analysis file for the thought process and walkthrough of the analysis. I also included the pdf file of the insights and  recommendations from the analysis.\n'], 'url_profile': 'https://github.com/piyanick', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Multiple-Linear-regression-Algorithm-in-Python\nImplemented the multiple linear regression algorithm in python and compared it with the algorithm in the standard libraries.\n'], 'url_profile': 'https://github.com/vamsiabbireddy', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020']}","{'location': 'Hanoi', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['Linear-Regression-simple-with-Rubik-AI\n'], 'url_profile': 'https://github.com/DinhHuySang', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020']}","{'location': 'Notre Dame, IN', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['Analyzing Correlated Data Using Logistic Regression Models\nOne assumption of logistic regssion is that all observations are independent of one another. Sometimes, this assumption is broken. This can happen when observations are repeated measures, or when observations are grouped by a certain factor (e.g. neighborhoods, hospitals).\nThis data analysis project uses two methods (generalized estimating equations and random effects models) to fit models on such correlated data using an example data set of a longitudinal MRI study.\nA Longitudinal MRI Study (https://www.kaggle.com/jboysen/mri-and-alzheimers#oasis_longitudinal.csv)\nThe data set contains information from a longitudinal MRI study done on 150 patients with and without dementia. Each patient was scanned during two or more visits, where visits were at least one year apart. The data was made available by the Open Access Series of Imaging Studies (OASIS) project and accessed on Kaggle. The data contains patients’ demographic and anatomic information. Speciﬁcally, the data set has 373 observations with the following features:\n\nSubject ID\nVisit Number\nGender\nAge\nYears of education\nWhether or not the patient’s group status changed at any point during the time of the study (group statuses are demented or not demented)\nClinical Dementia Rating (CDR)\nEstimated total intracranial volume in mm3 (eTIV)\nNormalized whole brain volume (nWBV)\n\nThe CDR has the following possible scores: 0 (no dementia), 0.5 (very mild dementia), 1 (mild dementia), 2 (moderate dementia), and 3 (severe dementia). Thus, I will convert this feature into a binary outcome variable so that a CDR score of 0 is no dementia (Class = 0) and any score above 0 is dementia (Class = 1).\nAs this data set has correlated observations (groups are subjects), the goal of this project will be to perform a GEE analysis and random eﬀects modeling analysis to predict whether or not a patient as dementia. Specifically, the analysis will focus on conducting variable and correlation structure selection for GEE, and on choosing fixed/random effects for random effects analysis. Pros and cons for selecting one method over the other for this particular data set are discussed.\nNote: In order to make the variables on similar scales so that model coeﬃcients are of similar magnitude, eTIV was divided by 1000 (units are now in cm3), nWBV was multipled by 10, and age was divided by 10.\nFor more on the methods, see:\nHosmer, D.W., Jr., Lemeshow, S. and Sturdivant, R.X. (2013). Logistic Regression Models for the Analysis of Correlated Data. In Applied Logistic Regression (eds D.W. Hosmer, S. Lemeshow and R.X. Sturdivant).\n'], 'url_profile': 'https://github.com/honokasuzuki', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020']}","{'location': 'Hyderabad, IN', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cpuranda', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NCR\nAn Efficient Estimate of Causal Effect using Multiple Negative Controls Regression in the presence of unobserved confounding\n'], 'url_profile': 'https://github.com/hhoulei', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', '1', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 28, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'R', 'Updated Mar 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['Prakriti-Data-Analytics-Hackathon-2020\nPrakriti is Asia\'s Largest Agri-Tech fest.\nThe problem statement and the dataset has been provided above.\nFollow the following steps to run the Code:\n\nRun the code only in google colab. The Link has already been provided.\nUpload the data set with the name: \'Data_set.xlsx\' in google colab in ""Choose file Section"".\nIn case the code is excecuted in .ipynb format/ .py format, make sure the libraries are updated to avoid any errors\nResults and outputs have already been printed in google colab.\nThe detail tabulation of the result can be looked in the Presentation and result pdf.\n\n'], 'url_profile': 'https://github.com/skp1999', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Tecniche-di-Deep-Learning-per-la-Previsione-di-Consumi-Energetici-\nUniversity individual project. First experience with regression techniques with no prior theoretical or practical knowledge about the argument.\nNotebook containing the python source code will be added soon.\n'], 'url_profile': 'https://github.com/EnricoGrazioli', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/santosh-balija', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'Tehran', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['pyspark-basic-examples\nBasic, Medior and Advenced examples of Pyspark Including:\nRDD transformation & actions, parallelism,\nSparkSQL and Dataframe,\nMLlib Examples such as Regression, Classification, etc\n'], 'url_profile': 'https://github.com/peymanbarca', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akshatAgrawal14', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'Berlin, Germany', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': [""Country Happiness Classifier\nThis project uses a simple Linear Regression Model to classify how happy or satisfied a country is given its GDP.\nThis example is taken from Aurélien Géron's Hands On Machine Learning book.\n\nDependencies\n\nPandas\nNumpy\nSciKitLearn\n\nInstallation\n\nRun python install -r requirements.txt\n\n""], 'url_profile': 'https://github.com/michaelokuboyejo', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'Liaoning Dalian', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chuwangBA', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['House_Pricing\nProjects based on Machine Learning supervised learning algorithms i.e. Logistic Regression , LDA, & KNN algo. in Python\nThe Dataset contains 506 observations of whether the property was sold within three months of getting listed.\nCorresponding to each house, data of 18 other variables are available on which it is suspected to depend.\n'], 'url_profile': 'https://github.com/Priyanka-Jogdand-DS', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['AI-analysis-of-complaints\nA regression neural net with multiple outputs to predict the volume and distribution of complaints in a hypothetical dataset.\n'], 'url_profile': 'https://github.com/casklord', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 17, 2020']}","{'location': 'Bhubaneswar, India', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shraja11', 'info_list': ['Jupyter Notebook', 'Updated Mar 19, 2020', 'Updated Mar 19, 2020', 'Python', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Apr 2, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', '1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'Jupyter Notebook', 'Updated Mar 21, 2020', '1', 'Python', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 17, 2020']}"
"{'location': 'Pakistan', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Human-Resource-Churn-Prediction\nCompanies want to improve and maintain a healthy relationship and environment for their employees. This can be achieved by recognizing and understanding the important factors that are associated with employee turnover and taking care of them might add into the productivity and growth of the employees. These insights can help managers for grabing the opportunity to take corrective steps to build and preserve their successful business.\nThe project that is based on understanding what factors contributed most to employee turnover and to create a model that can predict if a certain employee will leave the company or not.\nColumn Description\nsatisfacion_level: Showing satisfaction of a particular employee\nlast_evaluation: Showing last evaluation of a particular employee\nnumber_project: Showing number of projects handled a particular employee\naverage_montly_hours: Showing the monthly hours that were spent the particular emloyee\ntime_spend_company: Shows the number of years spent by the particular employee in the company.\nWork_accident: Showing an employee has whether been part of the company or not.\nleft: Tells either and employee has left the company or not. Shows two values 0= not left, 1= left\npromotion_last_5years: Shows that the whether the employee has got any promotion in the last 5 years or not.\ndept: Shows the departments\nsalary: Shows the salary type of the employee\n\n'], 'url_profile': 'https://github.com/YUSRA100', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated May 21, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Shell', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Jul 13, 2020', 'R', 'Updated Mar 21, 2020', 'Julia', 'MIT license', 'Updated Mar 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/crossedapex', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated May 21, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Shell', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Jul 13, 2020', 'R', 'Updated Mar 21, 2020', 'Julia', 'MIT license', 'Updated Mar 21, 2020']}","{'location': 'Dallas/Fort Worth area', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': [""fc-cincinnati-attendance-forecasting-model\nThis is a regression model to predict FC Cincinnati's game by game home attendance for the 2020 MLS regular season.\nThe raw data has been cleaned using Python and regression models tested using R.\n""], 'url_profile': 'https://github.com/vishaljiandani', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated May 21, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Shell', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Jul 13, 2020', 'R', 'Updated Mar 21, 2020', 'Julia', 'MIT license', 'Updated Mar 21, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '219 contributions\n        in the last year', 'description': ['IntroToMachineLearning\nThis repository contains implementation of logistic regression from scratch, neural networks, CNN, unsupervised learning, and Q learning algorithms\n'], 'url_profile': 'https://github.com/UtkarshBehre', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated May 21, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Shell', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Jul 13, 2020', 'R', 'Updated Mar 21, 2020', 'Julia', 'MIT license', 'Updated Mar 21, 2020']}","{'location': 'Tehran, Iran', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['Mc\nGutenberg-Richter Relationship and Plot.\nRequirements\nA single column (magnitude) catalog.\nGMT 5\n\nEdit the first few lines of the code before runing it.\ngood luck ;)\n'], 'url_profile': 'https://github.com/alirezaniki', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated May 21, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Shell', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Jul 13, 2020', 'R', 'Updated Mar 21, 2020', 'Julia', 'MIT license', 'Updated Mar 21, 2020']}","{'location': 'Cambridge', 'stats_list': [], 'contributions': '926 contributions\n        in the last year', 'description': ['Modeling the spread of COVID-19\nModel\nTo model the spread of COVID-19, the very simple and widely used lgoistic function\n\nwhere\n\nThe parameters of the model are determined by least square regression using the\ndataset of the Johns Hopkins University.\nResults\nThe estimate for the total number of cases, deaths, and the mortality for each country are given\nhere. Furthermore,\nplots of the regression are available in the plots folder. The green line indicates the date of inflection for both the cases and infections.\n\n\n'], 'url_profile': 'https://github.com/VincentStimper', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated May 21, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Shell', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Jul 13, 2020', 'R', 'Updated Mar 21, 2020', 'Julia', 'MIT license', 'Updated Mar 21, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '672 contributions\n        in the last year', 'description': ['Combined_Cycle_Power_Plant\nCoded Gradient Descent for N features and came up with predictions by trying and testing with various combinations of learning rates and number of iterations.Developed a model with accuracy 93.6%\n'], 'url_profile': 'https://github.com/Priyanshiguptaaa', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated May 21, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Shell', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Jul 13, 2020', 'R', 'Updated Mar 21, 2020', 'Julia', 'MIT license', 'Updated Mar 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '530 contributions\n        in the last year', 'description': [""SentimentAnalysis\nThis is a small NLP project which uses sentiment analysis and machine learning to classify words with positive or negative connotations.\nThis project is a part of the final project for the Getting started with NLP PES IO course.\nStarted: Mar 2020\nFinished: Mar 2020\nDescription:\nThis program reads in filtered data from the IMBD movie data set. The data set has been divided into test and train. It cleans the data and begins to process it. The model used in this project is linear regression. The first half of the data contains positive reviews and the second half contains negative reviews. Based on this information, the model is trained to recognize positive and negative words. Using this trained model, the program can assign a numerical value to the tone of a user's input.\nSteps:\nSTEP 1. Open the Test and train files and read them into an array\nSTEP 2. Clean the data by removing all unnecessary symbols and punctuation.\nSTEP 3. Neutralize the data by feeding it through a count vectorizer function\nSTEP 4. Split the data and train it based on negative and positve words\nSTEP 5. Traning the model using Logistic regression\nSTEP 7. Store the trained data in a dictionary with the word being the key\nSTEP 8. Accepting user input and determining a tone scale\nWhat I have learned:\nDuring this project I picked up a whole bunch of new skills. They are as follows:\n\nData filtering\nCount Vectorization\nSkLearn Library\nRegex\nLogistic Regression\n\nNote: I have used serveral sources from the web to build this project. As well as major thanks to my IO mentor.\n""], 'url_profile': 'https://github.com/Harsh188', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated May 21, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Shell', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Jul 13, 2020', 'R', 'Updated Mar 21, 2020', 'Julia', 'MIT license', 'Updated Mar 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['umojaHack2020-FireHotspots\nTeam Logistically Regressed\nTask Description\nEach year, thousands of fires blaze across the African continent. Some are natural occurrences, part of a ‘fire cycle’ that can actually benefit some dryland ecosystems. Many are started intentionally, used to clear land or to prepare fields for planting. And some are wildfires, which can rage over large areas and cause huge amounts of damage. Whatever the cause, fires pour vast amounts of CO2 into the atmosphere, along with smoke that degrades air quality for those living downwind.\nFiguring out the dynamics that influence where and when these fires will occur can help us to better understand their effects. And predicting how these dynamics will play out in the future, under different climatic conditions, could prove extremely useful. For this challenge, the goal is to do exactly that. We’ve aggregated data on burned areas across the whole of the DRC for each month since 1 April 2000. You’ll be given the burn area data up to the end of 2013, along with some additional information (such as rainfall, temperature, population density etc) that extends into the test period. The challenge is to build a model capable of predicting the burned area in different locations over the 2014 to 2016 test period based on only this information.\n'], 'url_profile': 'https://github.com/jardunn', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated May 21, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Shell', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Jul 13, 2020', 'R', 'Updated Mar 21, 2020', 'Julia', 'MIT license', 'Updated Mar 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mheiner', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated May 21, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Shell', 'Updated Mar 22, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Apr 10, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Jul 13, 2020', 'R', 'Updated Mar 21, 2020', 'Julia', 'MIT license', 'Updated Mar 21, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Regression Lines Lab\nIntroduction\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\nIn moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the graph here.\n\nDetermining Quality\nIn the file, movie_data.py you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies provided here.\nfrom movie_data import movies \nlen(movies)\n\nPress shift + enter\n\nmovies[0]\nmovies[0][\'budget\']/1000000\nThe numbers are in millions, so we will simplify things by dividing everything by a million\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key domgross.\nPlotting our data\nLet\'s write the code to plot this data set.\nAs a first task, convert the budget values of our scaled_movies to x_values, and convert the domgross values of the scaled_movies to y_values.\nx_values = None\ny_values = None\nx_values and x_values[0] # 13.0\ny_values and y_values[0] # 26.0\nAssign a variable called titles equal to the titles of the movies.\ntitles = None\ntitles and titles[0]\nGreat! Now we have the data necessary to make a trace of our data.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\nPlotting a regression line\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n\n$\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$.\n\n\n$\\hat{y} = 1.7x + 10$\n\n\nWrite a function called regression_formula that calculates our $\\hat{y}$ for any provided value of $x$.\ndef regression_formula(x):\n    pass\nCheck to see that the regression formula generates the correct outputs.\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\nCalculating errors of a regression Line\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called y_actual that given a data set of x_values and y_values, finds the actual y value, provided a value of x.\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\nWrite a function called error, that given a list of x_values, and a list of y_values, the values m and b of a regression line, and a value of x, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.\ndef error(x_values, y_values, m, b, x):\n    pass\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\nNow that we have a formula to calculate our errors, write a function called error_line_trace that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\nOk, so the function error_line_trace takes our dataset of x_values as the first argument and y_values as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\nThe return value is a dictionary that represents a trace, and looks like the following:\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\nThe trace represents the error line above. The data in x and y represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals \'lines\'.\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\nscaled_movies[17]\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\nFrom there, we can write a function called error_line_traces, that takes in a list of x_values as an argument, y_values as an argument, and returns a list of traces for every x value provided.\ndef error_line_traces(x_values, y_values, m, b):\n    pass\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\nerrors_for_regression and len(errors_for_regression) # 30\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n\nDon\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\nCalculating RSS\nNow write a function called squared_error, that given a value of x, returns the squared error at that x value.\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\ndef squared_error(x_values, y_values, m, b, x):\n    pass\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\ndef squared_errors(x_values, y_values, m, b):\n    pass\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\nNext, write a function called residual_sum_squares that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\nFinally, write a function called root_mean_squared_error that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that root_mean_squared_error is a way for us to measure the approximate error per data point.\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\nSome functions for your understanding\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\nregression_lines = [(1.7, 10), (1.9, 20)]\nThen we can return a list of the regression lines along with the associated RMSE.\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\nNow let\'s generate the RMSE values for each of these lines.\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\nNow we\'ll provide a couple functions for you:\n\na function called trace_rmse, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of x and y, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line ""fits"" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'Bonn, Germany', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': [""Customer-Churn-Prediction-using-Logistic-Regression-in-Python\nIn this, I developed a classifier that predicts  which customer will leave a particular company or stays with the company which is simply referred as Churns from the given customer dataset.\nThis is implemeted in Python by using Scikit-learn[sklearn], Pandas, Numpy, Matplotlib, Seaborn\nLogistic Regression is a variant of Linear Regression, and it is used when the dependent variable - 'y' is categorical or discrete. It predicts the probability of the class label as a function of the independent variables.\nLogistic regression fits a special s-shaped curve called Logistic Curve or Sigmoid Curve by taking the linear regression and transforming the numeric estimate into a probability with the function called sigmoid function 𝜎\nReference:\nhttps://www.coursera.org/\n""], 'url_profile': 'https://github.com/Jithsaavvy', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Intro to regression lines\nLearning Objectives\n\nUnderstand how regression lines can help us make predictions about data\nUnderstand how the components of slope and y-intercept determine the output of a regression line\nUnderstand how to represent a line as a function\n\nA not so simple problem\nNow we know a bit about plotting data and we have written some functions, like trace_values, layout, and plot, to help us do so.  You can view them here.\nThe benefit of a buck\nImagine we are hired as a consultant for a movie executive.  The movie executive receives a budget proposal, and wants to know how much money the movie might make.  We can help him by building a model of the relationship between the money spent on a movie and money made.\nRepresenting linear regression graphically\nTo predict movie revenue based on a budget, let's draw a single straight line that represents the relationship between how much a movie costs and how much it makes.\n\nEventually, we will want to train this model to match up against an actual data, but for now let's just draw a line to see how it can make estimates.\n\nfrom lib.graph import trace_values, plot, layout\n\nregression_trace = trace_values([0, 150], [0, 450], mode = 'lines', name = 'estimated revenue')\nmovie_layout = layout(options = {'title': 'Movie Spending and Revenue (in millions)'})\nplot([regression_trace], movie_layout)\nBy using a line, we can see how much money is earned for any point on this line.  All we need to do is look at a given $x$ value, and find the corresponding $y$ value at that point on the line.\n\nSpend 60 million, and expect to bring in about 180 million.\nSpend 20 million, and expect to bring in 60 million.\n\nThis approach of modeling a linear relationship (that is a drawing a straight line) between an input and an output is called linear regression.  We call the input our explanatory variable, and the output the dependent variable.  So here, we are saying budget explains our dependent variable, revenue.\nRepresenting linear regression with functions\nInstead of only representing this line visually, we also would like to represent this line with a function. That way, instead of having to see how an $x$ value points to a $y$ value along our line, we simply could punch this input into our function to calculate the proper output.\nA wrong guess\nLet's take an initial (wrong) guess at turning this line into a function.\nFirst, we represent the line as a mathematical formula.\n$y = x$\nThen, we turn this formula into a function:\ndef y(x):\n    return x\n\ny(0)\ny(10000000)\nThis is pretty nice.  We just wrote a function that automatically calculates the expected revenue given a certain movie budget.  This function says that for every value of $x$ that we input to the function, we get back an equal value $y$.  So according to the function, if the movie has a budget of $30$ million, it will earn $30$ million.\nA better guess: Matching lines to functions\nTake a look at the line that we drew.  Our line says something different.  The line says that spending 30 million brings predicted earnings of 90 million.  We need to change our function so that it matches our line.  In fact, we need a consistent way to turn lines into functions, and vice versa.  Let's get to it.\nWe start by turning our line into a chart below.  It shows how our line relates x-values and y-values, or our budgets and revenues.\n\n\n\nX (budget)\nY (revenue)\n\n\n\n\n0\n0\n\n\n30 million\n90 million\n\n\n60 million\n180 million\n\n\n\nNext, we need an equation that allows us to match this data.\n\ninput 0 and get back 0\ninput 30 million and get back 90 million\nand input 60 million and get back 180 million?\n\nWhat equation is that?  Well it's $y = 3x$.  Take a look to see for yourself.\n\n0 * 3 = 0\n30 million * 3 = 90 million\n60 million * 3 = 180 million\n\nLet's see it in the code.  This is what it looks like:\ndef y(x):\n    return 3*x\ny(30000000)\ny(0)\nProgress! We multiplied each $x$ value by 3 so that our function's outputs correspond to the $y$ values appearing along our graphed line.\nThe Slope Variable\nBy multiplying $x$ by 3, we just altered the slope variable.  The slope variable changes the inclination of the line in our graph.  Slope generally is represented by $m$ like so:\n$y = mx$\nLet's make sure we understand what all of our variables stand for.  Here they are:\n\n$y$: the output value returned by the function, also called the response variable, as it responds to values of $x$\n$x$: the input variable, also called the explanatory variable, as it explains the value of $y$\n$m$: the slope variable, determines how vertical or horizontal the line will appear\n\nLet's adapt these terms to our movie example.  The $y$ value is the revenue earned from the movie, which we say is in response to our budget.  The explanatory variable of our budget, $x$, represents our budget, and the $m$ corresponds to our value of 3, which describes how much money is earned for each dollar spent.  Therefore, with an $m$ of 3, our line says to expect to earn 3 dollars for each dollar spent making the movie.  Likewise, an $m$ of 2 suggests we earn 2 dollars for every dollar we spend.\n\nA higher value of $m$ means a steeper line.  It also means that we expect more money earned per dollar spent on our movies.  Imagine the line pivoting to a steeper tilt as we guess a higher amount of money earned per dollar spent.\n\nThe y-intercept\nThere is one more thing that we need to learn in order to describe every straight line in a two-dimensional world.  That is the y-intercept.\n\n\nThe y-intercept is the $y$ value of the line where it intersects the y-axis.\nOr, put another way, the y-intercept is the value of $y$ when $x$ equals zero.\n\n\nLet's add a trace with a higher y-intercept than our initial line to the movie plot.\nregression_trace_increased = trace_values([0, 150], [50, 500], mode = 'lines', name = 'increased est. revenue')\nplot([regression_trace_increased, regression_trace], movie_layout)\nWhat is the y-intercept of the original estimated revenue line?  Well, it's the value of $y$ when that line crosses the y-axis.  That value is zero.  Our second line is parallel to the first but is shifted higher so that the y-intercept increases up to 50 million.  Here, for every value of $x$, the corresponding value of $y$ is higher by 50 million.\n\nOur formula is no longer $y = 3x$.\nIt is $y = 3 x + 50,000,000$.\n\nIn addition to determining the y-intercept from a line on a graph, we can also see the y-intercept by looking at a chart of points.\n\nIn the chart below, we know that the y-intercept is 50 million because its corresponding $x$ value is zero.\n\n\n\n\nX\nY\n\n\n\n\n0\n50 million\n\n\n40 million\n170 million\n\n\n60 million\n230 million\n\n\n\nThe y-intercept of a line usually is represented by b.  Now we have all of the information needed to describe any straight line using the formula below:\n$$y = mx + b $$\nOnce more, in this formula:\n\n$m$ is our slope of the line, and\n$b$ is the value of $y$ when $x$ equals zero.\n\nSo thinking about it visually, increasing $m$ makes the line steeper, and increasing $b$ pushes the line higher.\nIn the context of our movies, we said that the the line with values of $m$ = 3 and $b$ = 50 million describes our line, giving us:\n$y = 3x + 50,000,000 $.\nLet's translate this into a function.  For any input of $x$ our function returns the value of $y$ along that line.\ndef y(x):\n    return 3*x + 50000000\ny(30000000)\ny(60000000)\nSummary\nIn this section, we saw how to estimate the relationship between an input variable and an output value.  We did so by drawing a straight line representing the relationship between a movie's budget and it's revenue.  We saw the output for a given input simply by looking at the y-value of the line at that input point of $x$.\nWe then learned how to represent a line as a mathematical formula, and ultimately a function.  We describe lines through the formula $y = mx + b $, with $m$ representing the slope of the line, and $b$ representing the value of $y$ when $x$ equals zero.  The $b$ variable shifts the line up or down while the $m$ variable tilts the line forwards or backwards.  Translating this formula into a function, we can write a function that returns an expected value of $y$ for an input value of $x$.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'HTML', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'Sri Lanka', 'stats_list': [], 'contributions': '1,804 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rohankumara', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K – 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['HR Analytics - Classification of retention of employees in a company using-Logistic-Regression\n'], 'url_profile': 'https://github.com/rahul01101991', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': ['Regression-Analysis-on-Red-Wine-Quality-Dataset-2018-\nResearch Objective : To develop a model estimating the quality of wine based on various predictors.\nData source :\nRed wine dataset we used for our project is from UCI Machine Learning Repository\n(http://archive.ics.uci.edu/ml/datasets/Wine+Quality)\nData Description:\nExperts conducted physicochemical tests (e.g. pH values and acidity... ) on the red wine samples from the north of Portugal and made an evaluation based on the median of 3 wine experts according to their sensory levels and each expert graded the wine quality between 0 (very bad) and 10 (very excellent).\nThe predictors we used in our project:\nResponse Variable: Wine Quality\nPredictor Variables : fixed acidity, volatile acidity ,citric acid ,residual sugar, chlorides ,free sulfur dioxide, total sulfur dioxide, Density, pH ,sulphate, alcohol.\n'], 'url_profile': 'https://github.com/Fatime116', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Apr 25, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Comedy Show Lab\nImagine that you are the producer for a comedy show at your school.  We need you to use knowledge of linear regression to make predictions as to the success of the show.\nWorking through a linear regression\nThe comedy show is trying to figure out how much money to spend on advertising in the student newspaper.  The newspaper tells the show that\n\nFor every two dollars spent on advertising, three students attend the show.\nIf no money is spent on advertising, no one will attend the show.\n\nWrite a linear regression function called attendance that shows the relationship between advertising and attendance expressed by the newspaper.\ndef attendance(advertising):\n    pass\nattendance(100) # 150\nattendance(50) # 75\nAs the old adage goes, ""Don\'t ask the barber if you need a haircut!"" Likewise, despite what the student newspaper says, the comedy show knows from experience that they\'ll still have a crowd even without an advertising budget.  Some of the comedians in the show have friends (believe it or not), and twenty of those friends will show up.  Write a function called attendance_with_friends that models the following:\n\nWhen the advertising budget is zero, 20 friends still attend\nThree additional people attend the show for every two dollars spent on advertising\n\ndef attendance_with_friends(advertising):\n    pass\nattendance_with_friends(100) # 170\nattendance_with_friends(50) # 95\nPlot it\nLet\'s help plot this line so you can get a sense of what your $m$ and $b$ values look like in graph form.\nOur x values can be a list of initial_sample_budgets,  equal to a list of our budgets.  And we can use the outputs of our attendance_with_friends function to determine the list of attendance_values, the attendance at each of those x values.\ninitial_sample_budgets = [0, 50, 100]\nattendance_values = [20, 95, 170]\nFirst we import the necessary plotly library, and graph_obs function, and setup plotly to be used without uploading our plots to its website.\nFinally, we plot out our regression line using our attendance_with_friends function.  Our x values will be the budgets.  For our y values, we need to use our attendance_with_friends function to create a list of y-value attendances for every input of x.\nimport plotly\nfrom plotly import graph_objs\nplotly.offline.init_notebook_mode(connected=True)\n\ntrace_of_attendance_with_friends = graph_objs.Scatter(\n    x=initial_sample_budgets,\n    y=attendance_values,\n)\n\nplotly.offline.iplot([trace_of_attendance_with_friends])\ntrace_of_attendance_with_friends\nNow let\'s write a couple functions that we can use going forward.  We\'ll write a function called m_b_data that given a slope of a line, $m$, a y-intercept, $b$, will return a dictionary that has a key of x pointing to a list of x_values, and a key of y that points to a list of y_values.  Each $y$ value should be the output of a regression line for the provided $m$ and $b$ values, for each of the provided x_values.\ndef m_b_data(m, b, x_values):\n    pass\nm_b_data(1.5, 20, [0, 50, 100]) # {\'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nNow let\'s write a function called m_b_trace that uses our m_b_data function to return a dictionary that includes keys of name and mode in addition to x and y.  The values of mode and name are provided as arguments.  When the mode argument is not provided, it has a default value of lines and when name is not provided, it has a default value of line function.\ndef m_b_trace(m, b, x_values, mode = \'lines\', name = \'line function\'):\n    pass\nm_b_trace(1.5, 20, [0, 50, 100]) \n# {\'mode\': \'line\', \'name\': \'line function\', \'x\': [0, 50, 100], \'y\': [20.0, 95.0, 170.0]}\nCalculating lines\nThe comedy show decides to advertise for two different shows.  The attendance looks like the following.\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n200\n400\n\n\n400\n700\n\n\n\nIn code, we represent this as the following:\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nWrite a function called marginal_return_on_budget that returns the expected amount of increase per every dollar spent on budget.\n\nThe function should use the formula for calculating the slope of a line provided two points.\n\ndef marginal_return_on_budget(first_show, second_show):\n    pass\nmarginal_return_on_budget(first_show, second_show) # 1.5\nfirst_show\nJust to check, let\'s use some different data to make sure our marginal_return_on_budget function calculates the slope properly.\nimaginary_third_show = {\'budget\': 300, \'attendance\': 500}\nimaginary_fourth_show = {\'budget\': 600, \'attendance\': 900}\nmarginal_return_on_budget(imaginary_third_show, imaginary_fourth_show) # 1.3333333333333333\nGreat!  Now we\'ll begin to write functions that we can use going forward.  The functions will calculate attributes of lines in general and can be used to predict the attendance of the comedy show.\nTake the following data.  The comedy show spends zero dollars on advertising for the next show.  The attendance chart now looks like the following:\n\n\n\nBudgets (dollars)\nAttendance\n\n\n\n\n0\n100\n\n\n200\n400\n\n\n400\n700\n\n\n\nbudgets = [0, 200, 400]\nattendance_numbers = [100, 400, 700]\nTo get you started, we\'ll provide a function called sorted_points that accepts a list of x values and a list of y values and returns a list of point coordinates sorted by their x values.  The return value is a list of sorted tuples.\ndef sorted_points(x_values, y_values):\n    values = list(zip(x_values, y_values))\n    sorted_values = sorted(values, key=lambda value: value[0])\n    return sorted_values\nsorted_points([4, 1, 6], [4, 6, 7])\nbuild_starting_line\nIn this section, we\'ll write a function called build_starting_line. The function that we end up building simply draws a line between our points with the highest and lowest x values.  We are selecting these points as an arbitrary ""starting"" point for our regression line.\n\nAs John von Neumann said, ""truth … is much too complicated to allow anything but approximations.""  All models are inherently wrong, but some are useful.  In future lessons, we will learn how to build a regression line that accurately matches our dataset. For now, we will focus on building a useful ""starting"" line using the first and last points along the x-axis.\n\nFirst, write a function called slope that, given a list of x values and a list of y values, will use the points with the lowest and highest x values to calculate the slope of a line.\ndef slope(x_values, y_values):\n    pass\nslope([200, 400], [400, 700]) # 1.5\nNow write a function called y_intercept.  Use the slope function to calculate the slope if it isn\'t provided as an argument. Then we will use the slope and the values of the point with the highest x value to return the y-intercept.\ndef y_intercept(x_values, y_values, m = None):\n    pass\ny_intercept([200, 400], [400, 700]) # 100\ny_intercept([0, 200, 400], [10, 400, 700]) # 10\nNow write a function called build_starting_line that given a list of x_values and a list of y_values returns a dictionary with a key of m and a key of b to return the m and b values of the calculated regression line.  Use the  slope and y_intercept functions to calculate the line.\ndef build_starting_line(x_values, y_values):\n    pass\nbuild_starting_line([0, 200, 400], [10, 400, 700]) # {\'b\': 10.0, \'m\': 1.725}\nFinally, let\'s write a function called expected_value_for_line that returns the expected attendance given the $m$, $b$, and $x$ $value$.\nfirst_show = {\'budget\': 300, \'attendance\': 700}\nsecond_show = {\'budget\': 400, \'attendance\': 900}\n\nshows = [first_show, second_show]\ndef expected_value_for_line(m, b, x_value):\n    pass\nexpected_value_for_line(1.5, 100, 100) # 250\nUsing our functions\nNow that we have built these functions, we can use them on our dataset.  Uncomment and run the lines below to see how we can use our functions going forward.\nfirst_show = {\'budget\': 200, \'attendance\': 400}\nsecond_show = {\'budget\': 400, \'attendance\': 700}\nthird_show = {\'budget\': 300, \'attendance\': 500}\nfourth_show = {\'budget\': 600, \'attendance\': 900}\n\ncomedy_shows = [first_show, second_show, third_show, fourth_show]\n\nshow_x_values = list(map(lambda show: show[\'budget\'], comedy_shows))\nshow_y_values = list(map(lambda show: show[\'attendance\'], comedy_shows))\ndef trace_values(x_values, y_values, mode = \'markers\', name=""data""):\n    return {\'x\': x_values, \'y\': y_values, \'mode\': mode, \'name\': name}\ndef plot(traces):\n    plotly.offline.iplot(traces)\ncomedy_show_trace = trace_values(show_x_values, show_y_values, name = \'comedy show data\')\ncomedy_show_trace\nshow_starting_line = build_starting_line(show_x_values, show_y_values)\nshow_starting_line\ntrace_show_line = m_b_trace(show_starting_line[\'m\'], show_starting_line[\'b\'], show_x_values, name = \'starting line\')\ntrace_show_line\nplot([comedy_show_trace, trace_show_line])\nAs we can see above, we built a ""starting"" regression line out of the points with the lowest and highest x values. We will learn in future lessons how to improve our line so that it becomes the ""best fit"" given all of our dataset, not just the first and last points. For now, this approach sufficed since our goal was to practice working with and plotting line functions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'HTML', 'Updated Dec 9, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Project: Regression Modeling with the Boston Housing Dataset\nIntroduction\nIn this lab, you\'ll apply the regression analysis and diagnostics techniques covered in this section to the ""Boston Housing"" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you\'ll use some of the features in this dataset to create a linear model to predict the house price!\nObjectives\nYou will be able to:\n\nPerform a linear regression using statsmodels\nDetermine if a particular set of data exhibits the assumptions of linear regression\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\nUse the coefficient of determination to determine model performance\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet\'s get started\nImport necessary libraries and load \'BostonHousing.csv\' as a pandas dataframe\n# Your code here\nThe columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value MEDV. The description of the other variables is available on KAGGLE.\nInspect the columns of the dataset and comment on type of variables present\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n# Record your observations here \nCreate histograms for all variables in the dataset and comment on their shape (uniform or not?)\n# Your code here \n\n# You observations here \nBased on this, we preselected some features  for you which appear to be more \'normal\' than others.\nCreate a new dataset with [\'crim\', \'dis\', \'rm\', \'zn\', \'age\', \'medv\']\n# Your code here\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ncrim\ndis\nrm\nzn\nage\nmedv\n\n\n\n\n0\n0.00632\n4.0900\n6.575\n18.0\n65.2\n24.0\n\n\n1\n0.02731\n4.9671\n6.421\n0.0\n78.9\n21.6\n\n\n2\n0.02729\n4.9671\n7.185\n0.0\n61.1\n34.7\n\n\n3\n0.03237\n6.0622\n6.998\n0.0\n45.8\n33.4\n\n\n4\n0.06905\n6.0622\n7.147\n0.0\n54.2\n36.2\n\n\n\n\nCheck the linearity assumption for all chosen features with target variable using scatter plots\n# Your code here \n\n\n\n\n\n# Your observations here \nClearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we\'ll use the dataset untouched and just move on with the regression. The assumptions are not exactly all fulfilled, but they still hold to a level that we can move on.\nLet\'s do Regression\nNow, let\'s perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You\'ll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n\nRun a simple OLS regression between independent and dependent variables\nPlot a regression line on the scatter plots\nPlot the residuals using sm.graphics.plot_regress_exog()\nPlot a Q-Q plot for regression residuals normality test\nStore following values in array for each iteration:\n\nIndependent Variable\nr_squared\'\nintercept\'\n\'slope\'\n\'p-value\'\n\'normality (JB)\'\n\n\nComment on each output\n\n# Your code here\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~crim\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~dis\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~rm\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~zn\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\nBoston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~age\n-------------------------------------------------------------------------------------\n\n\n\n\nPress Enter to continue...\n\npd.DataFrame(results)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nind_var\nr_squared\nintercept\nslope\np-value\nnormality (JB)\n\n\n1\ncrim\n0.15078\n24.0331\n-0.41519\n1.17399e-19\n295.404\n\n\n2\ndis\n0.0624644\n18.3901\n1.09161\n1.20661e-08\n305.104\n\n\n3\nrm\n0.483525\n-34.6706\n9.10211\n2.48723e-74\n612.449\n\n\n4\nzn\n0.129921\n20.9176\n0.14214\n5.71358e-17\n262.387\n\n\n5\nage\n0.142095\n30.9787\n-0.123163\n1.56998e-18\n456.983\n\n\n\n\n#Your observations here \nClearly, the results are not very reliable. The best R-Squared is witnessed with rm, so in this analysis, this is our best predictor.\nHow can you improve these results?\n\nPreprocessing\n\nThis is where the preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data.\n\nAdvanced Analytical Methods\n\nSimple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you\'ll explore multiple regression where you can use multiple features at once to define a relationship with the outcome. You\'ll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit.\nLevel up - Optional\nApply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit.\nSummary\nIn this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in the results. You\'ll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'HTML', 'Updated Dec 9, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 16, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': [""kaggle-house-price-prediction-regression-in-top28-leaderboard\n##House Price Prediction\nThis is a very interesting competition provided by Kaggle. I believe that all the entry level Machine Learning enthusiasts should definitely get their hands-on to this competition as well as the Titanic Survivor Prediction competition . One can also check my notebook on the same.\nClick to see my kaggle solution notebook\nComing back to this competition, as it says that the problem is predicting house price i.e. a continuous value. Hence it is a regression problem and thus all your basic regression knowledge will be tested here.\nNow, from a beginner's perspective how do you start with this? Well, I will show very easy steps with which you can easily jump from top 95% on leaderboard to top 28%. So, let's get started.\n\nLet's read about the data that is given. So, in the train.csv there are 1460 rows and 81 columns whereas in test.csv there are 1459 and 80 columns.\nAfter reading the file, we start data preprocessing and feature engineering. Before moving ahead we are combining both the dataframes so that changes can be done in both the dfs together.\nSo, with the help of heatmap we can check number of nulls in each feature. So, based on the column name and after reading the description in the file data_description.txt of that column, we can decide if we want to replace nulls by mean(), mode(), median() or 'NA' or something else.\nNow, we have to replace all the object/string values to numerical type using one hot encoding. While doing this many new columns will be created. So, we also run a code to drop all duplicate columns if any\nNow I am separating the train and test data from all_data and appending SalePrice column to the train data and the training models starts from here.\nWhile training the model, we are using Kfold cross validation for the better root mean squared log error as it is the evaluation criteria on the leaderboard.\nTraining with Simple linear regression model : r2_score that we got was a negative number thus we were getting error with rmsle as it was not able to handle negative number\nTraining with Lasso regression : r2_score was a positive number but rmsle was throwing error because of negative values in the data\nTraining with ridge model: rmsle, r2_score = 0.1562332068843027, 0.8350220277268043 respectively\nTraining with Elastic Net: rmsle, r2_score = 0.15524191588871572, 0.8355319656725282 respectively\nTraining with SVM algorithm: rmsle, r2_score = 0.42655965911720095, -0.21383191346231967 respectively\nTraining with Decision Tree: rmsle, r2_score = 0.20087638073362202, 0.7569636210379455 respectively\nTraining with Random Forest Regressor model: rmsle, r2_score = 0.17295960323425913, 0.8220478028104417 respectively\nTraining with Adaboost regressor model: rmsle, r2_score = 0.20880462846912984, 0.7886447451714611 respectively\nTraining with Bagging Classifier model: rmsle, r2_score = 0.14719806924931875, 0.8539938038399392 respectively\nTraining with XGboost model: rmsle, r2_score = 0.13372606548796895, 0.8742884283414025 respectively\nTraining with Gradient Boosting Regressor model: rmsle, r2_score = 0.14187482285610134, 0.8865067659268699 respectively\nNow you can do a submission and check your score on the leaderboard.\nAfter trying each model, try using the same one with hyper parameter tuning and you will be able to small improvement in rmsle and r2_score as well as your position on the leaderboard\nAt the end we have check the feature importance count and removed all the features with importance count less than 0.\n\nI hope this helps. Please comment below if you haven't understood anything from the above steps.\n""], 'url_profile': 'https://github.com/darshan-jain-29', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'HTML', 'Updated Dec 9, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Advanced-Regression-Techniques-based-on-Random-Forest-gbdt\n尝试了kaggle的house price prediction项目，用随机森林和梯度下降撸了个模型。\n'], 'url_profile': 'https://github.com/Chelseayyn', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'HTML', 'Updated Dec 9, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 16, 2020']}","{'location': 'Ranchi', 'stats_list': [], 'contributions': '150 contributions\n        in the last year', 'description': ['Machine-Learning-Practice-Corona-Vacation\nI am practising the concepts and knowldege I aquired in previous one year. Hence, I am doing converting my vacations productive ,which was declared because of COVID-19 outbreak.\n'], 'url_profile': 'https://github.com/dubeyakshat07', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'HTML', 'Updated Dec 9, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Select-Best-Parameter-Practice-for-Machine-Learning\nPractice of using cross-validation method to select the best low-rank approximation/ridge-regression parameter and predicting on new samples using the best parameter\n'], 'url_profile': 'https://github.com/Crabback', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'HTML', 'Updated Dec 9, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 16, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': [""Applying Machine Learning to Optimize Airbnb Listings\nBusiness Case: Can we apply machine learning to optimize our listing for Airbnb across the 4P's of marketing? Namely:\n\nPlace - What would be the best place to have our listing?\nPrice - What should be the optimal price to set?\nProduct - What features are important to have at a property?\nPromotion - How do we advertise our listing?\n\nFile run order:\n\nCleaning.ipynb\nNLP.ipynb\nModelling.ipynb\nAnalysis.ipynb (for Tableau output)\n\nI used linear regression to predict the base price for a given listing, Tableau visualizations to determine the business case for purchasing a property at a given location, and natural language processing to generate text once the optimal price, location and product has been determined.\n""], 'url_profile': 'https://github.com/xtzie', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'HTML', 'Updated Dec 9, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'HTML', 'Updated Dec 9, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 16, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '108 contributions\n        in the last year', 'description': ['optimizers-from-scratch\nWrite a range of Gradient Descent optimizers from scratch and compare them on a simple Convex optimization problem (Linear Regression) and show their essential properties\n'], 'url_profile': 'https://github.com/saykartik', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'HTML', 'Updated Dec 9, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 16, 2020']}","{'location': 'Mumbai, Maharashtra', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': [""Iris-Model-GCP-Deployment\nCreated a simple Iris SkLearn Logisitic Regression Model, but code is uniform with GCP trained model deployment best practices.\nGCP Deployment best practices essentially include creating clean, understandable code, that can be easily managed and handled by Google Cloud when it comes to Custom Prediction Pipelines.\nWhile Scikit-Learn is a supported package on Google's AI-Platform prediction instances (2.1 and 1.15, and even before), I wanted to try a Custom Prediction Pipeline because I code mostly with Pytorch and not Tensorflow, and will mostly be deploying Pytorch models on the cloud.\ncreateModel.py, preprocess.py are both files used to create the trained Logistic Regression model and preprocess the data in that order.\nThe really interesting file is custompredict.py, which is the file that AI-Platform will run in order to get custom online predictions from the deployed model on the cloud. It includes a .predict() method that is invoked when predictions are to be made, before which a classmethod .from_path() method is invoked to load the saved .pkl model (and any preprocessing .pkl models, not included here).\nsetup.py is a script that uses the setuptools package to create a tar-ball package of all the scripts that AIP requires to get custom online predictions. dist is the folder that contains this tar-ball package.\ninstances.json, inst2.json are JSON files that contain testing data in the exact format that is required by AIP to input into the model for predictions. AIP expects the data to be in a JSON format, which is then de-serialized automaticaly in the cloud, run through the model, re-serialized and sent back.\n""], 'url_profile': 'https://github.com/rohanath123', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Python', 'Updated Mar 19, 2020', '1', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Apr 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 21, 2020', 'HTML', 'Updated Dec 9, 2020', 'R', 'Updated Mar 19, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Heart Disease Classification\nOverview\nShowcase different feature selection methods for a binomial classification problem using both logistic regression and forward learning tree-based models in Python.\nFeature Selection Methods\n\nBackward Elimination\nANOVA\nRecursive Feature Elimination\nSelect From Model\n\nClassification Models\n\nLogistic Regression\nGradient Boosted Machines (h2o)\n\nConclusion\nEach of the feature selection methods resulted in similar outcomes for selected predictor variables. These methods should be used as a starting point, but should ultimately be validated during simulation testing.\nSince the Heart Disease example from Kaggle contains such as small sample set (300 records), more complex methods such as GBMs do not have the same advantage as they typically do with larger data sets. Even when applying cross-validation with a hyper parameter grid search, the logistic regression and GBM models performed about the same with respect to precision, recall, and F1 score.\n'], 'url_profile': 'https://github.com/cjayrans', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Mar 21, 2020', 'R', 'Updated Jan 29, 2021', 'Perl', 'GPL-3.0 license', 'Updated Mar 23, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '655 contributions\n        in the last year', 'description': ['machine-learning-classes\nA short project where I created 2 classes which combine many of the scikit-learn tools for both regression and classification tasks in order to simplify the process of creating and saving model results. In addition, I have integrated many visualisation and measurement functions within the class.\nClassification Class\nThis class is for classification problems. The below scikit-learn models are included:\n\nK Nearest Neighbors\nLogistic regression\nDecision Trees\nRandom Forest\nADA boosting\nGradient Boosting\nNaive Bayes\nLinear Support Vectors\nPolynomial Support Vectors\nGaussian Support Vectors\nNeural Networks - this is the only one which doesn\'t run automatically when you select run_all=True. You need to run this seperately.\n\nInstructions for classification class:\n\nAll you really need is a clean dataset (without NaNs) split into your X (predictors) and y (outcome variable). From here you can assign your class to an object as below:\ntest_model = full_classification(X, y, baseline=0.606, shuffle=True, stratify=y, save_it=True)\nAs you can see here, some additional settings have been specified. For a list of the settings - see the docstring below. Note that by default the class will print out everything that it does, you need to specifically turn this setting off with print_info = False.\nOnce you have run - only the last model will be saved to the object, in order to reassign a specific model to the object you can re-run any one model using the functions below.\ntest_model.knn_model()\ntest_model.decision_tree_model()\ntest_model.logistic_model()\ntest_model.random_forest_model()\ntest_model.ADAboosting_model()\ntest_model.GradientBoosting()\ntest_model.NaiveBayes()\ntest_model.LinearSVC()\ntest_model.PolynomialSVC()\ntest_model.GaussianSVC()\ntest_model.MLP_Neural_Net()\nYou will be able to see all of the settings that you are able to amend when you shift-tab on any of these, it is the standard scikit learn paramters.\nThe additional functions which have been included are:\n\ntest_model.knn_all_k() - this allows you to run a KNN model for a number of different Ks and will plot the accuracy curve over all of the Ks. the default is set to stop at K=50, but this can be changed in the settings.\ntest_model.decision_tree_model(print_tree=True, print_depth=3) - this allows you to not only run the decision tree, but also to graph out the tree. The default is to print to depth of 5.\ntest_model.NaiveBayes(power_transform=True) - it gives you the option to power transform the X data, Naive Bayes model works better with normally distributed data, and so this can help improve the model.\ntest_model.coefs() - only works on models which have coefs to show (logistic), will print dataframe of the coefs.\ntest_model.gridsearch() - automatically runs a gridsearch on your current selected model. Returns model_grid model with best parameters. Has default parameters for each model type, but you can set your own by passing a dict into params = {}. When you print out the results before running gridsearch it will give you a good estimate of how long the gridsearch will take in minutes.\ntest_model.matrix_n_graphs() - will print out a pretty confusion matrix and if possible will print out a ROC curve and a precision recall curve for you to further be able to analyse your model.\n\n\nThe class includes an option to save every model which is run into a dataframe. This will be assigned to a global variable called model_tracker so that it can then be used outside of the dataframe. This is very useful in keeping track of all of your models, and keeps track of how long they take to run and all of the scores.\nThere is a demo file in the repo which shows what the class looks like in action on a spam dataset.\nPlease feel free to reach out to me with any comments and ideas for improvements!\n\nDocstring for the Classification class:\n""""""A class which automatically does all classification models and gridsearches for you (logisitic default).\nNote: when you run a new model it will overwrite the previous model. You can access the current model with .model and .model_des.\nOther options:\n\nrun_all = default True, if set to false the class will not automatically run any models\nbaseline = default 0, set this to your baseline accuracy measure for comparision stats\nstandardize = default True, uses standard scaler and fit-transforms on train, transform on test if exists\ntest_size = default 0.15, decide the side of your test set - set to 0 if dont want test\nfolds = default 6, amount of folds for cross validation - integer > 1\nshuffle = default True, shuffle the data for test split and cross val\nstratify = default None, input the variable that you which to stratify the data by\nprint_info = default True, print all of the results out every time you run a model\nsave_it = default False, this adds functionality to be able to save down all model results into a dataframe, set as a global variable called model_tracker.\ncomment = default None, This is a comment field for the model_tracker\nGo to readme for further information: https://github.com/LukeBetham/machine-learning-classes/blob/master/README.md\nCreated by LukeBetham""""""\n\nRegression Class (work in progress)\nThis class focuses on linear regression and is to be used for predicting continuous variables. It is still a work in progress and will be updated soon!\n'], 'url_profile': 'https://github.com/LukeBetham', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Mar 21, 2020', 'R', 'Updated Jan 29, 2021', 'Perl', 'GPL-3.0 license', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bfann60', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Mar 21, 2020', 'R', 'Updated Jan 29, 2021', 'Perl', 'GPL-3.0 license', 'Updated Mar 23, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/boyasun', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Mar 21, 2020', 'R', 'Updated Jan 29, 2021', 'Perl', 'GPL-3.0 license', 'Updated Mar 23, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['House-Prices\nDetailed data analysis and exploration of a house prices data set from Kaggle. Followed by application of a linear regression model.\n'], 'url_profile': 'https://github.com/nissiea', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Mar 21, 2020', 'R', 'Updated Jan 29, 2021', 'Perl', 'GPL-3.0 license', 'Updated Mar 23, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '203 contributions\n        in the last year', 'description': [""NBA-2pt-vs-3pt\nNBA Three Points Evolution Trend analysis based on multiple factors (independent variables) with linear-regression model using Python (Pandas and Matplotlib).\nWe wanted to look at the evolutionary trend within the NBA, specifically how the use of the 3pt. has changed since it's been implemented. Players used to attempt to score closer proximity in order to ensure higher accuracy, but it’s been confirmed by sports analysts that players are attempting and making more 3pt shots throughout the years and the league has been in talks of creating a 4pt perimeter on the basketball court.\nOur Questions\n\nHow has the league overall attempts and percentages changed throughout the decades?\nWhat is the relationship between the 3pt. and a championship caliber team?\nHow have player roles changed through the years due to the 3pt?\n\nThree Point Change in Decades.\nWe used the NBA final data from 1980 to 2018, showcasing 4 decades. Using both the NBA final winner and the runner up we can see with the graphs below that there as significant increases in 3pt attempts and percentages.\n\n\nThe below linear regression model displays the year by year 3pt attempt. Here we can see that there is a steady increase in usage during the Finals. We can conclude here that the 3pt. has become more popular, but is there any relationship between the 3pt and a championship?\n\nThe graph below displays 3pt made throughout the years for the Finals along with their Percentages. Blue bars representing championship team and orange the runner up team. What we see here is that championship teams have both higher percentages and made shots. We were able to conclude here that the 3pt is one important factor that determines the championship however, there are a few years such as 1997, 2011 and 2016 where the runner up team had high stats for both, so we can’t say that it’s the sole factor.\n\n\nHow has the skill evolved as seasoned veterans’ game VS. a young player?\nHere we decided to investigate veteran players who came into the league from 2005 – 2019 and younger players who came into the league in 2012. What we see is that the veterans are peaking at over 40% and the younger at 30%. Also, there looks to be a decline within the use of the 2pt, given the rise in 3pt.\n\n\nFurther, the veterans are averaging and attempting more than the younger group. Both graphs display a dip, but looking closer at the data, after 2018 we see that there is a significant decline in the amount of minutes played in the latter part of the veterans year. Veterans losing more than 6 mins on the court by 2019. The younger players only lost about a minute. We concluded that veterans are focusing more attention than the younger players are, and this could be closely related to the longevity of their careers.\n\n\nCenters and Power Forwards – How have these roles changed?\nWe also wanted to look at the roles of certain players – specifically players who in the 80s and 90s would always stay in the paint and never step foot around the perimeter. Below we can see that after 2015 there is a sharp incline with Centers and PF attempts and made shots (over 4000 attempts and a little over half of those are made) – 35% from the 3 which is a good average for an effective shooter.\n\n\nThis is important because the role for the Center and PF has changed and it allows them to be an offense threat elsewhere which helps give coaches a more versatile playbook.\n""], 'url_profile': 'https://github.com/stevenluk', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Mar 21, 2020', 'R', 'Updated Jan 29, 2021', 'Perl', 'GPL-3.0 license', 'Updated Mar 23, 2020']}","{'location': 'Sheffield', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rtscowen', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Mar 21, 2020', 'R', 'Updated Jan 29, 2021', 'Perl', 'GPL-3.0 license', 'Updated Mar 23, 2020']}","{'location': 'Atlanta, USA', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['ML-Click-through-rate-prediction\nClassification Project on a dataset of 32M rows using models such as Decision Trees, Logisitic Regression, Random Forests, Neural Nets and XGBoost\nIntroduction:\nOnline advertising is a form of marketing which delivers promotional messages to consumers via the Internet. Due to the volume of advertisements being published & huge cost associated with marketing campaigns, serving the right advertisement to the right consumer is extremely important. Hence, the ability to predict whether a consumer will click an advertisement on a search engine such as Google will help the firm determine the right advertisement to serve the right consumer. Our project intends to unravel this prediction by creating a classification model, utilizing data of 9 days (Oct 21st - 29th 2014) during which more than 30 million instances of advertisements were served to various consumers.\nModels implemented:\nWe built and implemented models using the following algorithms:\nLogistic regression (With Lasso & Ridge Regularization)\nDecision Trees\nRandom Forest\nXGBoost\nThe Final Model, with a logloss of 0.39 on the test data, was submitted and won 3rd place in a class competition of 11 teams.\nMore on the dataset\nThis project involves predicting clicks for on-line advertisements. The training data consists of data for 9 days from October 21, 2014 to October 29, 2014. The variables in this file are as follows:\n-id = the identifier of the ad (this may, or may not be unique to each row) \n-click = 1 means the ad was clicked on. click = 0 means the ad was not clicked on \n-hour = the date and hour when the ad was displayed. Format is YYMMDDHH. \n-C1 = an anonymized categorical variable. \n-banner_pos = the position in the banner. \n-site_id = an identifier for the web site. \n-site_domain = an identifier for the site domain \n-site_category = a code for the site’s category. \n-app_id = an identifier for the application showing the ad. \n-app_domain = an identifier for the app’s domain. \n-app_category = a code for the category of the app. \n-device_id = an identifier for the device used. \n-device_ip = a code for the ip of the device. \n-device_model = the model of the device. \n-device_type = the type of the device. \n-device_conn_type = the type of the device’s connection \n-C14 – C21 = anonymized categorical variables \nThus, there are 24 columns in the dataset. The variable “click” is the Y-variable in the dataset.\n'], 'url_profile': 'https://github.com/Harshit-Arora', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Mar 21, 2020', 'R', 'Updated Jan 29, 2021', 'Perl', 'GPL-3.0 license', 'Updated Mar 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Mar 21, 2020', 'R', 'Updated Jan 29, 2021', 'Perl', 'GPL-3.0 license', 'Updated Mar 23, 2020']}","{'location': 'Piauí, Brasil', 'stats_list': [], 'contributions': '307 contributions\n        in the last year', 'description': ['LSM\nA simple statistics module, which provides 4 basic types of regressions using the Least Squares Method (LSM)\nThe file lsm_test.pl provides an unified interface for their use as a command line tool.\nNOTE: This module is still in the early stages of development, so efficiency and accuracy are not guaranteed.\n'], 'url_profile': 'https://github.com/LvMalware', 'info_list': ['Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', 'R', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 28, 2020', 'Jupyter Notebook', 'Updated Jun 3, 2020', 'R', 'Updated Mar 21, 2020', 'R', 'Updated Jan 29, 2021', 'Perl', 'GPL-3.0 license', 'Updated Mar 23, 2020']}"
"{'location': 'Hyderabad', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Navi433', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Predicting-House-Prices-on-Boston-Dataset-using-LinearRegression-using-SciKitLearn\nThis is a solution of training a simple ML model for predicting new house prices on Boston Data set using SciKit Learn\n'], 'url_profile': 'https://github.com/IshitPatel', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows × 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows × 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': [""COVID-19 Data Analysis\nIntro\nThis repository contains simple python scripts used to fit gompertz curve to cumulative confirmed Coronavirus cases.\nData\nData is fetched daily from Johns Hopkins University Center for Systems Science and Engineering,\nignoring data from China since their curve is skewed by a whole month, making it more difficult to fit curve for the rest of the world.\nResults\nCurrent data looks like this:\n\nNote: As more daily data arrives, the better estimation will be, since this is ongoing pandemic.\nThat's why there is second graph, showing history of these estimations:\n\nThis plot should converge to specific value, marking the end of the pandemic.\nAs of 3. May 2020. estimated end of pandemic is 3. July 2020.\n""], 'url_profile': 'https://github.com/TodorovicIgor', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['RedWineQuality\nPredict the quality of red wines using linear models.\nDataset: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009\n'], 'url_profile': 'https://github.com/simonefinelli', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '465 contributions\n        in the last year', 'description': [""MyPytorchWork\n\nThis repository contains implementation of the basic algorithms: Linear Regression, Customized ANN, Gradient Descent, CNN for classification, NLP, RNN, and Time Series Analysis.\nwith some of the well-known datasets found on the web.\nThe 'path' should be corrected according to directory of the desginated dataset folder.\nSome of the pretrained weights are provided.\n\n***Dataset Folder can be acquired from  here  ***\nRequirements\n\ntorch\nnumpy\npandas\nmatplotlib\nseaborn\nsklearn\nos\nPIL\n\n""], 'url_profile': 'https://github.com/TaoseefIshtiak', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Airline-Tweet-Sentiment-Analysis\nA ML model created using Voting Classifier taking Logistic Regression and Random Forest as samples for votes. Purpose of this model is to predict the sentiment of a tweet made against any US airline .\nThe model achieved high metric for both precision and recall for predicting the negative tweets while the same for positive tweets was moderate.\n'], 'url_profile': 'https://github.com/binayak042000', 'info_list': ['Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Python', 'Updated May 3, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Apr 2, 2020', '1', 'Jupyter Notebook', 'Updated Jun 24, 2020', '1', 'Jupyter Notebook', 'Updated Mar 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rahul01101991', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'C++', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 22, 2020']}","{'location': 'Helsinki, Finland', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MillerMatias', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'C++', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 22, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '254 contributions\n        in the last year', 'description': ['DTs\nDecision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n'], 'url_profile': 'https://github.com/tvelichkovt', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'C++', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['Machine_learning_supervised_learning\nTASK :\nTo classify suspected FNA cells to Benign (class 0) or Malignant (class 1) using logistic regression as the classifier. The dataset in use is the Wisconsin Diagnostic Breast Cancer (wdbc.dataset).\nPlan of work :\n\nExtract features values and Image Ids from the data: Process the original CSV data files\ninto a Numpy matrix or Pandas Dataframe.\nData Partitioning: Partition your data into training, validation and testing data. Randomly\nchoose 80% of the data for training and the rest for validation and testing.\nTrain using Logistic Regression: Use Gradient Descent for logistic regression to train the\nmodel using a group of hyperparameters.\nTune hyper-parameters: Validate the regression performance of your model on the validation\nset. Change your hyper-parameters. Try to find what values those hyper-parameters should take\nso as to give better performance on the validation set.\nTest your machine learning scheme on the testing set: After finishing all the above\nsteps, fix your hyper-parameters and model parameter and test your models performance on the\ntesting set.\n\nEVALUATION :\nPrecision, Recall, Accuracy\n'], 'url_profile': 'https://github.com/aashnamahajan', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'C++', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 22, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '120 contributions\n        in the last year', 'description': ['salary-prdiction\nLinear Regression model to learn the correlation between the number of years of experience of each employee and their respective salary. Once the model is trained, we will be able to do some sample predictions.\nA linear function has one independent variable and one dependent variable. The independent variable is x and the dependent variable is y.\n'], 'url_profile': 'https://github.com/GazalaSayyad', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'C++', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 22, 2020']}","{'location': 'Washington, D.C.', 'stats_list': [], 'contributions': '621 contributions\n        in the last year', 'description': ['\n\nProject File Summary\n\nREADME.md - A summary of all contents in this repository.\nData - All data called from the Spoonacular API saved out as .csv files.\nImages - Exported plots + additional graphics.\nLinear_Regression_Trial - Linear regression working code (not used for the final business recommendation).\nJupyter_Notebooks - Logistic regression code in Jupyter Notebooks.\nProject_Prompt - The prompt for this project.\nPython_Files - .py files loaded / imported in the Jupyter Notebooks.\n\n\nProject Members\n\nAlex Cheng\nNimu Sidhu\n\n\nProject Responsibilities\n\nAll project responsibilities were shared equally between Alex Cheng and Nimu Sidhu.\n\n\nProject Scenario\nTotal U.S. spending on food advertising was $151 billion dollars in 2018. This was a 4.1% increase from 2017. According to the New York Times, a person living in a city today sees over 5,000 ads per day, so how can we target successful ad placement in a world where food related ads are everywhere? As a business case study to address this, we work as an ads strategy consultant to businesses selling products and services related to the food industry, such as: Williams-Sonoma, KitchenAid, Blue Apron, and Hello Fresh. They are targeting Spoonacular.com to run ads. Spoonacular is a popular recipe aggregating website and app for users to look up recipes by ingredient, by nutritional content, by price, and more. The Spoonacular API includes over 360,000 recipes as well as an open source recipe database. Our clients only want to spend money to run ads on webpages that they know people will visit a lot. In this case, without knowing how many people visited each recipe page on Spoonacular, we will use “Likes” as our proxy metric for web-traffic.\n\nProject Goals\n\nWe want to predict if a recipe will be  ""liked"" a lot, to understand where to run ads.\nWe want to build a classification model that will predict whether a recipe will be ""highly liked"" or not.\nThis way, we will be able to determine where to run ads, when a new recipe is posted on Spoonacular.\n\nThis is an example of a recipe webpage on Spoonacular where we WOULD want to run ads. The recipe is very highly liked by Spoonacular users, which means that people are probably visiting this recipe page very often. Therefore, it is likely that our ad would be seen by lots of people!\n\nThis is an example of a recipe webpage on Spoonacular where we WOULD NOT want to run ads. This recipe only has 1 ""Like"". (Probably the person who made the recipe, honestly.) This means that people are probably not visiting this recipe page very often, so an ad on this page would not get seen very much.\n\n\nData\nWe used the Spoonacular API to collect a dataset of 1,000 unique recipes that have been aggregated on Spoonacular\'s website. Each recipe contained extensive information from nutritional content (calories, fat, vitamins, minerals, etc...) to dietary categories (paleo, vegan, whole30, etc...). A ""Likes"" count was also provided for each recipe. In lieu of webpage traffic, which would be ideal to predict users visiting a particular recipe\'s page, we decided to use ""Likes"" as our proxy target variable. From recipe information, we developed and/or utilized a total of 35 numerical predictors (indepedent variables) of ""Likes"".\n\nData Collection Process\nThe Spoonacular API enforces a pagination limit of 100 recipes per request, so we made 10 requests to pull our 1,000 unique recipes. We pulled recipes sorted by popularity in descending order to ensure that we would get recipes with high ""Like"" counts. After all, we want to be able to predict recipes with a high ""Like"" count to invest in website ads. However upon exploring, we discovered that our dataset contained some recipes with low like counts as well, so we ended up having a decent distribution of recipes with both high and low like counts.\nThe data collection process was completed in 2 steps. First, we retrieved the 1,000 most popular recipes - which included only general info about each recipe (id, title, image, etc...). Second, we harvested recipe ""ids"", and made a second round of API requests using ""ids"" to retrieve detailed information about each recipe. Finally, all of this information was merged together into one DataFrame using recipe ""id"" and was exported to a .CSV file for safe keeping.\n\nData Cleaning\nIn order to extract as many features as possible from the data, the Spoonacular JSON response needed to be keyed into a bit deeper to extract nutrition data and parse them into separate columns in our DataFrame. We dropped several columns that did not seem to be useful, such as the recipe source name and URL. There were also several columns where numerical data came in as the ""string"" datatype, and needed to be converted to the ""integer"" datatype in order to run aggregations.\n\nExploratory Data Analysis\nA Tableau dashboard was created to get a comprehensive understanding of the categorical and numerical data, all in one place. A static preview of this dashboard is displayed below. The fully interactive Tableau dashboard can be explored here: https://public.tableau.com/profile/alexander.cheng#!/vizhome/RecipeWebpageAds/RecipeWebpageAds\n\nSurprisingly, ""Price"" of a recipe does not seem to have a strong correlation with ""Likes"". The distribution seems to be scattered and not very clear. One might initially think that a higher cost of ingredients would decrease the popularity of a recipe.\n\nRecipe ""Calorie"" content does not seem to have a strong correlation with ""Likes"" either. One might presume that if a recipe is high-calorie, then it might be a bit of a turn off to health-conscious people - therefore being less popular. But on the flip-side, people also really like high-calorie foods. Perhaps high-calorie comfort foods like cakes and cookies are popular simply for sheer taste and pleasure, and people don\'t really care about calories to like a particular recipe.\n\nThere seemed to be some multicollinearity among the predicting variables. A collinearity heatmap helps to visualize the strength of correlation between all predicting variables.\n\nIn the histogram of ""Likes"" we can see that our target variable seems to have an exponential distribution.\n\nThe cumulative distribution function (CDF) plot of ""Likes"" in our dataset shows a logarithmic relationship between ""Likes"" and cumulative probability. This means that recipes get rarer and rarer to find as their number of ""Likes"" increases.\n\n\nData Preprocessing\nSince we had lots of numerical data that is measured on different scales, it was important to standardize this data using StandardScalar. This standardization allows the model to train on a ""level playing field"", so that no one variable\'s numerical scale has an unfair advantage or disadvantage compared to the others as the model adjusts weights during training. We also used Scikit-learn to train/test split our data using a 75/25 split, which allowed us to train our model sufficiently before making predictions.\n\nLogistic Regression Model\nWe used the median number of ""Likes"" in our dataset as a cutoff between recipes with high ""Likes"" and recipes with low ""Likes"". The median number of ""Likes"" was approximately 1,300. This helped ensure that our model trained on an even amount (50/50) of well-liked and less-liked recipes.\nIn developing this criteria, we assumed that our target variable (Likes) is binary. Of course, this target variable could be broken up into more categories than two. But for this project, binary classification gave us a clear yes/no on whether or not to invest in ads for a recipe\'s webpage.\nTo begin the modeling process, first we generated a vanilla logistic regression model using Statsmodels based on our binary classification criteria, and fit it to the training data using all 35 predicting features. We then calculated the Variance Inflation Factor (VIF) of all 35 predictors to see if there were any indications of multicollinearity. We eliminated all variables with a VIF greater than 8. (Note: in published literature, VIF cutoff ranges as low as 4 and as high as 10.) Next, using an initial alpha (confidence level) cutoff of 0.3, we eliminated more of these predictors. From here, VIF scores of all remaining features were less than 3, so VIF was no longer used to test for multicollinearity.\nTo continue reducing predictors, we created another logistic regression model and fit our data using our reduced list of features. Using an even lower alpha cutoff value of 0.1, we performed 2 more rounds of variable reduction and found 6 statistically significant predictors, including ""number of ingredients"", ""time to make"", ""saturated fat"", ""sodium"", ""vitamin K"", and ""fiber"".\nTo compare with our ""vanilla"" logistic regression model made with Statsmodels, we created an another logistic regression model using Scikit-learn and implemented hyperparameter tuning using RandomizedSearchCV. We selected the best model from the tuning process, tested this ""tuned"" model on the test data, and evaluated its accuracy.\n\nModel Results\nTuned Model - TRAIN DATA:\n\nAccuracy = 0.636\n\n3-Fold Cross-Validation, to ensure our model performance is not due to random chance:\n\nCV 1 - Accuracy = 0.62\nCV 2 - Accuracy = 0.62\nCV 3 - Accuracy = 0.648\nThis shows that our variance among these cross-validations was low.\nThis closely matches the mean accuracy (0.636) of the model.\nSo our model accuracy was probably not due to random chance.\n\nTuned Model - TEST DATA:\n\nAccuracy = 0.664\nThis accuracy of predicting test data is very close to that of the training data.\nThis is good! We did not overfit to the training data.\n\nTo graphically evaluate model performance on test data, we plotted the ""Receiver Operating Characteristic"" (ROC) Curve and calculated the ""Area Under the Curve"" (AUC).\n\nTrain AUC: 0.688\nTest AUC: 0.665\n\n\nTo deepen our understanding on model performance, we also plotted confusion matrices for model predictions on train data, and model predictions on test data.\nConfusion Matrix Results - TRAIN DATA\n\n\nOut of 750 observations:\n\n226 recipes were predicted as a ""winner"" when it really was a ""winner"". (True Positives)\n124 recipes were predicted as a ""winner"" when it really was a ""loser"". (False Positives)\n251 recipes were predicted as a ""loser"" when it really was a ""loser"". (True Negatives)\n149 recipes were predicted as a ""loser"" when it really was a ""winner"". (False Negatives)\n\n\n\nConfusion Matrix Results - TEST DATA\n\n\nOut of 250 observations:\n\n77 recipes were predicted as a ""winner"" when it really was a ""winner"". (True Positives)\n36 recipes were predicted as a ""winner"" when it really was a ""loser"". (False Positives)\n89 recipes were predicted as a ""loser"" when it really was a ""loser"". (True Negatives)\n48 recipes were predicted as a ""loser"" when it really was a ""winner"". (False Negatives)\n\n\n\n\nConclusions\n\nWe want to maximize the potential to predict correctly and make money.\nWe want to minimize the potential to predict wrong and waste money.\nSo, the “precision” of our model is the most important metric to consider.\n\nPrecision = Number Of Good Investments / Total Number Of Investments\nPrecision = True Positives / (True Positives + False Positives)\nPrecision = 77 / (77 + 36) = 68.1%\n\n\n\nRECOMMENDATION: With a precision of 68.1% on our test data, for every 3 ads invested, 2 ads will be a good investment and 1 ad may be a loss. With these odds, we recommend that our clients follow the model to invest in ads on Spoonacular!\n\nFuture Work\n\nIf possible, it would be better to obtain web-traffic metrics instead of using “Likes"" as a proxy.\nIt would help to pull more data (a greater number of recipes) from the Spoonacular database.\nNatural Language Processing may be useful to cluster text such as ingredient types - such as to predict the most visited recipe webpages with respect to the use of ""avocado"".\n\n'], 'url_profile': 'https://github.com/alexwcheng', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'C++', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 22, 2020']}","{'location': 'BELGAUM ', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['Autonomous_Mobile_Robot\nMade a autonomous mobile robot (a three wheeler car) which would avoid the obstacle by taking left or right\nw.r.t to the ultrasonic sensor relative distance correspondingly. This was done using Linear sepatrix method of regression model (Machine Learning).\nThis can be implemented using both Arduino (NodeMcu) as well as Rasberry Pi.\nAll the available codes are attached to the projected along with implementation done using Rasberry Pi\n'], 'url_profile': 'https://github.com/sourabhkulkarni-007', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'C++', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Ecommerce_company_prediction\nAn ecommerce company data is given in the file called Ecommerce Customer.csv. We are using Linear Regression Algorithm to predict, whether the company should focus more on mobile app or their online website.\n'], 'url_profile': 'https://github.com/tyajush', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'C++', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Materiais de estudos sobre Machine Learning\n . \nO aprendizado de máquina (machine learning) é um ramo da ciência da computação que oferece aos computadores a capacidade de fazer previsões sem serem explicitamente programados.\nTabela de Conteúdos\n\nPor Onde Começar\nÁreas de Aplicações de Machine Learning\nArtigos Científicos\nBases de Dados\nConferências\nCompetições\nCursos\nFrameworks\nJournals\nLivros\nMatemática para Machine Learning\nPesquisadores\nPodcasts\nPosts\nSlides\nSurveys\nVídeos\n\nAgradecimentos\nAgradecimentos especiais a todos que contribuíram para este projeto.\n\n\n\nNome\nBiografia\n\n\n\n\nLeanderson André\nProfessor@UNIVILLE/Doutorando@UDESC\n\n\n\n\n\n\n\n\n\n\n\nContatos e Comentários\nSe você tiver alguma sugestão (artigos ausentes, novos artigos, erros de digitação, etc), sinta-se à vontade para fazer uma nova issue.\nVocê também pode enviar um email para:\n\nLeanderson André leandersonandre@univille.br\n\n'], 'url_profile': 'https://github.com/univille-machine-learning', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'C++', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 22, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '389 contributions\n        in the last year', 'description': ['Machine-Learning-Coursera\nThis repository contains my submissions of all the assignments for a certified course on Coursera\n\nWeek-2 : Linear Regression\nWeek-3 : Logistic Regression\nWeek-4 : Multi class classification and Neural Networks\nWeek-5 : Neural Networks\nWeek-6 : Regularized linear regression and bias variance\nWeek-7 : Support Vector Machin\nWeek-8 : K-means Clustering and PCA\nWeek-9 : Anomaly Detection and Recommender System\n\n'], 'url_profile': 'https://github.com/KhushJain', 'info_list': ['Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020', 'Python', 'Updated Apr 3, 2020', 'Python', 'Updated Mar 17, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'C++', 'Updated Jul 21, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', '1', 'Updated Mar 22, 2020', 'MATLAB', 'Updated Mar 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': [""Pattern Recognition\nIn this repository I've implemented some algorithms of Statistical Pattern Recognition (SPR) from scratch.\nThe mentioned algorithms can be found in the table below that divided into 5 categories:\n\nRegression\nClassification\nClustering\nDecomposition\nmetrics\n\nInstallation\n# create a virtualenv\n$ virtualenv -p python3 .vnev\n\n# switch environment to virtualenv\n$ source .venv/bin/active\n\n# install requirements\n$ pip install -r requirements.txt\nImplemented Algorithms\n\n\n\nRegression\nClassification\nClustering\nDecomposition\nmetrics\n\n\n\n\nLinear\nBayesian\nK-Means\nPCA\nMSE\n\n\nLogistic\nKNN\n\n\nAccuracy\n\n\nSoftmax\nSVM\n\n\nTTS\n\n\n\n\n\n\nK-Fold\n\n\n\n""], 'url_profile': 'https://github.com/AlirezaKm', 'info_list': ['1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'JavaScript', 'Updated Jan 10, 2021', 'R', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Updated Mar 30, 2020', 'R', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Scala', 'Updated Mar 22, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 27, 2020']}","{'location': 'Pune (India)', 'stats_list': [], 'contributions': '279 contributions\n        in the last year', 'description': [""WILL BE HAVING ATLEAST ONE FILE FOR BELOW METHODS & ALGORITHMS\nIf you didn't find a file for any of the below topics please feel free to email me, so I will upload the same.\nemail: datascience.scout@gmail.com\n\nSUPERVISED LEARNING:\nREGRESSION: Linear  -  Polynomial  -  Ridge/Lasso\nCLASSIFICATION: K-NN  -  Naïve Bayes  -  Decision Tree  -  Logistic Regression  -  Confusion Matrix  -  SVM\nTIME SERIES ANALYSIS: Linear & Logistic Regr.  -  Autoregressive Model  -  ARIMA  -  Naïve  -  Smoothing Technique\n\nUNSUPERVISED LEARNING:\nCLUSTERING: K-Means  -  Agglomerative  -  Mean-Shift  -  Fuzzy C-Mean  -  DBSCAN   -    Hierarchical  -  Canopy\nPATTERN SEARCH: Apriori  -  FP-Growth  - Euclat\nDIMENSION REDUCTION: PCA  -  LSA  -  SVD  -  LDA  -  t-SNE\nRECOMMENDATION ENGINE: Association Rules  -  Market Basket Analysis  -  Apriori Algorithm  -  Real Rating Matrix  -  IBCF - (Item)  -  User-Based Collaborative Filtering UBCF - Method & Model\n\nENSEMBLE METHODS:\nBOOSTING: AdaBoost  -  XG Boost  -  LightGBM  -  CatBoost.\nBAGGING: Random Forest\nSTACKING\n""], 'url_profile': 'https://github.com/yash0422', 'info_list': ['1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'JavaScript', 'Updated Jan 10, 2021', 'R', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Updated Mar 30, 2020', 'R', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Scala', 'Updated Mar 22, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DanielPetelin', 'info_list': ['1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'JavaScript', 'Updated Jan 10, 2021', 'R', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Updated Mar 30, 2020', 'R', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Scala', 'Updated Mar 22, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cran', 'info_list': ['1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'JavaScript', 'Updated Jan 10, 2021', 'R', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Updated Mar 30, 2020', 'R', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Scala', 'Updated Mar 22, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '397 contributions\n        in the last year', 'description': ['python_deconvolution\nJupyter notebook walkthrough of how to blind deconvolve a BOLD timeseries to obtain underlying neuronal signal, in order to build regressors for gPPI analyses\n'], 'url_profile': 'https://github.com/achennings', 'info_list': ['1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'JavaScript', 'Updated Jan 10, 2021', 'R', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Updated Mar 30, 2020', 'R', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Scala', 'Updated Mar 22, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 27, 2020']}","{'location': 'Cincinnati', 'stats_list': [], 'contributions': '89 contributions\n        in the last year', 'description': ['Credit-card-default-prediction\nA detailed study and prediction modeling of Credit Card data sourced from Kaggle. (https://www.kaggle.com/rikdifos/credit-card-approval-prediction)\nPrediction models were generated using Logistic Regression, Classification Tree and Random Forest; best model was selected based on Recall and Area under the ROC curve.\nThis repository contains the final RMD code and the final report. Please, have a look at them.\nThank You!\nRegards,\nHasnat\n'], 'url_profile': 'https://github.com/hst22', 'info_list': ['1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'JavaScript', 'Updated Jan 10, 2021', 'R', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Updated Mar 30, 2020', 'R', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Scala', 'Updated Mar 22, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\nRRRR\n\n\n\n\nThe R package RRRR provides methods for estimating online Robust\nReduced-Rank Regression.\nTo cite package ‘RRRR’ in publications use:\n\nYangzhuoran Fin Yang and Ziping Zhao (2020). RRRR: Online Robust\nReduced-Rank Regression Estimation. R package version 1.0.0.\nhttps://pkg.yangzhuoranyang.com/RRRR/.\n\nInstallation\nYou can install the stable version on R\nCRAN.\ninstall.packages(""RRRR"")\nYou can install the development version from\nGithub with:\n# install.packages(""devtools"")\ndevtools::install_github(""FinYang/RRRR"")\nUsage\nThe R package RRRR provides the following estimation methods.\n\nReduced-Rank Regression using Gaussian MLE: RRR\nRobust Reduced-Rank Regression using Cauchy distribution and\nMajorisation-Minimisation: RRRR\nOnline Robust Reduced-Rank Regression: ORRRR\n\nSMM: Stochastic Majorisation-Minimisation\nSAA: Sample Average Approximation\n\n\nOnline update of the above model (except RRR): update.RRRR\n\nSee the vignette for a more detailed illustration.\nlibrary(RRRR)\nset.seed(2222)\ndata <- RRR_sim()\nres <- ORRRR(y=data$y, x=data$x, z=data$z)\nres\n#> Online Robust Reduced-Rank Regression\n#> ------\n#> Stochastic Majorisation-Minimisation\n#> ------------\n#> Specifications:\n#>            N            P            R            r initial_size        addon \n#>         1000            3            1            1          100           10 \n#> \n#> Coefficients:\n#>            mu        A        B       D    Sigma1    Sigma2    Sigma3\n#> [1,] 0.078343 -0.16766  1.55325 0.20475  0.656940 -0.044872  0.050316\n#> [2,] 0.139471  0.44229  0.91983 1.13833 -0.044872  0.657402 -0.063890\n#> [3,] 0.106746  0.80182 -0.69377 1.95502  0.050316 -0.063890  0.698777\nplot(res)\n\nnewdata <- RRR_sim(A = data$spec$A,\n                   B = data$spec$B,\n                   D = data$spec$D)\nres2 <- update(res, newy=newdata$y, newx=newdata$x, newz=newdata$z)\nres2\n#> Online Robust Reduced-Rank Regression\n#> ------\n#> Stochastic Majorisation-Minimisation\n#> ------------\n#> Specifications:\n#>            N            P            R            r initial_size        addon \n#>         2000            3            1            1         1010           10 \n#> \n#> Coefficients:\n#>            mu        A        B       D    Sigma1    Sigma2    Sigma3\n#> [1,] 0.073939 -0.15981  1.52031 0.20894  0.675436 -0.021789  0.040888\n#> [2,] 0.142791  0.45099  0.96270 1.11702 -0.021789  0.679136 -0.024140\n#> [3,] 0.107647  0.81759 -0.67044 1.95708  0.040888 -0.024140  0.703949\nplot(res2)\n\nLicense\nThis package is free and open source software, licensed under GPL-3.\n'], 'url_profile': 'https://github.com/cran', 'info_list': ['1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'JavaScript', 'Updated Jan 10, 2021', 'R', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Updated Mar 30, 2020', 'R', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Scala', 'Updated Mar 22, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 27, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/boyasun', 'info_list': ['1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'JavaScript', 'Updated Jan 10, 2021', 'R', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Updated Mar 30, 2020', 'R', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Scala', 'Updated Mar 22, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DreadlockDrew', 'info_list': ['1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'JavaScript', 'Updated Jan 10, 2021', 'R', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Updated Mar 30, 2020', 'R', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Scala', 'Updated Mar 22, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 27, 2020']}","{'location': 'Los Angeles, California', 'stats_list': [], 'contributions': '363 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/appielife', 'info_list': ['1', 'Python', 'Updated Mar 18, 2020', '1', 'Jupyter Notebook', 'Updated May 8, 2020', 'JavaScript', 'Updated Jan 10, 2021', 'R', 'Updated Dec 2, 2020', 'Jupyter Notebook', 'Updated Mar 19, 2020', '1', 'Updated Mar 30, 2020', 'R', 'Updated Jul 9, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Scala', 'Updated Mar 22, 2020', 'Python', 'Apache-2.0 license', 'Updated Mar 27, 2020']}"
"{'location': 'bangalore', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['KNN-Classification-ML\nK Nearest Neighbors The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other. Algorithm: The Distance Between two points can be calculated using Euclidean distance -Calculate the distance from x to all points in your data. -Sort the points in your data by increasing distance from x. -Predict the majority label of the k closest points. Note: k value defines accuracy of a model so try with all possible values of k\n'], 'url_profile': 'https://github.com/Sandhyakumari15', 'info_list': ['2', 'Python', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '174 contributions\n        in the last year', 'description': [""MAT-243 Applied Statistics for STEM I\nSource code (Jupyter Notebook) for my Projects in my MAT-243 STATS for STEM I course at SNHU as well as .csv files (if applicable) and Report.\nAll Jupyter notebook code is coded in Python. It says HTML because the visual file is an HTML file for each project. I will write and add a .py file with the code at some point and update the info here to reflect that change.\nUPDATE: Added .py files for the Jupyter Notebook for Project 1, Project 2 and Project 3.\nI found that installing Anaconda (most updated version for python) and running Visual Studio Code from Anaconda, then proceeding to open and run the .py file fixed the error for it not finding the 'numpy' module.\n""], 'url_profile': 'https://github.com/Lunarestia', 'info_list': ['2', 'Python', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/divyanshu4', 'info_list': ['2', 'Python', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Breast-Cancer-Detection\nAbstract—Breast Cancer is one of the most frequently occurring cancers in women and it can occur to men as well. If a patient has a tumor, it could be either malignant (cancerous) or benign. If we can identify at an early stage if the tumor is benign or malignant, the chances of survival are higher. In this project, an attempt to classify the tumors accurately using different machine learning algorithms is made. Different machine learning techniques such as Logistic Regression, K-Nearest Neighbors, Support Vector Machine, Decision Tree, Random Forest, and Naive Bayes are implemented in the project for the classification of benign and malignant tumors.\n'], 'url_profile': 'https://github.com/SayaliG0', 'info_list': ['2', 'Python', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kaushal-kulkarni', 'info_list': ['2', 'Python', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}","{'location': 'chennai', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': [""classifying_songs_based_on_audio_data\nOver the past few years, streaming services with huge catalogs have become the primary means through which most people listen to their favorite music. But at the same time, the sheer amount of music on offer can mean users might be a bit overwhelmed when trying to look for newer music that suits their tastes.  For this reason, streaming services have looked into means of categorizing music to allow for personalized recommendations. One method involves direct analysis of the raw audio information in a given song, scoring the raw data on a variety of metrics. Today, we'll be examining data compiled by a research group known as The Echo Nest. Our goal is to look through this dataset and classify songs as being either 'Hip-Hop' or 'Rock' - all without listening to a single one ourselves. In doing so, we will learn how to clean our data, do some exploratory data visualization, and use feature reduction towards the goal of feeding our data through some simple machine learning algorithms, such as decision trees and logistic regression.\n""], 'url_profile': 'https://github.com/jayesh265', 'info_list': ['2', 'Python', 'Updated Apr 1, 2020', '1', 'HTML', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated Mar 22, 2020', 'Jupyter Notebook', 'Updated May 27, 2020', 'Updated Mar 19, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020']}",,,,
